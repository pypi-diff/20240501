# Comparing `tmp/masr-2.3.7-py3-none-any.whl.zip` & `tmp/masr-2.3.8-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,17 +1,17 @@
-Zip file size: 1641222 bytes, number of entries: 82
--rw-rw-rw-  2.0 fat      134 b- defN 24-Apr-27 06:47 masr/__init__.py
+Zip file size: 1641919 bytes, number of entries: 82
+-rw-rw-rw-  2.0 fat      134 b- defN 24-May-01 03:16 masr/__init__.py
 -rw-rw-rw-  2.0 fat    18154 b- defN 23-Jul-17 11:28 masr/predict.py
--rw-rw-rw-  2.0 fat    38711 b- defN 24-Apr-21 12:12 masr/trainer.py
+-rw-rw-rw-  2.0 fat    41651 b- defN 24-Apr-27 14:39 masr/trainer.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:53 masr/data_utils/__init__.py
 -rw-rw-rw-  2.0 fat    23843 b- defN 24-Apr-27 05:35 masr/data_utils/audio.py
 -rw-rw-rw-  2.0 fat     2377 b- defN 22-Dec-03 14:13 masr/data_utils/binary.py
 -rw-rw-rw-  2.0 fat     1663 b- defN 23-Apr-05 02:33 masr/data_utils/collate_fn.py
 -rw-rw-rw-  2.0 fat     5191 b- defN 23-Jan-30 12:01 masr/data_utils/normalizer.py
--rw-rw-rw-  2.0 fat     4243 b- defN 23-Jan-30 12:01 masr/data_utils/reader.py
+-rw-rw-rw-  2.0 fat     4564 b- defN 24-Apr-27 14:37 masr/data_utils/reader.py
 -rw-rw-rw-  2.0 fat     7886 b- defN 22-Jul-12 12:53 masr/data_utils/sampler.py
 -rw-rw-rw-  2.0 fat    15934 b- defN 23-Dec-03 03:43 masr/data_utils/utils.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:53 masr/data_utils/augmentor/__init__.py
 -rw-rw-rw-  2.0 fat     5917 b- defN 23-Jan-30 12:01 masr/data_utils/augmentor/augmentation.py
 -rw-rw-rw-  2.0 fat      965 b- defN 22-Jul-12 12:53 masr/data_utils/augmentor/base.py
 -rw-rw-rw-  2.0 fat     2567 b- defN 23-Apr-08 01:31 masr/data_utils/augmentor/noise_perturb.py
 -rw-rw-rw-  2.0 fat      975 b- defN 22-Oct-01 05:25 masr/data_utils/augmentor/resample.py
@@ -31,54 +31,54 @@
 -rw-rw-rw-  2.0 fat     5158 b- defN 23-Jan-30 12:01 masr/infer_utils/inference_predictor.py
 -rw-rw-rw-  2.0 fat     4697 b- defN 23-Jan-30 12:01 masr/infer_utils/pun_predictor.py
 -rw-rw-rw-  2.0 fat  1807522 b- defN 22-Dec-03 14:13 masr/infer_utils/silero_vad.onnx
 -rw-rw-rw-  2.0 fat     8613 b- defN 23-Jan-30 12:01 masr/infer_utils/vad_predictor.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:53 masr/model_utils/__init__.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Dec-03 14:13 masr/model_utils/conformer/__init__.py
 -rw-rw-rw-  2.0 fat    11888 b- defN 23-Jan-30 12:01 masr/model_utils/conformer/attention.py
--rw-rw-rw-  2.0 fat     5291 b- defN 22-Dec-03 14:13 masr/model_utils/conformer/convolution.py
+-rw-rw-rw-  2.0 fat     5261 b- defN 24-Apr-29 13:27 masr/model_utils/conformer/convolution.py
 -rw-rw-rw-  2.0 fat     4869 b- defN 22-Dec-03 14:13 masr/model_utils/conformer/embedding.py
--rw-rw-rw-  2.0 fat    19467 b- defN 23-Mar-31 11:56 masr/model_utils/conformer/encoder.py
+-rw-rw-rw-  2.0 fat    19437 b- defN 24-Apr-29 13:27 masr/model_utils/conformer/encoder.py
 -rw-rw-rw-  2.0 fat     8388 b- defN 23-Jan-30 12:01 masr/model_utils/conformer/model.py
 -rw-rw-rw-  2.0 fat     1267 b- defN 22-Dec-03 14:13 masr/model_utils/conformer/positionwise.py
 -rw-rw-rw-  2.0 fat     8236 b- defN 22-Dec-03 14:13 masr/model_utils/conformer/subsampling.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:53 masr/model_utils/deepspeech2/__init__.py
 -rw-rw-rw-  2.0 fat      807 b- defN 22-Dec-03 14:13 masr/model_utils/deepspeech2/conv.py
 -rw-rw-rw-  2.0 fat     5537 b- defN 22-Dec-03 14:13 masr/model_utils/deepspeech2/encoder.py
 -rw-rw-rw-  2.0 fat      807 b- defN 22-Dec-03 14:13 masr/model_utils/deepspeech2/gru.py
 -rw-rw-rw-  2.0 fat     4039 b- defN 23-Jan-30 12:01 masr/model_utils/deepspeech2/model.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 12:01 masr/model_utils/efficient_conformer/__init__.py
 -rw-rw-rw-  2.0 fat     8517 b- defN 23-Apr-01 03:49 masr/model_utils/efficient_conformer/attention.py
--rw-rw-rw-  2.0 fat     5622 b- defN 23-Jan-30 12:01 masr/model_utils/efficient_conformer/convolution.py
--rw-rw-rw-  2.0 fat    25469 b- defN 23-Apr-01 03:17 masr/model_utils/efficient_conformer/encoder.py
+-rw-rw-rw-  2.0 fat     5592 b- defN 24-Apr-29 13:27 masr/model_utils/efficient_conformer/convolution.py
+-rw-rw-rw-  2.0 fat    25439 b- defN 24-Apr-29 13:27 masr/model_utils/efficient_conformer/encoder.py
 -rw-rw-rw-  2.0 fat     8470 b- defN 23-Jan-30 12:01 masr/model_utils/efficient_conformer/model.py
 -rw-rw-rw-  2.0 fat     2005 b- defN 23-Jan-30 12:01 masr/model_utils/efficient_conformer/subsampling.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Dec-03 14:13 masr/model_utils/loss/__init__.py
--rw-rw-rw-  2.0 fat     2851 b- defN 23-Mar-15 13:58 masr/model_utils/loss/ctc.py
+-rw-rw-rw-  2.0 fat     2821 b- defN 24-Apr-29 13:27 masr/model_utils/loss/ctc.py
 -rw-rw-rw-  2.0 fat     2963 b- defN 22-Dec-03 14:13 masr/model_utils/loss/label_smoothing_loss.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Dec-03 14:13 masr/model_utils/squeezeformer/__init__.py
 -rw-rw-rw-  2.0 fat     8368 b- defN 23-Jan-30 12:01 masr/model_utils/squeezeformer/attention.py
 -rw-rw-rw-  2.0 fat     1879 b- defN 22-Dec-03 14:13 masr/model_utils/squeezeformer/conv2d.py
--rw-rw-rw-  2.0 fat     6368 b- defN 22-Dec-03 14:13 masr/model_utils/squeezeformer/convolution.py
+-rw-rw-rw-  2.0 fat     6338 b- defN 24-Apr-29 13:27 masr/model_utils/squeezeformer/convolution.py
 -rw-rw-rw-  2.0 fat    22036 b- defN 23-Jan-30 12:01 masr/model_utils/squeezeformer/encoder.py
 -rw-rw-rw-  2.0 fat     8630 b- defN 23-Jan-30 12:01 masr/model_utils/squeezeformer/model.py
 -rw-rw-rw-  2.0 fat     2293 b- defN 22-Dec-03 14:13 masr/model_utils/squeezeformer/positionwise.py
 -rw-rw-rw-  2.0 fat     2827 b- defN 23-Jan-30 12:01 masr/model_utils/squeezeformer/subsampling.py
 -rw-rw-rw-  2.0 fat     8197 b- defN 23-Jan-30 12:01 masr/model_utils/squeezeformer/time_reduction.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Dec-03 14:13 masr/model_utils/transformer/__init__.py
--rw-rw-rw-  2.0 fat    16912 b- defN 22-Dec-03 14:13 masr/model_utils/transformer/decoder.py
+-rw-rw-rw-  2.0 fat    16859 b- defN 24-Apr-29 13:27 masr/model_utils/transformer/decoder.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Dec-03 14:13 masr/model_utils/utils/__init__.py
 -rw-rw-rw-  2.0 fat      953 b- defN 22-Dec-03 14:13 masr/model_utils/utils/cmvn.py
 -rw-rw-rw-  2.0 fat     4904 b- defN 22-Dec-03 14:13 masr/model_utils/utils/common.py
 -rw-rw-rw-  2.0 fat     6531 b- defN 22-Dec-03 14:13 masr/model_utils/utils/mask.py
 -rw-rw-rw-  2.0 fat        0 b- defN 23-Jan-30 12:01 masr/optimizer/__init__.py
--rw-rw-rw-  2.0 fat    10085 b- defN 23-Jan-30 12:01 masr/optimizer/scheduler.py
+-rw-rw-rw-  2.0 fat    10051 b- defN 24-Apr-27 15:00 masr/optimizer/scheduler.py
 -rw-rw-rw-  2.0 fat        0 b- defN 22-Jul-12 12:53 masr/utils/__init__.py
 -rw-rw-rw-  2.0 fat     2844 b- defN 23-Jan-30 12:01 masr/utils/logger.py
 -rw-rw-rw-  2.0 fat      893 b- defN 22-Jul-12 12:53 masr/utils/metrics.py
 -rw-rw-rw-  2.0 fat     3874 b- defN 23-Feb-22 12:00 masr/utils/utils.py
--rw-rw-rw-  2.0 fat    11558 b- defN 24-Apr-27 06:47 masr-2.3.7.dist-info/LICENSE
--rw-rw-rw-  2.0 fat     9795 b- defN 24-Apr-27 06:47 masr-2.3.7.dist-info/METADATA
--rw-rw-rw-  2.0 fat       92 b- defN 24-Apr-27 06:47 masr-2.3.7.dist-info/WHEEL
--rw-rw-rw-  2.0 fat        5 b- defN 24-Apr-27 06:47 masr-2.3.7.dist-info/top_level.txt
--rw-rw-r--  2.0 fat     7443 b- defN 24-Apr-27 06:47 masr-2.3.7.dist-info/RECORD
-82 files, 2250616 bytes uncompressed, 1629236 bytes compressed:  27.6%
+-rw-rw-rw-  2.0 fat    11558 b- defN 24-May-01 03:16 masr-2.3.8.dist-info/LICENSE
+-rw-rw-rw-  2.0 fat     9795 b- defN 24-May-01 03:16 masr-2.3.8.dist-info/METADATA
+-rw-rw-rw-  2.0 fat       92 b- defN 24-May-01 03:16 masr-2.3.8.dist-info/WHEEL
+-rw-rw-rw-  2.0 fat        5 b- defN 24-May-01 03:16 masr-2.3.8.dist-info/top_level.txt
+-rw-rw-r--  2.0 fat     7443 b- defN 24-May-01 03:16 masr-2.3.8.dist-info/RECORD
+82 files, 2253610 bytes uncompressed, 1629933 bytes compressed:  27.7%
```

## zipnote {}

```diff
@@ -225,23 +225,23 @@
 
 Filename: masr/utils/metrics.py
 Comment: 
 
 Filename: masr/utils/utils.py
 Comment: 
 
-Filename: masr-2.3.7.dist-info/LICENSE
+Filename: masr-2.3.8.dist-info/LICENSE
 Comment: 
 
-Filename: masr-2.3.7.dist-info/METADATA
+Filename: masr-2.3.8.dist-info/METADATA
 Comment: 
 
-Filename: masr-2.3.7.dist-info/WHEEL
+Filename: masr-2.3.8.dist-info/WHEEL
 Comment: 
 
-Filename: masr-2.3.7.dist-info/top_level.txt
+Filename: masr-2.3.8.dist-info/top_level.txt
 Comment: 
 
-Filename: masr-2.3.7.dist-info/RECORD
+Filename: masr-2.3.8.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## masr/__init__.py

```diff
@@ -1,3 +1,3 @@
-__version__ = "2.3.7"
+__version__ = "2.3.8"
 # 项目支持的模型
 SUPPORT_MODEL = ['squeezeformer', 'efficient_conformer', 'conformer', 'deepspeech2']
```

## masr/trainer.py

```diff
@@ -4,14 +4,15 @@
 import platform
 import shutil
 import time
 from collections import Counter
 from contextlib import nullcontext
 from datetime import timedelta
 
+import numpy as np
 import torch
 import torch.distributed as dist
 import yaml
 from torch.utils.data import DataLoader
 from tqdm import tqdm
 from visualdl import LogWriter
 
@@ -114,14 +115,59 @@
                                         min_duration=self.configs.dataset_conf.min_duration,
                                         max_duration=self.configs.dataset_conf.max_duration)
         self.test_loader = DataLoader(dataset=self.test_dataset,
                                       batch_size=self.configs.dataset_conf.batch_size,
                                       collate_fn=collate_fn,
                                       num_workers=self.configs.dataset_conf.num_workers)
 
+    # 提取特征保存文件
+    def extract_features(self, save_dir='dataset/features'):
+        for i, data_list_file in enumerate([self.configs.dataset_conf.train_manifest,
+                                            self.configs.dataset_conf.test_manifest]):
+            save_dir1 = os.path.join(save_dir, data_list_file.split('.')[-1])
+            os.makedirs(save_dir1, exist_ok=True)
+            test_dataset = MASRDataset(preprocess_configs=self.configs.preprocess_conf,
+                                       data_manifest=data_list_file,
+                                       vocab_filepath=self.configs.dataset_conf.dataset_vocab,
+                                       manifest_type=self.configs.dataset_conf.manifest_type,
+                                       max_duration=-1)
+            save_dir_num = f'{int(time.time())}'
+            os.makedirs(os.path.join(str(save_dir1), save_dir_num), exist_ok=True)
+            all_feature, time_sum, index = None, 0, 0
+            save_data_list = data_list_file.replace('manifest', 'manifest_features')
+            with open(save_data_list, 'w', encoding='utf-8') as f:
+                for i in tqdm(range(len(test_dataset))):
+                    feature, _ = test_dataset[i]
+                    data_list = test_dataset.get_one_list(idx=i)
+                    time_sum += data_list['duration']
+                    if all_feature is None:
+                        index += 1
+                        all_feature = feature
+                        if index >= 1000:
+                            index = 0
+                            save_dir_num = f'{int(time.time())}'
+                            os.makedirs(os.path.join(str(save_dir1), save_dir_num), exist_ok=True)
+                        save_path = os.path.join(str(save_dir1), save_dir_num,
+                                                 f'{int(time.time() * 1000)}.npy').replace('\\', '/')
+                    else:
+                        all_feature = np.concatenate((all_feature, feature), axis=0)
+                    new_data_list = {"audio_filepath": save_path,
+                                     "duration": data_list['duration'],
+                                     "text": data_list['text'],
+                                     "start_frame": all_feature.shape[0] - feature.shape[0],
+                                     "end_frame": all_feature.shape[0]}
+                    f.write(f'{json.dumps(new_data_list, ensure_ascii=False)}\n')
+                    if time_sum > 600:
+                        np.save(save_path, all_feature)
+                        all_feature, time_sum = None, 0
+                if all_feature is not None:
+                    np.save(save_path, all_feature)
+                    print(save_path)
+            logger.info(f'[{data_list_file}]列表中的数据已提取特征完成，新列表为：[{save_data_list}]')
+
     def __setup_model(self, input_dim, vocab_size, is_train=False):
         from masr.model_utils.squeezeformer.model import SqueezeformerModel
         from masr.model_utils.conformer.model import ConformerModel
         from masr.model_utils.efficient_conformer.model import EfficientConformerModel
         from masr.model_utils.deepspeech2.model import DeepSpeech2Model
         # 获取模型
         if self.configs.use_model == 'squeezeformer':
```

## masr/data_utils/reader.py

```diff
@@ -46,45 +46,53 @@
             # 获取二进制的数据列表
             self.dataset_reader = DatasetReader(data_path=data_manifest,
                                                 min_duration=min_duration,
                                                 max_duration=max_duration)
             self.data_list = self.dataset_reader.get_keys()
 
     def __getitem__(self, idx):
-        # 获取数据列表
-        if self.manifest_type == 'txt':
-            data_list = self.data_list[idx]
-        elif self.manifest_type == 'binary':
-            data_list = self.dataset_reader.get_data(self.data_list[idx])
-        else:
-            raise Exception(f'没有该类型：{self.manifest_type}')
-        if 'start_time' not in data_list.keys():
-            # 分割音频路径和标签
-            audio_file, transcript = data_list["audio_filepath"], data_list["text"]
-            # 读取音频
-            audio_segment = AudioSegment.from_file(audio_file)
+        data_list = self.get_one_list(idx)
+        # 分割音频路径和标签
+        audio_file, transcript = data_list["audio_filepath"], data_list["text"]
+        # 如果后缀名为.npy的文件，那么直接读取
+        if audio_file.endswith('.npy'):
+            start_frame, end_frame = data_list["start_frame"], data_list["end_frame"]
+            feature = np.load(audio_file)
+            feature = feature[start_frame:end_frame, :]
         else:
-            # 分割音频路径和标签
-            audio_file, transcript = data_list["audio_filepath"], data_list["text"]
-            start_time, end_time = data_list["start_time"], data_list["end_time"]
-            # 分割读取音频
-            audio_segment = AudioSegment.slice_from_file(audio_file, start=start_time, end=end_time)
-        # 音频增强
-        self._augmentation_pipeline.transform_audio(audio_segment)
-        # 预处理，提取特征
-        feature = self._audio_featurizer.featurize(audio_segment)
+            if 'start_time' not in data_list.keys():
+                # 读取音频
+                audio_segment = AudioSegment.from_file(audio_file)
+            else:
+                start_time, end_time = data_list["start_time"], data_list["end_time"]
+                # 分割读取音频
+                audio_segment = AudioSegment.slice_from_file(audio_file, start=start_time, end=end_time)
+            # 音频增强
+            self._augmentation_pipeline.transform_audio(audio_segment)
+            # 预处理，提取特征
+            feature = self._audio_featurizer.featurize(audio_segment)
         transcript = self._text_featurizer.featurize(transcript)
         # 特征增强
         feature = self._augmentation_pipeline.transform_feature(feature)
         transcript = np.array(transcript, dtype=np.int32)
         return feature.astype(np.float32), transcript
 
     def __len__(self):
         return len(self.data_list)
 
+    def get_one_list(self, idx):
+        # 获取数据列表
+        if self.manifest_type == 'txt':
+            data_list = self.data_list[idx]
+        elif self.manifest_type == 'binary':
+            data_list = self.dataset_reader.get_data(self.data_list[idx])
+        else:
+            raise Exception(f'没有该类型：{self.manifest_type}')
+        return data_list
+
     @property
     def feature_dim(self):
         """返回音频特征大小
 
         :return: 词汇表大小
         :rtype: int
         """
```

## masr/model_utils/conformer/convolution.py

```diff
@@ -1,19 +1,20 @@
 from typing import Tuple
 
 import torch
 from torch import nn
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 __all__ = ['ConvolutionModule']
 
 
 class ConvolutionModule(nn.Module):
     """ConvolutionModule in Conformer model."""
 
+    @typechecked
     def __init__(self,
                  channels: int,
                  kernel_size: int = 15,
                  activation: nn.Module = nn.ReLU(),
                  norm: str = "batch_norm",
                  causal: bool = False,
                  bias: bool = True):
@@ -22,15 +23,14 @@
             channels (int): The number of channels of conv layers.
             kernel_size (int): Kernel size of conv layers.
             activation (nn.Module): Activation Layer.
             norm (str): Normalization type, 'batch_norm' or 'layer_norm'
             causal (bool): Whether use causal convolution or not
             bias (bool): Whether Conv with bias or not
         """
-        assert check_argument_types()
         super().__init__()
         self.pointwise_conv1 = nn.Conv1d(
             channels,
             2 * channels,
             kernel_size=1,
             stride=1,
             padding=0,
```

## masr/model_utils/conformer/encoder.py

```diff
@@ -1,12 +1,12 @@
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 from masr.model_utils.conformer.attention import MultiHeadedAttention
 from masr.model_utils.conformer.attention import RelPositionMultiHeadedAttention
 from masr.model_utils.conformer.convolution import ConvolutionModule
 from masr.model_utils.conformer.embedding import NoPositionalEncoding
 from masr.model_utils.conformer.embedding import PositionalEncoding
 from masr.model_utils.conformer.embedding import RelPositionalEncoding
@@ -162,14 +162,15 @@
 
         return x, mask, new_att_cache, new_cnn_cache
 
 
 class ConformerEncoder(nn.Module):
     """Conformer encoder module."""
 
+    @typechecked
     def __init__(
             self,
             input_size: int,
             output_size: int = 256,
             attention_heads: int = 4,
             linear_units: int = 2048,
             num_blocks: int = 6,
@@ -228,15 +229,14 @@
             input_size to use_dynamic_chunk, see in BaseEncoder
             macaron_style (bool): Whether to use macaron style for positionwise layer.
             activation_type (str): Encoder activation function type.
             use_cnn_module (bool): Whether to use convolution module.
             cnn_module_kernel (int): Kernel size of convolution module.
             causal (bool): whether to use causal convolution or not.
         """
-        assert check_argument_types()
         super().__init__()
         self._output_size = output_size
 
         if pos_enc_layer_type == "abs_pos":
             pos_enc_class = PositionalEncoding
         elif pos_enc_layer_type == "rel_pos":
             pos_enc_class = RelPositionalEncoding
```

## masr/model_utils/efficient_conformer/convolution.py

```diff
@@ -1,17 +1,18 @@
 from typing import Tuple
 
 import torch
 from torch import nn
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 
 class ConvolutionModule(nn.Module):
     """ConvolutionModule in Conformer model."""
 
+    @typechecked
     def __init__(self,
                  channels: int,
                  kernel_size: int = 15,
                  activation: nn.Module = nn.ReLU(),
                  norm: str = "batch_norm",
                  causal: bool = False,
                  bias: bool = True,
@@ -19,15 +20,14 @@
         """Construct an ConvolutionModule object.
         Args:
             channels (int): The number of channels of conv layers.
             kernel_size (int): Kernel size of conv layers.
             causal (int): Whether use causal convolution or not
             stride (int): Stride Convolution, for efficient Conformer
         """
-        assert check_argument_types()
         super().__init__()
 
         self.pointwise_conv1 = nn.Conv1d(channels,
                                          2 * channels,
                                          kernel_size=1,
                                          stride=1,
                                          padding=0,
```

## masr/model_utils/efficient_conformer/encoder.py

```diff
@@ -1,13 +1,13 @@
 from typing import Tuple, Optional, List, Union
 
 import torch
 import torch.nn.functional as F
 from torch import nn
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 from masr.model_utils.conformer.attention import MultiHeadedAttention, RelPositionMultiHeadedAttention
 from masr.model_utils.conformer.embedding import PositionalEncoding, RelPositionalEncoding, NoPositionalEncoding
 from masr.model_utils.conformer.encoder import ConformerEncoderLayer
 from masr.model_utils.conformer.positionwise import PositionwiseFeedForward
 from masr.model_utils.conformer.subsampling import LinearNoSubsampling, Conv2dSubsampling4, Conv2dSubsampling6, \
     Conv2dSubsampling8
@@ -17,14 +17,15 @@
 from masr.model_utils.utils.common import get_activation
 from masr.model_utils.utils.mask import make_pad_mask, add_optional_chunk_mask
 
 
 class EfficientConformerEncoder(torch.nn.Module):
     """Conformer encoder module."""
 
+    @typechecked
     def __init__(
             self,
             input_size: int,
             output_size: int = 256,
             attention_heads: int = 4,
             linear_units: int = 2048,
             num_blocks: int = 6,
@@ -64,15 +65,14 @@
             causal (bool): whether to use causal convolution or not.
             stride_layer_idx (list): layer id with StrideConv, start from 0
             stride (list): stride size of each StrideConv in efficient conformer
             group_layer_idx (list): layer id with GroupedAttention, start from 0
             group_size (int): group size of every GroupedAttention layer
             stride_kernel (bool): default True. True: recompute cnn kernels with stride.
         """
-        assert check_argument_types()
         super().__init__()
         self._output_size = output_size
 
         if pos_enc_layer_type == "abs_pos":
             pos_enc_class = PositionalEncoding
         elif pos_enc_layer_type == "rel_pos":
             pos_enc_class = RelPositionalEncoding
```

## masr/model_utils/loss/ctc.py

```diff
@@ -1,30 +1,30 @@
 import torch
 import torch.nn.functional as F
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 
 class CTCLoss(torch.nn.Module):
     """CTC module"""
 
+    @typechecked
     def __init__(
             self,
             odim: int,
             encoder_output_size: int,
             dropout_rate: float = 0.0,
             reduce: bool = True,
     ):
         """ Construct CTC module
         Args:
             odim: dimension of outputs
             encoder_output_size: number of encoder projection units
             dropout_rate: dropout rate (0.0 ~ 1.0)
             reduce: reduce the CTC loss into a scalar
         """
-        assert check_argument_types()
         super().__init__()
         eprojs = encoder_output_size
         self.dropout_rate = dropout_rate
         self.ctc_lo = torch.nn.Linear(eprojs, odim)
 
         reduction_type = "sum" if reduce else "none"
         self.ctc_loss = torch.nn.CTCLoss(reduction=reduction_type)
```

## masr/model_utils/squeezeformer/convolution.py

```diff
@@ -1,17 +1,18 @@
 from typing import Tuple
 
 import torch
 from torch import nn
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 
 class ConvolutionModule(nn.Module):
     """ConvolutionModule in Conformer model."""
 
+    @typechecked
     def __init__(self,
                  channels: int,
                  kernel_size: int = 15,
                  activation: nn.Module = nn.ReLU(),
                  norm: str = "batch_norm",
                  causal: bool = False,
                  bias: bool = True,
@@ -19,15 +20,14 @@
                  init_weights: bool = False):
         """Construct an ConvolutionModule object.
         Args:
             channels (int): The number of channels of conv layers.
             kernel_size (int): Kernel size of conv layers.
             causal (int): Whether use causal convolution or not
         """
-        assert check_argument_types()
         super().__init__()
         self.bias = bias
         self.channels = channels
         self.kernel_size = kernel_size
         self.adaptive_scale = adaptive_scale
         self.ada_scale = torch.nn.Parameter(torch.ones([1, 1, channels]), requires_grad=adaptive_scale)
         self.ada_bias = torch.nn.Parameter(torch.zeros([1, 1, channels]), requires_grad=adaptive_scale)
```

## masr/model_utils/transformer/decoder.py

```diff
@@ -1,13 +1,13 @@
 from typing import List
 from typing import Optional, Tuple
 
 import torch
 from torch import nn
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 from masr.model_utils.conformer.attention import MultiHeadedAttention
 from masr.model_utils.conformer.embedding import PositionalEncoding
 from masr.model_utils.conformer.positionwise import PositionwiseFeedForward
 from masr.model_utils.utils.mask import (subsequent_mask, make_pad_mask)
 
 
@@ -29,14 +29,15 @@
             True: use layer_norm before each sub-block of a layer.
             False: use layer_norm after each sub-block of a layer.
         concat_after: whether to concat attention layer's input and output
             True: x -> x + linear(concat(x, att(x)))
             False: x -> x + att(x)
     """
 
+    @typechecked
     def __init__(self,
                  vocab_size: int,
                  encoder_output_size: int,
                  attention_heads: int = 4,
                  linear_units: int = 2048,
                  num_blocks: int = 6,
                  r_num_blocks: int = 0,
@@ -45,15 +46,14 @@
                  self_attention_dropout_rate: float = 0.0,
                  src_attention_dropout_rate: float = 0.0,
                  input_layer: str = "embed",
                  use_output_layer: bool = True,
                  normalize_before: bool = True,
                  concat_after: bool = False,
                  max_len: int = 5000):
-        assert check_argument_types()
         super().__init__()
         self.left_decoder = TransformerDecoder(
             vocab_size, encoder_output_size, attention_heads, linear_units,
             num_blocks, dropout_rate, positional_dropout_rate,
             self_attention_dropout_rate, src_attention_dropout_rate,
             input_layer, use_output_layer, normalize_before, concat_after,
             max_len)
@@ -139,14 +139,15 @@
             True: use layer_norm before each sub-block of a layer.
             False: use layer_norm after each sub-block of a layer.
         concat_after: whether to concat attention layer's input and output
             True: x -> x + linear(concat(x, att(x)))
             False: x -> x + att(x)
     """
 
+    @typechecked
     def __init__(self,
                  vocab_size: int,
                  encoder_output_size: int,
                  attention_heads: int = 4,
                  linear_units: int = 2048,
                  num_blocks: int = 6,
                  dropout_rate: float = 0.1,
@@ -154,16 +155,14 @@
                  self_attention_dropout_rate: float = 0.0,
                  src_attention_dropout_rate: float = 0.0,
                  input_layer: str = "embed",
                  use_output_layer: bool = True,
                  normalize_before: bool = True,
                  concat_after: bool = False,
                  max_len: int = 5000):
-
-        assert check_argument_types()
         super().__init__()
         attention_dim = encoder_output_size
 
         if input_layer == "embed":
             self.embed = nn.Sequential(
                 nn.Embedding(vocab_size, attention_dim),
                 PositionalEncoding(attention_dim, positional_dropout_rate, max_len=max_len), )
```

## masr/optimizer/scheduler.py

```diff
@@ -1,17 +1,17 @@
 import math
 from typing import Union
 
 import torch
-from torch.optim.lr_scheduler import _LRScheduler
+from torch.optim.lr_scheduler import LRScheduler
 
-from typeguard import check_argument_types
+from typeguard import typechecked
 
 
-class WarmupLR(_LRScheduler):
+class WarmupLR(LRScheduler):
     """The WarmupLR scheduler
 
     This scheduler is almost same as NoamLR Scheduler except for following
     difference:
 
     NoamLR:
         lr = optimizer.lr * model_size ** -0.5
@@ -20,22 +20,22 @@
         lr = optimizer.lr * warmup_step ** 0.5
              * min(step ** -0.5, step * warmup_step ** -1.5)
 
     Note that the maximum lr equals to optimizer.lr in this scheduler.
 
     """
 
+    @typechecked
     def __init__(
             self,
             optimizer: torch.optim.Optimizer,
             warmup_steps: Union[int, float] = 25000,
             min_lr=1e-5,
             last_epoch: int = -1,
     ):
-        assert check_argument_types()
         self.warmup_steps = warmup_steps
         self.min_lr = min_lr
         super().__init__(optimizer, last_epoch)
 
     def __repr__(self):
         return f"{self.__class__.__name__}(warmup_steps={self.warmup_steps}, lr={self.base_lr}, min_lr={self.min_lr}, last_epoch={self.last_epoch})"
 
@@ -58,15 +58,15 @@
                 lrs.append(lr)
             return lrs
 
     def set_step(self, step: int):
         self.last_epoch = step
 
 
-class NoamHoldAnnealing(_LRScheduler):
+class NoamHoldAnnealing(LRScheduler):
     def __init__(self, optimizer, max_steps=175680, warmup_steps=None, warmup_ratio=0.2, hold_steps=None,
                  hold_ratio=0.3, decay_rate=1.0, min_lr=1.e-5, last_epoch=-1):
         """
         From Nemo:
         Implementation of the Noam Hold Annealing policy from the SqueezeFormer paper.
 
         Unlike NoamAnnealing, the peak learning rate can be explicitly set for this scheduler.
@@ -184,15 +184,15 @@
         ]
         return new_lrs
 
     def set_step(self, step: int):
         self.last_epoch = step
 
 
-class CosineWithWarmup(_LRScheduler):
+class CosineWithWarmup(LRScheduler):
     def __init__(self, optimizer: torch.optim.Optimizer, T_max, eta_min=0, warmup_steps=None, warmup_ratio=0.2,
                  last_epoch=-1):
         """
         Set the learning rate using a cosine annealing schedule, where :math:`\eta_{max}` is set to
         the initial learning_rate.
 
         Args:
```

## Comparing `masr-2.3.7.dist-info/LICENSE` & `masr-2.3.8.dist-info/LICENSE`

 * *Files identical despite different names*

## Comparing `masr-2.3.7.dist-info/METADATA` & `masr-2.3.8.dist-info/METADATA`

 * *Files 2% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: masr
-Version: 2.3.7
+Version: 2.3.8
 Summary: Automatic speech recognition toolkit on Pytorch
 Home-page: https://github.com/yeyupiaoling/MASR
 Download-URL: https://github.com/yeyupiaoling/MASR.git
 Author: yeyupiaoling
 License: Apache License 2.0
 Keywords: asr,pytorch
 Classifier: Intended Audience :: Developers
@@ -28,15 +28,15 @@
 Requires-Dist: soundcard >=0.4.2
 Requires-Dist: resampy >=0.2.2
 Requires-Dist: zhconv >=1.4.2
 Requires-Dist: ijson >=3.1.4
 Requires-Dist: pyyaml >=5.4.1
 Requires-Dist: termcolor >=1.1.0
 Requires-Dist: scikit-learn >=1.0.2
-Requires-Dist: typeguard ==2.13.3
+Requires-Dist: typeguard >=2.13.3
 Requires-Dist: onnxruntime >=1.11.1
 Requires-Dist: av >=10.0.0
 
 ![python version](https://img.shields.io/badge/python-3.8+-orange.svg)
 ![GitHub forks](https://img.shields.io/github/forks/yeyupiaoling/MASR)
 ![GitHub Repo stars](https://img.shields.io/github/stars/yeyupiaoling/MASR)
 ![GitHub](https://img.shields.io/github/license/yeyupiaoling/MASR)
```

## Comparing `masr-2.3.7.dist-info/RECORD` & `masr-2.3.8.dist-info/RECORD`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-masr/__init__.py,sha256=3_N9RTC--p2evSUycIg8gURh_KbrML0eTP3zYoL1ISE,134
+masr/__init__.py,sha256=3NrRTuMQKg_WvS8Lp-DDoWLcrGZ7zXnaBuEhWuQbnSs,134
 masr/predict.py,sha256=MacUyNIJlSTs1pNWVZKG8N0HfCuzSDl5_pFFNwzQ1M8,18154
-masr/trainer.py,sha256=BDaHB3lYPG7lMvNaa1TucvZKtUKwucUiqlZv6zx8u78,38711
+masr/trainer.py,sha256=8xJQboyk2GckXJi-QukCmzZEACbbCFyez5g0roQOoy0,41651
 masr/data_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/data_utils/audio.py,sha256=Eg0oNlbFaFzGMYgrRvSOOSZWg4Vx1tIjxNMmjggfxKM,23843
 masr/data_utils/binary.py,sha256=qNXcZRcLcehLdGg4YwP5FBcLCpFp8Fd9YVSb5numyzs,2377
 masr/data_utils/collate_fn.py,sha256=tq1WMS5OpMVO_9aOzUDx_LnCyuisNIkEyPzW1w5VU0k,1663
 masr/data_utils/normalizer.py,sha256=srlTInnYb1voSGNqhw9g4HArtQkq8k_sPjVZ6zU0dco,5191
-masr/data_utils/reader.py,sha256=MjYIzYNO90uZd7qs9bxvCMG3ji5PonWAwVziHO4QpT4,4243
+masr/data_utils/reader.py,sha256=5BPv9xM4W0n04d6p12ECRf5WgqihhXmHIXZLToX4Tgg,4564
 masr/data_utils/sampler.py,sha256=R4QIdKhMUEhLkxE8vsDxYwSl6Zc8hgZ7ZZ3w9S1f-PY,7886
 masr/data_utils/utils.py,sha256=tmOvukFRCi9R9qtzKpKXA2dxAYF3sGG4pfYxnU3wLPI,15934
 masr/data_utils/augmentor/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/data_utils/augmentor/augmentation.py,sha256=c2KAySNcSg093fIrwdMt0JFXwiWdb9Yt8Z-g-zWp5t8,5917
 masr/data_utils/augmentor/base.py,sha256=Qw5AsIjpqDkggVkVyfPGTI_IwnfSuIWViRdgo7g2ze0,965
 masr/data_utils/augmentor/noise_perturb.py,sha256=ERthdAixMf6rCKSjxIwIvAObKpsFZ-_yw0upA9iS9wY,2567
 masr/data_utils/augmentor/resample.py,sha256=-KzF2dXZCL5lSITeFfld4kzwyzIsyy5JAsAsbnhoODU,975
@@ -30,53 +30,53 @@
 masr/infer_utils/inference_predictor.py,sha256=qpdVyb224PBkeLWM9paevYzwo3xLspbDvjflxIbCtbE,5158
 masr/infer_utils/pun_predictor.py,sha256=6Rc4T2ObeeIN0Rr_fIowXNnKfXdFJ8ajergH27ezn8E,4697
 masr/infer_utils/silero_vad.onnx,sha256=o16_Uv085fFGmyo2FY26dhvEe5c-ozgrMYbKFbH1ryg,1807522
 masr/infer_utils/vad_predictor.py,sha256=IUjAprF21U-azZtsc9rY2SPzxhYHdwIB4KlQMv8zzAU,8613
 masr/model_utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/model_utils/conformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/model_utils/conformer/attention.py,sha256=19SZSR7X1WD_q8bPosws4JXfu2JvdYs1fXuKLyyFv_A,11888
-masr/model_utils/conformer/convolution.py,sha256=JrBr2z4azjwzvTrEyo1Eaqm0VnBsY3Bi-KB3W0JU5wA,5291
+masr/model_utils/conformer/convolution.py,sha256=vbH7YPGwqruGHR5jv9t_8m7iK4ggHpFOb9u96BTlMAE,5261
 masr/model_utils/conformer/embedding.py,sha256=zGLpxwG-dlPMMabhjNgrF7sBXfAN1iD7kNEvSJcdAdU,4869
-masr/model_utils/conformer/encoder.py,sha256=ri6APca988uS8VLcTRpJuovWuGWgOqC82KNMQOTSx_Y,19467
+masr/model_utils/conformer/encoder.py,sha256=Oum6ar2Rp7_JDXgh5mNEh9tEYEDLU3fHdsk3kH2VBEk,19437
 masr/model_utils/conformer/model.py,sha256=exZrRE-YGcrRoyfRhdr3XqEzF7MsenoQEtBY21BRO0M,8388
 masr/model_utils/conformer/positionwise.py,sha256=Rr_hepeQYMWJrwtzu3CPfR2RgRTQpCqGet2Tm2o16g4,1267
 masr/model_utils/conformer/subsampling.py,sha256=dl9tjuWwkadJnqCaTs3NWMUpR711TCFnnMxP1HyVIY8,8236
 masr/model_utils/deepspeech2/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/model_utils/deepspeech2/conv.py,sha256=hA-GFuAA20I081iFGuvQLANmDYeQi6D1dqu1WqON6Ho,807
 masr/model_utils/deepspeech2/encoder.py,sha256=Biyz1SKDc7UuVk9KRwO_m2piKCwMarcBkvxVRm9m0XM,5537
 masr/model_utils/deepspeech2/gru.py,sha256=Q-XcOps_Gpqdv3WFBvs9siHBQ7phd4Ytk5WRmpGjKog,807
 masr/model_utils/deepspeech2/model.py,sha256=GANDGgPjrnS3E3c6zr3bqHnzJ-i1EUuwR1nJf17o8Vk,4039
 masr/model_utils/efficient_conformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/model_utils/efficient_conformer/attention.py,sha256=kWzmhIu0l86xPK5KsTz1o23DJzkWk_9loO3lMzF1q20,8517
-masr/model_utils/efficient_conformer/convolution.py,sha256=rTKhvJl5ecfpinayg2NURAcPPsWcI3b9slxc6-Tx6Nw,5622
-masr/model_utils/efficient_conformer/encoder.py,sha256=g_EMfzl2U2nJ8LvGxENI_Q5ToWKoL_yKp88EZEPWhZ0,25469
+masr/model_utils/efficient_conformer/convolution.py,sha256=URKE1fOqRBtS_uL_fGlOTcUBvP3LQXE-wXSeaHbSMMQ,5592
+masr/model_utils/efficient_conformer/encoder.py,sha256=OZC1zeLgYOgzKTJkcv5q1KRsmQouE6SI3nd6gbv8_Og,25439
 masr/model_utils/efficient_conformer/model.py,sha256=v93yxyFuBpj92wBQbPHhvKFSPIHEyZknd11L74t0S68,8470
 masr/model_utils/efficient_conformer/subsampling.py,sha256=-67sapk7BesRnkfaBFDOJ7TNQzCHTU92XwURBKsJiHs,2005
 masr/model_utils/loss/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-masr/model_utils/loss/ctc.py,sha256=aKydAPEQCXkPh2-csvAkr6UJPiSTS6cwXcnLbrxUS7U,2851
+masr/model_utils/loss/ctc.py,sha256=KT5suLUGtubOnAdZC933F7KZufC7t4cp4dBueG8ckzg,2821
 masr/model_utils/loss/label_smoothing_loss.py,sha256=In-E2glrfaP_nn2XNsV4a5iGkKvDOTLQNXG9fQbFu7M,2963
 masr/model_utils/squeezeformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/model_utils/squeezeformer/attention.py,sha256=88JR3rwnf3gt1n3RbJu_zwaOezr8li6P0mMbdCQgYo8,8368
 masr/model_utils/squeezeformer/conv2d.py,sha256=9XtqZkaz_GfT_T62gb9x2Fle1DMD4zr91HfrpQJlX9M,1879
-masr/model_utils/squeezeformer/convolution.py,sha256=b0puB3S9QARhMbszniqL2ZE9j5sziwl9Wuir-PjPh8c,6368
+masr/model_utils/squeezeformer/convolution.py,sha256=r90oM94VHQqV8s8O9CX52rXqMoKsFT3y4rrE6rpU8Ak,6338
 masr/model_utils/squeezeformer/encoder.py,sha256=LNlIjQxNdIUUbWnMSImY8GGsjpOUOUNbAXNftY5-z2E,22036
 masr/model_utils/squeezeformer/model.py,sha256=jj3VaWhapiS0jX0guI32BV5jRA5zJFrVIhY7x_AbjIs,8630
 masr/model_utils/squeezeformer/positionwise.py,sha256=e5Maoj5P4HmA2FMAsOQ9xKvHx0ScRYSpdh7aX4wY8Ts,2293
 masr/model_utils/squeezeformer/subsampling.py,sha256=X8p-qs048a4hk_o2HK9mwim5Jp-UgQZXvtdYOgpZRNQ,2827
 masr/model_utils/squeezeformer/time_reduction.py,sha256=qrX7om528ZDWarLUD1Pq3uR1h4_s01j_GkrWWHSPAk0,8197
 masr/model_utils/transformer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-masr/model_utils/transformer/decoder.py,sha256=FaW7_zaneziwD8S1gjshoNha_P3SVPCBFGlO8ApeZKw,16912
+masr/model_utils/transformer/decoder.py,sha256=Qwc-FdwJpTO-32UZ58e-xOVoC8gK6uIAAzdstVzU3FE,16859
 masr/model_utils/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/model_utils/utils/cmvn.py,sha256=uutu8u9y2QFLifX3GmDrKIVzhkLLJM42peSeEooiuwo,953
 masr/model_utils/utils/common.py,sha256=Yd7Khw7R55er4kxpyUfWcrUk55UcVV43RoVt-ulPRlg,4904
 masr/model_utils/utils/mask.py,sha256=33jM1OuwFd1WP56nxJ2uwQ2mhtZJxiG4Fms0RxbIBuQ,6531
 masr/optimizer/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
-masr/optimizer/scheduler.py,sha256=klQSh42wBpWgYA31jIBuWGoQUhNJTuQxbZF8_Ymi59w,10085
+masr/optimizer/scheduler.py,sha256=ysdoUNeSApR-TCPyIQrvkIdlncM_z2t3pVeM3QjNF9A,10051
 masr/utils/__init__.py,sha256=47DEQpj8HBSa-_TImW-5JCeuQeRkm5NMpJWZG3hSuFU,0
 masr/utils/logger.py,sha256=eHExExIfsMSQD7B0iCh0TSzx257JZIanKJCK7BWp23s,2844
 masr/utils/metrics.py,sha256=PGnCSSGnJH61xOzuM0iaNDLGwAllZh0ILPSCPRcGIAg,893
 masr/utils/utils.py,sha256=iFXLAwhUALrJ28Ayjkwf5kfad4Om84dfy4AEGZ2g6_4,3874
-masr-2.3.7.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
-masr-2.3.7.dist-info/METADATA,sha256=upWK9UZ2-wt4-n0IwnacTcDPRZ-vXnyiX4IEXC2HG2A,9795
-masr-2.3.7.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
-masr-2.3.7.dist-info/top_level.txt,sha256=ZSK_fJKFgHXpopi_6jgWHcdY4JqCVf5fXCvfge1Nc6s,5
-masr-2.3.7.dist-info/RECORD,,
+masr-2.3.8.dist-info/LICENSE,sha256=HrhfyXIkWY2tGFK11kg7vPCqhgh5DcxleloqdhrpyMY,11558
+masr-2.3.8.dist-info/METADATA,sha256=AJL6RN7B3odZrXSaHhMo0OP7U5wXtN7FXKQImbOnfJE,9795
+masr-2.3.8.dist-info/WHEEL,sha256=yQN5g4mg4AybRjkgi-9yy4iQEFibGQmlz78Pik5Or-A,92
+masr-2.3.8.dist-info/top_level.txt,sha256=ZSK_fJKFgHXpopi_6jgWHcdY4JqCVf5fXCvfge1Nc6s,5
+masr-2.3.8.dist-info/RECORD,,
```

