# Comparing `tmp/ms-swift-2.0.3.post1.tar.gz` & `tmp/ms-swift-2.0.4.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "dist/ms-swift-2.0.3.post1.tar", last modified: Wed Apr 24 11:17:10 2024, max compression
+gzip compressed data, was "dist/ms-swift-2.0.4.tar", last modified: Wed May  1 05:20:13 2024, max compression
```

## Comparing `ms-swift-2.0.3.post1.tar` & `ms-swift-2.0.4.tar`

### file list

```diff
@@ -1,228 +1,229 @@
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/
--rw-r--r--   0 runner    (1001) docker     (127)      154 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/MANIFEST.in
--rw-r--r--   0 runner    (1001) docker     (127)    59565 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)    53655 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/README.md
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/
--rw-r--r--   0 runner    (1001) docker     (127)    59565 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/PKG-INFO
--rw-r--r--   0 runner    (1001) docker     (127)     5577 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/SOURCES.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/dependency_links.txt
--rw-r--r--   0 runner    (1001) docker     (127)       51 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/entry_points.txt
--rw-r--r--   0 runner    (1001) docker     (127)        1 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/not-zip-safe
--rw-r--r--   0 runner    (1001) docker     (127)      784 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/requires.txt
--rw-r--r--   0 runner    (1001) docker     (127)       12 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/ms_swift.egg-info/top_level.txt
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/requirements/
--rw-r--r--   0 runner    (1001) docker     (127)       44 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/requirements/aigc.txt
--rw-r--r--   0 runner    (1001) docker     (127)      115 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/requirements/docs.txt
--rw-r--r--   0 runner    (1001) docker     (127)       15 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/requirements/eval.txt
--rw-r--r--   0 runner    (1001) docker     (127)      247 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/requirements/framework.txt
--rw-r--r--   0 runner    (1001) docker     (127)       85 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/requirements/llm.txt
--rw-r--r--   0 runner    (1001) docker     (127)      119 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/requirements/tests.txt
--rw-r--r--   0 runner    (1001) docker     (127)      772 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/setup.cfg
--rw-r--r--   0 runner    (1001) docker     (127)     6268 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/setup.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/
--rw-r--r--   0 runner    (1001) docker     (127)     3017 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/aigc/
--rw-r--r--   0 runner    (1001) docker     (127)     1931 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    23301 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/animatediff.py
--rw-r--r--   0 runner    (1001) docker     (127)     4669 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/animatediff_infer.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/
--rw-r--r--   0 runner    (1001) docker     (127)     1210 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     4037 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_controlnet.py
--rw-r--r--   0 runner    (1001) docker     (127)     3997 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_controlnet_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)     2474 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_dreambooth.py
--rw-r--r--   0 runner    (1001) docker     (127)     3367 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_dreambooth_lora.py
--rw-r--r--   0 runner    (1001) docker     (127)     3354 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_dreambooth_lora_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)     3395 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image.py
--rw-r--r--   0 runner    (1001) docker     (127)     3356 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image_lora.py
--rw-r--r--   0 runner    (1001) docker     (127)     3392 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image_lora_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)     3383 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)    48449 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_controlnet.py
--rw-r--r--   0 runner    (1001) docker     (127)    54249 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_controlnet_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)    60203 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_dreambooth.py
--rw-r--r--   0 runner    (1001) docker     (127)    58211 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_dreambooth_lora.py
--rw-r--r--   0 runner    (1001) docker     (127)    71598 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_dreambooth_lora_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)    46799 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image.py
--rw-r--r--   0 runner    (1001) docker     (127)    42456 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image_lora.py
--rw-r--r--   0 runner    (1001) docker     (127)    53924 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image_lora_sdxl.py
--rw-r--r--   0 runner    (1001) docker     (127)    57714 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image_sdxl.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/aigc/utils/
--rw-r--r--   0 runner    (1001) docker     (127)       70 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     5593 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/aigc/utils/argument.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/cli/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      130 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/app_ui.py
--rw-r--r--   0 runner    (1001) docker     (127)      130 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/deploy.py
--rw-r--r--   0 runner    (1001) docker     (127)      124 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/dpo.py
--rw-r--r--   0 runner    (1001) docker     (127)      126 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/eval.py
--rw-r--r--   0 runner    (1001) docker     (127)      130 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/export.py
--rw-r--r--   0 runner    (1001) docker     (127)      128 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/infer.py
--rw-r--r--   0 runner    (1001) docker     (127)     1981 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/main.py
--rw-r--r--   0 runner    (1001) docker     (127)      160 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/merge_lora.py
--rw-r--r--   0 runner    (1001) docker     (127)      124 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/sft.py
--rw-r--r--   0 runner    (1001) docker     (127)      123 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/cli/web_ui.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/hub/
--rw-r--r--   0 runner    (1001) docker     (127)      255 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    30552 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/api.py
--rw-r--r--   0 runner    (1001) docker     (127)     3852 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/check_model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1878 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     4061 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/errors.py
--rw-r--r--   0 runner    (1001) docker     (127)    12719 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/file_download.py
--rw-r--r--   0 runner    (1001) docker     (127)     8933 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/git.py
--rw-r--r--   0 runner    (1001) docker     (127)     7202 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/push_to_hub.py
--rw-r--r--   0 runner    (1001) docker     (127)    13006 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/repository.py
--rw-r--r--   0 runner    (1001) docker     (127)     7080 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/snapshot_download.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/hub/utils/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     9906 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/utils/caching.py
--rw-r--r--   0 runner    (1001) docker     (127)     3045 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/hub/utils/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/llm/
--rw-r--r--   0 runner    (1001) docker     (127)     1323 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      987 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/accelerator.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/llm/agent/
--rw-r--r--   0 runner    (1001) docker     (127)       40 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/agent/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2974 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/agent/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     4633 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/app_ui.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/llm/data/
--rw-r--r--   0 runner    (1001) docker     (127)    27203 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/data/self_cognition.jsonl
--rw-r--r--   0 runner    (1001) docker     (127)    20582 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/deploy.py
--rw-r--r--   0 runner    (1001) docker     (127)     8419 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/dpo.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/llm/ds_config/
--rw-r--r--   0 runner    (1001) docker     (127)     1275 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/ds_config/zero2.json
--rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/ds_config/zero3.json
--rw-r--r--   0 runner    (1001) docker     (127)     1544 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/ds_config/zero3_offload.json
--rw-r--r--   0 runner    (1001) docker     (127)     8248 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/eval.py
--rw-r--r--   0 runner    (1001) docker     (127)     6071 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/export.py
--rw-r--r--   0 runner    (1001) docker     (127)    20043 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/infer.py
--rw-r--r--   0 runner    (1001) docker     (127)     3809 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/rome.py
--rw-r--r--   0 runner    (1001) docker     (127)    13000 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/sft.py
--rw-r--r--   0 runner    (1001) docker     (127)    10756 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/tuner.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/llm/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     3045 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    53432 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/argument.py
--rw-r--r--   0 runner    (1001) docker     (127)     3274 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/client_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    59606 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)   136969 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     9427 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/preprocess.py
--rw-r--r--   0 runner    (1001) docker     (127)     3839 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/protocol.py
--rw-r--r--   0 runner    (1001) docker     (127)    60438 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/template.py
--rw-r--r--   0 runner    (1001) docker     (127)    31851 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    17458 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/llm/utils/vllm_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    11731 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/torchacc_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/trainers/
--rw-r--r--   0 runner    (1001) docker     (127)     1083 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1864 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/arguments.py
--rw-r--r--   0 runner    (1001) docker     (127)     3463 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/callback.py
--rw-r--r--   0 runner    (1001) docker     (127)    14917 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/dpo_trainers.py
--rw-r--r--   0 runner    (1001) docker     (127)    30575 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/mixin.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/
--rw-r--r--   0 runner    (1001) docker     (127)      763 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/__init__.py
--rwxr-xr-x   0 runner    (1001) docker     (127)    11202 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/adafactor.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     6229 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/adamw.py
--rw-r--r--   0 runner    (1001) docker     (127)     3901 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/adamw8bit.py
--rwxr-xr-x   0 runner    (1001) docker     (127)     5700 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/galore_projector.py
--rw-r--r--   0 runner    (1001) docker     (127)     7088 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)    14442 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/trainers.py
--rw-r--r--   0 runner    (1001) docker     (127)     2072 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/trainers/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/tuners/
--rw-r--r--   0 runner    (1001) docker     (127)     2725 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     8517 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/adapter.py
--rw-r--r--   0 runner    (1001) docker     (127)    43987 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/base.py
--rw-r--r--   0 runner    (1001) docker     (127)     7969 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/llamapro.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/tuners/longlora/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/longlora/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    19436 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/longlora/llama.py
--rw-r--r--   0 runner    (1001) docker     (127)     4032 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/longlora/longlora.py
--rw-r--r--   0 runner    (1001) docker     (127)     8358 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/lora.py
--rw-r--r--   0 runner    (1001) docker     (127)    40224 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/lora_layers.py
--rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     4957 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/module_mapping.py
--rw-r--r--   0 runner    (1001) docker     (127)     2604 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/neftune.py
--rw-r--r--   0 runner    (1001) docker     (127)    14325 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/peft.py
--rw-r--r--   0 runner    (1001) docker     (127)    10487 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/prompt.py
--rw-r--r--   0 runner    (1001) docker     (127)    17294 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/restuning.py
--rw-r--r--   0 runner    (1001) docker     (127)    13552 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/restuning_components.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/tuners/rome/
--rw-r--r--   0 runner    (1001) docker     (127)       85 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2273 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/compute_u.py
--rw-r--r--   0 runner    (1001) docker     (127)     9488 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/compute_v.py
--rw-r--r--   0 runner    (1001) docker     (127)      770 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/context_template.py
--rw-r--r--   0 runner    (1001) docker     (127)      471 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/hparams.py
--rw-r--r--   0 runner    (1001) docker     (127)    15874 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/nethook.py
--rw-r--r--   0 runner    (1001) docker     (127)     4971 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/repr_tools.py
--rw-r--r--   0 runner    (1001) docker     (127)     7583 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/rome.py
--rw-r--r--   0 runner    (1001) docker     (127)     1719 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/rome/rome_hparams.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/tuners/scetuning/
--rw-r--r--   0 runner    (1001) docker     (127)      100 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/scetuning/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)    11560 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/scetuning/scetuning.py
--rw-r--r--   0 runner    (1001) docker     (127)     4043 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/scetuning/scetuning_components.py
--rw-r--r--   0 runner    (1001) docker     (127)    10583 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/side.py
--rw-r--r--   0 runner    (1001) docker     (127)    16154 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/tuners/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/ui/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1598 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/app.py
--rw-r--r--   0 runner    (1001) docker     (127)     5942 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/base.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/ui/llm_infer/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_infer/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2364 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_infer/generate.py
--rw-r--r--   0 runner    (1001) docker     (127)    18470 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_infer/llm_infer.py
--rw-r--r--   0 runner    (1001) docker     (127)     6484 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_infer/model.py
--rw-r--r--   0 runner    (1001) docker     (127)    10844 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_infer/runtime.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     2903 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/advanced.py
--rw-r--r--   0 runner    (1001) docker     (127)     4017 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)     5476 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/hyper.py
--rw-r--r--   0 runner    (1001) docker     (127)    15408 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/llm_train.py
--rw-r--r--   0 runner    (1001) docker     (127)     3302 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/lora.py
--rw-r--r--   0 runner    (1001) docker     (127)     3991 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/model.py
--rw-r--r--   0 runner    (1001) docker     (127)     1583 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/quantization.py
--rw-r--r--   0 runner    (1001) docker     (127)    16741 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/runtime.py
--rw-r--r--   0 runner    (1001) docker     (127)     3186 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/save.py
--rw-r--r--   0 runner    (1001) docker     (127)     1945 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/self_cog.py
--rw-r--r--   0 runner    (1001) docker     (127)     1072 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/ui/llm_train/utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/swift/utils/
--rw-r--r--   0 runner    (1001) docker     (127)     1191 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      542 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/constants.py
--rw-r--r--   0 runner    (1001) docker     (127)     3361 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/hub.py
--rw-r--r--   0 runner    (1001) docker     (127)     2982 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/import_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1108 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/io_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2797 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/logger.py
--rw-r--r--   0 runner    (1001) docker     (127)     2938 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/metric.py
--rw-r--r--   0 runner    (1001) docker     (127)     1429 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/np_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1255 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/run_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     2446 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/tb_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5538 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/torch_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     6592 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/utils/utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      278 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/swift/version.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/tests/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/__init__.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/tests/hub/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/hub/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      778 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/hub/test_check_model.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/tests/llm/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/llm/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)      864 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/llm/test_dataset.py
--rw-r--r--   0 runner    (1001) docker     (127)    20381 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/llm/test_run.py
--rw-r--r--   0 runner    (1001) docker     (127)    27223 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/llm/test_template.py
--rw-r--r--   0 runner    (1001) docker     (127)     4007 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/llm/test_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     1155 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/llm/test_vllm_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)     5094 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/model_tag.py
--rw-r--r--   0 runner    (1001) docker     (127)    22281 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/run.py
--rw-r--r--   0 runner    (1001) docker     (127)    12312 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/test_utils.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/tests/tuners/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     3918 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_extra_state_dict.py
--rw-r--r--   0 runner    (1001) docker     (127)     1930 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_merged_linear.py
--rw-r--r--   0 runner    (1001) docker     (127)     5191 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_neft.py
--rw-r--r--   0 runner    (1001) docker     (127)     8475 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_peft.py
--rw-r--r--   0 runner    (1001) docker     (127)     1977 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_rome.py
--rw-r--r--   0 runner    (1001) docker     (127)     4428 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_scetuning.py
--rw-r--r--   0 runner    (1001) docker     (127)    23592 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_swift_base.py
--rw-r--r--   0 runner    (1001) docker     (127)     1942 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_swift_device_map.py
--rw-r--r--   0 runner    (1001) docker     (127)     5965 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/tuners/test_swift_restuning.py
-drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:10.000000 ms-swift-2.0.3.post1/tests/utils/
--rw-r--r--   0 runner    (1001) docker     (127)        0 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/utils/__init__.py
--rw-r--r--   0 runner    (1001) docker     (127)     1211 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/utils/test_io_utils.py
--rw-r--r--   0 runner    (1001) docker     (127)      404 2024-04-24 11:17:07.000000 ms-swift-2.0.3.post1/tests/utils/test_torch_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/
+-rw-r--r--   0 runner    (1001) docker     (127)      154 2024-05-01 05:20:08.000000 ms-swift-2.0.4/MANIFEST.in
+-rw-r--r--   0 runner    (1001) docker     (127)    60642 2024-05-01 05:20:13.000000 ms-swift-2.0.4/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)    54690 2024-05-01 05:20:08.000000 ms-swift-2.0.4/README.md
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/ms_swift.egg-info/
+-rw-r--r--   0 runner    (1001) docker     (127)    60642 2024-05-01 05:20:12.000000 ms-swift-2.0.4/ms_swift.egg-info/PKG-INFO
+-rw-r--r--   0 runner    (1001) docker     (127)     5609 2024-05-01 05:20:13.000000 ms-swift-2.0.4/ms_swift.egg-info/SOURCES.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-01 05:20:12.000000 ms-swift-2.0.4/ms_swift.egg-info/dependency_links.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       51 2024-05-01 05:20:12.000000 ms-swift-2.0.4/ms_swift.egg-info/entry_points.txt
+-rw-r--r--   0 runner    (1001) docker     (127)        1 2024-05-01 05:20:12.000000 ms-swift-2.0.4/ms_swift.egg-info/not-zip-safe
+-rw-r--r--   0 runner    (1001) docker     (127)      784 2024-05-01 05:20:12.000000 ms-swift-2.0.4/ms_swift.egg-info/requires.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       12 2024-05-01 05:20:12.000000 ms-swift-2.0.4/ms_swift.egg-info/top_level.txt
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/requirements/
+-rw-r--r--   0 runner    (1001) docker     (127)       44 2024-05-01 05:20:08.000000 ms-swift-2.0.4/requirements/aigc.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      115 2024-05-01 05:20:08.000000 ms-swift-2.0.4/requirements/docs.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       15 2024-05-01 05:20:08.000000 ms-swift-2.0.4/requirements/eval.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      247 2024-05-01 05:20:08.000000 ms-swift-2.0.4/requirements/framework.txt
+-rw-r--r--   0 runner    (1001) docker     (127)       85 2024-05-01 05:20:08.000000 ms-swift-2.0.4/requirements/llm.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      119 2024-05-01 05:20:08.000000 ms-swift-2.0.4/requirements/tests.txt
+-rw-r--r--   0 runner    (1001) docker     (127)      797 2024-05-01 05:20:13.000000 ms-swift-2.0.4/setup.cfg
+-rw-r--r--   0 runner    (1001) docker     (127)     6181 2024-05-01 05:20:08.000000 ms-swift-2.0.4/setup.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/
+-rw-r--r--   0 runner    (1001) docker     (127)     2946 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/aigc/
+-rw-r--r--   0 runner    (1001) docker     (127)     1895 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21701 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/animatediff.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4588 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/animatediff_infer.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/aigc/diffusers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1198 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3884 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_controlnet.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3844 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_controlnet_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2394 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_dreambooth.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3254 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_dreambooth_lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3241 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_dreambooth_lora_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3271 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3245 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image_lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3281 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image_lora_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3259 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)    45735 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_controlnet.py
+-rw-r--r--   0 runner    (1001) docker     (127)    51223 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_controlnet_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)    56676 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_dreambooth.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54718 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_dreambooth_lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)    67404 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_dreambooth_lora_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)    44087 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image.py
+-rw-r--r--   0 runner    (1001) docker     (127)    39653 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image_lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)    50800 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image_lora_sdxl.py
+-rw-r--r--   0 runner    (1001) docker     (127)    54357 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image_sdxl.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/aigc/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)       70 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5396 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/aigc/utils/argument.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/cli/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      130 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/app_ui.py
+-rw-r--r--   0 runner    (1001) docker     (127)      130 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/deploy.py
+-rw-r--r--   0 runner    (1001) docker     (127)      124 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/dpo.py
+-rw-r--r--   0 runner    (1001) docker     (127)      126 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)      130 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/export.py
+-rw-r--r--   0 runner    (1001) docker     (127)      128 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/infer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1941 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/main.py
+-rw-r--r--   0 runner    (1001) docker     (127)      160 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/merge_lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)      124 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/sft.py
+-rw-r--r--   0 runner    (1001) docker     (127)      123 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/cli/web_ui.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/hub/
+-rw-r--r--   0 runner    (1001) docker     (127)      255 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30386 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/api.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3666 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/check_model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1868 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3902 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/errors.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12118 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/file_download.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8365 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/git.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7037 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/push_to_hub.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12615 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/repository.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6593 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/snapshot_download.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/hub/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9537 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/utils/caching.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2879 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/hub/utils/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/llm/
+-rw-r--r--   0 runner    (1001) docker     (127)     1259 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      987 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/accelerator.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/llm/agent/
+-rw-r--r--   0 runner    (1001) docker     (127)       40 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/agent/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2879 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/agent/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4248 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/app_ui.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/llm/data/
+-rw-r--r--   0 runner    (1001) docker     (127)    27215 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/data/self_cognition.jsonl
+-rw-r--r--   0 runner    (1001) docker     (127)    18965 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/deploy.py
+-rw-r--r--   0 runner    (1001) docker     (127)     8241 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/dpo.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/llm/ds_config/
+-rw-r--r--   0 runner    (1001) docker     (127)     1275 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/ds_config/zero2.json
+-rw-r--r--   0 runner    (1001) docker     (127)     1545 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/ds_config/zero3.json
+-rw-r--r--   0 runner    (1001) docker     (127)     1544 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/ds_config/zero3_offload.json
+-rw-r--r--   0 runner    (1001) docker     (127)     7721 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/eval.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5782 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/export.py
+-rw-r--r--   0 runner    (1001) docker     (127)    19178 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/infer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3581 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/rome.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12450 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/sft.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13583 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/tuner.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/llm/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     2657 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    51785 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/argument.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3148 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/client_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    58552 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)   141321 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9117 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/preprocess.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3785 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/protocol.py
+-rw-r--r--   0 runner    (1001) docker     (127)    60732 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/template.py
+-rw-r--r--   0 runner    (1001) docker     (127)    30852 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3583 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/vision_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17049 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/llm/utils/vllm_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11181 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/torchacc_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/trainers/
+-rw-r--r--   0 runner    (1001) docker     (127)     1071 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1758 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/arguments.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3093 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/callback.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13725 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/dpo_trainers.py
+-rw-r--r--   0 runner    (1001) docker     (127)    28140 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/mixin.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/trainers/optimizers/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/
+-rw-r--r--   0 runner    (1001) docker     (127)      763 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/__init__.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)    10785 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/adafactor.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     6011 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/adamw.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3819 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/adamw8bit.py
+-rwxr-xr-x   0 runner    (1001) docker     (127)     5176 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/galore_projector.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6716 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/optimizers/galore/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13289 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/trainers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1989 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/trainers/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/tuners/
+-rw-r--r--   0 runner    (1001) docker     (127)     2589 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7604 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/adapter.py
+-rw-r--r--   0 runner    (1001) docker     (127)    41066 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7598 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/llamapro.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/tuners/longlora/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/longlora/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17854 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/longlora/llama.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3778 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/longlora/longlora.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7921 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)    38224 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/lora_layers.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1271 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4949 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/module_mapping.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2409 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/neftune.py
+-rw-r--r--   0 runner    (1001) docker     (127)    13430 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/peft.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9211 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/prompt.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15748 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/restuning.py
+-rw-r--r--   0 runner    (1001) docker     (127)    12682 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/restuning_components.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/tuners/rome/
+-rw-r--r--   0 runner    (1001) docker     (127)       85 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2184 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/compute_u.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9319 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/compute_v.py
+-rw-r--r--   0 runner    (1001) docker     (127)      770 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/context_template.py
+-rw-r--r--   0 runner    (1001) docker     (127)      471 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/hparams.py
+-rw-r--r--   0 runner    (1001) docker     (127)    15391 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/nethook.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4850 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/repr_tools.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7284 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/rome.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1698 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/rome/rome_hparams.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/tuners/scetuning/
+-rw-r--r--   0 runner    (1001) docker     (127)      100 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/scetuning/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10531 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/scetuning/scetuning.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3983 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/scetuning/scetuning_components.py
+-rw-r--r--   0 runner    (1001) docker     (127)     9442 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/side.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14976 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/tuners/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/ui/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1598 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/app.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5878 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/base.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/ui/llm_infer/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_infer/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2104 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_infer/generate.py
+-rw-r--r--   0 runner    (1001) docker     (127)    17439 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_infer/llm_infer.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6172 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_infer/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)    10424 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_infer/runtime.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/ui/llm_train/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2782 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/advanced.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3756 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4903 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/hyper.py
+-rw-r--r--   0 runner    (1001) docker     (127)    14491 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/llm_train.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3050 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/lora.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3747 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/model.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1583 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/quantization.py
+-rw-r--r--   0 runner    (1001) docker     (127)    16132 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/runtime.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3161 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/save.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1881 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/self_cog.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1054 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/ui/llm_train/utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/swift/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)     1071 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      542 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/constants.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3229 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/hub.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2834 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/import_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1070 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/io_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2762 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/logger.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2768 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/metric.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1353 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/np_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1203 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/run_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     2412 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/tb_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5419 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/torch_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     6248 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/utils/utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      272 2024-05-01 05:20:08.000000 ms-swift-2.0.4/swift/version.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/tests/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/__init__.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/tests/hub/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/hub/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      753 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/hub/test_check_model.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/tests/llm/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/llm/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)      813 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/llm/test_dataset.py
+-rw-r--r--   0 runner    (1001) docker     (127)    20012 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/llm/test_run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    26506 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/llm/test_template.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3881 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/llm/test_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1142 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/llm/test_vllm_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4894 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/model_tag.py
+-rw-r--r--   0 runner    (1001) docker     (127)    22281 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/run.py
+-rw-r--r--   0 runner    (1001) docker     (127)    11492 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/test_utils.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/tests/tuners/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     3452 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_extra_state_dict.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1811 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_merged_linear.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4898 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_neft.py
+-rw-r--r--   0 runner    (1001) docker     (127)     7771 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_peft.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1824 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_rome.py
+-rw-r--r--   0 runner    (1001) docker     (127)     4202 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_scetuning.py
+-rw-r--r--   0 runner    (1001) docker     (127)    21850 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_swift_base.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1793 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_swift_device_map.py
+-rw-r--r--   0 runner    (1001) docker     (127)     5741 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/tuners/test_swift_restuning.py
+drwxr-xr-x   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:13.000000 ms-swift-2.0.4/tests/utils/
+-rw-r--r--   0 runner    (1001) docker     (127)        0 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/utils/__init__.py
+-rw-r--r--   0 runner    (1001) docker     (127)     1184 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/utils/test_io_utils.py
+-rw-r--r--   0 runner    (1001) docker     (127)      391 2024-05-01 05:20:08.000000 ms-swift-2.0.4/tests/utils/test_torch_utils.py
```

### Comparing `ms-swift-2.0.3.post1/PKG-INFO` & `ms-swift-2.0.4/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ms-swift
-Version: 2.0.3.post1
+Version: 2.0.4
 Summary: Swift: Scalable lightWeight Infrastructure for Fine-Tuning
 Home-page: https://github.com/modelscope/swift
 Author: DAMO ModelScope teams
 Author-email: contact@modelscope.cn
 License: Apache License 2.0
 Description: # SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)
         
@@ -43,14 +43,17 @@
         SWIFT supports training, inference, evaluation and deployment of nearly **200 LLMs and MLLMs** (multimodal large models). Developers can directly apply our framework to their own research and production environments to realize the complete workflow from model training and evaluation to application. In addition to supporting the lightweight training solutions provided by [PEFT](https://github.com/huggingface/peft), we also provide a complete **Adapters library** to support the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This adapter library can be used directly in your own custom workflow without our training scripts.
         
         To facilitate use by users unfamiliar with deep learning, we provide a Gradio web-ui for controlling training and inference, as well as accompanying deep learning courses and best practices for beginners.
         
         Additionally, we are expanding capabilities for other modalities. Currently, we support full-parameter training and LoRA training for AnimateDiff.
         
         ##  News
+        - 2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model. For best practice, you can refer to [here](https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md).
+        - 2024.04.26: Support **LISA** and **unsloth** training! Specify `--lisa_activated_layers=2` to use LISA(to reduce the memory cost to 30 percent!), specify `--tuner_backend unsloth` to use unsloth to train a huge model(full or lora) with lesser memory(30 percent or lesser) and faster speed(5x)!
+        - 2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and Qwen1.5-110B-Chat model, use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh) to start training!
         - 2024.04.24: Support for inference and fine-tuning of Phi3 series models. Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/phi3_4b_4k_instruct/lora), phi3-4b-128k-instruct.
         - 2024.04.22: Support for inference, fine-tuning, and deployment of **chinese-llama-alpaca-2** series models. This includeschinese-llama-2-1.3b, chinese-llama-2-7b, chinese-llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-13b along with their corresponding 16k and 64k long text versions.
         - 2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-6b-128k, Openbuddy-Llama3.
         - 2024.04.20: Support for inference, fine-tuning, and deployment of **Atom** series models. This includes: Atom-7B and Atom-7B-Chat. use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/atom_7b_chat/lora/sft.sh) to train.
         - 2024.04.19: Support for single-card, DDP, ZeRO2, and ZeRO3 training and inference with NPU, please refer to [NPU Inference and Fine-tuning Best Practices](docs/source_en/LLM/NPU-best-practice.md).
         - 2024.04.19: Support for inference, fine-tuning, and deployment of **Llama3** series models. This includes: Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B, and Llama-3-70B-Instruct. use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/llama3_8b_instruct/lora/sft.sh) to train.
         - 2024.04.18: Supported models: wizardlm2-7b-awq, wizardlm2-8x22b, yi-6b-chat-awq, yi-6b-chat-int8, yi-34b-chat-awq, yi-34b-chat-int8. Supported `--deepspeed zero3-offload` and provided default zero3-offload configuration file for zero3+cpu offload usage.
@@ -444,15 +447,15 @@
         ### Supported Models
         The complete list of supported models and datasets can be found at [Supported Models and Datasets List](docs/source_en/LLM/Supported-models-datasets.md).
         
         #### LLMs
         
         | Model Type                                     | Model Introduction                                                     | Language           | Model Size                             | Model Type                                 |
         |------------------------------------------------|------------------------------------------------------------------------|--------------------|----------------------------------------|------------------------------------------- |
-        | Qwen<br>Qwen1.5                                   | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM)  | Chinese<br>English    | 0.5B-72B<br>including quantized versions | base model<br>chat model<br>MoE model<br>code model                      |
+        | Qwen<br>Qwen1.5                                   | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM)  | Chinese<br>English    | 0.5B-110B<br>including quantized versions | base model<br>chat model<br>MoE model<br>code model                      |
         | ChatGLM2<br>ChatGLM3<br>Codegeex2                    | [Zhipu ChatGLM series models](https://github.com/THUDM)               | Chinese<br>English    | 6B                                     | base model<br>chat model<br>code model<br>long text model  |
         | Baichuan/Baichuan2                             | [Baichuan 1 and Baichuan 2](https://github.com/baichuan-inc)           | Chinese<br>English    | 7B-13B<br>including quantized versions             | base model<br>chat model                       |
         | Yuan2                                          | [Langchao Yuan series models](https://github.com/IEIT-Yuan)             | Chinese<br>English    | 2B-102B                                | instruct model                                 |
         | XVerse                                         | [XVerse series models](https://github.com/xverse-ai)                    | Chinese<br>English    | 7B-65B                                 | base model<br>chat model<br>long text model<br>MoE model                |
         | LLaMA2                                         | [LLaMA2 series models](https://github.com/facebookresearch/llama)       | English            | 7B-70B<br>including quantized versions   | base model<br>chat model                       |
         | LLaMA3                   | [LLaMA3 series models](https://github.com/meta-llama/llama3)  | English       | 8B-70B<br>including quantized versions      | base model<br>chat model              |
         | Mistral<br>Mixtral                            | [Mistral series models](https://github.com/mistralai/mistral-src)       | English            | 7B-22B     | base model<br>instruct model<br>MoE model                     |
@@ -478,14 +481,15 @@
         | TeleChat | [Tele-AI](https://github.com/Tele-AI/Telechat) | Chinese<br>English | 7B-12B | chat model |
         | dbrx | [databricks](https://github.com/databricks/dbrx) | English | 132B | base model<br>chat model  |
         | mengzi3 | [Langboat](https://github.com/Langboat/Mengzi3) | Chinese<br>English | 13B | base model  |
         | c4ai-command-r | [c4ai](https://cohere.com/command) | Multilingual | 35B-104B | chat model  |
         | WizardLM2 | [WizardLM2 series models](https://github.com/nlpxucan/WizardLM) | English | 7B-8x22B<br>including quantized versions | chat model<br>MoE model |
         | Atom | [Atom](https://github.com/LlamaFamily/Llama-Chinese) | Chinese | 7B| base model<br>chat model|
         | Chinese-LLaMA-Alpaca-2 | [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) | Chinese | 1.3B-13B| base model<br>chat model<br>long text model |
+        | Chinese-LLaMA-Alpaca-3 | [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3) | Chinese | 8B| base model<br>chat model|
         | ModelScope-Agent | [ModelScope Agent series models](https://github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model |
         
         #### MLLMs
         
         | Model Type       | Model Introduction                                                     | Language           | Model Size        | Model Type         |
         |------------------|------------------------------------------------------------------------|--------------------|-------------------|------------------- |
         | Qwen-VL          | [Tongyi Qwen vision model](https://github.com/QwenLM)               | Chinese<br>English    | 7B<br>including quantized versions | base model<br>chat model |
@@ -493,14 +497,15 @@
         | YI-VL            | [01AI's YI series vision models](https://github.com/01-ai)             | Chinese<br>English    | 6B-34B            | chat model         |
         | XComposer2       | [Pujiang AI Lab InternLM vision model](https://github.com/InternLM/InternLM) | Chinese<br>English | 7B              | chat model         |
         | DeepSeek-VL      | [DeepSeek series vision models](https://github.com/deepseek-ai) | Chinese<br>English    | 1.3B-7B           | chat model         |
         | MiniCPM-V       | [OpenBmB MiniCPM vision model](https://github.com/OpenBMB/MiniCPM)     | Chinese<br>English    | 3B                | chat model         |
         | CogVLM<br>CogAgent  | [Zhipu ChatGLM visual QA and Agent model](https://github.com/THUDM/)   | English    | 17B-18B           | chat model         |
         | Llava      | [Llava series models](https://github.com/haotian-liu/LLaVA)                | English | 7B-34B               | chat model |
         | mPLUG-Owl      | [mPLUG-Owl series models](https://github.com/X-PLUG/mPLUG-Owl)         | English | 11B               | chat model |
+        | InternVL         | [InternVL](https://github.com/OpenGVLab/InternVL)                | Chinese<br>English | 25.5B | chat model |
         
         #### Diffusion Models
         
         | Model Type          | Model Introduction                                                    | Language | Model Type        |
         |---------------------|----------------------------------------------------------------------|----------|------------------ |
         | AnimateDiff         | [AnimateDiff animation model](https://github.com/guoyww/AnimateDiff) | English  | text-to-video     |
         | SD1.5/SD2.0/SDXL    | [StabilityAI series diffusion models](https://github.com/Stability-AI) | English | text-to-image    |
@@ -522,28 +527,29 @@
         | Quantization Assist | Quantization | pileval.                                                                                                                                                                                                                                                                                                             |
         | Other        | Fine-tuning    | finance-en, poetry-zh, webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh.                                                                                                                                                                                                                               |
         | Vision       | Fine-tuning    | coco-en, coco-mini-en, coco-mini-en-2, capcha-images.                                                                                                                                                                                                                                                              |
         | Audio        | Fine-tuning    | aishell1-zh, aishell1-mini-zh.                                                                                                                                                                                                                                                                                     |
         
         ### Supported Technologies
         
-        | Technology Name                                               |
-        |--------------------------------------------------------------- |
+        | Technology Name                                              |
+        | ------------------------------------------------------------ |
         | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685) |
         | LoRA+: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf) |
+        | GaLore:[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) |
+        | LISA: [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) |
+        | UnSloth: https://github.com/unslothai/unsloth               |
         | LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://arxiv.org/pdf/2401.02415.pdf) |
-        | SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  \|  [Project Page](https://scedit.github.io/) > |
+        | SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  \ |
         | NEFTune: [Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) |
-        | QA-LoRA:[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) |
         | LongLoRA: [Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307) |
-        | ROME: [Rank-One Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317) |
         | Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751) |
-        | Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) |
+        | Vision Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) |
         | Side: [Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503) |
-        | Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  \|  [Project Page](https://res-tuning.github.io/)  \|  [Usage](docs/source/GetStarted/ResTuning.md) > |
+        | Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  \ |
         | Tuners provided by [PEFT](https://github.com/huggingface/peft), such as IA3, AdaLoRA, etc. |
         
         ### Supported Hardware
         
         | Hardware Environment           | Notes                                           |
         |--------------------------------|-------------------------------------------------|
         | CPU                            |                                                 |
```

#### html2text {}

```diff
@@ -1,10 +1,10 @@
-Metadata-Version: 2.1 Name: ms-swift Version: 2.0.3.post1 Summary: Swift:
-Scalable lightWeight Infrastructure for Fine-Tuning Home-page: https://
-github.com/modelscope/swift Author: DAMO ModelScope teams Author-email:
+Metadata-Version: 2.1 Name: ms-swift Version: 2.0.4 Summary: Swift: Scalable
+lightWeight Infrastructure for Fine-Tuning Home-page: https://github.com/
+modelscope/swift Author: DAMO ModelScope teams Author-email:
 contact@modelscope.cn License: Apache License 2.0 Description: # SWIFT
 (Scalable lightWeight Infrastructure for Fine-Tuning)
 
                             [resources/banner.png]
                          _M_o_d_e_l_S_c_o_p_e_ _C_o_m_m_u_n_i_t_y_ _W_e_b_s_i_t_e
                            ______    English 
       [https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg][https://
@@ -26,16 +26,26 @@
 the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This
 adapter library can be used directly in your own custom workflow without our
 training scripts. To facilitate use by users unfamiliar with deep learning, we
 provide a Gradio web-ui for controlling training and inference, as well as
 accompanying deep learning courses and best practices for beginners.
 Additionally, we are expanding capabilities for other modalities. Currently, we
 support full-parameter training and LoRA training for AnimateDiff. ##  News
-- 2024.04.24: Support for inference and fine-tuning of Phi3 series models.
-Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/
+- 2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model.
+For best practice, you can refer to [here](https://github.com/modelscope/swift/
+tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md). -
+2024.04.26: Support **LISA** and **unsloth** training! Specify `--
+lisa_activated_layers=2` to use LISA(to reduce the memory cost to 30 percent!),
+specify `--tuner_backend unsloth` to use unsloth to train a huge model(full or
+lora) with lesser memory(30 percent or lesser) and faster speed(5x)! -
+2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and
+Qwen1.5-110B-Chat model, use [this script](https://github.com/modelscope/swift/
+blob/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh)
+to start training! - 2024.04.24: Support for inference and fine-tuning of Phi3
+series models. Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/
 phi3_4b_4k_instruct/lora), phi3-4b-128k-instruct. - 2024.04.22: Support for
 inference, fine-tuning, and deployment of **chinese-llama-alpaca-2** series
 models. This includeschinese-llama-2-1.3b, chinese-llama-2-7b, chinese-
 llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-
 13b along with their corresponding 16k and 64k long text versions. -
 2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-
 Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-
@@ -406,15 +416,15 @@
 models-datasets.md). #### LLMs | Model Type | Model Introduction | Language |
 Model Size | Model Type | |------------------------------------------------|---
 ---------------------------------------------------------------------|---------
 -----------|----------------------------------------|--------------------------
 ----------------- | | Qwen
 Qwen1.5 | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM) |
 Chinese
-English | 0.5B-72B
+English | 0.5B-110B
 including quantized versions | base model
 chat model
 MoE model
 code model | | ChatGLM2
 ChatGLM3
 Codegeex2 | [Zhipu ChatGLM series models](https://github.com/THUDM) | Chinese
 English | 6B | base model
@@ -515,16 +525,18 @@
 series models](https://github.com/nlpxucan/WizardLM) | English | 7B-8x22B
 including quantized versions | chat model
 MoE model | | Atom | [Atom](https://github.com/LlamaFamily/Llama-Chinese) |
 Chinese | 7B| base model
 chat model| | Chinese-LLaMA-Alpaca-2 | [Chinese-LLaMA-Alpaca-2](https://
 github.com/ymcui/Chinese-LLaMA-Alpaca-2) | Chinese | 1.3B-13B| base model
 chat model
-long text model | | ModelScope-Agent | [ModelScope Agent series models](https:/
-/github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model | ####
+long text model | | Chinese-LLaMA-Alpaca-3 | [Chinese-LLaMA-Alpaca-3](https://
+github.com/ymcui/Chinese-LLaMA-Alpaca-3) | Chinese | 8B| base model
+chat model| | ModelScope-Agent | [ModelScope Agent series models](https://
+github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model | ####
 MLLMs | Model Type | Model Introduction | Language | Model Size | Model Type |
 |------------------|-----------------------------------------------------------
 -------------|--------------------|-------------------|------------------- | |
 Qwen-VL | [Tongyi Qwen vision model](https://github.com/QwenLM) | Chinese
 English | 7B
 including quantized versions | base model
 chat model | | Qwen-Audio | [Tongyi Qwen speech model](https://github.com/
@@ -539,18 +551,20 @@
 English | 1.3B-7B | chat model | | MiniCPM-V | [OpenBmB MiniCPM vision model]
 (https://github.com/OpenBMB/MiniCPM) | Chinese
 English | 3B | chat model | | CogVLM
 CogAgent | [Zhipu ChatGLM visual QA and Agent model](https://github.com/THUDM/
 ) | English | 17B-18B | chat model | | Llava | [Llava series models](https://
 github.com/haotian-liu/LLaVA) | English | 7B-34B | chat model | | mPLUG-Owl |
 [mPLUG-Owl series models](https://github.com/X-PLUG/mPLUG-Owl) | English | 11B
-| chat model | #### Diffusion Models | Model Type | Model Introduction |
-Language | Model Type | |---------------------|--------------------------------
---------------------------------------|----------|------------------ | |
-AnimateDiff | [AnimateDiff animation model](https://github.com/guoyww/
+| chat model | | InternVL | [InternVL](https://github.com/OpenGVLab/InternVL) |
+Chinese
+English | 25.5B | chat model | #### Diffusion Models | Model Type | Model
+Introduction | Language | Model Type | |---------------------|-----------------
+-----------------------------------------------------|----------|--------------
+---- | | AnimateDiff | [AnimateDiff animation model](https://github.com/guoyww/
 AnimateDiff) | English | text-to-video | | SD1.5/SD2.0/SDXL | [StabilityAI
 series diffusion models](https://github.com/Stability-AI) | English | text-to-
 image | ### Supported Open Source Datasets | Dataset Type | Training Task |
 Documentation | |--------------|:---------------|------------------------------
 --------------------------------- | | General | Fine-tuning | ruozhiba,
 ms-bench, ms-bench-mini, alpaca-en(gpt4), alpaca-zh(gpt4),
 multi-alpaca-all, instinwild-en, instinwild-zh, cot-en, cot-zh, firefly-all-zh,
@@ -571,68 +585,68 @@
 context-en. | | Text Generation | Fine-tuning | advertise-gen-zh,
 dureader-robust-zh. | | Classification | Fine-tuning | cmnli-zh, cmnli-
 mini-zh, jd-sentiment-zh, hc3-zh, hc3-en. | | Quantization Assist |
 Quantization | pileval. | | Other | Fine-tuning | finance-en, poetry-zh,
 webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh. | | Vision |
 Fine-tuning | coco-en, coco-mini-en, coco-mini-en-2, capcha-images. | |
 Audio | Fine-tuning | aishell1-zh, aishell1-mini-zh. | ### Supported
-Technologies | Technology Name | |---------------------------------------------
------------------- | | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE
+Technologies | Technology Name | | --------------------------------------------
+---------------- | | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE
 MODELS](https://arxiv.org/abs/2106.09685) | | LoRA+: [LoRA+: Efficient Low
 Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf) | |
-LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://
-arxiv.org/pdf/2401.02415.pdf) | | SCEdit: [SCEdit: Efficient and
-Controllable Image Diffusion Generation via Skip Connection Editing](https://
-arxiv.org/abs/2312.11392) < [arXiv](https://arxiv.org/abs/2312.11392) \|
-[Project Page](https://scedit.github.io/) > | | NEFTune: [Noisy Embeddings
-Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) | | QA-LoRA:
-[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://
-arxiv.org/abs/2309.14717) | | LongLoRA: [Efficient Fine-tuning of Long-Context
-Large Language Models](https://arxiv.org/abs/2309.12307) | | ROME: [Rank-One
-Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317) | |
-Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/
-1902.00751) | | Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/
-2203.12119) | | Side: [Side-Tuning: A Baseline for Network Adaptation via
-Additive Side Networks](https://arxiv.org/abs/1912.13503) | | Res-Tuning: [Res-
-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from
-Backbone](https://arxiv.org/abs/2310.19859) < [arXiv](https://arxiv.org/abs/
-2310.19859) \| [Project Page](https://res-tuning.github.io/) \| [Usage](docs/
-source/GetStarted/ResTuning.md) > | | Tuners provided by [PEFT](https://
-github.com/huggingface/peft), such as IA3, AdaLoRA, etc. | ### Supported
-Hardware | Hardware Environment | Notes | |--------------------------------|---
-----------------------------------------------| | CPU | | | RTX 20/30/40
-series, etc. | After 30 series, BF16 and FlashAttn can be used | | Computing
-cards T4/V100, etc. | BF16 and FlashAttn not supported | | Computing cards A10/
-A100, etc. | Support BF16 and FlashAttn | | Huawei Ascend NPU | | ## 
-Documentation ### Documentation Compiling ```shell make docs # Check docs/
-build/html/index.html in web-browser ``` ### User Guide | Document Name | | ---
---------------------------------------------------------- | | [Using Web-UI]
-(docs/source_en/GetStarted/Web-ui.md) | | [Using Tuners](docs/source_en/
-GetStarted/Tuners.md) | | [LLM Inference](docs/source_en/LLM/LLM-inference.md)
-| | [LLM Fine-tuning](docs/source_en/LLM/LLM-fine-tuning.md) | | [LLM
-Evaluation](docs/source_en/LLM/LLM-eval.md) | | [LLM Quantization](docs/
-source_en/LLM/LLM-quantization.md) | | [LLM Deployment](docs/source_en/LLM/
-VLLM-inference-acceleration-and-deployment.md) | | [DPO Human Alignment
-Training](docs/source_en/LLM/RLHF.md) | | [AnimateDiff Training](docs/
-source_en/AIGC/AnimateDiff-train-infer.md) | ### Reference Documentation |
-Document Name | | -----------------------------------------------------------
-- | | [Command Line Arguments](docs/source_en/LLM/Command-line-parameters.md) |
-| [Supported Models and Datasets List](docs/source_en/LLM/Supported-models-
-datasets.md) | | [Customizing New Models and Datasets](docs/source_en/LLM/
-Customization.md) | | [Runtime Speed and Memory Benchmark](docs/source_en/LLM/
-Benchmark.md) | ### Best Practices | Best Practices Name | | ------------------
------------------------------------------- | | [Agent Fine-Tuning Best
-Practice](docs/source_en/LLM/Agent-best-practice.md) | | [Self-Cognition Fine-
-Tuning Best Practice](docs/source_en/LLM/Self-cognition-best-practice.md) | |
-[Qwen1.5 Best Practice](docs/source_en/LLM/Qwen1.5-best-practice.md) | |
-[Multi-Modal Model Training Best Practice](docs/source_en/Multi-Modal/index.md)
-| | [NPU Best Practice](docs/source_en/LLM/NPU-best-practice.md) | ### Deep
-Learning Tutorials | Tutorial Name | |-----------------------------------------
---------------------- | | [Introduction to Deep Learning](https://github.com/
-modelscope/modelscope-classroom/blob/main/LLM-tutorial/
+GaLore:[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank
+Projection](https://arxiv.org/abs/2403.03507) | | LISA: [LISA: Layerwise
+Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning]
+(https://arxiv.org/abs/2403.17919) | | UnSloth: https://github.com/
+unslothai/unsloth | | LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block
+Expansion](https://arxiv.org/pdf/2401.02415.pdf) | | SCEdit: [SCEdit:
+Efficient and Controllable Image Diffusion Generation via Skip Connection
+Editing](https://arxiv.org/abs/2312.11392) < [arXiv](https://arxiv.org/abs/
+2312.11392) \ | | NEFTune: [Noisy Embeddings Improve Instruction
+Finetuning](https://arxiv.org/abs/2310.05914) | | LongLoRA: [Efficient Fine-
+tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
+| | Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/
+abs/1902.00751) | | Vision Prompt Tuning: [Visual Prompt Tuning](https://
+arxiv.org/abs/2203.12119) | | Side: [Side-Tuning: A Baseline for Network
+Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503) | |
+Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding
+Tuner from Backbone](https://arxiv.org/abs/2310.19859) < [arXiv](https://
+arxiv.org/abs/2310.19859) \ | | Tuners provided by [PEFT](https://github.com/
+huggingface/peft), such as IA3, AdaLoRA, etc. | ### Supported Hardware |
+Hardware Environment | Notes | |--------------------------------|--------------
+-----------------------------------| | CPU | | | RTX 20/30/40 series, etc. |
+After 30 series, BF16 and FlashAttn can be used | | Computing cards T4/V100,
+etc. | BF16 and FlashAttn not supported | | Computing cards A10/A100, etc. |
+Support BF16 and FlashAttn | | Huawei Ascend NPU | | ##  Documentation ###
+Documentation Compiling ```shell make docs # Check docs/build/html/index.html
+in web-browser ``` ### User Guide | Document Name | | -------------------------
+----------------------------------- | | [Using Web-UI](docs/source_en/
+GetStarted/Web-ui.md) | | [Using Tuners](docs/source_en/GetStarted/Tuners.md) |
+| [LLM Inference](docs/source_en/LLM/LLM-inference.md) | | [LLM Fine-tuning]
+(docs/source_en/LLM/LLM-fine-tuning.md) | | [LLM Evaluation](docs/source_en/
+LLM/LLM-eval.md) | | [LLM Quantization](docs/source_en/LLM/LLM-quantization.md)
+| | [LLM Deployment](docs/source_en/LLM/VLLM-inference-acceleration-and-
+deployment.md) | | [DPO Human Alignment Training](docs/source_en/LLM/RLHF.md) |
+| [AnimateDiff Training](docs/source_en/AIGC/AnimateDiff-train-infer.md) | ###
+Reference Documentation | Document Name | | -----------------------------------
+------------------------- | | [Command Line Arguments](docs/source_en/LLM/
+Command-line-parameters.md) | | [Supported Models and Datasets List](docs/
+source_en/LLM/Supported-models-datasets.md) | | [Customizing New Models and
+Datasets](docs/source_en/LLM/Customization.md) | | [Runtime Speed and Memory
+Benchmark](docs/source_en/LLM/Benchmark.md) | ### Best Practices | Best
+Practices Name | | -----------------------------------------------------------
+- | | [Agent Fine-Tuning Best Practice](docs/source_en/LLM/Agent-best-
+practice.md) | | [Self-Cognition Fine-Tuning Best Practice](docs/source_en/LLM/
+Self-cognition-best-practice.md) | | [Qwen1.5 Best Practice](docs/source_en/
+LLM/Qwen1.5-best-practice.md) | | [Multi-Modal Model Training Best Practice]
+(docs/source_en/Multi-Modal/index.md) | | [NPU Best Practice](docs/source_en/
+LLM/NPU-best-practice.md) | ### Deep Learning Tutorials | Tutorial Name | |----
+---------------------------------------------------------- | | [Introduction to
+Deep Learning](https://github.com/modelscope/modelscope-classroom/blob/main/
+LLM-tutorial/
 A.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md)
 | | [Large Model Basics](https://github.com/modelscope/modelscope-classroom/
 blob/main/LLM-tutorial/
 B.%E9%AD%94%E6%90%AD%E7%A4%BE%E5%8C%BA%E5%92%8CLLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.md)
 | | [Prompt Engineering](https://github.com/modelscope/modelscope-classroom/
 blob/main/LLM-tutorial/C.%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-
 prompt%20engineering.md) | | [Transformer Architecture Introduction](https://
```

### Comparing `ms-swift-2.0.3.post1/README.md` & `ms-swift-2.0.4/README.md`

 * *Files 2% similar despite different names*

```diff
@@ -35,14 +35,17 @@
 SWIFT supports training, inference, evaluation and deployment of nearly **200 LLMs and MLLMs** (multimodal large models). Developers can directly apply our framework to their own research and production environments to realize the complete workflow from model training and evaluation to application. In addition to supporting the lightweight training solutions provided by [PEFT](https://github.com/huggingface/peft), we also provide a complete **Adapters library** to support the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This adapter library can be used directly in your own custom workflow without our training scripts.
 
 To facilitate use by users unfamiliar with deep learning, we provide a Gradio web-ui for controlling training and inference, as well as accompanying deep learning courses and best practices for beginners.
 
 Additionally, we are expanding capabilities for other modalities. Currently, we support full-parameter training and LoRA training for AnimateDiff.
 
 ##  News
+- 2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model. For best practice, you can refer to [here](https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md).
+- 2024.04.26: Support **LISA** and **unsloth** training! Specify `--lisa_activated_layers=2` to use LISA(to reduce the memory cost to 30 percent!), specify `--tuner_backend unsloth` to use unsloth to train a huge model(full or lora) with lesser memory(30 percent or lesser) and faster speed(5x)!
+- 2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and Qwen1.5-110B-Chat model, use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh) to start training!
 - 2024.04.24: Support for inference and fine-tuning of Phi3 series models. Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/phi3_4b_4k_instruct/lora), phi3-4b-128k-instruct.
 - 2024.04.22: Support for inference, fine-tuning, and deployment of **chinese-llama-alpaca-2** series models. This includeschinese-llama-2-1.3b, chinese-llama-2-7b, chinese-llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-13b along with their corresponding 16k and 64k long text versions.
 - 2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-6b-128k, Openbuddy-Llama3.
 - 2024.04.20: Support for inference, fine-tuning, and deployment of **Atom** series models. This includes: Atom-7B and Atom-7B-Chat. use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/atom_7b_chat/lora/sft.sh) to train.
 - 2024.04.19: Support for single-card, DDP, ZeRO2, and ZeRO3 training and inference with NPU, please refer to [NPU Inference and Fine-tuning Best Practices](docs/source_en/LLM/NPU-best-practice.md).
 - 2024.04.19: Support for inference, fine-tuning, and deployment of **Llama3** series models. This includes: Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B, and Llama-3-70B-Instruct. use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/llama3_8b_instruct/lora/sft.sh) to train.
 - 2024.04.18: Supported models: wizardlm2-7b-awq, wizardlm2-8x22b, yi-6b-chat-awq, yi-6b-chat-int8, yi-34b-chat-awq, yi-34b-chat-int8. Supported `--deepspeed zero3-offload` and provided default zero3-offload configuration file for zero3+cpu offload usage.
@@ -436,15 +439,15 @@
 ### Supported Models
 The complete list of supported models and datasets can be found at [Supported Models and Datasets List](docs/source_en/LLM/Supported-models-datasets.md).
 
 #### LLMs
 
 | Model Type                                     | Model Introduction                                                     | Language           | Model Size                             | Model Type                                 |
 |------------------------------------------------|------------------------------------------------------------------------|--------------------|----------------------------------------|------------------------------------------- |
-| Qwen<br>Qwen1.5                                   | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM)  | Chinese<br>English    | 0.5B-72B<br>including quantized versions | base model<br>chat model<br>MoE model<br>code model                      |
+| Qwen<br>Qwen1.5                                   | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM)  | Chinese<br>English    | 0.5B-110B<br>including quantized versions | base model<br>chat model<br>MoE model<br>code model                      |
 | ChatGLM2<br>ChatGLM3<br>Codegeex2                    | [Zhipu ChatGLM series models](https://github.com/THUDM)               | Chinese<br>English    | 6B                                     | base model<br>chat model<br>code model<br>long text model  |
 | Baichuan/Baichuan2                             | [Baichuan 1 and Baichuan 2](https://github.com/baichuan-inc)           | Chinese<br>English    | 7B-13B<br>including quantized versions             | base model<br>chat model                       |
 | Yuan2                                          | [Langchao Yuan series models](https://github.com/IEIT-Yuan)             | Chinese<br>English    | 2B-102B                                | instruct model                                 |
 | XVerse                                         | [XVerse series models](https://github.com/xverse-ai)                    | Chinese<br>English    | 7B-65B                                 | base model<br>chat model<br>long text model<br>MoE model                |
 | LLaMA2                                         | [LLaMA2 series models](https://github.com/facebookresearch/llama)       | English            | 7B-70B<br>including quantized versions   | base model<br>chat model                       |
 | LLaMA3                   | [LLaMA3 series models](https://github.com/meta-llama/llama3)  | English       | 8B-70B<br>including quantized versions      | base model<br>chat model              |
 | Mistral<br>Mixtral                            | [Mistral series models](https://github.com/mistralai/mistral-src)       | English            | 7B-22B     | base model<br>instruct model<br>MoE model                     |
@@ -470,14 +473,15 @@
 | TeleChat | [Tele-AI](https://github.com/Tele-AI/Telechat) | Chinese<br>English | 7B-12B | chat model |
 | dbrx | [databricks](https://github.com/databricks/dbrx) | English | 132B | base model<br>chat model  |
 | mengzi3 | [Langboat](https://github.com/Langboat/Mengzi3) | Chinese<br>English | 13B | base model  |
 | c4ai-command-r | [c4ai](https://cohere.com/command) | Multilingual | 35B-104B | chat model  |
 | WizardLM2 | [WizardLM2 series models](https://github.com/nlpxucan/WizardLM) | English | 7B-8x22B<br>including quantized versions | chat model<br>MoE model |
 | Atom | [Atom](https://github.com/LlamaFamily/Llama-Chinese) | Chinese | 7B| base model<br>chat model|
 | Chinese-LLaMA-Alpaca-2 | [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) | Chinese | 1.3B-13B| base model<br>chat model<br>long text model |
+| Chinese-LLaMA-Alpaca-3 | [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3) | Chinese | 8B| base model<br>chat model|
 | ModelScope-Agent | [ModelScope Agent series models](https://github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model |
 
 #### MLLMs
 
 | Model Type       | Model Introduction                                                     | Language           | Model Size        | Model Type         |
 |------------------|------------------------------------------------------------------------|--------------------|-------------------|------------------- |
 | Qwen-VL          | [Tongyi Qwen vision model](https://github.com/QwenLM)               | Chinese<br>English    | 7B<br>including quantized versions | base model<br>chat model |
@@ -485,14 +489,15 @@
 | YI-VL            | [01AI's YI series vision models](https://github.com/01-ai)             | Chinese<br>English    | 6B-34B            | chat model         |
 | XComposer2       | [Pujiang AI Lab InternLM vision model](https://github.com/InternLM/InternLM) | Chinese<br>English | 7B              | chat model         |
 | DeepSeek-VL      | [DeepSeek series vision models](https://github.com/deepseek-ai) | Chinese<br>English    | 1.3B-7B           | chat model         |
 | MiniCPM-V       | [OpenBmB MiniCPM vision model](https://github.com/OpenBMB/MiniCPM)     | Chinese<br>English    | 3B                | chat model         |
 | CogVLM<br>CogAgent  | [Zhipu ChatGLM visual QA and Agent model](https://github.com/THUDM/)   | English    | 17B-18B           | chat model         |
 | Llava      | [Llava series models](https://github.com/haotian-liu/LLaVA)                | English | 7B-34B               | chat model |
 | mPLUG-Owl      | [mPLUG-Owl series models](https://github.com/X-PLUG/mPLUG-Owl)         | English | 11B               | chat model |
+| InternVL         | [InternVL](https://github.com/OpenGVLab/InternVL)                | Chinese<br>English | 25.5B | chat model |
 
 #### Diffusion Models
 
 | Model Type          | Model Introduction                                                    | Language | Model Type        |
 |---------------------|----------------------------------------------------------------------|----------|------------------ |
 | AnimateDiff         | [AnimateDiff animation model](https://github.com/guoyww/AnimateDiff) | English  | text-to-video     |
 | SD1.5/SD2.0/SDXL    | [StabilityAI series diffusion models](https://github.com/Stability-AI) | English | text-to-image    |
@@ -514,28 +519,29 @@
 | Quantization Assist | Quantization | pileval.                                                                                                                                                                                                                                                                                                             |
 | Other        | Fine-tuning    | finance-en, poetry-zh, webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh.                                                                                                                                                                                                                               |
 | Vision       | Fine-tuning    | coco-en, coco-mini-en, coco-mini-en-2, capcha-images.                                                                                                                                                                                                                                                              |
 | Audio        | Fine-tuning    | aishell1-zh, aishell1-mini-zh.                                                                                                                                                                                                                                                                                     |
 
 ### Supported Technologies
 
-| Technology Name                                               |
-|--------------------------------------------------------------- |
+| Technology Name                                              |
+| ------------------------------------------------------------ |
 | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685) |
 | LoRA+: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf) |
+| GaLore:[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) |
+| LISA: [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) |
+| UnSloth: https://github.com/unslothai/unsloth               |
 | LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://arxiv.org/pdf/2401.02415.pdf) |
-| SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  \|  [Project Page](https://scedit.github.io/) > |
+| SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  \ |
 | NEFTune: [Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) |
-| QA-LoRA:[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) |
 | LongLoRA: [Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307) |
-| ROME: [Rank-One Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317) |
 | Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751) |
-| Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) |
+| Vision Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) |
 | Side: [Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503) |
-| Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  \|  [Project Page](https://res-tuning.github.io/)  \|  [Usage](docs/source/GetStarted/ResTuning.md) > |
+| Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  \ |
 | Tuners provided by [PEFT](https://github.com/huggingface/peft), such as IA3, AdaLoRA, etc. |
 
 ### Supported Hardware
 
 | Hardware Environment           | Notes                                           |
 |--------------------------------|-------------------------------------------------|
 | CPU                            |                                                 |
```

#### html2text {}

```diff
@@ -22,16 +22,26 @@
 the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This
 adapter library can be used directly in your own custom workflow without our
 training scripts. To facilitate use by users unfamiliar with deep learning, we
 provide a Gradio web-ui for controlling training and inference, as well as
 accompanying deep learning courses and best practices for beginners.
 Additionally, we are expanding capabilities for other modalities. Currently, we
 support full-parameter training and LoRA training for AnimateDiff. ##  News
-- 2024.04.24: Support for inference and fine-tuning of Phi3 series models.
-Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/
+- 2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model.
+For best practice, you can refer to [here](https://github.com/modelscope/swift/
+tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md). -
+2024.04.26: Support **LISA** and **unsloth** training! Specify `--
+lisa_activated_layers=2` to use LISA(to reduce the memory cost to 30 percent!),
+specify `--tuner_backend unsloth` to use unsloth to train a huge model(full or
+lora) with lesser memory(30 percent or lesser) and faster speed(5x)! -
+2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and
+Qwen1.5-110B-Chat model, use [this script](https://github.com/modelscope/swift/
+blob/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh)
+to start training! - 2024.04.24: Support for inference and fine-tuning of Phi3
+series models. Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/
 phi3_4b_4k_instruct/lora), phi3-4b-128k-instruct. - 2024.04.22: Support for
 inference, fine-tuning, and deployment of **chinese-llama-alpaca-2** series
 models. This includeschinese-llama-2-1.3b, chinese-llama-2-7b, chinese-
 llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-
 13b along with their corresponding 16k and 64k long text versions. -
 2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-
 Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-
@@ -402,15 +412,15 @@
 models-datasets.md). #### LLMs | Model Type | Model Introduction | Language |
 Model Size | Model Type | |------------------------------------------------|---
 ---------------------------------------------------------------------|---------
 -----------|----------------------------------------|--------------------------
 ----------------- | | Qwen
 Qwen1.5 | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM) |
 Chinese
-English | 0.5B-72B
+English | 0.5B-110B
 including quantized versions | base model
 chat model
 MoE model
 code model | | ChatGLM2
 ChatGLM3
 Codegeex2 | [Zhipu ChatGLM series models](https://github.com/THUDM) | Chinese
 English | 6B | base model
@@ -511,16 +521,18 @@
 series models](https://github.com/nlpxucan/WizardLM) | English | 7B-8x22B
 including quantized versions | chat model
 MoE model | | Atom | [Atom](https://github.com/LlamaFamily/Llama-Chinese) |
 Chinese | 7B| base model
 chat model| | Chinese-LLaMA-Alpaca-2 | [Chinese-LLaMA-Alpaca-2](https://
 github.com/ymcui/Chinese-LLaMA-Alpaca-2) | Chinese | 1.3B-13B| base model
 chat model
-long text model | | ModelScope-Agent | [ModelScope Agent series models](https:/
-/github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model | ####
+long text model | | Chinese-LLaMA-Alpaca-3 | [Chinese-LLaMA-Alpaca-3](https://
+github.com/ymcui/Chinese-LLaMA-Alpaca-3) | Chinese | 8B| base model
+chat model| | ModelScope-Agent | [ModelScope Agent series models](https://
+github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model | ####
 MLLMs | Model Type | Model Introduction | Language | Model Size | Model Type |
 |------------------|-----------------------------------------------------------
 -------------|--------------------|-------------------|------------------- | |
 Qwen-VL | [Tongyi Qwen vision model](https://github.com/QwenLM) | Chinese
 English | 7B
 including quantized versions | base model
 chat model | | Qwen-Audio | [Tongyi Qwen speech model](https://github.com/
@@ -535,18 +547,20 @@
 English | 1.3B-7B | chat model | | MiniCPM-V | [OpenBmB MiniCPM vision model]
 (https://github.com/OpenBMB/MiniCPM) | Chinese
 English | 3B | chat model | | CogVLM
 CogAgent | [Zhipu ChatGLM visual QA and Agent model](https://github.com/THUDM/
 ) | English | 17B-18B | chat model | | Llava | [Llava series models](https://
 github.com/haotian-liu/LLaVA) | English | 7B-34B | chat model | | mPLUG-Owl |
 [mPLUG-Owl series models](https://github.com/X-PLUG/mPLUG-Owl) | English | 11B
-| chat model | #### Diffusion Models | Model Type | Model Introduction |
-Language | Model Type | |---------------------|--------------------------------
---------------------------------------|----------|------------------ | |
-AnimateDiff | [AnimateDiff animation model](https://github.com/guoyww/
+| chat model | | InternVL | [InternVL](https://github.com/OpenGVLab/InternVL) |
+Chinese
+English | 25.5B | chat model | #### Diffusion Models | Model Type | Model
+Introduction | Language | Model Type | |---------------------|-----------------
+-----------------------------------------------------|----------|--------------
+---- | | AnimateDiff | [AnimateDiff animation model](https://github.com/guoyww/
 AnimateDiff) | English | text-to-video | | SD1.5/SD2.0/SDXL | [StabilityAI
 series diffusion models](https://github.com/Stability-AI) | English | text-to-
 image | ### Supported Open Source Datasets | Dataset Type | Training Task |
 Documentation | |--------------|:---------------|------------------------------
 --------------------------------- | | General | Fine-tuning | ruozhiba,
 ms-bench, ms-bench-mini, alpaca-en(gpt4), alpaca-zh(gpt4),
 multi-alpaca-all, instinwild-en, instinwild-zh, cot-en, cot-zh, firefly-all-zh,
@@ -567,68 +581,68 @@
 context-en. | | Text Generation | Fine-tuning | advertise-gen-zh,
 dureader-robust-zh. | | Classification | Fine-tuning | cmnli-zh, cmnli-
 mini-zh, jd-sentiment-zh, hc3-zh, hc3-en. | | Quantization Assist |
 Quantization | pileval. | | Other | Fine-tuning | finance-en, poetry-zh,
 webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh. | | Vision |
 Fine-tuning | coco-en, coco-mini-en, coco-mini-en-2, capcha-images. | |
 Audio | Fine-tuning | aishell1-zh, aishell1-mini-zh. | ### Supported
-Technologies | Technology Name | |---------------------------------------------
------------------- | | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE
+Technologies | Technology Name | | --------------------------------------------
+---------------- | | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE
 MODELS](https://arxiv.org/abs/2106.09685) | | LoRA+: [LoRA+: Efficient Low
 Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf) | |
-LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://
-arxiv.org/pdf/2401.02415.pdf) | | SCEdit: [SCEdit: Efficient and
-Controllable Image Diffusion Generation via Skip Connection Editing](https://
-arxiv.org/abs/2312.11392) < [arXiv](https://arxiv.org/abs/2312.11392) \|
-[Project Page](https://scedit.github.io/) > | | NEFTune: [Noisy Embeddings
-Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) | | QA-LoRA:
-[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://
-arxiv.org/abs/2309.14717) | | LongLoRA: [Efficient Fine-tuning of Long-Context
-Large Language Models](https://arxiv.org/abs/2309.12307) | | ROME: [Rank-One
-Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317) | |
-Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/
-1902.00751) | | Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/
-2203.12119) | | Side: [Side-Tuning: A Baseline for Network Adaptation via
-Additive Side Networks](https://arxiv.org/abs/1912.13503) | | Res-Tuning: [Res-
-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from
-Backbone](https://arxiv.org/abs/2310.19859) < [arXiv](https://arxiv.org/abs/
-2310.19859) \| [Project Page](https://res-tuning.github.io/) \| [Usage](docs/
-source/GetStarted/ResTuning.md) > | | Tuners provided by [PEFT](https://
-github.com/huggingface/peft), such as IA3, AdaLoRA, etc. | ### Supported
-Hardware | Hardware Environment | Notes | |--------------------------------|---
-----------------------------------------------| | CPU | | | RTX 20/30/40
-series, etc. | After 30 series, BF16 and FlashAttn can be used | | Computing
-cards T4/V100, etc. | BF16 and FlashAttn not supported | | Computing cards A10/
-A100, etc. | Support BF16 and FlashAttn | | Huawei Ascend NPU | | ## 
-Documentation ### Documentation Compiling ```shell make docs # Check docs/
-build/html/index.html in web-browser ``` ### User Guide | Document Name | | ---
---------------------------------------------------------- | | [Using Web-UI]
-(docs/source_en/GetStarted/Web-ui.md) | | [Using Tuners](docs/source_en/
-GetStarted/Tuners.md) | | [LLM Inference](docs/source_en/LLM/LLM-inference.md)
-| | [LLM Fine-tuning](docs/source_en/LLM/LLM-fine-tuning.md) | | [LLM
-Evaluation](docs/source_en/LLM/LLM-eval.md) | | [LLM Quantization](docs/
-source_en/LLM/LLM-quantization.md) | | [LLM Deployment](docs/source_en/LLM/
-VLLM-inference-acceleration-and-deployment.md) | | [DPO Human Alignment
-Training](docs/source_en/LLM/RLHF.md) | | [AnimateDiff Training](docs/
-source_en/AIGC/AnimateDiff-train-infer.md) | ### Reference Documentation |
-Document Name | | -----------------------------------------------------------
-- | | [Command Line Arguments](docs/source_en/LLM/Command-line-parameters.md) |
-| [Supported Models and Datasets List](docs/source_en/LLM/Supported-models-
-datasets.md) | | [Customizing New Models and Datasets](docs/source_en/LLM/
-Customization.md) | | [Runtime Speed and Memory Benchmark](docs/source_en/LLM/
-Benchmark.md) | ### Best Practices | Best Practices Name | | ------------------
------------------------------------------- | | [Agent Fine-Tuning Best
-Practice](docs/source_en/LLM/Agent-best-practice.md) | | [Self-Cognition Fine-
-Tuning Best Practice](docs/source_en/LLM/Self-cognition-best-practice.md) | |
-[Qwen1.5 Best Practice](docs/source_en/LLM/Qwen1.5-best-practice.md) | |
-[Multi-Modal Model Training Best Practice](docs/source_en/Multi-Modal/index.md)
-| | [NPU Best Practice](docs/source_en/LLM/NPU-best-practice.md) | ### Deep
-Learning Tutorials | Tutorial Name | |-----------------------------------------
---------------------- | | [Introduction to Deep Learning](https://github.com/
-modelscope/modelscope-classroom/blob/main/LLM-tutorial/
+GaLore:[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank
+Projection](https://arxiv.org/abs/2403.03507) | | LISA: [LISA: Layerwise
+Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning]
+(https://arxiv.org/abs/2403.17919) | | UnSloth: https://github.com/
+unslothai/unsloth | | LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block
+Expansion](https://arxiv.org/pdf/2401.02415.pdf) | | SCEdit: [SCEdit:
+Efficient and Controllable Image Diffusion Generation via Skip Connection
+Editing](https://arxiv.org/abs/2312.11392) < [arXiv](https://arxiv.org/abs/
+2312.11392) \ | | NEFTune: [Noisy Embeddings Improve Instruction
+Finetuning](https://arxiv.org/abs/2310.05914) | | LongLoRA: [Efficient Fine-
+tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
+| | Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/
+abs/1902.00751) | | Vision Prompt Tuning: [Visual Prompt Tuning](https://
+arxiv.org/abs/2203.12119) | | Side: [Side-Tuning: A Baseline for Network
+Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503) | |
+Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding
+Tuner from Backbone](https://arxiv.org/abs/2310.19859) < [arXiv](https://
+arxiv.org/abs/2310.19859) \ | | Tuners provided by [PEFT](https://github.com/
+huggingface/peft), such as IA3, AdaLoRA, etc. | ### Supported Hardware |
+Hardware Environment | Notes | |--------------------------------|--------------
+-----------------------------------| | CPU | | | RTX 20/30/40 series, etc. |
+After 30 series, BF16 and FlashAttn can be used | | Computing cards T4/V100,
+etc. | BF16 and FlashAttn not supported | | Computing cards A10/A100, etc. |
+Support BF16 and FlashAttn | | Huawei Ascend NPU | | ##  Documentation ###
+Documentation Compiling ```shell make docs # Check docs/build/html/index.html
+in web-browser ``` ### User Guide | Document Name | | -------------------------
+----------------------------------- | | [Using Web-UI](docs/source_en/
+GetStarted/Web-ui.md) | | [Using Tuners](docs/source_en/GetStarted/Tuners.md) |
+| [LLM Inference](docs/source_en/LLM/LLM-inference.md) | | [LLM Fine-tuning]
+(docs/source_en/LLM/LLM-fine-tuning.md) | | [LLM Evaluation](docs/source_en/
+LLM/LLM-eval.md) | | [LLM Quantization](docs/source_en/LLM/LLM-quantization.md)
+| | [LLM Deployment](docs/source_en/LLM/VLLM-inference-acceleration-and-
+deployment.md) | | [DPO Human Alignment Training](docs/source_en/LLM/RLHF.md) |
+| [AnimateDiff Training](docs/source_en/AIGC/AnimateDiff-train-infer.md) | ###
+Reference Documentation | Document Name | | -----------------------------------
+------------------------- | | [Command Line Arguments](docs/source_en/LLM/
+Command-line-parameters.md) | | [Supported Models and Datasets List](docs/
+source_en/LLM/Supported-models-datasets.md) | | [Customizing New Models and
+Datasets](docs/source_en/LLM/Customization.md) | | [Runtime Speed and Memory
+Benchmark](docs/source_en/LLM/Benchmark.md) | ### Best Practices | Best
+Practices Name | | -----------------------------------------------------------
+- | | [Agent Fine-Tuning Best Practice](docs/source_en/LLM/Agent-best-
+practice.md) | | [Self-Cognition Fine-Tuning Best Practice](docs/source_en/LLM/
+Self-cognition-best-practice.md) | | [Qwen1.5 Best Practice](docs/source_en/
+LLM/Qwen1.5-best-practice.md) | | [Multi-Modal Model Training Best Practice]
+(docs/source_en/Multi-Modal/index.md) | | [NPU Best Practice](docs/source_en/
+LLM/NPU-best-practice.md) | ### Deep Learning Tutorials | Tutorial Name | |----
+---------------------------------------------------------- | | [Introduction to
+Deep Learning](https://github.com/modelscope/modelscope-classroom/blob/main/
+LLM-tutorial/
 A.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md)
 | | [Large Model Basics](https://github.com/modelscope/modelscope-classroom/
 blob/main/LLM-tutorial/
 B.%E9%AD%94%E6%90%AD%E7%A4%BE%E5%8C%BA%E5%92%8CLLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.md)
 | | [Prompt Engineering](https://github.com/modelscope/modelscope-classroom/
 blob/main/LLM-tutorial/C.%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-
 prompt%20engineering.md) | | [Transformer Architecture Introduction](https://
```

### Comparing `ms-swift-2.0.3.post1/ms_swift.egg-info/PKG-INFO` & `ms-swift-2.0.4/ms_swift.egg-info/PKG-INFO`

 * *Files 1% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: ms-swift
-Version: 2.0.3.post1
+Version: 2.0.4
 Summary: Swift: Scalable lightWeight Infrastructure for Fine-Tuning
 Home-page: https://github.com/modelscope/swift
 Author: DAMO ModelScope teams
 Author-email: contact@modelscope.cn
 License: Apache License 2.0
 Description: # SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)
         
@@ -43,14 +43,17 @@
         SWIFT supports training, inference, evaluation and deployment of nearly **200 LLMs and MLLMs** (multimodal large models). Developers can directly apply our framework to their own research and production environments to realize the complete workflow from model training and evaluation to application. In addition to supporting the lightweight training solutions provided by [PEFT](https://github.com/huggingface/peft), we also provide a complete **Adapters library** to support the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This adapter library can be used directly in your own custom workflow without our training scripts.
         
         To facilitate use by users unfamiliar with deep learning, we provide a Gradio web-ui for controlling training and inference, as well as accompanying deep learning courses and best practices for beginners.
         
         Additionally, we are expanding capabilities for other modalities. Currently, we support full-parameter training and LoRA training for AnimateDiff.
         
         ##  News
+        - 2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model. For best practice, you can refer to [here](https://github.com/modelscope/swift/tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md).
+        - 2024.04.26: Support **LISA** and **unsloth** training! Specify `--lisa_activated_layers=2` to use LISA(to reduce the memory cost to 30 percent!), specify `--tuner_backend unsloth` to use unsloth to train a huge model(full or lora) with lesser memory(30 percent or lesser) and faster speed(5x)!
+        - 2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and Qwen1.5-110B-Chat model, use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh) to start training!
         - 2024.04.24: Support for inference and fine-tuning of Phi3 series models. Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/phi3_4b_4k_instruct/lora), phi3-4b-128k-instruct.
         - 2024.04.22: Support for inference, fine-tuning, and deployment of **chinese-llama-alpaca-2** series models. This includeschinese-llama-2-1.3b, chinese-llama-2-7b, chinese-llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-13b along with their corresponding 16k and 64k long text versions.
         - 2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-6b-128k, Openbuddy-Llama3.
         - 2024.04.20: Support for inference, fine-tuning, and deployment of **Atom** series models. This includes: Atom-7B and Atom-7B-Chat. use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/atom_7b_chat/lora/sft.sh) to train.
         - 2024.04.19: Support for single-card, DDP, ZeRO2, and ZeRO3 training and inference with NPU, please refer to [NPU Inference and Fine-tuning Best Practices](docs/source_en/LLM/NPU-best-practice.md).
         - 2024.04.19: Support for inference, fine-tuning, and deployment of **Llama3** series models. This includes: Llama-3-8B, Llama-3-8B-Instruct, Llama-3-70B, and Llama-3-70B-Instruct. use [this script](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/llama3_8b_instruct/lora/sft.sh) to train.
         - 2024.04.18: Supported models: wizardlm2-7b-awq, wizardlm2-8x22b, yi-6b-chat-awq, yi-6b-chat-int8, yi-34b-chat-awq, yi-34b-chat-int8. Supported `--deepspeed zero3-offload` and provided default zero3-offload configuration file for zero3+cpu offload usage.
@@ -444,15 +447,15 @@
         ### Supported Models
         The complete list of supported models and datasets can be found at [Supported Models and Datasets List](docs/source_en/LLM/Supported-models-datasets.md).
         
         #### LLMs
         
         | Model Type                                     | Model Introduction                                                     | Language           | Model Size                             | Model Type                                 |
         |------------------------------------------------|------------------------------------------------------------------------|--------------------|----------------------------------------|------------------------------------------- |
-        | Qwen<br>Qwen1.5                                   | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM)  | Chinese<br>English    | 0.5B-72B<br>including quantized versions | base model<br>chat model<br>MoE model<br>code model                      |
+        | Qwen<br>Qwen1.5                                   | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM)  | Chinese<br>English    | 0.5B-110B<br>including quantized versions | base model<br>chat model<br>MoE model<br>code model                      |
         | ChatGLM2<br>ChatGLM3<br>Codegeex2                    | [Zhipu ChatGLM series models](https://github.com/THUDM)               | Chinese<br>English    | 6B                                     | base model<br>chat model<br>code model<br>long text model  |
         | Baichuan/Baichuan2                             | [Baichuan 1 and Baichuan 2](https://github.com/baichuan-inc)           | Chinese<br>English    | 7B-13B<br>including quantized versions             | base model<br>chat model                       |
         | Yuan2                                          | [Langchao Yuan series models](https://github.com/IEIT-Yuan)             | Chinese<br>English    | 2B-102B                                | instruct model                                 |
         | XVerse                                         | [XVerse series models](https://github.com/xverse-ai)                    | Chinese<br>English    | 7B-65B                                 | base model<br>chat model<br>long text model<br>MoE model                |
         | LLaMA2                                         | [LLaMA2 series models](https://github.com/facebookresearch/llama)       | English            | 7B-70B<br>including quantized versions   | base model<br>chat model                       |
         | LLaMA3                   | [LLaMA3 series models](https://github.com/meta-llama/llama3)  | English       | 8B-70B<br>including quantized versions      | base model<br>chat model              |
         | Mistral<br>Mixtral                            | [Mistral series models](https://github.com/mistralai/mistral-src)       | English            | 7B-22B     | base model<br>instruct model<br>MoE model                     |
@@ -478,14 +481,15 @@
         | TeleChat | [Tele-AI](https://github.com/Tele-AI/Telechat) | Chinese<br>English | 7B-12B | chat model |
         | dbrx | [databricks](https://github.com/databricks/dbrx) | English | 132B | base model<br>chat model  |
         | mengzi3 | [Langboat](https://github.com/Langboat/Mengzi3) | Chinese<br>English | 13B | base model  |
         | c4ai-command-r | [c4ai](https://cohere.com/command) | Multilingual | 35B-104B | chat model  |
         | WizardLM2 | [WizardLM2 series models](https://github.com/nlpxucan/WizardLM) | English | 7B-8x22B<br>including quantized versions | chat model<br>MoE model |
         | Atom | [Atom](https://github.com/LlamaFamily/Llama-Chinese) | Chinese | 7B| base model<br>chat model|
         | Chinese-LLaMA-Alpaca-2 | [Chinese-LLaMA-Alpaca-2](https://github.com/ymcui/Chinese-LLaMA-Alpaca-2) | Chinese | 1.3B-13B| base model<br>chat model<br>long text model |
+        | Chinese-LLaMA-Alpaca-3 | [Chinese-LLaMA-Alpaca-3](https://github.com/ymcui/Chinese-LLaMA-Alpaca-3) | Chinese | 8B| base model<br>chat model|
         | ModelScope-Agent | [ModelScope Agent series models](https://github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model |
         
         #### MLLMs
         
         | Model Type       | Model Introduction                                                     | Language           | Model Size        | Model Type         |
         |------------------|------------------------------------------------------------------------|--------------------|-------------------|------------------- |
         | Qwen-VL          | [Tongyi Qwen vision model](https://github.com/QwenLM)               | Chinese<br>English    | 7B<br>including quantized versions | base model<br>chat model |
@@ -493,14 +497,15 @@
         | YI-VL            | [01AI's YI series vision models](https://github.com/01-ai)             | Chinese<br>English    | 6B-34B            | chat model         |
         | XComposer2       | [Pujiang AI Lab InternLM vision model](https://github.com/InternLM/InternLM) | Chinese<br>English | 7B              | chat model         |
         | DeepSeek-VL      | [DeepSeek series vision models](https://github.com/deepseek-ai) | Chinese<br>English    | 1.3B-7B           | chat model         |
         | MiniCPM-V       | [OpenBmB MiniCPM vision model](https://github.com/OpenBMB/MiniCPM)     | Chinese<br>English    | 3B                | chat model         |
         | CogVLM<br>CogAgent  | [Zhipu ChatGLM visual QA and Agent model](https://github.com/THUDM/)   | English    | 17B-18B           | chat model         |
         | Llava      | [Llava series models](https://github.com/haotian-liu/LLaVA)                | English | 7B-34B               | chat model |
         | mPLUG-Owl      | [mPLUG-Owl series models](https://github.com/X-PLUG/mPLUG-Owl)         | English | 11B               | chat model |
+        | InternVL         | [InternVL](https://github.com/OpenGVLab/InternVL)                | Chinese<br>English | 25.5B | chat model |
         
         #### Diffusion Models
         
         | Model Type          | Model Introduction                                                    | Language | Model Type        |
         |---------------------|----------------------------------------------------------------------|----------|------------------ |
         | AnimateDiff         | [AnimateDiff animation model](https://github.com/guoyww/AnimateDiff) | English  | text-to-video     |
         | SD1.5/SD2.0/SDXL    | [StabilityAI series diffusion models](https://github.com/Stability-AI) | English | text-to-image    |
@@ -522,28 +527,29 @@
         | Quantization Assist | Quantization | pileval.                                                                                                                                                                                                                                                                                                             |
         | Other        | Fine-tuning    | finance-en, poetry-zh, webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh.                                                                                                                                                                                                                               |
         | Vision       | Fine-tuning    | coco-en, coco-mini-en, coco-mini-en-2, capcha-images.                                                                                                                                                                                                                                                              |
         | Audio        | Fine-tuning    | aishell1-zh, aishell1-mini-zh.                                                                                                                                                                                                                                                                                     |
         
         ### Supported Technologies
         
-        | Technology Name                                               |
-        |--------------------------------------------------------------- |
+        | Technology Name                                              |
+        | ------------------------------------------------------------ |
         | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685) |
         | LoRA+: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf) |
+        | GaLore:[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection](https://arxiv.org/abs/2403.03507) |
+        | LISA: [LISA: Layerwise Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning](https://arxiv.org/abs/2403.17919) |
+        | UnSloth: https://github.com/unslothai/unsloth               |
         | LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://arxiv.org/pdf/2401.02415.pdf) |
-        | SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  \|  [Project Page](https://scedit.github.io/) > |
+        | SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  \ |
         | NEFTune: [Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) |
-        | QA-LoRA:[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717) |
         | LongLoRA: [Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307) |
-        | ROME: [Rank-One Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317) |
         | Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751) |
-        | Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) |
+        | Vision Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119) |
         | Side: [Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503) |
-        | Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  \|  [Project Page](https://res-tuning.github.io/)  \|  [Usage](docs/source/GetStarted/ResTuning.md) > |
+        | Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  \ |
         | Tuners provided by [PEFT](https://github.com/huggingface/peft), such as IA3, AdaLoRA, etc. |
         
         ### Supported Hardware
         
         | Hardware Environment           | Notes                                           |
         |--------------------------------|-------------------------------------------------|
         | CPU                            |                                                 |
```

#### html2text {}

```diff
@@ -1,10 +1,10 @@
-Metadata-Version: 2.1 Name: ms-swift Version: 2.0.3.post1 Summary: Swift:
-Scalable lightWeight Infrastructure for Fine-Tuning Home-page: https://
-github.com/modelscope/swift Author: DAMO ModelScope teams Author-email:
+Metadata-Version: 2.1 Name: ms-swift Version: 2.0.4 Summary: Swift: Scalable
+lightWeight Infrastructure for Fine-Tuning Home-page: https://github.com/
+modelscope/swift Author: DAMO ModelScope teams Author-email:
 contact@modelscope.cn License: Apache License 2.0 Description: # SWIFT
 (Scalable lightWeight Infrastructure for Fine-Tuning)
 
                             [resources/banner.png]
                          _M_o_d_e_l_S_c_o_p_e_ _C_o_m_m_u_n_i_t_y_ _W_e_b_s_i_t_e
                            ______    English 
       [https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg][https://
@@ -26,16 +26,26 @@
 the latest training techniques such as NEFTune, LoRA+, LLaMA-PRO, etc. This
 adapter library can be used directly in your own custom workflow without our
 training scripts. To facilitate use by users unfamiliar with deep learning, we
 provide a Gradio web-ui for controlling training and inference, as well as
 accompanying deep learning courses and best practices for beginners.
 Additionally, we are expanding capabilities for other modalities. Currently, we
 support full-parameter training and LoRA training for AnimateDiff. ##  News
-- 2024.04.24: Support for inference and fine-tuning of Phi3 series models.
-Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/
+- 2024.04.29: Supports inference and fine-tuning of InternVL-Chat-V1.5 model.
+For best practice, you can refer to [here](https://github.com/modelscope/swift/
+tree/main/docs/source_en/Multi-Modal/internvl-best-practice.md). -
+2024.04.26: Support **LISA** and **unsloth** training! Specify `--
+lisa_activated_layers=2` to use LISA(to reduce the memory cost to 30 percent!),
+specify `--tuner_backend unsloth` to use unsloth to train a huge model(full or
+lora) with lesser memory(30 percent or lesser) and faster speed(5x)! -
+2024.04.26: Support the fine-tuning and inference of Qwen1.5-110B and
+Qwen1.5-110B-Chat model, use [this script](https://github.com/modelscope/swift/
+blob/main/examples/pytorch/llm/scripts/qwen1half_110b_chat/lora_ddp_ds/sft.sh)
+to start training! - 2024.04.24: Support for inference and fine-tuning of Phi3
+series models. Including: [phi3-4b-4k-instruct](examples/pytorch/llm/scripts/
 phi3_4b_4k_instruct/lora), phi3-4b-128k-instruct. - 2024.04.22: Support for
 inference, fine-tuning, and deployment of **chinese-llama-alpaca-2** series
 models. This includeschinese-llama-2-1.3b, chinese-llama-2-7b, chinese-
 llama-2-13b, chinese-alpaca-2-1.3b, chinese-alpaca-2-7b and chinese-alpaca-2-
 13b along with their corresponding 16k and 64k long text versions. -
 2024.04.22: Support for inference and fine-tuning of Llama3 GPTQ-Int4, GPTQ-
 Int8, and AWQ series models. Support for inference and fine-tuning of chatglm3-
@@ -406,15 +416,15 @@
 models-datasets.md). #### LLMs | Model Type | Model Introduction | Language |
 Model Size | Model Type | |------------------------------------------------|---
 ---------------------------------------------------------------------|---------
 -----------|----------------------------------------|--------------------------
 ----------------- | | Qwen
 Qwen1.5 | [Tongyi Qwen 1.0 and 1.5 series models](https://github.com/QwenLM) |
 Chinese
-English | 0.5B-72B
+English | 0.5B-110B
 including quantized versions | base model
 chat model
 MoE model
 code model | | ChatGLM2
 ChatGLM3
 Codegeex2 | [Zhipu ChatGLM series models](https://github.com/THUDM) | Chinese
 English | 6B | base model
@@ -515,16 +525,18 @@
 series models](https://github.com/nlpxucan/WizardLM) | English | 7B-8x22B
 including quantized versions | chat model
 MoE model | | Atom | [Atom](https://github.com/LlamaFamily/Llama-Chinese) |
 Chinese | 7B| base model
 chat model| | Chinese-LLaMA-Alpaca-2 | [Chinese-LLaMA-Alpaca-2](https://
 github.com/ymcui/Chinese-LLaMA-Alpaca-2) | Chinese | 1.3B-13B| base model
 chat model
-long text model | | ModelScope-Agent | [ModelScope Agent series models](https:/
-/github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model | ####
+long text model | | Chinese-LLaMA-Alpaca-3 | [Chinese-LLaMA-Alpaca-3](https://
+github.com/ymcui/Chinese-LLaMA-Alpaca-3) | Chinese | 8B| base model
+chat model| | ModelScope-Agent | [ModelScope Agent series models](https://
+github.com/modelscope/modelscope-agent) | Chinese | 7B-14B| agent model | ####
 MLLMs | Model Type | Model Introduction | Language | Model Size | Model Type |
 |------------------|-----------------------------------------------------------
 -------------|--------------------|-------------------|------------------- | |
 Qwen-VL | [Tongyi Qwen vision model](https://github.com/QwenLM) | Chinese
 English | 7B
 including quantized versions | base model
 chat model | | Qwen-Audio | [Tongyi Qwen speech model](https://github.com/
@@ -539,18 +551,20 @@
 English | 1.3B-7B | chat model | | MiniCPM-V | [OpenBmB MiniCPM vision model]
 (https://github.com/OpenBMB/MiniCPM) | Chinese
 English | 3B | chat model | | CogVLM
 CogAgent | [Zhipu ChatGLM visual QA and Agent model](https://github.com/THUDM/
 ) | English | 17B-18B | chat model | | Llava | [Llava series models](https://
 github.com/haotian-liu/LLaVA) | English | 7B-34B | chat model | | mPLUG-Owl |
 [mPLUG-Owl series models](https://github.com/X-PLUG/mPLUG-Owl) | English | 11B
-| chat model | #### Diffusion Models | Model Type | Model Introduction |
-Language | Model Type | |---------------------|--------------------------------
---------------------------------------|----------|------------------ | |
-AnimateDiff | [AnimateDiff animation model](https://github.com/guoyww/
+| chat model | | InternVL | [InternVL](https://github.com/OpenGVLab/InternVL) |
+Chinese
+English | 25.5B | chat model | #### Diffusion Models | Model Type | Model
+Introduction | Language | Model Type | |---------------------|-----------------
+-----------------------------------------------------|----------|--------------
+---- | | AnimateDiff | [AnimateDiff animation model](https://github.com/guoyww/
 AnimateDiff) | English | text-to-video | | SD1.5/SD2.0/SDXL | [StabilityAI
 series diffusion models](https://github.com/Stability-AI) | English | text-to-
 image | ### Supported Open Source Datasets | Dataset Type | Training Task |
 Documentation | |--------------|:---------------|------------------------------
 --------------------------------- | | General | Fine-tuning | ruozhiba,
 ms-bench, ms-bench-mini, alpaca-en(gpt4), alpaca-zh(gpt4),
 multi-alpaca-all, instinwild-en, instinwild-zh, cot-en, cot-zh, firefly-all-zh,
@@ -571,68 +585,68 @@
 context-en. | | Text Generation | Fine-tuning | advertise-gen-zh,
 dureader-robust-zh. | | Classification | Fine-tuning | cmnli-zh, cmnli-
 mini-zh, jd-sentiment-zh, hc3-zh, hc3-en. | | Quantization Assist |
 Quantization | pileval. | | Other | Fine-tuning | finance-en, poetry-zh,
 webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh. | | Vision |
 Fine-tuning | coco-en, coco-mini-en, coco-mini-en-2, capcha-images. | |
 Audio | Fine-tuning | aishell1-zh, aishell1-mini-zh. | ### Supported
-Technologies | Technology Name | |---------------------------------------------
------------------- | | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE
+Technologies | Technology Name | | --------------------------------------------
+---------------- | | LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE
 MODELS](https://arxiv.org/abs/2106.09685) | | LoRA+: [LoRA+: Efficient Low
 Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf) | |
-LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://
-arxiv.org/pdf/2401.02415.pdf) | | SCEdit: [SCEdit: Efficient and
-Controllable Image Diffusion Generation via Skip Connection Editing](https://
-arxiv.org/abs/2312.11392) < [arXiv](https://arxiv.org/abs/2312.11392) \|
-[Project Page](https://scedit.github.io/) > | | NEFTune: [Noisy Embeddings
-Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914) | | QA-LoRA:
-[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://
-arxiv.org/abs/2309.14717) | | LongLoRA: [Efficient Fine-tuning of Long-Context
-Large Language Models](https://arxiv.org/abs/2309.12307) | | ROME: [Rank-One
-Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317) | |
-Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/
-1902.00751) | | Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/
-2203.12119) | | Side: [Side-Tuning: A Baseline for Network Adaptation via
-Additive Side Networks](https://arxiv.org/abs/1912.13503) | | Res-Tuning: [Res-
-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from
-Backbone](https://arxiv.org/abs/2310.19859) < [arXiv](https://arxiv.org/abs/
-2310.19859) \| [Project Page](https://res-tuning.github.io/) \| [Usage](docs/
-source/GetStarted/ResTuning.md) > | | Tuners provided by [PEFT](https://
-github.com/huggingface/peft), such as IA3, AdaLoRA, etc. | ### Supported
-Hardware | Hardware Environment | Notes | |--------------------------------|---
-----------------------------------------------| | CPU | | | RTX 20/30/40
-series, etc. | After 30 series, BF16 and FlashAttn can be used | | Computing
-cards T4/V100, etc. | BF16 and FlashAttn not supported | | Computing cards A10/
-A100, etc. | Support BF16 and FlashAttn | | Huawei Ascend NPU | | ## 
-Documentation ### Documentation Compiling ```shell make docs # Check docs/
-build/html/index.html in web-browser ``` ### User Guide | Document Name | | ---
---------------------------------------------------------- | | [Using Web-UI]
-(docs/source_en/GetStarted/Web-ui.md) | | [Using Tuners](docs/source_en/
-GetStarted/Tuners.md) | | [LLM Inference](docs/source_en/LLM/LLM-inference.md)
-| | [LLM Fine-tuning](docs/source_en/LLM/LLM-fine-tuning.md) | | [LLM
-Evaluation](docs/source_en/LLM/LLM-eval.md) | | [LLM Quantization](docs/
-source_en/LLM/LLM-quantization.md) | | [LLM Deployment](docs/source_en/LLM/
-VLLM-inference-acceleration-and-deployment.md) | | [DPO Human Alignment
-Training](docs/source_en/LLM/RLHF.md) | | [AnimateDiff Training](docs/
-source_en/AIGC/AnimateDiff-train-infer.md) | ### Reference Documentation |
-Document Name | | -----------------------------------------------------------
-- | | [Command Line Arguments](docs/source_en/LLM/Command-line-parameters.md) |
-| [Supported Models and Datasets List](docs/source_en/LLM/Supported-models-
-datasets.md) | | [Customizing New Models and Datasets](docs/source_en/LLM/
-Customization.md) | | [Runtime Speed and Memory Benchmark](docs/source_en/LLM/
-Benchmark.md) | ### Best Practices | Best Practices Name | | ------------------
------------------------------------------- | | [Agent Fine-Tuning Best
-Practice](docs/source_en/LLM/Agent-best-practice.md) | | [Self-Cognition Fine-
-Tuning Best Practice](docs/source_en/LLM/Self-cognition-best-practice.md) | |
-[Qwen1.5 Best Practice](docs/source_en/LLM/Qwen1.5-best-practice.md) | |
-[Multi-Modal Model Training Best Practice](docs/source_en/Multi-Modal/index.md)
-| | [NPU Best Practice](docs/source_en/LLM/NPU-best-practice.md) | ### Deep
-Learning Tutorials | Tutorial Name | |-----------------------------------------
---------------------- | | [Introduction to Deep Learning](https://github.com/
-modelscope/modelscope-classroom/blob/main/LLM-tutorial/
+GaLore:[GaLore: Memory-Efficient LLM Training by Gradient Low-Rank
+Projection](https://arxiv.org/abs/2403.03507) | | LISA: [LISA: Layerwise
+Importance Sampling for Memory-Efficient Large Language Model Fine-Tuning]
+(https://arxiv.org/abs/2403.17919) | | UnSloth: https://github.com/
+unslothai/unsloth | | LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block
+Expansion](https://arxiv.org/pdf/2401.02415.pdf) | | SCEdit: [SCEdit:
+Efficient and Controllable Image Diffusion Generation via Skip Connection
+Editing](https://arxiv.org/abs/2312.11392) < [arXiv](https://arxiv.org/abs/
+2312.11392) \ | | NEFTune: [Noisy Embeddings Improve Instruction
+Finetuning](https://arxiv.org/abs/2310.05914) | | LongLoRA: [Efficient Fine-
+tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
+| | Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/
+abs/1902.00751) | | Vision Prompt Tuning: [Visual Prompt Tuning](https://
+arxiv.org/abs/2203.12119) | | Side: [Side-Tuning: A Baseline for Network
+Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503) | |
+Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding
+Tuner from Backbone](https://arxiv.org/abs/2310.19859) < [arXiv](https://
+arxiv.org/abs/2310.19859) \ | | Tuners provided by [PEFT](https://github.com/
+huggingface/peft), such as IA3, AdaLoRA, etc. | ### Supported Hardware |
+Hardware Environment | Notes | |--------------------------------|--------------
+-----------------------------------| | CPU | | | RTX 20/30/40 series, etc. |
+After 30 series, BF16 and FlashAttn can be used | | Computing cards T4/V100,
+etc. | BF16 and FlashAttn not supported | | Computing cards A10/A100, etc. |
+Support BF16 and FlashAttn | | Huawei Ascend NPU | | ##  Documentation ###
+Documentation Compiling ```shell make docs # Check docs/build/html/index.html
+in web-browser ``` ### User Guide | Document Name | | -------------------------
+----------------------------------- | | [Using Web-UI](docs/source_en/
+GetStarted/Web-ui.md) | | [Using Tuners](docs/source_en/GetStarted/Tuners.md) |
+| [LLM Inference](docs/source_en/LLM/LLM-inference.md) | | [LLM Fine-tuning]
+(docs/source_en/LLM/LLM-fine-tuning.md) | | [LLM Evaluation](docs/source_en/
+LLM/LLM-eval.md) | | [LLM Quantization](docs/source_en/LLM/LLM-quantization.md)
+| | [LLM Deployment](docs/source_en/LLM/VLLM-inference-acceleration-and-
+deployment.md) | | [DPO Human Alignment Training](docs/source_en/LLM/RLHF.md) |
+| [AnimateDiff Training](docs/source_en/AIGC/AnimateDiff-train-infer.md) | ###
+Reference Documentation | Document Name | | -----------------------------------
+------------------------- | | [Command Line Arguments](docs/source_en/LLM/
+Command-line-parameters.md) | | [Supported Models and Datasets List](docs/
+source_en/LLM/Supported-models-datasets.md) | | [Customizing New Models and
+Datasets](docs/source_en/LLM/Customization.md) | | [Runtime Speed and Memory
+Benchmark](docs/source_en/LLM/Benchmark.md) | ### Best Practices | Best
+Practices Name | | -----------------------------------------------------------
+- | | [Agent Fine-Tuning Best Practice](docs/source_en/LLM/Agent-best-
+practice.md) | | [Self-Cognition Fine-Tuning Best Practice](docs/source_en/LLM/
+Self-cognition-best-practice.md) | | [Qwen1.5 Best Practice](docs/source_en/
+LLM/Qwen1.5-best-practice.md) | | [Multi-Modal Model Training Best Practice]
+(docs/source_en/Multi-Modal/index.md) | | [NPU Best Practice](docs/source_en/
+LLM/NPU-best-practice.md) | ### Deep Learning Tutorials | Tutorial Name | |----
+---------------------------------------------------------- | | [Introduction to
+Deep Learning](https://github.com/modelscope/modelscope-classroom/blob/main/
+LLM-tutorial/
 A.%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E4%BB%8B%E7%BB%8D.md)
 | | [Large Model Basics](https://github.com/modelscope/modelscope-classroom/
 blob/main/LLM-tutorial/
 B.%E9%AD%94%E6%90%AD%E7%A4%BE%E5%8C%BA%E5%92%8CLLM%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86.md)
 | | [Prompt Engineering](https://github.com/modelscope/modelscope-classroom/
 blob/main/LLM-tutorial/C.%E6%8F%90%E7%A4%BA%E8%AF%8D%E5%B7%A5%E7%A8%8B-
 prompt%20engineering.md) | | [Transformer Architecture Introduction](https://
```

### Comparing `ms-swift-2.0.3.post1/ms_swift.egg-info/SOURCES.txt` & `ms-swift-2.0.4/ms_swift.egg-info/SOURCES.txt`

 * *Files 0% similar despite different names*

```diff
@@ -88,14 +88,15 @@
 swift/llm/utils/client_utils.py
 swift/llm/utils/dataset.py
 swift/llm/utils/model.py
 swift/llm/utils/preprocess.py
 swift/llm/utils/protocol.py
 swift/llm/utils/template.py
 swift/llm/utils/utils.py
+swift/llm/utils/vision_utils.py
 swift/llm/utils/vllm_utils.py
 swift/trainers/__init__.py
 swift/trainers/arguments.py
 swift/trainers/callback.py
 swift/trainers/dpo_trainers.py
 swift/trainers/mixin.py
 swift/trainers/trainers.py
```

### Comparing `ms-swift-2.0.3.post1/ms_swift.egg-info/requires.txt` & `ms-swift-2.0.4/ms_swift.egg-info/requires.txt`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/setup.py` & `ms-swift-2.0.4/setup.py`

 * *Files 2% similar despite different names*

```diff
@@ -63,31 +63,29 @@
 
                 info['package'] = parts[0]
                 if len(parts) > 1:
                     op, rest = parts[1:]
                     if ';' in rest:
                         # Handle platform specific dependencies
                         # http://setuptools.readthedocs.io/en/latest/setuptools.html#declaring-platform-specific-dependencies
-                        version, platform_deps = map(str.strip,
-                                                     rest.split(';'))
+                        version, platform_deps = map(str.strip, rest.split(';'))
                         info['platform_deps'] = platform_deps
                     else:
                         version = rest  # NOQA
                     info['version'] = (op, version)
             yield info
 
     def parse_require_file(fpath):
         with open(fpath, 'r', encoding='utf-8') as f:
             for line in f.readlines():
                 line = line.strip()
                 if line.startswith('http'):
                     print('skip http requirements %s' % line)
                     continue
-                if line and not line.startswith('#') and not line.startswith(
-                        '--'):
+                if line and not line.startswith('#') and not line.startswith('--'):
                     for info in parse_line(line):
                         yield info
                 elif line and line.startswith('--find-links'):
                     eles = line.split()
                     for e in eles:
                         e = e.strip()
                         if 'http' in e:
@@ -129,16 +127,15 @@
     all_requires.extend(extra_requires['aigc'])
     all_requires.extend(extra_requires['eval'])
     extra_requires['all'] = all_requires
 
     setup(
         name='ms-swift',
         version=get_version(),
-        description=
-        'Swift: Scalable lightWeight Infrastructure for Fine-Tuning',
+        description='Swift: Scalable lightWeight Infrastructure for Fine-Tuning',
         long_description=readme(),
         long_description_content_type='text/markdown',
         author='DAMO ModelScope teams',
         author_email='contact@modelscope.cn',
         keywords='python, petl, efficient tuners',
         url='https://github.com/modelscope/swift',
         packages=find_packages(exclude=('configs', 'demo')),
```

### Comparing `ms-swift-2.0.3.post1/swift/__init__.py` & `ms-swift-2.0.4/swift/__init__.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,59 +1,43 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import TYPE_CHECKING
 
 from .utils.import_utils import _LazyModule
 
 if TYPE_CHECKING:
     from .version import __version__, __release_datetime__
-    from .tuners import (
-        Adapter, AdapterConfig, AdapterModule, SwiftModel, LoRA, LoRAConfig,
-        SWIFT_MAPPING, AdaLoraConfig, IA3Config, LoftQConfig, LoHaConfig,
-        LoKrConfig, LoraConfig, OFTConfig, PeftConfig, PeftModel,
-        PeftModelForCausalLM, ResTuningConfig, SideConfig,
-        PeftModelForSeq2SeqLM, PeftModelForSequenceClassification,
-        PeftModelForTokenClassification, PrefixTuningConfig,
-        PromptEncoderConfig, PromptLearningConfig, PromptTuningConfig,
-        get_peft_config, get_peft_model, get_peft_model_state_dict, Prompt,
-        PromptConfig, PromptModule, SwiftConfig, SwiftOutput, Swift,
-        SwiftTuners, LongLoRAConfig, LongLoRA, LongLoRAModelType, SCETuning,
-        SCETuningConfig)
+    from .tuners import (Adapter, AdapterConfig, AdapterModule, SwiftModel, LoRA, LoRAConfig, SWIFT_MAPPING,
+                         AdaLoraConfig, IA3Config, LoftQConfig, LoHaConfig, LoKrConfig, LoraConfig, OFTConfig,
+                         PeftConfig, PeftModel, PeftModelForCausalLM, ResTuningConfig, SideConfig,
+                         PeftModelForSeq2SeqLM, PeftModelForSequenceClassification, PeftModelForTokenClassification,
+                         PrefixTuningConfig, PromptEncoderConfig, PromptLearningConfig, PromptTuningConfig,
+                         get_peft_config, get_peft_model, get_peft_model_state_dict, Prompt, PromptConfig, PromptModule,
+                         SwiftConfig, SwiftOutput, Swift, SwiftTuners, LongLoRAConfig, LongLoRA, LongLoRAModelType,
+                         SCETuning, SCETuningConfig)
     from .hub import snapshot_download, push_to_hub, push_to_hub_async, push_to_hub_in_queue
-    from .trainers import (EvaluationStrategy, FSDPOption, HPSearchBackend,
-                           HubStrategy, IntervalStrategy, SchedulerType,
-                           ShardedDDPOption, TrainingArguments,
-                           Seq2SeqTrainingArguments, Trainer, Seq2SeqTrainer)
+    from .trainers import (EvaluationStrategy, FSDPOption, HPSearchBackend, HubStrategy, IntervalStrategy,
+                           SchedulerType, ShardedDDPOption, TrainingArguments, Seq2SeqTrainingArguments, Trainer,
+                           Seq2SeqTrainer)
     from .utils import get_logger
 else:
     _import_structure = {
         'version': ['__release_datetime__', '__version__'],
-        'hub': [
-            'snapshot_download', 'push_to_hub', 'push_to_hub_async',
-            'push_to_hub_in_queue'
-        ],
+        'hub': ['snapshot_download', 'push_to_hub', 'push_to_hub_async', 'push_to_hub_in_queue'],
         'tuners': [
-            'Adapter', 'AdapterConfig', 'AdapterModule', 'SwiftModel', 'LoRA',
-            'LoRAConfig', 'SWIFT_MAPPING', 'LoraConfig', 'AdaLoraConfig',
-            'IA3Config', 'LoftQConfig', 'LoHaConfig', 'LoKrConfig',
-            'OFTConfig', 'PeftConfig', 'ResTuningConfig', 'SideConfig',
-            'PeftModel', 'PeftModelForCausalLM', 'PeftModelForSeq2SeqLM',
-            'PeftModelForSequenceClassification',
-            'PeftModelForTokenClassification', 'PrefixTuningConfig',
-            'PromptEncoderConfig', 'PromptLearningConfig',
-            'PromptTuningConfig', 'get_peft_config', 'get_peft_model',
-            'get_peft_model_state_dict', 'Prompt', 'PromptConfig',
-            'PromptModule', 'SwiftConfig', 'SwiftOutput', 'Swift',
-            'SwiftTuners', 'LongLoRAConfig', 'LongLoRA', 'LongLoRAModelType',
-            'SCETuning', 'SCETuningConfig'
+            'Adapter', 'AdapterConfig', 'AdapterModule', 'SwiftModel', 'LoRA', 'LoRAConfig', 'SWIFT_MAPPING',
+            'LoraConfig', 'AdaLoraConfig', 'IA3Config', 'LoftQConfig', 'LoHaConfig', 'LoKrConfig', 'OFTConfig',
+            'PeftConfig', 'ResTuningConfig', 'SideConfig', 'PeftModel', 'PeftModelForCausalLM', 'PeftModelForSeq2SeqLM',
+            'PeftModelForSequenceClassification', 'PeftModelForTokenClassification', 'PrefixTuningConfig',
+            'PromptEncoderConfig', 'PromptLearningConfig', 'PromptTuningConfig', 'get_peft_config', 'get_peft_model',
+            'get_peft_model_state_dict', 'Prompt', 'PromptConfig', 'PromptModule', 'SwiftConfig', 'SwiftOutput',
+            'Swift', 'SwiftTuners', 'LongLoRAConfig', 'LongLoRA', 'LongLoRAModelType', 'SCETuning', 'SCETuningConfig'
         ],
         'trainers': [
-            'EvaluationStrategy', 'FSDPOption', 'HPSearchBackend',
-            'HubStrategy', 'IntervalStrategy', 'SchedulerType',
-            'ShardedDDPOption', 'TrainingArguments',
-            'Seq2SeqTrainingArguments', 'Trainer', 'Seq2SeqTrainer'
+            'EvaluationStrategy', 'FSDPOption', 'HPSearchBackend', 'HubStrategy', 'IntervalStrategy', 'SchedulerType',
+            'ShardedDDPOption', 'TrainingArguments', 'Seq2SeqTrainingArguments', 'Trainer', 'Seq2SeqTrainer'
         ],
         'utils': ['get_logger']
     }
 
     import sys
 
     sys.modules[__name__] = _LazyModule(
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/__init__.py` & `ms-swift-2.0.4/swift/aigc/__init__.py`

 * *Files 4% similar despite different names*

```diff
@@ -14,22 +14,19 @@
         infer_dreambooth, infer_dreambooth_lora, infer_dreambooth_lora_sdxl
     from .utils import AnimateDiffArguments, AnimateDiffInferArguments
 else:
     _import_structure = {
         'animatediff': ['animatediff_sft', 'animatediff_main'],
         'animatediff_infer': ['animatediff_infer', 'animatediff_infer_main'],
         'diffusers': [
-            'train_text_to_image', 'train_text_to_image_lora',
-            'train_text_to_image_lora_sdxl', 'train_text_to_image_sdxl',
-            'infer_text_to_image', 'infer_text_to_image_lora',
-            'infer_text_to_image_sdxl', 'infer_text_to_image_lora_sdxl',
-            'train_controlnet', 'train_controlnet_sdxl', 'train_dreambooth',
-            'train_dreambooth_lora', 'train_dreambooth_lora_sdxl',
-            'infer_controlnet', 'infer_controlnet_sdxl', 'infer_dreambooth',
-            'infer_dreambooth_lora', 'infer_dreambooth_lora_sdxl'
+            'train_text_to_image', 'train_text_to_image_lora', 'train_text_to_image_lora_sdxl',
+            'train_text_to_image_sdxl', 'infer_text_to_image', 'infer_text_to_image_lora', 'infer_text_to_image_sdxl',
+            'infer_text_to_image_lora_sdxl', 'train_controlnet', 'train_controlnet_sdxl', 'train_dreambooth',
+            'train_dreambooth_lora', 'train_dreambooth_lora_sdxl', 'infer_controlnet', 'infer_controlnet_sdxl',
+            'infer_dreambooth', 'infer_dreambooth_lora', 'infer_dreambooth_lora_sdxl'
         ],
         'utils': ['AnimateDiffArguments', 'AnimateDiffInferArguments'],
     }
 
     import sys
 
     sys.modules[__name__] = _LazyModule(
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/animatediff.py` & `ms-swift-2.0.4/swift/aigc/animatediff.py`

 * *Files 6% similar despite different names*

```diff
@@ -12,16 +12,15 @@
 import numpy as np
 import torch
 import torch.distributed as dist
 import torch.nn.functional as F
 import torchvision
 import torchvision.transforms as transforms
 from decord import VideoReader
-from diffusers import (AutoencoderKL, DDIMScheduler, MotionAdapter,
-                       UNet2DConditionModel, UNetMotionModel)
+from diffusers import AutoencoderKL, DDIMScheduler, MotionAdapter, UNet2DConditionModel, UNetMotionModel
 from diffusers.optimization import get_scheduler
 from diffusers.pipelines import AnimateDiffPipeline
 from diffusers.utils import export_to_gif
 from diffusers.utils.import_utils import is_xformers_available
 from einops import rearrange
 from modelscope import snapshot_download
 from torch.nn.parallel import DistributedDataParallel as DDP
@@ -58,58 +57,48 @@
             self.dataset = list(csv.DictReader(csvfile))
         dataset = []
         for d in tqdm(self.dataset):
             content_url = d[self.CONTENT_URL]
             file_name = content_url.split('/')[-1]
             if os.path.isfile(os.path.join(video_folder, file_name)):
                 dataset.append(d)
-            if dataset_sample_size is not None and len(
-                    dataset) > dataset_sample_size:
+            if dataset_sample_size is not None and len(dataset) > dataset_sample_size:
                 break
 
         self.dataset = dataset
         self.length = len(self.dataset)
         print(f'data scale: {self.length}')
 
         self.video_folder = video_folder
         self.sample_stride = sample_stride
         self.sample_n_frames = sample_n_frames
 
-        sample_size = tuple(sample_size) if not isinstance(
-            sample_size, int) else (sample_size, sample_size)
+        sample_size = tuple(sample_size) if not isinstance(sample_size, int) else (sample_size, sample_size)
         self.pixel_transforms = transforms.Compose([
             transforms.RandomHorizontalFlip(),
             transforms.Resize(sample_size[0]),
             transforms.CenterCrop(sample_size),
-            transforms.Normalize(
-                mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),
+            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], inplace=True),
         ])
 
     def get_batch(self, idx):
         video_dict: Dict[str, str] = self.dataset[idx]
         name = video_dict[self.NAME]
 
         content_url = video_dict[self.CONTENT_URL]
         file_name = content_url.split('/')[-1]
         video_dir = os.path.join(self.video_folder, file_name)
         video_reader = VideoReader(video_dir)
         video_length = len(video_reader)
 
-        clip_length = min(video_length,
-                          (self.sample_n_frames - 1) * self.sample_stride + 1)
+        clip_length = min(video_length, (self.sample_n_frames - 1) * self.sample_stride + 1)
         start_idx = random.randint(0, video_length - clip_length)
-        batch_index = np.linspace(
-            start_idx,
-            start_idx + clip_length - 1,
-            self.sample_n_frames,
-            dtype=int)
-
-        pixel_values = torch.from_numpy(
-            video_reader.get_batch(batch_index).asnumpy()).permute(
-                0, 3, 1, 2).contiguous()
+        batch_index = np.linspace(start_idx, start_idx + clip_length - 1, self.sample_n_frames, dtype=int)
+
+        pixel_values = torch.from_numpy(video_reader.get_batch(batch_index).asnumpy()).permute(0, 3, 1, 2).contiguous()
         pixel_values = pixel_values / 255.
         del video_reader
         return pixel_values, name
 
     def __len__(self):
         return self.length
 
@@ -124,19 +113,15 @@
                 idx = random.randint(0, self.length - 1)
 
         pixel_values = self.pixel_transforms(pixel_values)
         sample = dict(pixel_values=pixel_values, text=name)
         return sample
 
 
-def save_videos_grid(videos: torch.Tensor,
-                     path: str,
-                     rescale=False,
-                     n_rows=6,
-                     duration=4):
+def save_videos_grid(videos: torch.Tensor, path: str, rescale=False, n_rows=6, duration=4):
     import imageio
     videos = rearrange(videos, 'b c t h w -> t b c h w')
     outputs = []
     for x in videos:
         x = torchvision.utils.make_grid(x, nrow=n_rows)
         x = x.transpose(0, 1).transpose(1, 2).squeeze(-1)
         if rescale:
@@ -195,33 +180,27 @@
         beta_start=args.beta_start,
         beta_end=args.beta_end,
         beta_schedule=args.beta_schedule,
         steps_offset=args.steps_offset,
         clip_sample=args.clip_sample,
     )
     if not os.path.exists(args.model_id_or_path):
-        pretrained_model_path = snapshot_download(
-            args.model_id_or_path, revision=args.model_revision)
+        pretrained_model_path = snapshot_download(args.model_id_or_path, revision=args.model_revision)
     vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder='vae')
-    tokenizer = CLIPTokenizer.from_pretrained(
-        pretrained_model_path, subfolder='tokenizer')
-    text_encoder = CLIPTextModel.from_pretrained(
-        pretrained_model_path, subfolder='text_encoder')
+    tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder='tokenizer')
+    text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder='text_encoder')
 
     motion_adapter = None
     if args.motion_adapter_id_or_path is not None:
         if not os.path.exists(args.motion_adapter_id_or_path):
             args.motion_adapter_id_or_path = snapshot_download(
-                args.motion_adapter_id_or_path,
-                revision=args.motion_adapter_revision)
-        motion_adapter = MotionAdapter.from_pretrained(
-            args.motion_adapter_id_or_path)
+                args.motion_adapter_id_or_path, revision=args.motion_adapter_revision)
+        motion_adapter = MotionAdapter.from_pretrained(args.motion_adapter_id_or_path)
     unet: UNetMotionModel = UNetMotionModel.from_unet2d(
-        UNet2DConditionModel.from_pretrained(
-            pretrained_model_path, subfolder='unet'),
+        UNet2DConditionModel.from_pretrained(pretrained_model_path, subfolder='unet'),
         motion_adapter=motion_adapter,
         load_weights=True,
     )
 
     # Freeze vae and text_encoder
     vae.requires_grad_(False)
     text_encoder.requires_grad_(False)
@@ -231,47 +210,41 @@
     for name, param in unet.named_parameters():
         if re.fullmatch(args.trainable_modules, name):
             param.requires_grad = True
 
     # Preparing LoRA
     if args.sft_type == 'lora':
         if args.motion_adapter_id_or_path is None:
-            raise ValueError(
-                'No AnimateDiff weight found, Please do not use LoRA.')
+            raise ValueError('No AnimateDiff weight found, Please do not use LoRA.')
         lora_config = LoRAConfig(
             r=args.lora_rank,
             target_modules=args.trainable_modules,
             lora_alpha=args.lora_alpha,
             lora_dtype=args.lora_dtype,
             lora_dropout=args.lora_dropout_p)
         unet = Swift.prepare_model(unet, lora_config)
         logger.info(f'lora_config: {lora_config}')
 
-    trainable_params = list(
-        filter(lambda p: p.requires_grad, unet.parameters()))
+    trainable_params = list(filter(lambda p: p.requires_grad, unet.parameters()))
     optimizer = torch.optim.AdamW(
         trainable_params,
         lr=args.learning_rate,
         weight_decay=args.weight_decay,
     )
 
     if is_main_process:
         print(f'trainable params number: {len(trainable_params)}')
-        print(
-            f'trainable params scale: {sum(p.numel() for p in trainable_params) / 1e6:.3f} M'
-        )
+        print(f'trainable params scale: {sum(p.numel() for p in trainable_params) / 1e6:.3f} M')
 
     # Enable xformers
     if args.enable_xformers_memory_efficient_attention:
         if is_xformers_available():
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     # Enable gradient checkpointing
     if args.gradient_checkpointing:
         unet.enable_gradient_checkpointing()
 
     # Move models to GPU
     vae.to(local_rank)
@@ -287,19 +260,15 @@
         dataset_sample_size=args.dataset_sample_size,
     )
 
     if not is_dist():
         sampler = RandomSampler(train_dataset)
     else:
         sampler = DistributedSampler(
-            train_dataset,
-            num_replicas=num_processes,
-            rank=global_rank,
-            shuffle=True,
-            seed=global_seed)
+            train_dataset, num_replicas=num_processes, rank=global_rank, shuffle=True, seed=global_seed)
 
     # DataLoaders creation:
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         batch_size=args.batch_size,
         shuffle=False,
         sampler=sampler,
@@ -312,16 +281,15 @@
     max_train_steps = args.num_train_epochs * len(train_dataloader)
     print(f'max_train_steps: {max_train_steps}')
 
     # Scheduler
     lr_scheduler = get_scheduler(
         args.lr_scheduler_type,
         optimizer=optimizer,
-        num_warmup_steps=int(args.warmup_ratio * max_train_steps)
-        // args.gradient_accumulation_steps,
+        num_warmup_steps=int(args.warmup_ratio * max_train_steps) // args.gradient_accumulation_steps,
         num_training_steps=max_train_steps // args.gradient_accumulation_steps,
     )
 
     unet.to(local_rank)
     if is_dist():
         unet = DDP(unet, device_ids=[local_rank], output_device=local_rank)
 
@@ -330,90 +298,69 @@
     # Train!
     total_batch_size = args.batch_size * num_processes * args.gradient_accumulation_steps
 
     if is_main_process:
         logging.info('***** Running training *****')
         logging.info(f'  Num examples = {len(train_dataset)}')
         logging.info(f'  Num Epochs = {num_train_epochs}')
-        logging.info(
-            f'  Instantaneous batch size per device = {args.batch_size}')
-        logging.info(
-            f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-        )
-        logging.info(
-            f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}'
-        )
+        logging.info(f'  Instantaneous batch size per device = {args.batch_size}')
+        logging.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+        logging.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
         logging.info(f'  Total optimization steps = {max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Only show the progress bar once on each machine.
-    progress_bar = tqdm(
-        range(global_step, max_train_steps), disable=not is_main_process)
+    progress_bar = tqdm(range(global_step, max_train_steps), disable=not is_main_process)
     progress_bar.set_description('Steps')
 
     # Support mixed-precision training
     scaler = torch.cuda.amp.GradScaler() if args.mixed_precision else None
 
     for epoch in range(first_epoch, num_train_epochs):
         if is_dist():
             train_dataloader.sampler.set_epoch(epoch)
 
         unet.train()
 
         for step, batch in enumerate(train_dataloader):
             if args.text_dropout_rate > 0:
-                batch['text'] = [
-                    name if random.random() > args.text_dropout_rate else ''
-                    for name in batch['text']
-                ]
+                batch['text'] = [name if random.random() > args.text_dropout_rate else '' for name in batch['text']]
 
             # Data batch sanity check
             if epoch == first_epoch and step == 0:
-                pixel_values, texts = batch['pixel_values'].cpu(
-                ), batch['text']
-                pixel_values = rearrange(pixel_values,
-                                         'b f c h w -> b c f h w')
-                for idx, (pixel_value,
-                          text) in enumerate(zip(pixel_values, texts)):
+                pixel_values, texts = batch['pixel_values'].cpu(), batch['text']
+                pixel_values = rearrange(pixel_values, 'b f c h w -> b c f h w')
+                for idx, (pixel_value, text) in enumerate(zip(pixel_values, texts)):
                     pixel_value = pixel_value[None, ...]
-                    file_name = '-'.join(text.replace('/', '').split(
-                    )[:10]) if not text == '' else f'{global_rank}-{idx}'
-                    save_videos_grid(
-                        pixel_value,
-                        f'{output_dir}/sanity_check/{file_name}.gif',
-                        rescale=True)
+                    file_name = '-'.join(text.replace('/',
+                                                      '').split()[:10]) if not text == '' else f'{global_rank}-{idx}'
+                    save_videos_grid(pixel_value, f'{output_dir}/sanity_check/{file_name}.gif', rescale=True)
 
             # Convert videos to latent space
             pixel_values = batch['pixel_values'].to(local_rank)
             video_length = pixel_values.shape[1]
             with torch.no_grad():
-                pixel_values = rearrange(pixel_values,
-                                         'b f c h w -> (b f) c h w')
+                pixel_values = rearrange(pixel_values, 'b f c h w -> (b f) c h w')
                 latents = vae.encode(pixel_values).latent_dist
                 latents = latents.sample()
-                latents = rearrange(
-                    latents, '(b f) c h w -> b c f h w', f=video_length)
+                latents = rearrange(latents, '(b f) c h w -> b c f h w', f=video_length)
                 latents = latents * 0.18215
 
             # Sample noise that we'll add to the latents
             noise = torch.randn_like(latents)
             bsz = latents.shape[0]
 
             # Sample a random timestep for each video
-            timesteps = torch.randint(
-                0,
-                noise_scheduler.config.num_train_timesteps, (bsz, ),
-                device=latents.device)
+            timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=latents.device)
             timesteps = timesteps.long()
 
             # Add noise to the latents according to the noise magnitude at each timestep
             # (this is the forward diffusion process)
-            noisy_latents = noise_scheduler.add_noise(latents, noise,
-                                                      timesteps)
+            noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
 
             # Get the text embedding for conditioning
             with torch.no_grad():
                 prompt_ids = tokenizer(
                     batch['text'],
                     max_length=tokenizer.model_max_length,
                     padding='max_length',
@@ -423,93 +370,79 @@
 
             # Get the target for loss depending on the prediction type
             if noise_scheduler.config.prediction_type == 'epsilon':
                 target = noise
             elif noise_scheduler.config.prediction_type == 'v_prediction':
                 raise NotImplementedError
             else:
-                raise ValueError(
-                    f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                )
+                raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
             # Predict the noise residual and compute loss
             # Mixed-precision training
             with torch.cuda.amp.autocast(enabled=args.mixed_precision):
-                model_pred = unet(noisy_latents, timesteps,
-                                  encoder_hidden_states).sample
-                loss = F.mse_loss(
-                    model_pred.float(), target.float(), reduction='mean')
+                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
+                loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
 
             # Backpropagate
             if args.mixed_precision:
                 scaler.scale(loss).backward()
             else:
                 loss.backward()
 
             if step % args.gradient_accumulation_steps == 0:
                 # Backpropagate
                 if args.mixed_precision:
                     scaler.unscale_(optimizer)
-                    torch.nn.utils.clip_grad_norm_(unet.parameters(),
-                                                   args.max_grad_norm)
+                    torch.nn.utils.clip_grad_norm_(unet.parameters(), args.max_grad_norm)
                     scaler.step(optimizer)
                     scaler.update()
                 else:
-                    torch.nn.utils.clip_grad_norm_(unet.parameters(),
-                                                   args.max_grad_norm)
+                    torch.nn.utils.clip_grad_norm_(unet.parameters(), args.max_grad_norm)
                     optimizer.step()
                 optimizer.zero_grad()
                 lr_scheduler.step()
 
             progress_bar.update(1)
             global_step += 1
 
             # Wandb logging
             if is_main_process and args.use_wandb:
                 wandb.log({'train_loss': loss.item()}, step=global_step)
 
             # Save checkpoint
-            if is_main_process and (global_step % args.save_steps == 0
-                                    or step == len(train_dataloader) - 1):
+            if is_main_process and (global_step % args.save_steps == 0 or step == len(train_dataloader) - 1):
                 save_path = os.path.join(output_dir, 'checkpoints')
                 if step == len(train_dataloader) - 1:
                     if isinstance(unet, DDP):
-                        unet.module.save_pretrained(
-                            os.path.join(save_path, 'iter-last'))
+                        unet.module.save_pretrained(os.path.join(save_path, 'iter-last'))
                     else:
-                        unet.save_pretrained(
-                            os.path.join(save_path, 'iter-last'))
+                        unet.save_pretrained(os.path.join(save_path, 'iter-last'))
                     if args.push_to_hub:
                         push_to_hub(
                             repo_name=args.hub_model_id,
                             output_dir=os.path.join(save_path, 'iter-last'),
                             token=args.hub_token,
                             private=True,
                         )
-                    logging.info(
-                        f'Saved state to {os.path.join(save_path, "iter-last")} on the last step'
-                    )
+                    logging.info(f'Saved state to {os.path.join(save_path, "iter-last")} on the last step')
                 else:
-                    iter_save_path = os.path.join(save_path,
-                                                  f'iter-{global_step}')
+                    iter_save_path = os.path.join(save_path, f'iter-{global_step}')
                     if isinstance(unet, DDP):
                         unet.module.save_pretrained(iter_save_path)
                     else:
                         unet.save_pretrained(iter_save_path)
                     if args.push_to_hub and args.push_hub_strategy == 'all_checkpoints':
                         push_to_hub(
                             repo_name=args.hub_model_id,
-                            output_dir=os.path.join(save_path,
-                                                    f'iter-{global_step}'),
+                            output_dir=os.path.join(save_path, f'iter-{global_step}'),
                             token=args.hub_token,
                             private=True,
                         )
                     logging.info(
-                        f'Saved state to {os.path.join(save_path, f"iter-{global_step}")} (global_step: {global_step})'
-                    )
+                        f'Saved state to {os.path.join(save_path, f"iter-{global_step}")} (global_step: {global_step})')
 
             # Periodically validation
             if is_main_process and global_step % args.eval_steps == 0:
 
                 generator = torch.Generator(device=latents.device)
                 generator.manual_seed(global_seed)
                 Swift.merge(unet)
@@ -522,46 +455,39 @@
                                prefix='',
                                keep_vars=False,
                                adapter_name: str = None,
                                **kwargs):
                     state_dict = self.state_dict_origin()
                     return {
                         key.replace('base_layer.', ''): value
-                        for key, value in state_dict.items()
-                        if 'lora' not in key
+                        for key, value in state_dict.items() if 'lora' not in key
                     }
 
                 motion_adapter = MotionAdapter(
                     motion_num_attention_heads=args.motion_num_attention_heads,
                     motion_max_seq_length=args.motion_max_seq_length)
 
                 module = unet if not isinstance(unet, DDP) else unet.module
-                motion_adapter.mid_block.motion_modules = deepcopy(
-                    module.mid_block.motion_modules)
+                motion_adapter.mid_block.motion_modules = deepcopy(module.mid_block.motion_modules)
                 motion_adapter.mid_block.motion_modules.state_dict_origin = \
                     motion_adapter.mid_block.motion_modules.state_dict
-                motion_adapter.mid_block.motion_modules.state_dict = MethodType(
-                    state_dict, motion_adapter.mid_block.motion_modules)
-                for db1, db2 in zip(motion_adapter.down_blocks,
-                                    module.down_blocks):
+                motion_adapter.mid_block.motion_modules.state_dict = MethodType(state_dict,
+                                                                                motion_adapter.mid_block.motion_modules)
+                for db1, db2 in zip(motion_adapter.down_blocks, module.down_blocks):
                     db1.motion_modules = deepcopy(db2.motion_modules)
                     db1.motion_modules.state_dict_origin = db1.motion_modules.state_dict
-                    db1.motion_modules.state_dict = MethodType(
-                        state_dict, db1.motion_modules)
-                for db1, db2 in zip(motion_adapter.up_blocks,
-                                    module.up_blocks):
+                    db1.motion_modules.state_dict = MethodType(state_dict, db1.motion_modules)
+                for db1, db2 in zip(motion_adapter.up_blocks, module.up_blocks):
                     db1.motion_modules = deepcopy(db2.motion_modules)
                     db1.motion_modules.state_dict_origin = db1.motion_modules.state_dict
-                    db1.motion_modules.state_dict = MethodType(
-                        state_dict, db1.motion_modules)
+                    db1.motion_modules.state_dict = MethodType(state_dict, db1.motion_modules)
 
                 Swift.unmerge(unet)
                 validation_pipeline = AnimateDiffPipeline(
-                    unet=UNet2DConditionModel.from_pretrained(
-                        pretrained_model_path, subfolder='unet'),
+                    unet=UNet2DConditionModel.from_pretrained(pretrained_model_path, subfolder='unet'),
                     vae=vae,
                     tokenizer=tokenizer,
                     motion_adapter=motion_adapter,
                     text_encoder=text_encoder,
                     scheduler=noise_scheduler,
                 ).to('cuda')
                 validation_pipeline.enable_vae_slicing()
@@ -572,27 +498,21 @@
                         prompt=prompt,
                         negative_prompt='bad quality, worse quality',
                         num_frames=args.sample_n_frames,
                         height=height,
                         width=width,
                         guidance_scale=args.guidance_scale,
                         num_inference_steps=args.num_inference_steps,
-                        generator=torch.Generator('cpu').manual_seed(
-                            global_seed),
+                        generator=torch.Generator('cpu').manual_seed(global_seed),
                     )
                     frames = output.frames[0]
-                    export_to_gif(
-                        frames,
-                        f'{output_dir}/samples/sample-{global_step}-{idx}.gif')
+                    export_to_gif(frames, f'{output_dir}/samples/sample-{global_step}-{idx}.gif')
                 unet.train()
 
-            logs = {
-                'step_loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'step_loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
 
             if global_step >= max_train_steps:
                 break
 
     if is_dist():
         dist.destroy_process_group()
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/animatediff_infer.py` & `ms-swift-2.0.4/swift/aigc/animatediff_infer.py`

 * *Files 3% similar despite different names*

```diff
@@ -24,27 +24,24 @@
         beta_end=args.beta_end,
         beta_schedule=args.beta_schedule,
         steps_offset=args.steps_offset,
         clip_sample=args.clip_sample,
     )
 
     if not os.path.exists(args.model_id_or_path):
-        pretrained_model_path = snapshot_download(
-            args.model_id_or_path, revision=args.model_revision)
+        pretrained_model_path = snapshot_download(args.model_id_or_path, revision=args.model_revision)
     else:
         pretrained_model_path = args.model_id_or_path
 
     motion_adapter = None
     if args.motion_adapter_id_or_path is not None:
         if not os.path.exists(args.motion_adapter_id_or_path):
             args.motion_adapter_id_or_path = snapshot_download(
-                args.motion_adapter_id_or_path,
-                revision=args.motion_adapter_revision)
-        motion_adapter = MotionAdapter.from_pretrained(
-            args.motion_adapter_id_or_path)
+                args.motion_adapter_id_or_path, revision=args.motion_adapter_revision)
+        motion_adapter = MotionAdapter.from_pretrained(args.motion_adapter_id_or_path)
     if args.sft_type == 'full':
         motion_adapter_dir = args.ckpt_dir if args.ckpt_dir is not None else os.path.join(
             pretrained_model_path, 'motion_adapter')
         motion_adapter = MotionAdapter.from_pretrained(motion_adapter_dir)
 
     validation_pipeline = AnimateDiffPipeline.from_pretrained(
         pretrained_model_path,
@@ -59,18 +56,17 @@
             merged_lora_path = os.path.join(ckpt_dir, f'{ckpt_name}-merged')
             logger.info(f'merged_lora_path: `{merged_lora_path}`')
             logger.info("Setting args.sft_type: 'full'")
             logger.info(f'Setting args.ckpt_dir: {merged_lora_path}')
             args.sft_type = 'full'
             args.ckpt_dir = merged_lora_path
             if os.path.exists(args.ckpt_dir) and not args.replace_if_exists:
-                logger.warn(
-                    f'The weight directory for the merged LoRA already exists in {args.ckpt_dir}, '
-                    'skipping the saving process. '
-                    'you can pass `replace_if_exists=True` to overwrite it.')
+                logger.warn(f'The weight directory for the merged LoRA already exists in {args.ckpt_dir}, '
+                            'skipping the saving process. '
+                            'you can pass `replace_if_exists=True` to overwrite it.')
                 return
 
             Swift.merge_and_unload(model)
             validation_pipeline.unet = model.model
             validation_pipeline.save_pretrained(args.ckpt_dir)
 
     validation_pipeline.enable_vae_slicing()
@@ -85,16 +81,15 @@
                 negative_prompt='bad quality, worse quality',
                 generator=generator,
                 num_frames=args.sample_n_frames,
                 num_inference_steps=args.num_inference_steps,
                 guidance_scale=args.guidance_scale,
             ).frames[0]
             os.makedirs(args.output_path, exist_ok=True)
-            logger.info(
-                f'Output saved to: {f"{args.output_path}/output-{idx}.gif"}')
+            logger.info(f'Output saved to: {f"{args.output_path}/output-{idx}.gif"}')
             export_to_gif(sample, f'{args.output_path}/output-{idx}.gif')
             idx += 1
     else:
         with open(args.validation_prompts_path, 'r') as f:
             validation_data = f.readlines()
 
         for idx, prompt in enumerate(validation_data):
@@ -103,13 +98,12 @@
                 negative_prompt='bad quality, worse quality',
                 generator=generator,
                 num_frames=args.sample_n_frames,
                 num_inference_steps=args.num_inference_steps,
                 guidance_scale=args.guidance_scale,
             ).frames[0]
             os.makedirs(args.output_path, exist_ok=True)
-            logger.info(
-                f'Output saved to: {f"{args.output_path}/output-{idx}.gif"}')
+            logger.info(f'Output saved to: {f"{args.output_path}/output-{idx}.gif"}')
             export_to_gif(sample, f'{args.output_path}/output-{idx}.gif')
 
 
 animatediff_infer_main = get_main(AnimateDiffInferArguments, animatediff_infer)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/__init__.py` & `ms-swift-2.0.4/swift/aigc/diffusers/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,18 @@
 from .infer_controlnet import main as infer_controlnet
 from .infer_controlnet_sdxl import main as infer_controlnet_sdxl
 from .infer_dreambooth import main as infer_dreambooth
 from .infer_dreambooth_lora import main as infer_dreambooth_lora
 from .infer_dreambooth_lora_sdxl import main as infer_dreambooth_lora_sdxl
 from .infer_text_to_image import main as infer_text_to_image
 from .infer_text_to_image_lora import main as infer_text_to_image_lora
-from .infer_text_to_image_lora_sdxl import \
-    main as infer_text_to_image_lora_sdxl
+from .infer_text_to_image_lora_sdxl import main as infer_text_to_image_lora_sdxl
 from .infer_text_to_image_sdxl import main as infer_text_to_image_sdxl
 from .train_controlnet import main as train_controlnet
 from .train_controlnet_sdxl import main as train_controlnet_sdxl
 from .train_dreambooth import main as train_dreambooth
 from .train_dreambooth_lora import main as train_dreambooth_lora
 from .train_dreambooth_lora_sdxl import main as train_dreambooth_lora_sdxl
 from .train_text_to_image import main as train_text_to_image
 from .train_text_to_image_lora import main as train_text_to_image_lora
-from .train_text_to_image_lora_sdxl import \
-    main as train_text_to_image_lora_sdxl
+from .train_text_to_image_lora_sdxl import main as train_text_to_image_lora_sdxl
 from .train_text_to_image_sdxl import main as train_text_to_image_sdxl
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_controlnet.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_controlnet.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,51 +1,46 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
 import os
 
 import torch
-from diffusers import (ControlNetModel, StableDiffusionControlNetPipeline,
-                       UniPCMultistepScheduler)
+from diffusers import ControlNetModel, StableDiffusionControlNetPipeline, UniPCMultistepScheduler
 from diffusers.utils import load_image
 from modelscope import snapshot_download
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a ControlNet inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a ControlNet inference.')
     parser.add_argument(
         '--base_model_path',
         type=str,
         default='AI-ModelScope/stable-diffusion-v1-5',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--controlnet_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained controlnet model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--control_image_path',
         type=str,
         default=None,
         required=True,
         help='The path to conditioning image.',
@@ -58,74 +53,63 @@
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
-    parser.add_argument(
-        '--seed', type=int, default=None, help='A seed for inference.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for inference.')
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=20,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.base_model_path):
         base_model_path = args.base_model_path
     else:
-        base_model_path = snapshot_download(
-            args.base_model_path, revision=args.revision)
+        base_model_path = snapshot_download(args.base_model_path, revision=args.revision)
 
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    controlnet = ControlNetModel.from_pretrained(
-        args.controlnet_path, torch_dtype=torch_dtype)
+    controlnet = ControlNetModel.from_pretrained(args.controlnet_path, torch_dtype=torch_dtype)
     pipe = StableDiffusionControlNetPipeline.from_pretrained(
         base_model_path, controlnet=controlnet, torch_dtype=torch_dtype)
 
     # speed up diffusion process with faster scheduler and memory optimization
     pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
 
     # memory optimization.
     pipe.enable_model_cpu_offload()
 
     control_image = load_image(args.control_image_path)
 
     # generate image
     generator = torch.manual_seed(args.seed)
     image = pipe(
-        args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        generator=generator,
-        image=control_image).images[0]
+        args.prompt, num_inference_steps=args.num_inference_steps, generator=generator, image=control_image).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_controlnet_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_controlnet_sdxl.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,50 +1,45 @@
 import argparse
 import os
 
 import torch
-from diffusers import (ControlNetModel, StableDiffusionXLControlNetPipeline,
-                       UniPCMultistepScheduler)
+from diffusers import ControlNetModel, StableDiffusionXLControlNetPipeline, UniPCMultistepScheduler
 from diffusers.utils import load_image
 from modelscope import snapshot_download
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a ControlNet inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a ControlNet inference.')
     parser.add_argument(
         '--base_model_path',
         type=str,
         default='AI-ModelScope/stable-diffusion-xl-base-1.0',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--controlnet_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained controlnet model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--control_image_path',
         type=str,
         default=None,
         required=True,
         help='The path to conditioning image.',
@@ -57,73 +52,62 @@
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
-    parser.add_argument(
-        '--seed', type=int, default=None, help='A seed for inference.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for inference.')
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=20,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.base_model_path):
         base_model_path = args.base_model_path
     else:
-        base_model_path = snapshot_download(
-            args.base_model_path, revision=args.revision)
+        base_model_path = snapshot_download(args.base_model_path, revision=args.revision)
 
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    controlnet = ControlNetModel.from_pretrained(
-        args.controlnet_path, torch_dtype=torch_dtype)
+    controlnet = ControlNetModel.from_pretrained(args.controlnet_path, torch_dtype=torch_dtype)
     pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
         base_model_path, controlnet=controlnet, torch_dtype=torch_dtype)
 
     # speed up diffusion process with faster scheduler and memory optimization
     pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)
     # memory optimization.
     pipe.enable_model_cpu_offload()
 
     control_image = load_image(args.control_image_path)
 
     # generate image
     generator = torch.manual_seed(args.seed)
     image = pipe(
-        args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        generator=generator,
-        image=control_image).images[0]
+        args.prompt, num_inference_steps=args.num_inference_steps, generator=generator, image=control_image).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_dreambooth.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_dreambooth.py`

 * *Files 8% similar despite different names*

```diff
@@ -4,65 +4,58 @@
 
 import torch
 from diffusers import StableDiffusionPipeline
 from modelscope import snapshot_download
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a dreambooth inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a dreambooth inference.')
     parser.add_argument(
         '--model_path',
         type=str,
         default=None,
         required=True,
         help='Path to trained model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=50,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
@@ -71,16 +64,13 @@
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = StableDiffusionPipeline.from_pretrained(
-        args.model_path, torch_dtype=torch_dtype).to('cuda')
+    pipe = StableDiffusionPipeline.from_pretrained(args.model_path, torch_dtype=torch_dtype).to('cuda')
 
     image = pipe(
-        args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        guidance_scale=args.guidance_scale).images[0]
+        args.prompt, num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale).images[0]
 
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_dreambooth_lora.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_dreambooth_lora_sdxl.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,112 +1,97 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
 import os
 
 import torch
-from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
+from diffusers import DiffusionPipeline
 from modelscope import snapshot_download
 
 from swift import Swift
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a text to image inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a text to image inference.')
     parser.add_argument(
         '--base_model_path',
         type=str,
-        default='AI-ModelScope/stable-diffusion-v1-5',
+        default='AI-ModelScope/stable-diffusion-xl-base-1.0',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--lora_model_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained lora model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
+    parser.add_argument('--seed', type=int, default=None, help='A seed for inference.')
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=50,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.base_model_path):
         base_model_path = args.base_model_path
     else:
-        base_model_path = snapshot_download(
-            args.base_model_path, revision=args.revision)
+        base_model_path = snapshot_download(args.base_model_path, revision=args.revision)
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = DiffusionPipeline.from_pretrained(
-        base_model_path, torch_dtype=torch_dtype)
-    pipe.scheduler = DPMSolverMultistepScheduler.from_config(
-        pipe.scheduler.config)
+    pipe = DiffusionPipeline.from_pretrained(base_model_path, torch_dtype=torch_dtype)
     if args.lora_model_path is not None:
         pipe.unet = Swift.from_pretrained(pipe.unet, args.lora_model_path)
-    pipe.to('cuda')
-
-    image = pipe(
-        args.prompt, num_inference_steps=args.num_inference_steps).images[0]
-
+    pipe = pipe.to('cuda')
+    image = pipe(args.prompt, num_inference_steps=args.num_inference_steps).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_dreambooth_lora_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_dreambooth_lora.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,110 +1,99 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import argparse
 import os
 
 import torch
-from diffusers import DiffusionPipeline
+from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler
 from modelscope import snapshot_download
 
 from swift import Swift
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a text to image inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a text to image inference.')
     parser.add_argument(
         '--base_model_path',
         type=str,
-        default='AI-ModelScope/stable-diffusion-xl-base-1.0',
+        default='AI-ModelScope/stable-diffusion-v1-5',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--lora_model_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained lora model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
     parser.add_argument(
-        '--seed', type=int, default=None, help='A seed for inference.')
-    parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=50,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.base_model_path):
         base_model_path = args.base_model_path
     else:
-        base_model_path = snapshot_download(
-            args.base_model_path, revision=args.revision)
+        base_model_path = snapshot_download(args.base_model_path, revision=args.revision)
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = DiffusionPipeline.from_pretrained(
-        base_model_path, torch_dtype=torch_dtype)
+    pipe = DiffusionPipeline.from_pretrained(base_model_path, torch_dtype=torch_dtype)
+    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
     if args.lora_model_path is not None:
         pipe.unet = Swift.from_pretrained(pipe.unet, args.lora_model_path)
-    pipe = pipe.to('cuda')
-    image = pipe(
-        args.prompt, num_inference_steps=args.num_inference_steps).images[0]
+    pipe.to('cuda')
+
+    image = pipe(args.prompt, num_inference_steps=args.num_inference_steps).images[0]
+
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image.py`

 * *Files 6% similar despite different names*

```diff
@@ -4,107 +4,93 @@
 
 import torch
 from diffusers import StableDiffusionPipeline, UNet2DConditionModel
 from modelscope import snapshot_download
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a text to image inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a text to image inference.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default='AI-ModelScope/stable-diffusion-v1-5',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--unet_model_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained unet model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=50,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.pretrained_model_name_or_path):
         model_path = args.pretrained_model_name_or_path
     else:
-        model_path = snapshot_download(
-            args.pretrained_model_name_or_path, revision=args.revision)
+        model_path = snapshot_download(args.pretrained_model_name_or_path, revision=args.revision)
 
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = StableDiffusionPipeline.from_pretrained(
-        model_path, torch_dtype=torch_dtype)
+    pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch_dtype)
     if args.unet_model_path is not None:
-        pipe.unet = UNet2DConditionModel.from_pretrained(
-            args.unet_model_path, torch_dtype=torch_dtype)
+        pipe.unet = UNet2DConditionModel.from_pretrained(args.unet_model_path, torch_dtype=torch_dtype)
     pipe.to('cuda')
     image = pipe(
-        prompt=args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        guidance_scale=args.guidance_scale).images[0]
+        prompt=args.prompt, num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image_lora.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image_lora.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,106 +6,93 @@
 from diffusers import StableDiffusionPipeline
 from modelscope import snapshot_download
 
 from swift import Swift
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a text to image inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a text to image inference.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default='AI-ModelScope/stable-diffusion-v1-5',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--lora_model_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained lora model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=30,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.pretrained_model_name_or_path):
         model_path = args.pretrained_model_name_or_path
     else:
-        model_path = snapshot_download(
-            args.pretrained_model_name_or_path, revision=args.revision)
+        model_path = snapshot_download(args.pretrained_model_name_or_path, revision=args.revision)
 
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = StableDiffusionPipeline.from_pretrained(
-        model_path, torch_dtype=torch_dtype)
+    pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch_dtype)
     if args.lora_model_path is not None:
         pipe.unet = Swift.from_pretrained(pipe.unet, args.lora_model_path)
     pipe.to('cuda')
     image = pipe(
-        prompt=args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        guidance_scale=args.guidance_scale).images[0]
+        prompt=args.prompt, num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image_lora_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image_lora_sdxl.py`

 * *Files 6% similar despite different names*

```diff
@@ -6,106 +6,93 @@
 from diffusers import StableDiffusionXLPipeline, UNet2DConditionModel
 from modelscope import snapshot_download
 
 from swift import Swift
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a text to image lora sdxl inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a text to image lora sdxl inference.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default='AI-ModelScope/stable-diffusion-v1-5',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--lora_model_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained lora model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=30,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.pretrained_model_name_or_path):
         model_path = args.pretrained_model_name_or_path
     else:
-        model_path = snapshot_download(
-            args.pretrained_model_name_or_path, revision=args.revision)
+        model_path = snapshot_download(args.pretrained_model_name_or_path, revision=args.revision)
 
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = StableDiffusionXLPipeline.from_pretrained(
-        model_path, torch_dtype=torch_dtype)
+    pipe = StableDiffusionXLPipeline.from_pretrained(model_path, torch_dtype=torch_dtype)
     if args.lora_model_path is not None:
         pipe.unet = Swift.from_pretrained(pipe.unet, args.lora_model_path)
     pipe.to('cuda')
     image = pipe(
-        prompt=args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        guidance_scale=args.guidance_scale).images[0]
+        prompt=args.prompt, num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/infer_text_to_image_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/infer_text_to_image_sdxl.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,107 +4,93 @@
 
 import torch
 from diffusers import DiffusionPipeline, UNet2DConditionModel
 from modelscope import snapshot_download
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a text to image inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a text to image inference.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default='AI-ModelScope/stable-diffusion-v1-5',
         required=True,
-        help=
-        'Path to pretrained model or model identifier from modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from modelscope.cn/models.',
+        help='Revision of pretrained model identifier from modelscope.cn/models.',
     )
     parser.add_argument(
         '--unet_model_path',
         type=str,
         default=None,
         required=False,
         help='The path to trained unet model.',
     )
     parser.add_argument(
         '--prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        'The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
+        help='The prompt or prompts to guide image generation. If not defined, you need to pass `prompt_embeds`',
     )
     parser.add_argument(
         '--image_save_path',
         type=str,
         default=None,
         required=True,
         help='The path to save generated image',
     )
     parser.add_argument(
         '--torch_dtype',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
-         ' mixed_precision passed with the `accelerate.launch` command in training script.'
-         ),
+        help=('Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to the value of the'
+              ' mixed_precision passed with the `accelerate.launch` command in training script.'),
     )
     parser.add_argument(
         '--num_inference_steps',
         type=int,
         default=30,
-        help=
-        ('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
+        help=('The number of denoising steps. More denoising steps usually lead to a higher quality image at the \
                 expense of slower inference.'),
     )
     parser.add_argument(
         '--guidance_scale',
         type=float,
         default=7.5,
-        help=
-        ('A higher guidance scale value encourages the model to generate images closely linked to the text \
-                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'
-         ),
+        help=('A higher guidance scale value encourages the model to generate images closely linked to the text \
+                `prompt` at the expense of lower image quality. Guidance scale is enabled when `guidance_scale > 1`.'),
     )
 
     args = parser.parse_args()
     return args
 
 
 def main():
     args = parse_args()
 
     if os.path.exists(args.pretrained_model_name_or_path):
         model_path = args.pretrained_model_name_or_path
     else:
-        model_path = snapshot_download(
-            args.pretrained_model_name_or_path, revision=args.revision)
+        model_path = snapshot_download(args.pretrained_model_name_or_path, revision=args.revision)
 
     if args.torch_dtype == 'fp16':
         torch_dtype = torch.float16
     elif args.torch_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = torch.float32
 
-    pipe = DiffusionPipeline.from_pretrained(
-        model_path, torch_dtype=torch_dtype)
+    pipe = DiffusionPipeline.from_pretrained(model_path, torch_dtype=torch_dtype)
     if args.unet_model_path is not None:
-        pipe.unet = UNet2DConditionModel.from_pretrained(
-            args.unet_model_path, torch_dtype=torch_dtype)
+        pipe.unet = UNet2DConditionModel.from_pretrained(args.unet_model_path, torch_dtype=torch_dtype)
     pipe.to('cuda')
     image = pipe(
-        prompt=args.prompt,
-        num_inference_steps=args.num_inference_steps,
-        guidance_scale=args.guidance_scale).images[0]
+        prompt=args.prompt, num_inference_steps=args.num_inference_steps, guidance_scale=args.guidance_scale).images[0]
     image.save(args.image_save_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_controlnet.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_controlnet.py`

 * *Files 5% similar despite different names*

```diff
@@ -27,17 +27,16 @@
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import ProjectConfiguration, set_seed
 from datasets import load_dataset
-from diffusers import (AutoencoderKL, ControlNetModel, DDPMScheduler,
-                       StableDiffusionControlNetPipeline, UNet2DConditionModel,
-                       UniPCMultistepScheduler)
+from diffusers import (AutoencoderKL, ControlNetModel, DDPMScheduler, StableDiffusionControlNetPipeline,
+                       UNet2DConditionModel, UniPCMultistepScheduler)
 from diffusers.optimization import get_scheduler
 from diffusers.utils import is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from modelscope import AutoTokenizer, MsDataset
 from packaging import version
 from PIL import Image
 from torchvision import transforms
@@ -59,16 +58,15 @@
     grid = Image.new('RGB', size=(cols * w, rows * h))
 
     for i, img in enumerate(imgs):
         grid.paste(img, box=(i % cols * w, i // cols * h))
     return grid
 
 
-def log_validation(vae, text_encoder, tokenizer, unet, controlnet, args,
-                   accelerator, weight_dtype, step):
+def log_validation(vae, text_encoder, tokenizer, unet, controlnet, args, accelerator, weight_dtype, step):
     logger.info('Running validation... ')
 
     controlnet = accelerator.unwrap_model(controlnet)
 
     pipeline = StableDiffusionControlNetPipeline.from_pretrained(
         args.pretrained_model_name_or_path,
         vae=vae,
@@ -77,58 +75,50 @@
         unet=unet,
         controlnet=controlnet,
         safety_checker=None,
         revision=args.revision,
         variant=args.variant,
         torch_dtype=weight_dtype,
     )
-    pipeline.scheduler = UniPCMultistepScheduler.from_config(
-        pipeline.scheduler.config)
+    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)
     pipeline = pipeline.to(accelerator.device)
     pipeline.set_progress_bar_config(disable=True)
 
     if args.enable_xformers_memory_efficient_attention:
         pipeline.enable_xformers_memory_efficient_attention()
 
     if args.seed is None:
         generator = None
     else:
-        generator = torch.Generator(device=accelerator.device).manual_seed(
-            args.seed)
+        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)
 
     if len(args.validation_image) == len(args.validation_prompt):
         validation_images = args.validation_image
         validation_prompts = args.validation_prompt
     elif len(args.validation_image) == 1:
         validation_images = args.validation_image * len(args.validation_prompt)
         validation_prompts = args.validation_prompt
     elif len(args.validation_prompt) == 1:
         validation_images = args.validation_image
-        validation_prompts = args.validation_prompt * len(
-            args.validation_image)
+        validation_prompts = args.validation_prompt * len(args.validation_image)
     else:
         raise ValueError(
-            'number of `args.validation_image` and `args.validation_prompt` should be checked in `parse_args`'
-        )
+            'number of `args.validation_image` and `args.validation_prompt` should be checked in `parse_args`')
 
     image_logs = []
 
-    for validation_prompt, validation_image in zip(validation_prompts,
-                                                   validation_images):
+    for validation_prompt, validation_image in zip(validation_prompts, validation_images):
         validation_image = Image.open(validation_image).convert('RGB')
 
         images = []
 
         for _ in range(args.num_validation_images):
             with torch.autocast('cuda'):
                 image = pipeline(
-                    validation_prompt,
-                    validation_image,
-                    num_inference_steps=20,
-                    generator=generator).images[0]
+                    validation_prompt, validation_image, num_inference_steps=20, generator=generator).images[0]
 
             images.append(image)
 
         image_logs.append({
             'validation_image': validation_image,
             'images': images,
             'validation_prompt': validation_prompt
@@ -146,44 +136,37 @@
                 formatted_images.append(np.asarray(validation_image))
 
                 for image in images:
                     formatted_images.append(np.asarray(image))
 
                 formatted_images = np.stack(formatted_images)
 
-                tracker.writer.add_images(
-                    validation_prompt,
-                    formatted_images,
-                    step,
-                    dataformats='NHWC')
+                tracker.writer.add_images(validation_prompt, formatted_images, step, dataformats='NHWC')
         elif tracker.name == 'wandb':
             formatted_images = []
 
             for log in image_logs:
                 images = log['images']
                 validation_prompt = log['validation_prompt']
                 validation_image = log['validation_image']
 
-                formatted_images.append(
-                    wandb.Image(
-                        validation_image, caption='Controlnet conditioning'))
+                formatted_images.append(wandb.Image(validation_image, caption='Controlnet conditioning'))
 
                 for image in images:
                     image = wandb.Image(image, caption=validation_prompt)
                     formatted_images.append(image)
 
             tracker.log({'validation': formatted_images})
         else:
             logger.warn(f'image logging not implemented for {tracker.name}')
 
         return image_logs
 
 
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str, revision: str):
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):
     text_encoder_config = PretrainedConfig.from_pretrained(
         pretrained_model_name_or_path,
         subfolder='text_encoder',
         revision=revision,
     )
     model_class = text_encoder_config.architectures[0]
 
@@ -195,31 +178,26 @@
         from diffusers.pipelines.alt_diffusion.modeling_roberta_series import RobertaSeriesModelWithTransformation
 
         return RobertaSeriesModelWithTransformation
     else:
         raise ValueError(f'{model_class} is not supported.')
 
 
-def save_model_card(repo_id: str,
-                    image_logs=None,
-                    base_model=str,
-                    repo_folder=None):
+def save_model_card(repo_id: str, image_logs=None, base_model=str, repo_folder=None):
     img_str = ''
     if image_logs is not None:
         img_str = 'You can find some example images below.\n'
         for i, log in enumerate(image_logs):
             images = log['images']
             validation_prompt = log['validation_prompt']
             validation_image = log['validation_image']
-            validation_image.save(
-                os.path.join(repo_folder, 'image_control.png'))
+            validation_image.save(os.path.join(repo_folder, 'image_control.png'))
             img_str += f'prompt: {validation_prompt}\n'
             images = [validation_image] + images
-            image_grid(images, 1, len(images)).save(
-                os.path.join(repo_folder, f'images_{i}.png'))
+            image_grid(images, 1, len(images)).save(os.path.join(repo_folder, f'images_{i}.png'))
             img_str += f'![images_{i})](./images_{i}.png)\n'
 
     yaml = f"""
 ---
 license: creativeml-openrail-m
 base_model: {base_model}
 tags:
@@ -238,433 +216,332 @@
 {img_str}
 """
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a ControlNet training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a ControlNet training script.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--controlnet_model_name_or_path',
         type=str,
         default=None,
-        help=
-        'Path to pretrained controlnet model or model identifier from huggingface.co/models or modelscope.cn/models.'
-        ' If not specified controlnet weights are initialized from unet.',
+        help='Path to pretrained controlnet model or model identifier from huggingface.co/models or '
+        'modelscope.cn/models. If not specified controlnet weights are initialized from unet.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--tokenizer_name',
         type=str,
         default=None,
         help='Pretrained tokenizer name or path if not the same as model_name',
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='controlnet-model',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
     parser.add_argument(
         '--cache_dir',
         type=str,
         default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        help='The directory where the downloaded models and datasets will be stored.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=512,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for the training dataloader.')
+        '--train_batch_size', type=int, default=4, help='Batch size (per device) for the training dataloader.')
     parser.add_argument('--num_train_epochs', type=int, default=1)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
         help=
         ('Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training '
          'via `--resume_from_checkpoint`. '
          'In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for '
          'inference.'
          'Using a checkpoint for inference requires separate loading of the original pipeline and the individual '
          'checkpointed model components.'
          'See https://huggingface.co/docs/diffusers/main/en/training/dreambooth'
-         '#performing-inference-using-a-saved-checkpoint for step by step instructions.'
-         ),
+         '#performing-inference-using-a-saved-checkpoint for step by step instructions.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=5e-6,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--lr_num_cycles',
         type=int,
         default=1,
-        help=
-        'Number of hard resets of the lr in cosine_with_restarts scheduler.',
+        help='Number of hard resets of the lr in cosine_with_restarts scheduler.',
     )
+    parser.add_argument('--lr_power', type=float, default=1.0, help='Power factor of the polynomial scheduler.')
     parser.add_argument(
-        '--lr_power',
-        type=float,
-        default=1.0,
-        help='Power factor of the polynomial scheduler.')
-    parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
-    )
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
+    )
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
     parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
     parser.add_argument(
         '--set_grads_to_none',
         action='store_true',
-        help=
-        ('Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain'
-         ' behaviors, so disable this argument if it causes any problems. More info:'
-         ' https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'
-         ),
+        help=('Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain'
+              ' behaviors, so disable this argument if it causes any problems. More info:'
+              ' https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'),
     )
     parser.add_argument(
         '--dataset_name',
         type=str,
         default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
+        help=('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
     )
     parser.add_argument(
         '--dataset_config_name',
         type=str,
         default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
         '--train_data_dir',
         type=str,
         default=None,
-        help=
-        ('A folder containing the training data. Folder contents must follow the structure described in'
-         ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
-         ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'
-         ),
+        help=('A folder containing the training data. Folder contents must follow the structure described in'
+              ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
+              ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'),
     )
     parser.add_argument(
-        '--image_column',
-        type=str,
-        default='image',
-        help='The column of the dataset containing the target image.')
+        '--image_column', type=str, default='image', help='The column of the dataset containing the target image.')
     parser.add_argument(
         '--conditioning_image_column',
         type=str,
         default='conditioning_image',
-        help=
-        'The column of the dataset containing the controlnet conditioning image.',
+        help='The column of the dataset containing the controlnet conditioning image.',
     )
     parser.add_argument(
         '--caption_column',
         type=str,
         default='text',
-        help=
-        'The column of the dataset containing a caption or a list of captions.',
+        help='The column of the dataset containing a caption or a list of captions.',
     )
     parser.add_argument(
         '--max_train_samples',
         type=int,
         default=None,
-        help=
-        ('For debugging purposes or quicker training, truncate the number of training examples to this '
-         'value if set.'),
+        help=('For debugging purposes or quicker training, truncate the number of training examples to this '
+              'value if set.'),
     )
     parser.add_argument(
         '--proportion_empty_prompts',
         type=float,
         default=0,
-        help=
-        'Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).',
+        help='Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).',
     )
     parser.add_argument(
         '--validation_prompt',
         type=str,
         default=None,
         nargs='+',
-        help=
-        ('A set of prompts evaluated every `--validation_steps` and logged to `--report_to`.'
-         ' Provide either a matching number of `--validation_image`s, a single `--validation_image`'
-         ' to be used with all prompts, or a single prompt that will be used with all `--validation_image`s.'
-         ),
+        help=('A set of prompts evaluated every `--validation_steps` and logged to `--report_to`.'
+              ' Provide either a matching number of `--validation_image`s, a single `--validation_image`'
+              ' to be used with all prompts, or a single prompt that will be used with all `--validation_image`s.'),
     )
     parser.add_argument(
         '--validation_image',
         type=str,
         default=None,
         nargs='+',
-        help=
-        ('A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`'
-         ' and logged to `--report_to`. Provide either a matching number of `--validation_prompt`s, a'
-         ' a single `--validation_prompt` to be used with all `--validation_image`s, or a single'
-         ' `--validation_image` that will be used with all `--validation_prompt`s.'
-         ),
+        help=('A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`'
+              ' and logged to `--report_to`. Provide either a matching number of `--validation_prompt`s, a'
+              ' a single `--validation_prompt` to be used with all `--validation_image`s, or a single'
+              ' `--validation_image` that will be used with all `--validation_prompt`s.'),
     )
     parser.add_argument(
         '--num_validation_images',
         type=int,
         default=4,
-        help=
-        'Number of images to be generated for each `--validation_image`, `--validation_prompt` pair',
+        help='Number of images to be generated for each `--validation_image`, `--validation_prompt` pair',
     )
     parser.add_argument(
         '--validation_steps',
         type=int,
         default=100,
-        help=
-        ('Run validation every X steps. Validation consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`'
-         ' and logging the images.'),
+        help=('Run validation every X steps. Validation consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`'
+              ' and logging the images.'),
     )
     parser.add_argument(
         '--tracker_project_name',
         type=str,
         default='train_controlnet',
-        help=
-        ('The `project_name` argument passed to Accelerator.init_trackers for'
-         ' more information see '
-         'https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator'
-         ),
+        help=('The `project_name` argument passed to Accelerator.init_trackers for'
+              ' more information see '
+              'https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator'),
     )
 
     if input_args is not None:
         args = parser.parse_args(input_args)
     else:
         args = parser.parse_args()
 
     if args.dataset_name is None and args.train_data_dir is None:
-        raise ValueError(
-            'Specify either `--dataset_name` or `--train_data_dir`')
+        raise ValueError('Specify either `--dataset_name` or `--train_data_dir`')
 
     if args.dataset_name is not None and args.train_data_dir is not None:
-        raise ValueError(
-            'Specify only one of `--dataset_name` or `--train_data_dir`')
+        raise ValueError('Specify only one of `--dataset_name` or `--train_data_dir`')
 
     if args.proportion_empty_prompts < 0 or args.proportion_empty_prompts > 1:
-        raise ValueError(
-            '`--proportion_empty_prompts` must be in the range [0, 1].')
+        raise ValueError('`--proportion_empty_prompts` must be in the range [0, 1].')
 
     if args.validation_prompt is not None and args.validation_image is None:
-        raise ValueError(
-            '`--validation_image` must be set if `--validation_prompt` is set')
+        raise ValueError('`--validation_image` must be set if `--validation_prompt` is set')
 
     if args.validation_prompt is None and args.validation_image is not None:
-        raise ValueError(
-            '`--validation_prompt` must be set if `--validation_image` is set')
+        raise ValueError('`--validation_prompt` must be set if `--validation_image` is set')
 
-    if (args.validation_image is not None
-            and args.validation_prompt is not None
-            and len(args.validation_image) != 1
-            and len(args.validation_prompt) != 1
-            and len(args.validation_image) != len(args.validation_prompt)):
-        raise ValueError(
-            'Must provide either 1 `--validation_image`, 1 `--validation_prompt`,'
-            ' or the same number of `--validation_prompt`s and `--validation_image`s'
-        )
+    if (args.validation_image is not None and args.validation_prompt is not None and len(args.validation_image) != 1
+            and len(args.validation_prompt) != 1 and len(args.validation_image) != len(args.validation_prompt)):
+        raise ValueError('Must provide either 1 `--validation_image`, 1 `--validation_prompt`,'
+                         ' or the same number of `--validation_prompt`s and `--validation_image`s')
 
     if args.resolution % 8 != 0:
-        raise ValueError(
-            '`--resolution` must be divisible by 8 for consistently sized encoded images between '
-            'the VAE and the controlnet encoder.')
+        raise ValueError('`--resolution` must be divisible by 8 for consistently sized encoded images between '
+                         'the VAE and the controlnet encoder.')
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
 
-    if args.controlnet_model_name_or_path and not os.path.exists(
-            args.controlnet_model_name_or_path):
-        args.controlnet_model_name_or_path = snapshot_download(
-            args.controlnet_model_name_or_path)
+    if args.controlnet_model_name_or_path and not os.path.exists(args.controlnet_model_name_or_path):
+        args.controlnet_model_name_or_path = snapshot_download(args.controlnet_model_name_or_path)
 
     return args
 
 
 def make_train_dataset(args, tokenizer, accelerator):
     # Get the datasets: you can either provide your own training and evaluation files (see below)
     # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).
@@ -674,18 +551,15 @@
     if args.dataset_name is not None:
         # Downloading and loading a dataset from the hub.
         dataset = MsDataset.load(
             args.dataset_name,
             args.dataset_config_name,
         )
         if isinstance(dataset, dict):
-            dataset = {
-                key: value.to_hf_dataset()
-                for key, value in dataset.items()
-            }
+            dataset = {key: value.to_hf_dataset() for key, value in dataset.items()}
         else:
             dataset = {'train': dataset.to_hf_dataset()}
     else:
         if args.train_data_dir is not None:
             dataset = load_dataset(
                 args.train_data_dir,
                 cache_dir=args.cache_dir,
@@ -700,33 +574,29 @@
     # 6. Get the column names for input/target.
     if args.image_column is None:
         image_column = column_names[0]
         logger.info(f'image column defaulting to {image_column}')
     else:
         image_column = args.image_column
         if image_column not in column_names:
-            raise ValueError(
-                f"`--image_column` value '{args.image_column}' not found in dataset columns."
-                f" Dataset columns are: {', '.join(column_names)}")
+            raise ValueError(f"`--image_column` value '{args.image_column}' not found in dataset columns."
+                             f" Dataset columns are: {', '.join(column_names)}")
 
     if args.caption_column is None:
         caption_column = column_names[1]
         logger.info(f'caption column defaulting to {caption_column}')
     else:
         caption_column = args.caption_column
         if caption_column not in column_names:
-            raise ValueError(
-                f"`--caption_column` value '{args.caption_column}' not found in dataset columns. "
-                f"Dataset columns are: {', '.join(column_names)}")
+            raise ValueError(f"`--caption_column` value '{args.caption_column}' not found in dataset columns. "
+                             f"Dataset columns are: {', '.join(column_names)}")
 
     if args.conditioning_image_column is None:
         conditioning_image_column = column_names[2]
-        logger.info(
-            f'conditioning image column defaulting to {conditioning_image_column}'
-        )
+        logger.info(f'conditioning image column defaulting to {conditioning_image_column}')
     else:
         conditioning_image_column = args.conditioning_image_column
         if conditioning_image_column not in column_names:
             raise ValueError(
                 f"`--conditioning_image_column` value '{args.conditioning_image_column}' not found in dataset columns. "
                 f"Dataset columns are: {', '.join(column_names)}")
 
@@ -735,100 +605,78 @@
         for caption in examples[caption_column]:
             if random.random() < args.proportion_empty_prompts:
                 captions.append('')
             elif isinstance(caption, str):
                 captions.append(caption)
             elif isinstance(caption, (list, np.ndarray)):
                 # take a random caption if there are multiple
-                captions.append(
-                    random.choice(caption) if is_train else caption[0])
+                captions.append(random.choice(caption) if is_train else caption[0])
             else:
                 raise ValueError(
-                    f'Caption column `{caption_column}` should contain either strings or lists of strings.'
-                )
+                    f'Caption column `{caption_column}` should contain either strings or lists of strings.')
         inputs = tokenizer(
-            captions,
-            max_length=tokenizer.model_max_length,
-            padding='max_length',
-            truncation=True,
-            return_tensors='pt')
+            captions, max_length=tokenizer.model_max_length, padding='max_length', truncation=True, return_tensors='pt')
         return inputs.input_ids
 
     image_transforms = transforms.Compose([
-        transforms.Resize(
-            args.resolution,
-            interpolation=transforms.InterpolationMode.BILINEAR),
+        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
         transforms.CenterCrop(args.resolution),
         transforms.ToTensor(),
         transforms.Normalize([0.5], [0.5]),
     ])
 
     conditioning_image_transforms = transforms.Compose([
-        transforms.Resize(
-            args.resolution,
-            interpolation=transforms.InterpolationMode.BILINEAR),
+        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
         transforms.CenterCrop(args.resolution),
         transforms.ToTensor(),
     ])
 
     def preprocess_train(examples):
         images = [image.convert('RGB') for image in examples[image_column]]
         images = [image_transforms(image) for image in images]
 
-        conditioning_images = [
-            image.convert('RGB')
-            for image in examples[conditioning_image_column]
-        ]
-        conditioning_images = [
-            conditioning_image_transforms(image)
-            for image in conditioning_images
-        ]
+        conditioning_images = [image.convert('RGB') for image in examples[conditioning_image_column]]
+        conditioning_images = [conditioning_image_transforms(image) for image in conditioning_images]
 
         examples['pixel_values'] = images
         examples['conditioning_pixel_values'] = conditioning_images
         examples['input_ids'] = tokenize_captions(examples)
 
         return examples
 
     with accelerator.main_process_first():
         if args.max_train_samples is not None:
-            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(
-                range(args.max_train_samples))
+            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(range(args.max_train_samples))
         # Set the training transforms
         train_dataset = dataset['train'].with_transform(preprocess_train)
 
     return train_dataset
 
 
 def collate_fn(examples):
-    pixel_values = torch.stack(
-        [example['pixel_values'] for example in examples])
-    pixel_values = pixel_values.to(
-        memory_format=torch.contiguous_format).float()
-
-    conditioning_pixel_values = torch.stack(
-        [example['conditioning_pixel_values'] for example in examples])
-    conditioning_pixel_values = conditioning_pixel_values.to(
-        memory_format=torch.contiguous_format).float()
+    pixel_values = torch.stack([example['pixel_values'] for example in examples])
+    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
+
+    conditioning_pixel_values = torch.stack([example['conditioning_pixel_values'] for example in examples])
+    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()
 
     input_ids = torch.stack([example['input_ids'] for example in examples])
 
     return {
         'pixel_values': pixel_values,
         'conditioning_pixel_values': conditioning_pixel_values,
         'input_ids': input_ids,
     }
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
@@ -854,51 +702,38 @@
     # Handle the repository creation
     if accelerator.is_main_process:
         if args.output_dir is not None:
             os.makedirs(args.output_dir, exist_ok=True)
 
     # Load the tokenizer
     if args.tokenizer_name:
-        tokenizer = AutoTokenizer.from_pretrained(
-            args.tokenizer_name, revision=args.revision, use_fast=False)
+        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)
     elif args.pretrained_model_name_or_path:
         tokenizer = AutoTokenizer.from_pretrained(
             args.pretrained_model_name_or_path,
             subfolder='tokenizer',
             revision=args.revision,
             use_fast=False,
         )
 
     # import correct text encoder class
-    text_encoder_cls = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     text_encoder = text_encoder_cls.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
     vae = AutoencoderKL.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='vae',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='vae', revision=args.revision, variant=args.variant)
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
     if args.controlnet_model_name_or_path:
         logger.info('Loading existing controlnet weights')
-        controlnet = ControlNetModel.from_pretrained(
-            args.controlnet_model_name_or_path)
+        controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path)
     else:
         logger.info('Initializing controlnet weights from unet')
         controlnet = ControlNetModel.from_unet(unet)
 
     # `accelerate` 0.16.0 will have better support for customized saving
     if version.parse(accelerate.__version__) >= version.parse('0.16.0'):
         # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
@@ -917,16 +752,15 @@
 
         def load_model_hook(models, input_dir):
             while len(models) > 0:
                 # pop models so that they are not loaded again
                 model = models.pop()
 
                 # load diffusers style into model
-                load_model = ControlNetModel.from_pretrained(
-                    input_dir, subfolder='controlnet')
+                load_model = ControlNetModel.from_pretrained(input_dir, subfolder='controlnet')
                 model.register_to_config(**load_model.config)
 
                 model.load_state_dict(load_model.state_dict())
                 del load_model
 
         accelerator.register_save_state_pre_hook(save_model_hook)
         accelerator.register_load_state_pre_hook(load_model_hook)
@@ -941,55 +775,47 @@
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
                 logger.warn(
                     'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training,'
                     ' please update xFormers to at least 0.0.17. '
-                    'See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                    'See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
             controlnet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     if args.gradient_checkpointing:
         controlnet.enable_gradient_checkpointing()
 
     # Check that all trainable models are in full precision
     low_precision_error_string = (
         ' Please make sure to always have all model weights in full float32 precision when starting training - even if'
-        ' doing mixed precision training, copy of the weights should still be float32.'
-    )
+        ' doing mixed precision training, copy of the weights should still be float32.')
 
     if accelerator.unwrap_model(controlnet).dtype != torch.float32:
         raise ValueError(
-            f'Controlnet loaded as datatype {accelerator.unwrap_model(controlnet).dtype}. {low_precision_error_string}'
-        )
+            f'Controlnet loaded as datatype {accelerator.unwrap_model(controlnet).dtype}. {low_precision_error_string}')
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
-            raise ImportError(
-                'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-            )
+            raise ImportError('To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
         optimizer_class = bnb.optim.AdamW8bit
     else:
         optimizer_class = torch.optim.AdamW
 
     # Optimizer creation
     params_to_optimize = controlnet.parameters()
@@ -1009,32 +835,31 @@
         collate_fn=collate_fn,
         batch_size=args.train_batch_size,
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
         num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
         num_training_steps=args.max_train_steps * accelerator.num_processes,
         num_cycles=args.lr_num_cycles,
         power=args.lr_power,
     )
 
     # Prepare everything with our `accelerator`.
-    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-        controlnet, optimizer, train_dataloader, lr_scheduler)
+    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(controlnet, optimizer, train_dataloader,
+                                                                                lr_scheduler)
 
     # For mixed precision training we cast the text_encoder and vae weights to half-precision
     # as these models are only used for inference, keeping weights in full precision is not required.
     weight_dtype = torch.float32
     if accelerator.mixed_precision == 'fp16':
         weight_dtype = torch.float16
     elif accelerator.mixed_precision == 'bf16':
@@ -1042,48 +867,41 @@
 
     # Move vae, unet and text_encoder to device and cast to weight_dtype
     vae.to(accelerator.device, dtype=weight_dtype)
     unet.to(accelerator.device, dtype=weight_dtype)
     text_encoder.to(accelerator.device, dtype=weight_dtype)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         tracker_config = dict(vars(args))
 
         # tensorboard cannot handle list types for config
         tracker_config.pop('validation_prompt')
         tracker_config.pop('validation_image')
 
-        accelerator.init_trackers(
-            args.tracker_project_name, config=tracker_config)
+        accelerator.init_trackers(args.tracker_project_name, config=tracker_config)
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num batches each epoch = {len(train_dataloader)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1093,16 +911,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -1120,38 +937,32 @@
     )
 
     image_logs = None
     for epoch in range(first_epoch, args.num_train_epochs):
         for step, batch in enumerate(train_dataloader):
             with accelerator.accumulate(controlnet):
                 # Convert images to latent space
-                latents = vae.encode(batch['pixel_values'].to(
-                    dtype=weight_dtype)).latent_dist.sample()
+                latents = vae.encode(batch['pixel_values'].to(dtype=weight_dtype)).latent_dist.sample()
                 latents = latents * vae.config.scaling_factor
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(latents)
                 bsz = latents.shape[0]
                 # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=latents.device)
+                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=latents.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the latents according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_latents = noise_scheduler.add_noise(
-                    latents, noise, timesteps)
+                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
 
                 # Get the text embedding for conditioning
                 encoder_hidden_states = text_encoder(batch['input_ids'])[0]
 
-                controlnet_image = batch['conditioning_pixel_values'].to(
-                    dtype=weight_dtype)
+                controlnet_image = batch['conditioning_pixel_values'].to(dtype=weight_dtype)
 
                 down_block_res_samples, mid_block_res_sample = controlnet(
                     noisy_latents,
                     timesteps,
                     encoder_hidden_states=encoder_hidden_states,
                     controlnet_cond=controlnet_image,
                     return_dict=False,
@@ -1159,86 +970,64 @@
 
                 # Predict the noise residual
                 model_pred = unet(
                     noisy_latents,
                     timesteps,
                     encoder_hidden_states=encoder_hidden_states,
                     down_block_additional_residuals=[
-                        sample.to(dtype=weight_dtype)
-                        for sample in down_block_res_samples
+                        sample.to(dtype=weight_dtype) for sample in down_block_res_samples
                     ],
-                    mid_block_additional_residual=mid_block_res_sample.to(
-                        dtype=weight_dtype),
+                    mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),
                 ).sample
 
                 # Get the target for loss depending on the prediction type
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        latents, noise, timesteps)
+                    target = noise_scheduler.get_velocity(latents, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
-                loss = F.mse_loss(
-                    model_pred.float(), target.float(), reduction='mean')
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
+                loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
 
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
                     params_to_clip = controlnet.parameters()
-                    accelerator.clip_grad_norm_(params_to_clip,
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad(set_to_none=args.set_grads_to_none)
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint,
                             # we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
                         logger.info(f'Saved state to {save_path}')
 
                     if args.validation_prompt is not None and global_step % args.validation_steps == 0:
                         image_logs = log_validation(
                             vae,
                             text_encoder,
@@ -1247,18 +1036,15 @@
                             controlnet,
                             args,
                             accelerator,
                             weight_dtype,
                             global_step,
                         )
 
-            logs = {
-                'loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
             accelerator.log(logs, step=global_step)
 
             if global_step >= args.max_train_steps:
                 break
 
     # Create the pipeline using using the trained modules and save it.
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_controlnet_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image_sdxl.py`

 * *Files 8% similar despite different names*

```diff
@@ -7,768 +7,451 @@
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
+# limitations under the License.
+"""Fine-tuning script for Stable Diffusion XL for text2image."""
 
 import argparse
 import functools
 import gc
 import logging
 import math
 import os
 import random
 import shutil
 from pathlib import Path
 
 import accelerate
+import datasets
 import diffusers
 import numpy as np
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import ProjectConfiguration, set_seed
 from datasets import load_dataset
-from diffusers import (AutoencoderKL, ControlNetModel, DDPMScheduler,
-                       StableDiffusionXLControlNetPipeline,
-                       UNet2DConditionModel, UniPCMultistepScheduler)
+from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionXLPipeline, UNet2DConditionModel
 from diffusers.optimization import get_scheduler
-from diffusers.utils import is_wandb_available, make_image_grid
+from diffusers.training_utils import EMAModel, compute_snr
+from diffusers.utils import is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from modelscope import AutoTokenizer, MsDataset
 from packaging import version
 from PIL import Image
 from torchvision import transforms
+from torchvision.transforms.functional import crop
 from tqdm.auto import tqdm
 from transformers import PretrainedConfig
 
 from swift import push_to_hub, snapshot_download
 
-if is_wandb_available():
-    import wandb
-
 logger = get_logger(__name__)
 
+DATASET_NAME_MAPPING = {
+    'AI-ModelScope/pokemon-blip-captions': ('text', 'image:FILE'),
+}
 
-def log_validation(vae, unet, controlnet, args, accelerator, weight_dtype,
-                   step):
-    logger.info('Running validation... ')
-
-    controlnet = accelerator.unwrap_model(controlnet)
-
-    pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(
-        args.pretrained_model_name_or_path,
-        vae=vae,
-        unet=unet,
-        controlnet=controlnet,
-        revision=args.revision,
-        variant=args.variant,
-        torch_dtype=weight_dtype,
-    )
-    pipeline.scheduler = UniPCMultistepScheduler.from_config(
-        pipeline.scheduler.config)
-    pipeline = pipeline.to(accelerator.device)
-    pipeline.set_progress_bar_config(disable=True)
-
-    if args.enable_xformers_memory_efficient_attention:
-        pipeline.enable_xformers_memory_efficient_attention()
-
-    if args.seed is None:
-        generator = None
-    else:
-        generator = torch.Generator(device=accelerator.device).manual_seed(
-            args.seed)
-
-    if len(args.validation_image) == len(args.validation_prompt):
-        validation_images = args.validation_image
-        validation_prompts = args.validation_prompt
-    elif len(args.validation_image) == 1:
-        validation_images = args.validation_image * len(args.validation_prompt)
-        validation_prompts = args.validation_prompt
-    elif len(args.validation_prompt) == 1:
-        validation_images = args.validation_image
-        validation_prompts = args.validation_prompt * len(
-            args.validation_image)
-    else:
-        raise ValueError(
-            'number of `args.validation_image` and `args.validation_prompt` should be checked in `parse_args`'
-        )
-
-    image_logs = []
-
-    for validation_prompt, validation_image in zip(validation_prompts,
-                                                   validation_images):
-        validation_image = Image.open(validation_image).convert('RGB')
-        validation_image = validation_image.resize(
-            (args.resolution, args.resolution))
-
-        images = []
-
-        for _ in range(args.num_validation_images):
-            with torch.autocast('cuda'):
-                image = pipeline(
-                    prompt=validation_prompt,
-                    image=validation_image,
-                    num_inference_steps=20,
-                    generator=generator).images[0]
-            images.append(image)
-
-        image_logs.append({
-            'validation_image': validation_image,
-            'images': images,
-            'validation_prompt': validation_prompt
-        })
-
-    for tracker in accelerator.trackers:
-        if tracker.name == 'tensorboard':
-            for log in image_logs:
-                images = log['images']
-                validation_prompt = log['validation_prompt']
-                validation_image = log['validation_image']
-
-                formatted_images = []
-
-                formatted_images.append(np.asarray(validation_image))
-
-                for image in images:
-                    formatted_images.append(np.asarray(image))
-
-                formatted_images = np.stack(formatted_images)
-
-                tracker.writer.add_images(
-                    validation_prompt,
-                    formatted_images,
-                    step,
-                    dataformats='NHWC')
-        elif tracker.name == 'wandb':
-            formatted_images = []
-
-            for log in image_logs:
-                images = log['images']
-                validation_prompt = log['validation_prompt']
-                validation_image = log['validation_image']
-
-                formatted_images.append(
-                    wandb.Image(
-                        validation_image, caption='Controlnet conditioning'))
-
-                for image in images:
-                    image = wandb.Image(image, caption=validation_prompt)
-                    formatted_images.append(image)
-
-            tracker.log({'validation': formatted_images})
-        else:
-            logger.warn(f'image logging not implemented for {tracker.name}')
-
-        del pipeline
-        gc.collect()
-        torch.cuda.empty_cache()
-
-        return image_logs
-
-
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str,
-        revision: str,
-        subfolder: str = 'text_encoder'):
-    text_encoder_config = PretrainedConfig.from_pretrained(
-        pretrained_model_name_or_path, subfolder=subfolder, revision=revision)
-    model_class = text_encoder_config.architectures[0]
 
-    if model_class == 'CLIPTextModel':
-        from transformers import CLIPTextModel
-
-        return CLIPTextModel
-    elif model_class == 'CLIPTextModelWithProjection':
-        from transformers import CLIPTextModelWithProjection
-
-        return CLIPTextModelWithProjection
-    else:
-        raise ValueError(f'{model_class} is not supported.')
-
-
-def save_model_card(repo_id: str,
-                    image_logs=None,
-                    base_model=str,
-                    repo_folder=None):
+def save_model_card(
+    repo_id: str,
+    images=None,
+    validation_prompt=None,
+    base_model=str,
+    dataset_name=str,
+    repo_folder=None,
+    vae_path=None,
+):
     img_str = ''
-    if image_logs is not None:
-        img_str = 'You can find some example images below.\n'
-        for i, log in enumerate(image_logs):
-            images = log['images']
-            validation_prompt = log['validation_prompt']
-            validation_image = log['validation_image']
-            validation_image.save(
-                os.path.join(repo_folder, 'image_control.png'))
-            img_str += f'prompt: {validation_prompt}\n'
-            images = [validation_image] + images
-            make_image_grid(images, 1, len(images)).save(
-                os.path.join(repo_folder, f'images_{i}.png'))
-            img_str += f'![images_{i})](./images_{i}.png)\n'
+    for i, image in enumerate(images):
+        image.save(os.path.join(repo_folder, f'image_{i}.png'))
+        img_str += f'![img_{i}](./image_{i}.png)\n'
 
     yaml = f"""
 ---
-license: openrail++
+license: creativeml-openrail-m
 base_model: {base_model}
+dataset: {dataset_name}
 tags:
 - stable-diffusion-xl
 - stable-diffusion-xl-diffusers
 - text-to-image
 - diffusers
-- controlnet
 inference: true
 ---
     """
     model_card = f"""
-# controlnet-{repo_id}
+# Text-to-image finetuning - {repo_id}
 
-These are controlnet weights trained on {base_model} with new type of conditioning.
+This pipeline was finetuned from **{base_model}** on the **{args.dataset_name}** dataset. Below are some example images
+generated with the finetuned pipeline using the following prompt: {validation_prompt}: \n
 {img_str}
-"""
 
+Special VAE used for training: {vae_path}.
+"""
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str,
+                                               revision: str,
+                                               subfolder: str = 'text_encoder'):
+    text_encoder_config = PretrainedConfig.from_pretrained(
+        pretrained_model_name_or_path, subfolder=subfolder, revision=revision)
+    model_class = text_encoder_config.architectures[0]
+
+    if model_class == 'CLIPTextModel':
+        from transformers import CLIPTextModel
+
+        return CLIPTextModel
+    elif model_class == 'CLIPTextModelWithProjection':
+        from transformers import CLIPTextModelWithProjection
+
+        return CLIPTextModelWithProjection
+    else:
+        raise ValueError(f'{model_class} is not supported.')
+
+
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a ControlNet training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a training script.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--pretrained_vae_model_name_or_path',
         type=str,
         default=None,
-        help='Path to an improved VAE to stabilize training. '
-        'For more details check out: https://github.com/huggingface/diffusers/pull/4038.',
+        help='Path to pretrained VAE model with better numerical stability. \
+        More details: https://github.com/huggingface/diffusers/pull/4038.',
     )
     parser.add_argument(
-        '--controlnet_model_name_or_path',
+        '--revision',
         type=str,
         default=None,
-        help=
-        'Path to pretrained controlnet model or model identifier from huggingface.co/models or modelscope.cn/models.'
-        ' If not specified controlnet weights are initialized from unet.',
+        required=False,
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
-        '--revision',
+        '--dataset_name',
         type=str,
         default=None,
-        required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help=('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
     )
     parser.add_argument(
-        '--tokenizer_name',
+        '--dataset_config_name',
         type=str,
         default=None,
-        help='Pretrained tokenizer name or path if not the same as model_name',
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
-        '--output_dir',
+        '--train_data_dir',
         type=str,
-        default='controlnet-model',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        default=None,
+        help=('A folder containing the training data. Folder contents must follow the structure described in'
+              ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
+              ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'),
     )
     parser.add_argument(
-        '--cache_dir',
+        '--image_column', type=str, default='image:FILE', help='The column of the dataset containing an image.')
+    parser.add_argument(
+        '--caption_column',
         type=str,
-        default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        default='text',
+        help='The column of the dataset containing a caption or a list of captions.',
     )
     parser.add_argument(
-        '--seed',
-        type=int,
+        '--validation_prompt',
+        type=str,
         default=None,
-        help='A seed for reproducible training.')
+        help='A prompt that is used during validation to verify that the model is learning.',
+    )
     parser.add_argument(
-        '--resolution',
+        '--num_validation_images',
         type=int,
-        default=512,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        default=4,
+        help='Number of images that should be generated during validation with `validation_prompt`.',
     )
     parser.add_argument(
-        '--crops_coords_top_left_h',
+        '--validation_epochs',
         type=int,
-        default=0,
-        help=
-        ('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'
-         ),
+        default=1,
+        help=('Run fine-tuning validation every X epochs. The validation process consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`.'),
     )
     parser.add_argument(
-        '--crops_coords_top_left_w',
+        '--max_train_samples',
         type=int,
+        default=None,
+        help=('For debugging purposes or quicker training, truncate the number of training examples to this '
+              'value if set.'),
+    )
+    parser.add_argument(
+        '--proportion_empty_prompts',
+        type=float,
         default=0,
-        help=
-        ('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'
-         ),
+        help='Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).',
     )
     parser.add_argument(
-        '--train_batch_size',
+        '--output_dir',
+        type=str,
+        default='sdxl-model-finetuned',
+        help='The output directory where the model predictions and checkpoints will be written.',
+    )
+    parser.add_argument(
+        '--cache_dir',
+        type=str,
+        default=None,
+        help='The directory where the downloaded models and datasets will be stored.',
+    )
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
+    parser.add_argument(
+        '--resolution',
         type=int,
-        default=4,
-        help='Batch size (per device) for the training dataloader.')
-    parser.add_argument('--num_train_epochs', type=int, default=1)
+        default=1024,
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
+    )
+    parser.add_argument(
+        '--center_crop',
+        default=False,
+        action='store_true',
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
+    )
+    parser.add_argument(
+        '--random_flip',
+        action='store_true',
+        help='whether to randomly flip images horizontally',
+    )
+    parser.add_argument(
+        '--train_batch_size', type=int, default=16, help='Batch size (per device) for the training dataloader.')
+    parser.add_argument('--num_train_epochs', type=int, default=100)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
-        help=
-        ('Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training '
-         'via `--resume_from_checkpoint`. '
-         'In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for '
-         'inference.'
-         'Using a checkpoint for inference requires separate loading of the original pipeline '
-         'and the individual checkpointed model components.'
-         'See https://huggingface.co/docs/diffusers/main/en/training/dreambooth'
-         '#performing-inference-using-a-saved-checkpoint for step by step'
-         'instructions.'),
+        help=('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
+              ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
+              ' training using `--resume_from_checkpoint`.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
-        default=5e-6,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        default=1e-4,
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
-        '--lr_num_cycles',
-        type=int,
-        default=1,
-        help=
-        'Number of hard resets of the lr in cosine_with_restarts scheduler.',
+        '--timestep_bias_strategy',
+        type=str,
+        default='none',
+        choices=['earlier', 'later', 'range', 'none'],
+        help=(
+            'The timestep bias strategy, which may help direct the model toward learning low or high frequency details.'
+            " Choices: ['earlier', 'later', 'range', 'none']."
+            " The default is 'none', which means no bias is applied, and training proceeds normally."
+            " The value of 'later' will increase the frequency of the model's final training timesteps."),
     )
     parser.add_argument(
-        '--lr_power',
+        '--timestep_bias_multiplier',
         type=float,
         default=1.0,
-        help='Power factor of the polynomial scheduler.')
-    parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
+        help=('The multiplier for the bias. Defaults to 1.0, which means no bias is applied.'
+              ' A value of 2.0 will double the weight of the bias, and a value of 0.5 will halve it.'),
+    )
     parser.add_argument(
-        '--dataloader_num_workers',
+        '--timestep_bias_begin',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
+        help=('When using `--timestep_bias_strategy=range`, the beginning (inclusive) timestep to bias.'
+              ' Defaults to zero, which equates to having no specific bias.'),
     )
     parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
+        '--timestep_bias_end',
+        type=int,
+        default=1000,
+        help=('When using `--timestep_bias_strategy=range`, the final timestep (inclusive) to bias.'
+              ' Defaults to 1000, which is the number of timesteps that Stable Diffusion is trained on.'),
+    )
     parser.add_argument(
-        '--adam_weight_decay',
+        '--timestep_bias_portion',
         type=float,
-        default=1e-2,
-        help='Weight decay to use.')
+        default=0.25,
+        help=('The portion of timesteps to bias. Defaults to 0.25, which 25% of timesteps will be biased.'
+              ' A value of 0.5 will bias one half of the timesteps. '
+              'The value provided for `--timestep_bias_strategy` determines'
+              ' whether the biased portions are in the earlier or later timesteps.'),
+    )
     parser.add_argument(
-        '--adam_epsilon',
+        '--snr_gamma',
         type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
-    parser.add_argument(
-        '--hub_model_id',
-        type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
-    )
-    parser.add_argument(
-        '--logging_dir',
-        type=str,
-        default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help='SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
+        'More details here: https://arxiv.org/abs/2303.09556.',
     )
+    parser.add_argument('--use_ema', action='store_true', help='Whether to use EMA model.')
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
-        '--report_to',
-        type=str,
-        default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
-    )
-    parser.add_argument(
-        '--mixed_precision',
-        type=str,
-        default=None,
-        choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
-    )
-    parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
-    parser.add_argument(
-        '--set_grads_to_none',
-        action='store_true',
-        help=
-        ('Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain'
-         ' behaviors, so disable this argument if it causes any problems. More info:'
-         ' https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'
-         ),
+        '--dataloader_num_workers',
+        type=int,
+        default=0,
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
     )
     parser.add_argument(
-        '--dataset_name',
-        type=str,
-        default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
-    )
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
-        '--dataset_config_name',
+        '--prediction_type',
         type=str,
         default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
+        help="The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or \
+        leave `None`. If left to `None` the default prediction type of the scheduler: \
+        `noise_scheduler.config.prediciton_type` is chosen.",
     )
     parser.add_argument(
-        '--train_data_dir',
+        '--hub_model_id',
         type=str,
         default=None,
-        help=
-        ('A folder containing the training data. Folder contents must follow the structure described in'
-         ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
-         ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'
-         ),
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
-        '--image_column',
-        type=str,
-        default='image',
-        help='The column of the dataset containing the target image.')
-    parser.add_argument(
-        '--conditioning_image_column',
-        type=str,
-        default='conditioning_image',
-        help=
-        'The column of the dataset containing the controlnet conditioning image.',
-    )
-    parser.add_argument(
-        '--caption_column',
+        '--logging_dir',
         type=str,
-        default='text',
-        help=
-        'The column of the dataset containing a caption or a list of captions.',
-    )
-    parser.add_argument(
-        '--max_train_samples',
-        type=int,
-        default=None,
-        help=
-        ('For debugging purposes or quicker training, truncate the number of training examples to this '
-         'value if set.'),
-    )
-    parser.add_argument(
-        '--proportion_empty_prompts',
-        type=float,
-        default=0,
-        help=
-        'Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).',
+        default='logs',
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
-        '--validation_prompt',
+        '--report_to',
         type=str,
-        default=None,
-        nargs='+',
-        help=
-        ('A set of prompts evaluated every `--validation_steps` and logged to `--report_to`.'
-         ' Provide either a matching number of `--validation_image`s, a single `--validation_image`'
-         ' to be used with all prompts, or a single prompt that will be used with all `--validation_image`s.'
-         ),
+        default='tensorboard',
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
-        '--validation_image',
+        '--mixed_precision',
         type=str,
         default=None,
-        nargs='+',
-        help=
-        ('A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`'
-         ' and logged to `--report_to`. Provide either a matching number of `--validation_prompt`s, a'
-         ' a single `--validation_prompt` to be used with all `--validation_image`s, or a single'
-         ' `--validation_image` that will be used with all `--validation_prompt`s.'
-         ),
-    )
-    parser.add_argument(
-        '--num_validation_images',
-        type=int,
-        default=4,
-        help=
-        'Number of images to be generated for each `--validation_image`, `--validation_prompt` pair',
-    )
-    parser.add_argument(
-        '--validation_steps',
-        type=int,
-        default=100,
-        help=
-        ('Run validation every X steps. Validation consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`'
-         ' and logging the images.'),
+        choices=['no', 'fp16', 'bf16'],
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
-        '--tracker_project_name',
-        type=str,
-        default='sd_xl_train_controlnet',
-        help=
-        ('The `project_name` argument passed to Accelerator.init_trackers for'
-         ' more information see '
-         'https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator'
-         ),
-    )
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
+    parser.add_argument('--noise_offset', type=float, default=0, help='The scale of noise offset.')
 
     if input_args is not None:
         args = parser.parse_args(input_args)
     else:
         args = parser.parse_args()
 
-    if args.dataset_name is None and args.train_data_dir is None:
-        raise ValueError(
-            'Specify either `--dataset_name` or `--train_data_dir`')
+    env_local_rank = int(os.environ.get('LOCAL_RANK', -1))
+    if env_local_rank != -1 and env_local_rank != args.local_rank:
+        args.local_rank = env_local_rank
 
-    if args.dataset_name is not None and args.train_data_dir is not None:
-        raise ValueError(
-            'Specify only one of `--dataset_name` or `--train_data_dir`')
+    # Sanity checks
+    if args.dataset_name is None and args.train_data_dir is None:
+        raise ValueError('Need either a dataset name or a training folder.')
 
     if args.proportion_empty_prompts < 0 or args.proportion_empty_prompts > 1:
-        raise ValueError(
-            '`--proportion_empty_prompts` must be in the range [0, 1].')
-
-    if args.validation_prompt is not None and args.validation_image is None:
-        raise ValueError(
-            '`--validation_image` must be set if `--validation_prompt` is set')
-
-    if args.validation_prompt is None and args.validation_image is not None:
-        raise ValueError(
-            '`--validation_prompt` must be set if `--validation_image` is set')
-
-    if (args.validation_image is not None
-            and args.validation_prompt is not None
-            and len(args.validation_image) != 1
-            and len(args.validation_prompt) != 1
-            and len(args.validation_image) != len(args.validation_prompt)):
-        raise ValueError(
-            'Must provide either 1 `--validation_image`, 1 `--validation_prompt`,'
-            ' or the same number of `--validation_prompt`s and `--validation_image`s'
-        )
-
-    if args.resolution % 8 != 0:
-        raise ValueError(
-            '`--resolution` must be divisible by 8 for consistently sized encoded images '
-            'between the VAE and the controlnet encoder.')
+        raise ValueError('`--proportion_empty_prompts` must be in the range [0, 1].')
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
 
-    if args.controlnet_model_name_or_path and not os.path.exists(
-            args.controlnet_model_name_or_path):
-        args.controlnet_model_name_or_path = snapshot_download(
-            args.controlnet_model_name_or_path)
-
-    if args.pretrained_vae_model_name_or_path and not os.path.exists(
-            args.pretrained_vae_model_name_or_path):
-        args.pretrained_vae_model_name_or_path = snapshot_download(
-            args.pretrained_vae_model_name_or_path)
-
+    args.vae_base_model_id = args.pretrained_vae_model_name_or_path
+    if args.pretrained_vae_model_name_or_path and not os.path.exists(args.pretrained_vae_model_name_or_path):
+        args.pretrained_vae_model_name_or_path = snapshot_download(args.pretrained_vae_model_name_or_path)
     return args
 
 
-def get_train_dataset(args, accelerator):
-    # Get the datasets: you can either provide your own training and evaluation files (see below)
-    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).
-
-    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
-    # download the dataset.
-    if args.dataset_name is not None:
-        # Downloading and loading a dataset from the hub.
-        dataset = MsDataset.load(
-            args.dataset_name,
-            args.dataset_config_name,
-        )
-        if isinstance(dataset, dict):
-            dataset = {
-                key: value.to_hf_dataset()
-                for key, value in dataset.items()
-            }
-        else:
-            dataset = {'train': dataset.to_hf_dataset()}
-    else:
-        if args.train_data_dir is not None:
-            dataset = load_dataset(
-                args.train_data_dir,
-                cache_dir=args.cache_dir,
-            )
-        # See more about loading custom images at
-        # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script
-
-    # Preprocessing the datasets.
-    # We need to tokenize inputs and targets.
-    column_names = dataset['train'].column_names
-
-    # 6. Get the column names for input/target.
-    if args.image_column is None:
-        image_column = column_names[0]
-        logger.info(f'image column defaulting to {image_column}')
-    else:
-        image_column = args.image_column
-        if image_column not in column_names:
-            raise ValueError(
-                f"`--image_column` value '{args.image_column}' not found in dataset columns. "
-                f"Dataset columns are: {', '.join(column_names)}")
-
-    if args.caption_column is None:
-        caption_column = column_names[1]
-        logger.info(f'caption column defaulting to {caption_column}')
-    else:
-        caption_column = args.caption_column
-        if caption_column not in column_names:
-            raise ValueError(
-                f"`--caption_column` value '{args.caption_column}' not found in dataset columns. "
-                f"Dataset columns are: {', '.join(column_names)}")
-
-    if args.conditioning_image_column is None:
-        conditioning_image_column = column_names[2]
-        logger.info(
-            f'conditioning image column defaulting to {conditioning_image_column}'
-        )
-    else:
-        conditioning_image_column = args.conditioning_image_column
-        if conditioning_image_column not in column_names:
-            raise ValueError(
-                f"`--conditioning_image_column` value '{args.conditioning_image_column}' not found in dataset columns."
-                f" Dataset columns are: {', '.join(column_names)}")
-
-    with accelerator.main_process_first():
-        train_dataset = dataset['train'].shuffle(seed=args.seed)
-        if args.max_train_samples is not None:
-            train_dataset = train_dataset.select(range(args.max_train_samples))
-    return train_dataset
-
-
 # Adapted from pipelines.StableDiffusionXLPipeline.encode_prompt
-def encode_prompt(prompt_batch,
-                  text_encoders,
-                  tokenizers,
-                  proportion_empty_prompts,
-                  is_train=True):
+def encode_prompt(batch, text_encoders, tokenizers, proportion_empty_prompts, caption_column, is_train=True):
     prompt_embeds_list = []
+    prompt_batch = batch[caption_column]
 
     captions = []
     for caption in prompt_batch:
         if random.random() < proportion_empty_prompts:
             captions.append('')
         elif isinstance(caption, str):
             captions.append(caption)
@@ -796,116 +479,101 @@
             prompt_embeds = prompt_embeds.hidden_states[-2]
             bs_embed, seq_len, _ = prompt_embeds.shape
             prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)
             prompt_embeds_list.append(prompt_embeds)
 
     prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)
     pooled_prompt_embeds = pooled_prompt_embeds.view(bs_embed, -1)
-    return prompt_embeds, pooled_prompt_embeds
-
+    return {'prompt_embeds': prompt_embeds.cpu(), 'pooled_prompt_embeds': pooled_prompt_embeds.cpu()}
 
-def prepare_train_dataset(args, dataset, accelerator):
-    image_transforms = transforms.Compose([
-        transforms.Resize(
-            args.resolution,
-            interpolation=transforms.InterpolationMode.BILINEAR),
-        transforms.CenterCrop(args.resolution),
-        transforms.ToTensor(),
-        transforms.Normalize([0.5], [0.5]),
-    ])
-
-    conditioning_image_transforms = transforms.Compose([
-        transforms.Resize(
-            args.resolution,
-            interpolation=transforms.InterpolationMode.BILINEAR),
-        transforms.CenterCrop(args.resolution),
-        transforms.ToTensor(),
-    ])
 
-    def preprocess_train(examples):
-        images = [
-            image.convert('RGB') for image in examples[args.image_column]
-        ]
-        images = [image_transforms(image) for image in images]
-
-        conditioning_images = [
-            image.convert('RGB')
-            for image in examples[args.conditioning_image_column]
-        ]
-        conditioning_images = [
-            conditioning_image_transforms(image)
-            for image in conditioning_images
-        ]
-
-        examples['pixel_values'] = images
-        examples['conditioning_pixel_values'] = conditioning_images
-
-        return examples
+def compute_vae_encodings(batch, vae):
+    images = batch.pop('pixel_values')
+    pixel_values = torch.stack(list(images))
+    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
+    pixel_values = pixel_values.to(vae.device, dtype=vae.dtype)
 
-    with accelerator.main_process_first():
-        dataset = dataset.with_transform(preprocess_train)
+    with torch.no_grad():
+        model_input = vae.encode(pixel_values).latent_dist.sample()
+    model_input = model_input * vae.config.scaling_factor
+    return {'model_input': model_input.cpu()}
+
+
+def generate_timestep_weights(args, num_timesteps):
+    weights = torch.ones(num_timesteps)
+
+    # Determine the indices to bias
+    num_to_bias = int(args.timestep_bias_portion * num_timesteps)
+
+    if args.timestep_bias_strategy == 'later':
+        bias_indices = slice(-num_to_bias, None)
+    elif args.timestep_bias_strategy == 'earlier':
+        bias_indices = slice(0, num_to_bias)
+    elif args.timestep_bias_strategy == 'range':
+        # Out of the possible 1000 timesteps, we might want to focus on eg. 200-500.
+        range_begin = args.timestep_bias_begin
+        range_end = args.timestep_bias_end
+        if range_begin < 0:
+            raise ValueError(
+                'When using the range strategy for timestep bias, you must provide a beginning timestep greater \
+                or equal to zero.')
+        if range_end > num_timesteps:
+            raise ValueError(
+                'When using the range strategy for timestep bias, you must provide an ending timestep smaller than \
+                the number of timesteps.')
+        bias_indices = slice(range_begin, range_end)
+    else:  # 'none' or any other string
+        return weights
+    if args.timestep_bias_multiplier <= 0:
+        return ValueError(
+            'The parameter --timestep_bias_multiplier is not intended to be used to disable the training of specific '
+            'timesteps.'
+            ' If it was intended to disable timestep bias, use `--timestep_bias_strategy none` instead.'
+            ' A timestep bias multiplier less than or equal to 0 is not allowed.')
 
-    return dataset
+    # Apply the bias
+    weights[bias_indices] *= args.timestep_bias_multiplier
 
+    # Normalize
+    weights /= weights.sum()
 
-def collate_fn(examples):
-    pixel_values = torch.stack(
-        [example['pixel_values'] for example in examples])
-    pixel_values = pixel_values.to(
-        memory_format=torch.contiguous_format).float()
-
-    conditioning_pixel_values = torch.stack(
-        [example['conditioning_pixel_values'] for example in examples])
-    conditioning_pixel_values = conditioning_pixel_values.to(
-        memory_format=torch.contiguous_format).float()
-
-    prompt_ids = torch.stack(
-        [torch.tensor(example['prompt_embeds']) for example in examples])
-
-    add_text_embeds = torch.stack(
-        [torch.tensor(example['text_embeds']) for example in examples])
-    add_time_ids = torch.stack(
-        [torch.tensor(example['time_ids']) for example in examples])
-
-    return {
-        'pixel_values': pixel_values,
-        'conditioning_pixel_values': conditioning_pixel_values,
-        'prompt_ids': prompt_ids,
-        'unet_added_conditions': {
-            'text_embeds': add_text_embeds,
-            'time_ids': add_time_ids
-        },
-    }
+    return weights
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
 
+    if args.report_to == 'wandb':
+        if not is_wandb_available():
+            raise ImportError('Make sure to install wandb if you want to use it for logging during training.')
+        import wandb
+
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
     )
     logger.info(accelerator.state, main_process_only=False)
     if accelerator.is_local_main_process:
+        datasets.utils.logging.set_verbosity_warning()
         transformers.utils.logging.set_verbosity_warning()
         diffusers.utils.logging.set_verbosity_info()
     else:
+        datasets.utils.logging.set_verbosity_error()
         transformers.utils.logging.set_verbosity_error()
         diffusers.utils.logging.set_verbosity_error()
 
     # If passed along, set the training seed now.
     if args.seed is not None:
         set_seed(args.seed)
 
@@ -925,309 +593,338 @@
         args.pretrained_model_name_or_path,
         subfolder='tokenizer_2',
         revision=args.revision,
         use_fast=False,
     )
 
     # import correct text encoder classes
-    text_encoder_cls_one = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls_one = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
     text_encoder_cls_two = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path,
-        args.revision,
-        subfolder='text_encoder_2')
+        args.pretrained_model_name_or_path, args.revision, subfolder='text_encoder_2')
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
+    # Check for terminal SNR in combination with SNR Gamma
     text_encoder_one = text_encoder_cls_one.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
     text_encoder_two = text_encoder_cls_two.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder_2',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder_2', revision=args.revision, variant=args.variant)
     vae_path = (
         args.pretrained_model_name_or_path
-        if args.pretrained_vae_model_name_or_path is None else
-        args.pretrained_vae_model_name_or_path)
+        if args.pretrained_vae_model_name_or_path is None else args.pretrained_vae_model_name_or_path)
     vae = AutoencoderKL.from_pretrained(
         vae_path,
-        subfolder='vae'
-        if args.pretrained_vae_model_name_or_path is None else None,
+        subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
         revision=args.revision,
         variant=args.variant,
     )
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
-    if args.controlnet_model_name_or_path:
-        logger.info('Loading existing controlnet weights')
-        controlnet = ControlNetModel.from_pretrained(
-            args.controlnet_model_name_or_path)
-    else:
-        logger.info('Initializing controlnet weights from unet')
-        controlnet = ControlNetModel.from_unet(unet)
+    # Freeze vae and text encoders.
+    vae.requires_grad_(False)
+    text_encoder_one.requires_grad_(False)
+    text_encoder_two.requires_grad_(False)
+    # Set unet as trainable.
+    unet.train()
+
+    # For mixed precision training we cast all non-trainable weigths to half-precision
+    # as these weights are only used for inference, keeping weights in full precision is not required.
+    weight_dtype = torch.float32
+    if accelerator.mixed_precision == 'fp16':
+        weight_dtype = torch.float16
+    elif accelerator.mixed_precision == 'bf16':
+        weight_dtype = torch.bfloat16
+
+    # Move unet, vae and text_encoder to device and cast to weight_dtype
+    # The VAE is in float32 to avoid NaN losses.
+    vae.to(accelerator.device, dtype=torch.float32)
+    text_encoder_one.to(accelerator.device, dtype=weight_dtype)
+    text_encoder_two.to(accelerator.device, dtype=weight_dtype)
+
+    # Create EMA for the unet.
+    if args.use_ema:
+        ema_unet = UNet2DConditionModel.from_pretrained(
+            args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
+        ema_unet = EMAModel(ema_unet.parameters(), model_cls=UNet2DConditionModel, model_config=ema_unet.config)
+
+    if args.enable_xformers_memory_efficient_attention:
+        if is_xformers_available():
+            import xformers
+
+            xformers_version = version.parse(xformers.__version__)
+            if xformers_version == version.parse('0.0.16'):
+                logger.warn(
+                    'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training,'
+                    ' please update xFormers to at least 0.0.17. '
+                    'See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
+            unet.enable_xformers_memory_efficient_attention()
+        else:
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     # `accelerate` 0.16.0 will have better support for customized saving
     if version.parse(accelerate.__version__) >= version.parse('0.16.0'):
         # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
         def save_model_hook(models, weights, output_dir):
             if accelerator.is_main_process:
-                i = len(weights) - 1
-
-                while len(weights) > 0:
-                    weights.pop()
-                    model = models[i]
+                if args.use_ema:
+                    ema_unet.save_pretrained(os.path.join(output_dir, 'unet_ema'))
 
-                    sub_dir = 'controlnet'
-                    model.save_pretrained(os.path.join(output_dir, sub_dir))
+                for i, model in enumerate(models):
+                    model.save_pretrained(os.path.join(output_dir, 'unet'))
 
-                    i -= 1
+                    # make sure to pop weight so that corresponding model is not saved again
+                    weights.pop()
 
         def load_model_hook(models, input_dir):
-            while len(models) > 0:
+            if args.use_ema:
+                load_model = EMAModel.from_pretrained(os.path.join(input_dir, 'unet_ema'), UNet2DConditionModel)
+                ema_unet.load_state_dict(load_model.state_dict())
+                ema_unet.to(accelerator.device)
+                del load_model
+
+            for i in range(len(models)):
                 # pop models so that they are not loaded again
                 model = models.pop()
 
                 # load diffusers style into model
-                load_model = ControlNetModel.from_pretrained(
-                    input_dir, subfolder='controlnet')
+                load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder='unet')
                 model.register_to_config(**load_model.config)
 
                 model.load_state_dict(load_model.state_dict())
                 del load_model
 
         accelerator.register_save_state_pre_hook(save_model_hook)
         accelerator.register_load_state_pre_hook(load_model_hook)
 
-    vae.requires_grad_(False)
-    unet.requires_grad_(False)
-    text_encoder_one.requires_grad_(False)
-    text_encoder_two.requires_grad_(False)
-    controlnet.train()
-
-    if args.enable_xformers_memory_efficient_attention:
-        if is_xformers_available():
-            import xformers
-
-            xformers_version = version.parse(xformers.__version__)
-            if xformers_version == version.parse('0.0.16'):
-                logger.warn(
-                    'xFormers 0.0.16 cannot be used for training in some GPUs. '
-                    'If you observe problems during training, please update xFormers to at least 0.0.17. '
-                    'See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
-            unet.enable_xformers_memory_efficient_attention()
-            controlnet.enable_xformers_memory_efficient_attention()
-        else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
-
     if args.gradient_checkpointing:
-        controlnet.enable_gradient_checkpointing()
         unet.enable_gradient_checkpointing()
 
-    # Check that all trainable models are in full precision
-    low_precision_error_string = (
-        ' Please make sure to always have all model weights in full float32 precision when starting training - even if'
-        ' doing mixed precision training, copy of the weights should still be float32.'
-    )
-
-    if accelerator.unwrap_model(controlnet).dtype != torch.float32:
-        raise ValueError(
-            f'Controlnet loaded as datatype {accelerator.unwrap_model(controlnet).dtype}. {low_precision_error_string}'
-        )
-
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
-            raise ImportError(
-                'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-            )
+            raise ImportError('To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
         optimizer_class = bnb.optim.AdamW8bit
     else:
         optimizer_class = torch.optim.AdamW
 
     # Optimizer creation
-    params_to_optimize = controlnet.parameters()
+    params_to_optimize = unet.parameters()
     optimizer = optimizer_class(
         params_to_optimize,
         lr=args.learning_rate,
         betas=(args.adam_beta1, args.adam_beta2),
         weight_decay=args.adam_weight_decay,
         eps=args.adam_epsilon,
     )
 
-    # For mixed precision training we cast the text_encoder and vae weights to half-precision
-    # as these models are only used for inference, keeping weights in full precision is not required.
-    weight_dtype = torch.float32
-    if accelerator.mixed_precision == 'fp16':
-        weight_dtype = torch.float16
-    elif accelerator.mixed_precision == 'bf16':
-        weight_dtype = torch.bfloat16
+    # Get the datasets: you can either provide your own training and evaluation files (see below)
+    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).
 
-    # Move vae, unet and text_encoder to device and cast to weight_dtype
-    # The VAE is in float32 to avoid NaN losses.
-    if args.pretrained_vae_model_name_or_path is not None:
-        vae.to(accelerator.device, dtype=weight_dtype)
+    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
+    # download the dataset.
+    def path_to_img(example):
+        example['image'] = Image.open(example['image:FILE'])
+        return example
+
+    if args.dataset_name is not None:
+        # Downloading and loading a dataset from the hub.
+        dataset = MsDataset.load(
+            args.dataset_name,
+            args.dataset_config_name,
+            data_dir=args.train_data_dir,
+        )
+        if isinstance(dataset, dict):
+            dataset = {key: value.to_hf_dataset() for key, value in dataset.items()}
+        else:
+            dataset = {'train': dataset.to_hf_dataset()}
     else:
-        vae.to(accelerator.device, dtype=torch.float32)
-    unet.to(accelerator.device, dtype=weight_dtype)
-    text_encoder_one.to(accelerator.device, dtype=weight_dtype)
-    text_encoder_two.to(accelerator.device, dtype=weight_dtype)
+        data_files = {}
+        if args.train_data_dir is not None:
+            data_files['train'] = os.path.join(args.train_data_dir, '**')
+        dataset = load_dataset(
+            'imagefolder',
+            data_files=data_files,
+            cache_dir=args.cache_dir,
+        )
+        # See more about loading custom images at
+        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder
 
-    # Here, we compute not just the text embeddings but also the additional embeddings
-    # needed for the SD XL UNet to operate.
-    def compute_embeddings(batch,
-                           proportion_empty_prompts,
-                           text_encoders,
-                           tokenizers,
-                           is_train=True):
-        original_size = (args.resolution, args.resolution)
-        target_size = (args.resolution, args.resolution)
-        crops_coords_top_left = (args.crops_coords_top_left_h,
-                                 args.crops_coords_top_left_w)
-        prompt_batch = batch[args.caption_column]
-
-        prompt_embeds, pooled_prompt_embeds = encode_prompt(
-            prompt_batch, text_encoders, tokenizers, proportion_empty_prompts,
-            is_train)
-        add_text_embeds = pooled_prompt_embeds
-
-        # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids
-        add_time_ids = list(original_size + crops_coords_top_left
-                            + target_size)
-        add_time_ids = torch.tensor([add_time_ids])
-
-        prompt_embeds = prompt_embeds.to(accelerator.device)
-        add_text_embeds = add_text_embeds.to(accelerator.device)
-        add_time_ids = add_time_ids.repeat(len(prompt_batch), 1)
-        add_time_ids = add_time_ids.to(
-            accelerator.device, dtype=prompt_embeds.dtype)
-        unet_added_cond_kwargs = {
-            'text_embeds': add_text_embeds,
-            'time_ids': add_time_ids
-        }
+    # Preprocessing the datasets.
+    # We need to tokenize inputs and targets.
+    column_names = dataset['train'].column_names
+
+    # 6. Get the column names for input/target.
+    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)
+    if args.image_column is None:
+        image_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
+    else:
+        image_column = args.image_column
+        if image_column not in column_names:
+            raise ValueError(
+                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}")
+    if args.caption_column is None:
+        caption_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
+    else:
+        caption_column = args.caption_column
+        if caption_column not in column_names:
+            raise ValueError(
+                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}")
+    if image_column.endswith(':FILE'):
+        dataset['train'] = dataset['train'].map(path_to_img)
+        image_column = 'image'
 
-        return {'prompt_embeds': prompt_embeds, **unet_added_cond_kwargs}
+    # Preprocessing the datasets.
+    train_resize = transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR)
+    train_crop = transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution)
+    train_flip = transforms.RandomHorizontalFlip(p=1.0)
+    train_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])
+
+    def preprocess_train(examples):
+        images = [image.convert('RGB') for image in examples[image_column]]
+        # image aug
+        original_sizes = []
+        all_images = []
+        crop_top_lefts = []
+        for image in images:
+            original_sizes.append((image.height, image.width))
+            image = train_resize(image)
+            if args.center_crop:
+                y1 = max(0, int(round((image.height - args.resolution) / 2.0)))
+                x1 = max(0, int(round((image.width - args.resolution) / 2.0)))
+                image = train_crop(image)
+            else:
+                y1, x1, h, w = train_crop.get_params(image, (args.resolution, args.resolution))
+                image = crop(image, y1, x1, h, w)
+            if args.random_flip and random.random() < 0.5:
+                # flip
+                x1 = image.width - x1
+                image = train_flip(image)
+            crop_top_left = (y1, x1)
+            crop_top_lefts.append(crop_top_left)
+            image = train_transforms(image)
+            all_images.append(image)
+
+        examples['original_sizes'] = original_sizes
+        examples['crop_top_lefts'] = crop_top_lefts
+        examples['pixel_values'] = all_images
+        return examples
+
+    with accelerator.main_process_first():
+        if args.max_train_samples is not None:
+            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(range(args.max_train_samples))
+        # Set the training transforms
+        train_dataset = dataset['train'].with_transform(preprocess_train)
 
     # Let's first compute all the embeddings so that we can free up the text encoders
-    # from memory.
+    # from memory. We will pre-compute the VAE encodings too.
     text_encoders = [text_encoder_one, text_encoder_two]
     tokenizers = [tokenizer_one, tokenizer_two]
-    train_dataset = get_train_dataset(args, accelerator)
     compute_embeddings_fn = functools.partial(
-        compute_embeddings,
+        encode_prompt,
         text_encoders=text_encoders,
         tokenizers=tokenizers,
         proportion_empty_prompts=args.proportion_empty_prompts,
+        caption_column=args.caption_column,
     )
+    compute_vae_encodings_fn = functools.partial(compute_vae_encodings, vae=vae)
     with accelerator.main_process_first():
         from datasets.fingerprint import Hasher
 
         # fingerprint used by the cache for the other processes to load the result
         # details: https://github.com/huggingface/diffusers/pull/4038#discussion_r1266078401
         new_fingerprint = Hasher.hash(args)
+        new_fingerprint_for_vae = Hasher.hash('vae')
+        train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)
         train_dataset = train_dataset.map(
-            compute_embeddings_fn,
+            compute_vae_encodings_fn,
             batched=True,
-            new_fingerprint=new_fingerprint)
+            batch_size=args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps,
+            new_fingerprint=new_fingerprint_for_vae,
+        )
 
-    del text_encoders, tokenizers
+    del text_encoders, tokenizers, vae
     gc.collect()
     torch.cuda.empty_cache()
 
-    # Then get the training dataset ready to be passed to the dataloader.
-    train_dataset = prepare_train_dataset(args, train_dataset, accelerator)
+    def collate_fn(examples):
+        model_input = torch.stack([torch.tensor(example['model_input']) for example in examples])
+        original_sizes = [example['original_sizes'] for example in examples]
+        crop_top_lefts = [example['crop_top_lefts'] for example in examples]
+        prompt_embeds = torch.stack([torch.tensor(example['prompt_embeds']) for example in examples])
+        pooled_prompt_embeds = torch.stack([torch.tensor(example['pooled_prompt_embeds']) for example in examples])
+
+        return {
+            'model_input': model_input,
+            'prompt_embeds': prompt_embeds,
+            'pooled_prompt_embeds': pooled_prompt_embeds,
+            'original_sizes': original_sizes,
+            'crop_top_lefts': crop_top_lefts,
+        }
 
+    # DataLoaders creation:
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         shuffle=True,
         collate_fn=collate_fn,
         batch_size=args.train_batch_size,
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
-        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
-        num_training_steps=args.max_train_steps * accelerator.num_processes,
-        num_cycles=args.lr_num_cycles,
-        power=args.lr_power,
+        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
+        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
     )
 
     # Prepare everything with our `accelerator`.
-    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-        controlnet, optimizer, train_dataloader, lr_scheduler)
+    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                          lr_scheduler)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
-        tracker_config = dict(vars(args))
-
-        # tensorboard cannot handle list types for config
-        tracker_config.pop('validation_prompt')
-        tracker_config.pop('validation_image')
-
-        accelerator.init_trackers(
-            args.tracker_project_name, config=tracker_config)
+        accelerator.init_trackers('text2image-fine-tune-sdxl', config=vars(args))
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
-    logger.info(f'  Num batches each epoch = {len(train_dataloader)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1237,183 +934,286 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
             initial_global_step = global_step
             first_epoch = global_step // num_update_steps_per_epoch
+
     else:
         initial_global_step = 0
 
     progress_bar = tqdm(
         range(0, args.max_train_steps),
         initial=initial_global_step,
         desc='Steps',
         # Only show the progress bar once on each machine.
         disable=not accelerator.is_local_main_process,
     )
 
-    image_logs = None
     for epoch in range(first_epoch, args.num_train_epochs):
+        train_loss = 0.0
         for step, batch in enumerate(train_dataloader):
-            with accelerator.accumulate(controlnet):
-                # Convert images to latent space
-                if args.pretrained_vae_model_name_or_path is not None:
-                    pixel_values = batch['pixel_values'].to(dtype=weight_dtype)
-                else:
-                    pixel_values = batch['pixel_values']
-                latents = vae.encode(pixel_values).latent_dist.sample()
-                latents = latents * vae.config.scaling_factor
-                if args.pretrained_vae_model_name_or_path is None:
-                    latents = latents.to(weight_dtype)
-
+            with accelerator.accumulate(unet):
                 # Sample noise that we'll add to the latents
-                noise = torch.randn_like(latents)
-                bsz = latents.shape[0]
-
-                # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=latents.device)
-                timesteps = timesteps.long()
+                model_input = batch['model_input'].to(accelerator.device)
+                noise = torch.randn_like(model_input)
+                if args.noise_offset:
+                    # https://www.crosslabs.org//blog/diffusion-with-offset-noise
+                    noise += args.noise_offset * torch.randn(
+                        (model_input.shape[0], model_input.shape[1], 1, 1), device=model_input.device)
+
+                bsz = model_input.shape[0]
+                if args.timestep_bias_strategy == 'none':
+                    # Sample a random timestep for each image without bias.
+                    timesteps = torch.randint(
+                        0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=model_input.device)
+                else:
+                    # Sample a random timestep for each image, potentially biased by the timestep weights.
+                    # Biasing the timestep weights allows us to spend less time training irrelevant timesteps.
+                    weights = generate_timestep_weights(args, noise_scheduler.config.num_train_timesteps).to(
+                        model_input.device)
+                    timesteps = torch.multinomial(weights, bsz, replacement=True).long()
 
-                # Add noise to the latents according to the noise magnitude at each timestep
+                # Add noise to the model input according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_latents = noise_scheduler.add_noise(
-                    latents, noise, timesteps)
+                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)
 
-                # ControlNet conditioning.
-                controlnet_image = batch['conditioning_pixel_values'].to(
-                    dtype=weight_dtype)
-                down_block_res_samples, mid_block_res_sample = controlnet(
-                    noisy_latents,
-                    timesteps,
-                    encoder_hidden_states=batch['prompt_ids'],
-                    added_cond_kwargs=batch['unet_added_conditions'],
-                    controlnet_cond=controlnet_image,
-                    return_dict=False,
-                )
+                # time ids
+                def compute_time_ids(original_size, crops_coords_top_left):
+                    # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids
+                    target_size = (args.resolution, args.resolution)
+                    add_time_ids = list(original_size + crops_coords_top_left + target_size)
+                    add_time_ids = torch.tensor([add_time_ids])
+                    add_time_ids = add_time_ids.to(accelerator.device, dtype=weight_dtype)
+                    return add_time_ids
+
+                add_time_ids = torch.cat(
+                    [compute_time_ids(s, c) for s, c in zip(batch['original_sizes'], batch['crop_top_lefts'])])
 
                 # Predict the noise residual
+                unet_added_conditions = {'time_ids': add_time_ids}
+                prompt_embeds = batch['prompt_embeds'].to(accelerator.device)
+                pooled_prompt_embeds = batch['pooled_prompt_embeds'].to(accelerator.device)
+                unet_added_conditions.update({'text_embeds': pooled_prompt_embeds})
                 model_pred = unet(
-                    noisy_latents,
-                    timesteps,
-                    encoder_hidden_states=batch['prompt_ids'],
-                    added_cond_kwargs=batch['unet_added_conditions'],
-                    down_block_additional_residuals=[
-                        sample.to(dtype=weight_dtype)
-                        for sample in down_block_res_samples
-                    ],
-                    mid_block_additional_residual=mid_block_res_sample.to(
-                        dtype=weight_dtype),
-                ).sample
+                    noisy_model_input, timesteps, prompt_embeds, added_cond_kwargs=unet_added_conditions).sample
 
                 # Get the target for loss depending on the prediction type
+                if args.prediction_type is not None:
+                    # set prediction_type of scheduler if defined
+                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)
+
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        latents, noise, timesteps)
+                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)
+                elif noise_scheduler.config.prediction_type == 'sample':
+                    # We set the target to latents here, but the model_pred will return the noise sample prediction.
+                    target = model_input
+                    # We will have to subtract the noise residual from the prediction to get the target sample.
+                    model_pred = model_pred - noise
+                else:
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
+
+                if args.snr_gamma is None:
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
-                loss = F.mse_loss(
-                    model_pred.float(), target.float(), reduction='mean')
+                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
+                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.
+                    # This is discussed in Section 4.2 of the same paper.
+                    snr = compute_snr(noise_scheduler, timesteps)
+                    if noise_scheduler.config.prediction_type == 'v_prediction':
+                        # Velocity objective requires that we add one to SNR values before we divide by them.
+                        snr = snr + 1
+                    mse_loss_weights = (
+                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr)
+
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='none')
+                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
+                    loss = loss.mean()
+
+                # Gather the losses across all processes for logging (if we use distributed training).
+                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
+                train_loss += avg_loss.item() / args.gradient_accumulation_steps
 
+                # Backpropagate
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
-                    params_to_clip = controlnet.parameters()
-                    accelerator.clip_grad_norm_(params_to_clip,
-                                                args.max_grad_norm)
+                    params_to_clip = unet.parameters()
+                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
-                optimizer.zero_grad(set_to_none=args.set_grads_to_none)
+                optimizer.zero_grad()
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
+                accelerator.log({'train_loss': train_loss}, step=global_step)
+                train_loss = 0.0
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
-
-                            # before we save the new checkpoint,
-                            # we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
+
+                            # before we save the new checkpoint, we need to have at _most_ \
+                            # `checkpoints_total_limit - 1` checkpoints
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
                         logger.info(f'Saved state to {save_path}')
 
-                    if args.validation_prompt is not None and global_step % args.validation_steps == 0:
-                        image_logs = log_validation(vae, unet, controlnet,
-                                                    args, accelerator,
-                                                    weight_dtype, global_step)
-
-            logs = {
-                'loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'step_loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
-            accelerator.log(logs, step=global_step)
 
             if global_step >= args.max_train_steps:
                 break
 
-    # Create the pipeline using using the trained modules and save it.
+        if accelerator.is_main_process:
+            if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
+                logger.info(f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
+                            f' {args.validation_prompt}.')
+                if args.use_ema:
+                    # Store the UNet parameters temporarily and load the EMA parameters to perform inference.
+                    ema_unet.store(unet.parameters())
+                    ema_unet.copy_to(unet.parameters())
+
+                # create pipeline
+                vae = AutoencoderKL.from_pretrained(
+                    vae_path,
+                    subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
+                    revision=args.revision,
+                    variant=args.variant,
+                )
+                pipeline = StableDiffusionXLPipeline.from_pretrained(
+                    args.pretrained_model_name_or_path,
+                    vae=vae,
+                    unet=accelerator.unwrap_model(unet),
+                    revision=args.revision,
+                    variant=args.variant,
+                    torch_dtype=weight_dtype,
+                )
+                if args.prediction_type is not None:
+                    scheduler_args = {'prediction_type': args.prediction_type}
+                    pipeline.scheduler = pipeline.scheduler.from_config(pipeline.scheduler.config, **scheduler_args)
+
+                pipeline = pipeline.to(accelerator.device)
+                pipeline.set_progress_bar_config(disable=True)
+
+                # run inference
+                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
+                pipeline_args = {'prompt': args.validation_prompt}
+
+                with torch.cuda.amp.autocast():
+                    images = [
+                        pipeline(**pipeline_args, generator=generator, num_inference_steps=25).images[0]
+                        for _ in range(args.num_validation_images)
+                    ]
+
+                for tracker in accelerator.trackers:
+                    if tracker.name == 'tensorboard':
+                        np_images = np.stack([np.asarray(img) for img in images])
+                        tracker.writer.add_images('validation', np_images, epoch, dataformats='NHWC')
+                    if tracker.name == 'wandb':
+                        tracker.log({
+                            'validation': [
+                                wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
+                                for i, image in enumerate(images)
+                            ]
+                        })
+
+                del pipeline
+                torch.cuda.empty_cache()
+
     accelerator.wait_for_everyone()
     if accelerator.is_main_process:
-        controlnet = accelerator.unwrap_model(controlnet)
-        controlnet.save_pretrained(args.output_dir)
+        unet = accelerator.unwrap_model(unet)
+        if args.use_ema:
+            ema_unet.copy_to(unet.parameters())
+
+        # Serialize pipeline.
+        vae = AutoencoderKL.from_pretrained(
+            vae_path,
+            subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
+            revision=args.revision,
+            variant=args.variant,
+            torch_dtype=weight_dtype,
+        )
+        pipeline = StableDiffusionXLPipeline.from_pretrained(
+            args.pretrained_model_name_or_path,
+            unet=unet,
+            vae=vae,
+            revision=args.revision,
+            variant=args.variant,
+            torch_dtype=weight_dtype,
+        )
+        if args.prediction_type is not None:
+            scheduler_args = {'prediction_type': args.prediction_type}
+            pipeline.scheduler = pipeline.scheduler.from_config(pipeline.scheduler.config, **scheduler_args)
+        pipeline.save_pretrained(args.output_dir)
+
+        # run inference
+        images = []
+        if args.validation_prompt and args.num_validation_images > 0:
+            pipeline = pipeline.to(accelerator.device)
+            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
+            with torch.cuda.amp.autocast():
+                images = [
+                    pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]
+                    for _ in range(args.num_validation_images)
+                ]
+
+            for tracker in accelerator.trackers:
+                if tracker.name == 'tensorboard':
+                    np_images = np.stack([np.asarray(img) for img in images])
+                    tracker.writer.add_images('test', np_images, epoch, dataformats='NHWC')
+                if tracker.name == 'wandb':
+                    tracker.log({
+                        'test': [
+                            wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
+                            for i, image in enumerate(images)
+                        ]
+                    })
+
         if args.push_to_hub:
             save_model_card(
-                args.hub_model_id,
-                image_logs=image_logs,
+                repo_id=args.hub_model_id,
+                images=images,
+                validation_prompt=args.validation_prompt,
                 base_model=args.base_model_id,
+                dataset_name=args.dataset_name,
                 repo_folder=args.output_dir,
+                vae_path=args.vae_base_model_id,
+            )
+            push_to_hub(
+                args.hub_model_id,
+                args.output_dir,
+                args.hub_token,
             )
-            push_to_hub(args.hub_model_id, args.output_dir, args.hub_token)
 
     accelerator.end_training()
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_dreambooth.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_dreambooth.py`

 * *Files 5% similar despite different names*

```diff
@@ -29,16 +29,15 @@
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import ProjectConfiguration, set_seed
-from diffusers import (AutoencoderKL, DDPMScheduler, DiffusionPipeline,
-                       StableDiffusionPipeline, UNet2DConditionModel)
+from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, StableDiffusionPipeline, UNet2DConditionModel
 from diffusers.optimization import get_scheduler
 from diffusers.training_utils import compute_snr
 from diffusers.utils import is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from huggingface_hub.utils import insecure_hashlib
 from modelscope import AutoTokenizer
 from packaging import version
@@ -107,17 +106,16 @@
     args,
     accelerator,
     weight_dtype,
     global_step,
     prompt_embeds,
     negative_prompt_embeds,
 ):
-    logger.info(
-        f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
-        f' {args.validation_prompt}.')
+    logger.info(f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
+                f' {args.validation_prompt}.')
 
     pipeline_args = {}
 
     if vae is not None:
         pipeline_args['vae'] = vae
 
     if text_encoder is not None:
@@ -145,68 +143,57 @@
         if variance_type in ['learned', 'learned_range']:
             variance_type = 'fixed_small'
 
         scheduler_args['variance_type'] = variance_type
 
     module = importlib.import_module('diffusers')
     scheduler_class = getattr(module, args.validation_scheduler)
-    pipeline.scheduler = scheduler_class.from_config(pipeline.scheduler.config,
-                                                     **scheduler_args)
+    pipeline.scheduler = scheduler_class.from_config(pipeline.scheduler.config, **scheduler_args)
     pipeline = pipeline.to(accelerator.device)
     pipeline.set_progress_bar_config(disable=True)
 
     if args.pre_compute_text_embeddings:
         pipeline_args = {
             'prompt_embeds': prompt_embeds,
             'negative_prompt_embeds': negative_prompt_embeds,
         }
     else:
         pipeline_args = {'prompt': args.validation_prompt}
 
     # run inference
-    generator = None if args.seed is None else torch.Generator(
-        device=accelerator.device).manual_seed(args.seed)
+    generator = None if args.seed is None else torch.Generator(device=accelerator.device).manual_seed(args.seed)
     images = []
     if args.validation_images is None:
         for _ in range(args.num_validation_images):
             with torch.autocast('cuda'):
-                image = pipeline(
-                    **pipeline_args,
-                    num_inference_steps=25,
-                    generator=generator).images[0]
+                image = pipeline(**pipeline_args, num_inference_steps=25, generator=generator).images[0]
             images.append(image)
     else:
         for image in args.validation_images:
             image = Image.open(image)
-            image = pipeline(
-                **pipeline_args, image=image, generator=generator).images[0]
+            image = pipeline(**pipeline_args, image=image, generator=generator).images[0]
             images.append(image)
 
     for tracker in accelerator.trackers:
         if tracker.name == 'tensorboard':
             np_images = np.stack([np.asarray(img) for img in images])
-            tracker.writer.add_images(
-                'validation', np_images, global_step, dataformats='NHWC')
+            tracker.writer.add_images('validation', np_images, global_step, dataformats='NHWC')
         if tracker.name == 'wandb':
             tracker.log({
-                'validation': [
-                    wandb.Image(
-                        image, caption=f'{i}: {args.validation_prompt}')
-                    for i, image in enumerate(images)
-                ]
+                'validation':
+                [wandb.Image(image, caption=f'{i}: {args.validation_prompt}') for i, image in enumerate(images)]
             })
 
     del pipeline
     torch.cuda.empty_cache()
 
     return images
 
 
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str, revision: str):
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):
     text_encoder_config = PretrainedConfig.from_pretrained(
         pretrained_model_name_or_path,
         subfolder='text_encoder',
         revision=revision,
     )
     model_class = text_encoder_config.architectures[0]
 
@@ -223,38 +210,34 @@
 
         return T5EncoderModel
     else:
         raise ValueError(f'{model_class} is not supported.')
 
 
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a training script.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--tokenizer_name',
         type=str,
         default=None,
         help='Pretrained tokenizer name or path if not the same as model_name',
     )
@@ -279,329 +262,239 @@
         required=True,
         help='The prompt with identifier specifying the instance',
     )
     parser.add_argument(
         '--class_prompt',
         type=str,
         default=None,
-        help=
-        'The prompt to specify images in the same class as provided instance images.',
+        help='The prompt to specify images in the same class as provided instance images.',
     )
     parser.add_argument(
         '--with_prior_preservation',
         default=False,
         action='store_true',
         help='Flag to add prior preservation loss.',
     )
-    parser.add_argument(
-        '--prior_loss_weight',
-        type=float,
-        default=1.0,
-        help='The weight of prior preservation loss.')
+    parser.add_argument('--prior_loss_weight', type=float, default=1.0, help='The weight of prior preservation loss.')
     parser.add_argument(
         '--num_class_images',
         type=int,
         default=100,
-        help=
-        ('Minimal class images for prior preservation loss. If there are not enough images already present in'
-         ' class_data_dir, additional images will be sampled with class_prompt.'
-         ),
+        help=('Minimal class images for prior preservation loss. If there are not enough images already present in'
+              ' class_data_dir, additional images will be sampled with class_prompt.'),
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='dreambooth-model',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=512,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
         '--center_crop',
         default=False,
         action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
     )
     parser.add_argument(
         '--train_text_encoder',
         action='store_true',
-        help=
-        'Whether to train the text encoder. If set, the text encoder should be float32 precision.',
+        help='Whether to train the text encoder. If set, the text encoder should be float32 precision.',
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for the training dataloader.')
-    parser.add_argument(
-        '--sample_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for sampling images.')
+        '--train_batch_size', type=int, default=4, help='Batch size (per device) for the training dataloader.')
+    parser.add_argument('--sample_batch_size', type=int, default=4, help='Batch size (per device) for sampling images.')
     parser.add_argument('--num_train_epochs', type=int, default=1)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
         help=
         ('Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training '
          'via `--resume_from_checkpoint`. '
          'In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for '
          'inference.'
          'Using a checkpoint for inference requires separate loading of the original pipeline '
          'and the individual checkpointed model components.'
          'See https://huggingface.co/docs/diffusers/main/en/training/dreambooth'
-         '#performing-inference-using-a-saved-checkpoint for step by step instructions.'
-         ),
+         '#performing-inference-using-a-saved-checkpoint for step by step instructions.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
-        help=
-        ('Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`.'
-         ' See Accelerator::save_state '
-         'https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state'
-         ' for more details'),
+        help=('Max number of checkpoints to store. Passed as `total_limit` to the `Accelerator` `ProjectConfiguration`.'
+              ' See Accelerator::save_state '
+              'https://huggingface.co/docs/accelerate/package_reference/accelerator#accelerate.Accelerator.save_state'
+              ' for more details'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=5e-6,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--lr_num_cycles',
         type=int,
         default=1,
-        help=
-        'Number of hard resets of the lr in cosine_with_restarts scheduler.',
+        help='Number of hard resets of the lr in cosine_with_restarts scheduler.',
     )
+    parser.add_argument('--lr_power', type=float, default=1.0, help='Power factor of the polynomial scheduler.')
     parser.add_argument(
-        '--lr_power',
-        type=float,
-        default=1.0,
-        help='Power factor of the polynomial scheduler.')
-    parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
-    )
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
+    )
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
         '--validation_prompt',
         type=str,
         default=None,
-        help=
-        'A prompt that is used during validation to verify that the model is learning.',
+        help='A prompt that is used during validation to verify that the model is learning.',
     )
     parser.add_argument(
         '--num_validation_images',
         type=int,
         default=4,
-        help=
-        'Number of images that should be generated during validation with `validation_prompt`.',
+        help='Number of images that should be generated during validation with `validation_prompt`.',
     )
     parser.add_argument(
         '--validation_steps',
         type=int,
         default=100,
-        help=
-        ('Run validation every X steps. Validation consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`'
-         ' and logging the images.'),
+        help=('Run validation every X steps. Validation consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`'
+              ' and logging the images.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
     parser.add_argument(
         '--prior_generation_precision',
         type=str,
         default=None,
         choices=['no', 'fp32', 'fp16', 'bf16'],
-        help=
-        ('Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.'
-         ),
+        help=('Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.'),
     )
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
-        '--local_rank',
-        type=int,
-        default=-1,
-        help='For distributed training: local_rank')
-    parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
     parser.add_argument(
         '--set_grads_to_none',
         action='store_true',
-        help=
-        ('Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain'
-         ' behaviors, so disable this argument if it causes any problems. More info:'
-         ' https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'
-         ),
+        help=('Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain'
+              ' behaviors, so disable this argument if it causes any problems. More info:'
+              ' https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'),
     )
 
     parser.add_argument(
         '--offset_noise',
         action='store_true',
         default=False,
-        help=
-        ('Fine-tuning against a modified noise'
-         ' See: https://www.crosslabs.org//blog/diffusion-with-offset-noise for more information.'
-         ),
+        help=('Fine-tuning against a modified noise'
+              ' See: https://www.crosslabs.org//blog/diffusion-with-offset-noise for more information.'),
     )
     parser.add_argument(
         '--snr_gamma',
         type=float,
         default=None,
-        help=
-        'SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
+        help='SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
         'More details here: https://arxiv.org/abs/2303.09556.',
     )
     parser.add_argument(
         '--pre_compute_text_embeddings',
         action='store_true',
         help='Whether or not to pre-compute text embeddings. '
         'If text embeddings are pre-computed, the text encoder will not be kept in memory during training '
@@ -609,83 +502,69 @@
         'This is not compatible with `--train_text_encoder`.',
     )
     parser.add_argument(
         '--tokenizer_max_length',
         type=int,
         default=None,
         required=False,
-        help=
-        "The maximum length of the tokenizer. If not set, will default to the tokenizer's max length.",
+        help="The maximum length of the tokenizer. If not set, will default to the tokenizer's max length.",
     )
     parser.add_argument(
         '--text_encoder_use_attention_mask',
         action='store_true',
         required=False,
         help='Whether to use attention mask for the text encoder',
     )
     parser.add_argument(
-        '--skip_save_text_encoder',
-        action='store_true',
-        required=False,
-        help='Set to not save text encoder')
+        '--skip_save_text_encoder', action='store_true', required=False, help='Set to not save text encoder')
     parser.add_argument(
         '--validation_images',
         required=False,
         default=None,
         nargs='+',
-        help=
-        'Optional set of images to use for validation. Used when the target pipeline takes an initial image '
+        help='Optional set of images to use for validation. Used when the target pipeline takes an initial image '
         'as input such as when training image variation or superresolution.',
     )
     parser.add_argument(
         '--class_labels_conditioning',
         required=False,
         default=None,
-        help=
-        'The optional `class_label` conditioning to pass to the unet, available values are `timesteps`.',
+        help='The optional `class_label` conditioning to pass to the unet, available values are `timesteps`.',
     )
     parser.add_argument(
         '--validation_scheduler',
         type=str,
         default='DPMSolverMultistepScheduler',
         choices=['DPMSolverMultistepScheduler', 'DDPMScheduler'],
-        help=
-        'Select which scheduler to use for validation. DDPMScheduler is recommended for DeepFloyd IF.',
+        help='Select which scheduler to use for validation. DDPMScheduler is recommended for DeepFloyd IF.',
     )
 
     if input_args is not None:
         args = parser.parse_args(input_args)
     else:
         args = parser.parse_args()
 
     env_local_rank = int(os.environ.get('LOCAL_RANK', -1))
     if env_local_rank != -1 and env_local_rank != args.local_rank:
         args.local_rank = env_local_rank
 
     if args.with_prior_preservation:
         if args.class_data_dir is None:
-            raise ValueError(
-                'You must specify a data directory for class images.')
+            raise ValueError('You must specify a data directory for class images.')
         if args.class_prompt is None:
             raise ValueError('You must specify prompt for class images.')
     else:
         # logger is not available yet
         if args.class_data_dir is not None:
-            warnings.warn(
-                'You need not use --class_data_dir without --with_prior_preservation.'
-            )
+            warnings.warn('You need not use --class_data_dir without --with_prior_preservation.')
         if args.class_prompt is not None:
-            warnings.warn(
-                'You need not use --class_prompt without --with_prior_preservation.'
-            )
+            warnings.warn('You need not use --class_prompt without --with_prior_preservation.')
 
     if args.train_text_encoder and args.pre_compute_text_embeddings:
-        raise ValueError(
-            '`--train_text_encoder` cannot be used with `--pre_compute_text_embeddings`'
-        )
+        raise ValueError('`--train_text_encoder` cannot be used with `--pre_compute_text_embeddings`')
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
 
     return args
@@ -716,118 +595,100 @@
         self.tokenizer = tokenizer
         self.encoder_hidden_states = encoder_hidden_states
         self.class_prompt_encoder_hidden_states = class_prompt_encoder_hidden_states
         self.tokenizer_max_length = tokenizer_max_length
 
         self.instance_data_root = Path(instance_data_root)
         if not self.instance_data_root.exists():
-            raise ValueError(
-                f"Instance {self.instance_data_root} images root doesn't exists."
-            )
+            raise ValueError(f"Instance {self.instance_data_root} images root doesn't exists.")
 
         self.instance_images_path = list(Path(instance_data_root).iterdir())
         self.num_instance_images = len(self.instance_images_path)
         self.instance_prompt = instance_prompt
         self._length = self.num_instance_images
 
         if class_data_root is not None:
             self.class_data_root = Path(class_data_root)
             self.class_data_root.mkdir(parents=True, exist_ok=True)
             self.class_images_path = list(self.class_data_root.iterdir())
             if class_num is not None:
-                self.num_class_images = min(
-                    len(self.class_images_path), class_num)
+                self.num_class_images = min(len(self.class_images_path), class_num)
             else:
                 self.num_class_images = len(self.class_images_path)
             self._length = max(self.num_class_images, self.num_instance_images)
             self.class_prompt = class_prompt
         else:
             self.class_data_root = None
 
         self.image_transforms = transforms.Compose([
-            transforms.Resize(
-                size, interpolation=transforms.InterpolationMode.BILINEAR),
-            transforms.CenterCrop(size)
-            if center_crop else transforms.RandomCrop(size),
+            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),
+            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),
             transforms.ToTensor(),
             transforms.Normalize([0.5], [0.5]),
         ])
 
     def __len__(self):
         return self._length
 
     def __getitem__(self, index):
         example = {}
-        instance_image = Image.open(
-            self.instance_images_path[index % self.num_instance_images])
+        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])
         instance_image = exif_transpose(instance_image)
 
         if not instance_image.mode == 'RGB':
             instance_image = instance_image.convert('RGB')
         example['instance_images'] = self.image_transforms(instance_image)
 
         if self.encoder_hidden_states is not None:
             example['instance_prompt_ids'] = self.encoder_hidden_states
         else:
             text_inputs = tokenize_prompt(
-                self.tokenizer,
-                self.instance_prompt,
-                tokenizer_max_length=self.tokenizer_max_length)
+                self.tokenizer, self.instance_prompt, tokenizer_max_length=self.tokenizer_max_length)
             example['instance_prompt_ids'] = text_inputs.input_ids
             example['instance_attention_mask'] = text_inputs.attention_mask
 
         if self.class_data_root:
-            class_image = Image.open(
-                self.class_images_path[index % self.num_class_images])
+            class_image = Image.open(self.class_images_path[index % self.num_class_images])
             class_image = exif_transpose(class_image)
 
             if not class_image.mode == 'RGB':
                 class_image = class_image.convert('RGB')
             example['class_images'] = self.image_transforms(class_image)
 
             if self.class_prompt_encoder_hidden_states is not None:
-                example[
-                    'class_prompt_ids'] = self.class_prompt_encoder_hidden_states
+                example['class_prompt_ids'] = self.class_prompt_encoder_hidden_states
             else:
                 class_text_inputs = tokenize_prompt(
-                    self.tokenizer,
-                    self.class_prompt,
-                    tokenizer_max_length=self.tokenizer_max_length)
+                    self.tokenizer, self.class_prompt, tokenizer_max_length=self.tokenizer_max_length)
                 example['class_prompt_ids'] = class_text_inputs.input_ids
-                example[
-                    'class_attention_mask'] = class_text_inputs.attention_mask
+                example['class_attention_mask'] = class_text_inputs.attention_mask
 
         return example
 
 
 def collate_fn(examples, with_prior_preservation=False):
     has_attention_mask = 'instance_attention_mask' in examples[0]
 
     input_ids = [example['instance_prompt_ids'] for example in examples]
     pixel_values = [example['instance_images'] for example in examples]
 
     if has_attention_mask:
-        attention_mask = [
-            example['instance_attention_mask'] for example in examples
-        ]
+        attention_mask = [example['instance_attention_mask'] for example in examples]
 
     # Concat class and instance examples for prior preservation.
     # We do this to avoid doing two forward passes.
     if with_prior_preservation:
         input_ids += [example['class_prompt_ids'] for example in examples]
         pixel_values += [example['class_images'] for example in examples]
 
         if has_attention_mask:
-            attention_mask += [
-                example['class_attention_mask'] for example in examples
-            ]
+            attention_mask += [example['class_attention_mask'] for example in examples]
 
     pixel_values = torch.stack(pixel_values)
-    pixel_values = pixel_values.to(
-        memory_format=torch.contiguous_format).float()
+    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
 
     input_ids = torch.cat(input_ids, dim=0)
 
     batch = {
         'input_ids': input_ids,
         'pixel_values': pixel_values,
     }
@@ -855,16 +716,15 @@
         example['index'] = index
         return example
 
 
 def model_has_vae(args):
     config_file_name = os.path.join('vae', AutoencoderKL.config_name)
     if os.path.isdir(args.pretrained_model_name_or_path):
-        config_file_name = os.path.join(args.pretrained_model_name_or_path,
-                                        config_file_name)
+        config_file_name = os.path.join(args.pretrained_model_name_or_path, config_file_name)
         return os.path.isfile(config_file_name)
     else:
         raise NotImplementedError()
 
 
 def tokenize_prompt(tokenizer, prompt, tokenizer_max_length=None):
     if tokenizer_max_length is not None:
@@ -879,18 +739,15 @@
         max_length=max_length,
         return_tensors='pt',
     )
 
     return text_inputs
 
 
-def encode_prompt(text_encoder,
-                  input_ids,
-                  attention_mask,
-                  text_encoder_use_attention_mask=None):
+def encode_prompt(text_encoder, input_ids, attention_mask, text_encoder_use_attention_mask=None):
     text_input_ids = input_ids.to(text_encoder.device)
 
     if text_encoder_use_attention_mask:
         attention_mask = attention_mask.to(text_encoder.device)
     else:
         attention_mask = None
 
@@ -903,38 +760,34 @@
     return prompt_embeds
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
 
     if args.report_to == 'wandb':
         if not is_wandb_available():
-            raise ImportError(
-                'Make sure to install wandb if you want to use it for logging during training.'
-            )
+            raise ImportError('Make sure to install wandb if you want to use it for logging during training.')
 
     # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate
     # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.
     # TODO (patil-suraj): Remove this check when gradient accumulation with two models is enabled in accelerate.
     if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:
         raise ValueError(
             'Gradient accumulation is not supported when training the text encoder in distributed training. '
-            'Please set gradient_accumulation_steps to 1. This feature will be supported in the future.'
-        )
+            'Please set gradient_accumulation_steps to 1. This feature will be supported in the future.')
 
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
     )
@@ -974,107 +827,87 @@
             )
             pipeline.set_progress_bar_config(disable=True)
 
             num_new_images = args.num_class_images - cur_class_images
             logger.info(f'Number of class images to sample: {num_new_images}.')
 
             sample_dataset = PromptDataset(args.class_prompt, num_new_images)
-            sample_dataloader = torch.utils.data.DataLoader(
-                sample_dataset, batch_size=args.sample_batch_size)
+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)
 
             sample_dataloader = accelerator.prepare(sample_dataloader)
             pipeline.to(accelerator.device)
 
             for example in tqdm(
-                    sample_dataloader,
-                    desc='Generating class images',
-                    disable=not accelerator.is_local_main_process):
+                    sample_dataloader, desc='Generating class images', disable=not accelerator.is_local_main_process):
                 images = pipeline(example['prompt']).images
 
                 for i, image in enumerate(images):
-                    hash_image = insecure_hashlib.sha1(
-                        image.tobytes()).hexdigest()
+                    hash_image = insecure_hashlib.sha1(image.tobytes()).hexdigest()
                     image_filename = class_images_dir / f"{example['index'][i] + cur_class_images}-{hash_image}.jpg"
                     image.save(image_filename)
 
             del pipeline
             if torch.cuda.is_available():
                 torch.cuda.empty_cache()
 
     # Handle the repository creation
     if accelerator.is_main_process:
         if args.output_dir is not None:
             os.makedirs(args.output_dir, exist_ok=True)
 
     # Load the tokenizer
     if args.tokenizer_name:
-        tokenizer = AutoTokenizer.from_pretrained(
-            args.tokenizer_name, revision=args.revision, use_fast=False)
+        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)
     elif args.pretrained_model_name_or_path:
         tokenizer = AutoTokenizer.from_pretrained(
             args.pretrained_model_name_or_path,
             subfolder='tokenizer',
             revision=args.revision,
             use_fast=False,
         )
 
     # import correct text encoder class
-    text_encoder_cls = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     text_encoder = text_encoder_cls.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
 
     if model_has_vae(args):
         vae = AutoencoderKL.from_pretrained(
-            args.pretrained_model_name_or_path,
-            subfolder='vae',
-            revision=args.revision,
-            variant=args.variant)
+            args.pretrained_model_name_or_path, subfolder='vae', revision=args.revision, variant=args.variant)
     else:
         vae = None
 
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
     # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
     def save_model_hook(models, weights, output_dir):
         if accelerator.is_main_process:
             for model in models:
-                sub_dir = 'unet' if isinstance(
-                    model, type(
-                        accelerator.unwrap_model(unet))) else 'text_encoder'
+                sub_dir = 'unet' if isinstance(model, type(accelerator.unwrap_model(unet))) else 'text_encoder'
                 model.save_pretrained(os.path.join(output_dir, sub_dir))
 
                 # make sure to pop weight so that corresponding model is not saved again
                 weights.pop()
 
     def load_model_hook(models, input_dir):
         while len(models) > 0:
             # pop models so that they are not loaded again
             model = models.pop()
 
             if isinstance(model, type(accelerator.unwrap_model(text_encoder))):
                 # load transformers style into model
-                load_model = text_encoder_cls.from_pretrained(
-                    input_dir, subfolder='text_encoder')
+                load_model = text_encoder_cls.from_pretrained(input_dir, subfolder='text_encoder')
                 model.config = load_model.config
             else:
                 # load diffusers style into model
-                load_model = UNet2DConditionModel.from_pretrained(
-                    input_dir, subfolder='unet')
+                load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder='unet')
                 model.register_to_config(**load_model.config)
 
             model.load_state_dict(load_model.state_dict())
             del load_model
 
     accelerator.register_save_state_pre_hook(save_model_hook)
     accelerator.register_load_state_pre_hook(load_model_hook)
@@ -1090,110 +923,92 @@
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
                 logger.warn(
                     'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training,'
                     ' please update xFormers to at least 0.0.17. See '
-                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     if args.gradient_checkpointing:
         unet.enable_gradient_checkpointing()
         if args.train_text_encoder:
             text_encoder.gradient_checkpointing_enable()
 
     # Check that all trainable models are in full precision
     low_precision_error_string = (
         'Please make sure to always have all model weights in full float32 precision when starting training - even if'
-        ' doing mixed precision training. copy of the weights should still be float32.'
-    )
+        ' doing mixed precision training. copy of the weights should still be float32.')
 
     if accelerator.unwrap_model(unet).dtype != torch.float32:
         raise ValueError(
-            f'Unet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}'
-        )
+            f'Unet loaded as datatype {accelerator.unwrap_model(unet).dtype}. {low_precision_error_string}')
 
-    if args.train_text_encoder and accelerator.unwrap_model(
-            text_encoder).dtype != torch.float32:
-        raise ValueError(
-            f'Text encoder loaded as datatype {accelerator.unwrap_model(text_encoder).dtype}.'
-            f' {low_precision_error_string}')
+    if args.train_text_encoder and accelerator.unwrap_model(text_encoder).dtype != torch.float32:
+        raise ValueError(f'Text encoder loaded as datatype {accelerator.unwrap_model(text_encoder).dtype}.'
+                         f' {low_precision_error_string}')
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
-            raise ImportError(
-                'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-            )
+            raise ImportError('To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
         optimizer_class = bnb.optim.AdamW8bit
     else:
         optimizer_class = torch.optim.AdamW
 
     # Optimizer creation
     params_to_optimize = (
-        itertools.chain(unet.parameters(), text_encoder.parameters())
-        if args.train_text_encoder else unet.parameters())
+        itertools.chain(unet.parameters(), text_encoder.parameters()) if args.train_text_encoder else unet.parameters())
     optimizer = optimizer_class(
         params_to_optimize,
         lr=args.learning_rate,
         betas=(args.adam_beta1, args.adam_beta2),
         weight_decay=args.adam_weight_decay,
         eps=args.adam_epsilon,
     )
 
     if args.pre_compute_text_embeddings:
 
         def compute_text_embeddings(prompt):
             with torch.no_grad():
-                text_inputs = tokenize_prompt(
-                    tokenizer,
-                    prompt,
-                    tokenizer_max_length=args.tokenizer_max_length)
+                text_inputs = tokenize_prompt(tokenizer, prompt, tokenizer_max_length=args.tokenizer_max_length)
                 prompt_embeds = encode_prompt(
                     text_encoder,
                     text_inputs.input_ids,
                     text_inputs.attention_mask,
-                    text_encoder_use_attention_mask=args.
-                    text_encoder_use_attention_mask,
+                    text_encoder_use_attention_mask=args.text_encoder_use_attention_mask,
                 )
 
             return prompt_embeds
 
-        pre_computed_encoder_hidden_states = compute_text_embeddings(
-            args.instance_prompt)
+        pre_computed_encoder_hidden_states = compute_text_embeddings(args.instance_prompt)
         validation_prompt_negative_prompt_embeds = compute_text_embeddings('')
 
         if args.validation_prompt is not None:
-            validation_prompt_encoder_hidden_states = compute_text_embeddings(
-                args.validation_prompt)
+            validation_prompt_encoder_hidden_states = compute_text_embeddings(args.validation_prompt)
         else:
             validation_prompt_encoder_hidden_states = None
 
         if args.class_prompt is not None:
-            pre_computed_class_prompt_encoder_hidden_states = compute_text_embeddings(
-                args.class_prompt)
+            pre_computed_class_prompt_encoder_hidden_states = compute_text_embeddings(args.class_prompt)
         else:
             pre_computed_class_prompt_encoder_hidden_states = None
 
         text_encoder = None
         tokenizer = None
 
         gc.collect()
@@ -1204,40 +1019,36 @@
         validation_prompt_negative_prompt_embeds = None
         pre_computed_class_prompt_encoder_hidden_states = None
 
     # Dataset and DataLoaders creation:
     train_dataset = DreamBoothDataset(
         instance_data_root=args.instance_data_dir,
         instance_prompt=args.instance_prompt,
-        class_data_root=args.class_data_dir
-        if args.with_prior_preservation else None,
+        class_data_root=args.class_data_dir if args.with_prior_preservation else None,
         class_prompt=args.class_prompt,
         class_num=args.num_class_images,
         tokenizer=tokenizer,
         size=args.resolution,
         center_crop=args.center_crop,
         encoder_hidden_states=pre_computed_encoder_hidden_states,
-        class_prompt_encoder_hidden_states=
-        pre_computed_class_prompt_encoder_hidden_states,
+        class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states,
         tokenizer_max_length=args.tokenizer_max_length,
     )
 
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         batch_size=args.train_batch_size,
         shuffle=True,
-        collate_fn=lambda examples: collate_fn(examples, args.
-                                               with_prior_preservation),
+        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
@@ -1248,16 +1059,16 @@
     )
 
     # Prepare everything with our `accelerator`.
     if args.train_text_encoder:
         unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
             unet, text_encoder, optimizer, train_dataloader, lr_scheduler)
     else:
-        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-            unet, optimizer, train_dataloader, lr_scheduler)
+        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                              lr_scheduler)
 
     # For mixed precision training we cast all non-trainable weights
     # (vae, non-lora text_encoder and non-lora unet) to half-precision
     # as these weights are only used for inference, keeping weights in full precision is not required.
     weight_dtype = torch.float32
     if accelerator.mixed_precision == 'fp16':
         weight_dtype = torch.float16
@@ -1268,21 +1079,19 @@
     if vae is not None:
         vae.to(accelerator.device, dtype=weight_dtype)
 
     if not args.train_text_encoder and text_encoder is not None:
         text_encoder.to(accelerator.device, dtype=weight_dtype)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         tracker_config = vars(copy.deepcopy(args))
         tracker_config.pop('validation_images')
         accelerator.init_trackers('dreambooth', config=tracker_config)
@@ -1290,21 +1099,17 @@
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num batches each epoch = {len(train_dataloader)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1314,16 +1119,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -1346,182 +1150,138 @@
             text_encoder.train()
         for step, batch in enumerate(train_dataloader):
             with accelerator.accumulate(unet):
                 pixel_values = batch['pixel_values'].to(dtype=weight_dtype)
 
                 if vae is not None:
                     # Convert images to latent space
-                    model_input = vae.encode(batch['pixel_values'].to(
-                        dtype=weight_dtype)).latent_dist.sample()
+                    model_input = vae.encode(batch['pixel_values'].to(dtype=weight_dtype)).latent_dist.sample()
                     model_input = model_input * vae.config.scaling_factor
                 else:
                     model_input = pixel_values
 
                 # Sample noise that we'll add to the model input
                 if args.offset_noise:
                     noise = torch.randn_like(model_input) + 0.1 * torch.randn(
-                        model_input.shape[0],
-                        model_input.shape[1],
-                        1,
-                        1,
-                        device=model_input.device)
+                        model_input.shape[0], model_input.shape[1], 1, 1, device=model_input.device)
                 else:
                     noise = torch.randn_like(model_input)
                 bsz, channels, height, width = model_input.shape
                 # Sample a random timestep for each image
                 timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=model_input.device)
+                    0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=model_input.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the model input according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_model_input = noise_scheduler.add_noise(
-                    model_input, noise, timesteps)
+                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)
 
                 # Get the text embedding for conditioning
                 if args.pre_compute_text_embeddings:
                     encoder_hidden_states = batch['input_ids']
                 else:
                     encoder_hidden_states = encode_prompt(
                         text_encoder,
                         batch['input_ids'],
                         batch['attention_mask'],
-                        text_encoder_use_attention_mask=args.
-                        text_encoder_use_attention_mask,
+                        text_encoder_use_attention_mask=args.text_encoder_use_attention_mask,
                     )
 
-                if accelerator.unwrap_model(
-                        unet).config.in_channels == channels * 2:
-                    noisy_model_input = torch.cat(
-                        [noisy_model_input, noisy_model_input], dim=1)
+                if accelerator.unwrap_model(unet).config.in_channels == channels * 2:
+                    noisy_model_input = torch.cat([noisy_model_input, noisy_model_input], dim=1)
 
                 if args.class_labels_conditioning == 'timesteps':
                     class_labels = timesteps
                 else:
                     class_labels = None
 
                 # Predict the noise residual
-                model_pred = unet(
-                    noisy_model_input,
-                    timesteps,
-                    encoder_hidden_states,
-                    class_labels=class_labels).sample
+                model_pred = unet(noisy_model_input, timesteps, encoder_hidden_states, class_labels=class_labels).sample
 
                 if model_pred.shape[1] == 6:
                     model_pred, _ = torch.chunk(model_pred, 2, dim=1)
 
                 # Get the target for loss depending on the prediction type
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        model_input, noise, timesteps)
+                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
                 if args.with_prior_preservation:
                     # Chunk the noise and model_pred into two parts and compute the loss on each part separately.
-                    model_pred, model_pred_prior = torch.chunk(
-                        model_pred, 2, dim=0)
+                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
                     target, target_prior = torch.chunk(target, 2, dim=0)
                     # Compute prior loss
-                    prior_loss = F.mse_loss(
-                        model_pred_prior.float(),
-                        target_prior.float(),
-                        reduction='mean')
+                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction='mean')
 
                 # Compute instance loss
                 if args.snr_gamma is None:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
                 else:
                     # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
                     # Since we predict the noise instead of x_0, the original formulation is slightly changed.
                     # This is discussed in Section 4.2 of the same paper.
                     snr = compute_snr(noise_scheduler, timesteps)
                     base_weight = (
-                        torch.stack(
-                            [snr, args.snr_gamma * torch.ones_like(timesteps)],
-                            dim=1).min(dim=1)[0] / snr)
+                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr)
 
                     if noise_scheduler.config.prediction_type == 'v_prediction':
                         # Velocity objective needs to be floored to an SNR weight of one.
                         mse_loss_weights = base_weight + 1
                     else:
                         # Epsilon and sample both use the same loss weights.
                         mse_loss_weights = base_weight
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='none')
-                    loss = loss.mean(
-                        dim=list(range(1, len(loss.shape)))) * mse_loss_weights
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='none')
+                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                     loss = loss.mean()
 
                 if args.with_prior_preservation:
                     # Add the prior loss to the instance loss.
                     loss = loss + args.prior_loss_weight * prior_loss
 
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
                     params_to_clip = (
-                        itertools.chain(unet.parameters(),
-                                        text_encoder.parameters())
+                        itertools.chain(unet.parameters(), text_encoder.parameters())
                         if args.train_text_encoder else unet.parameters())
-                    accelerator.clip_grad_norm_(params_to_clip,
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad(set_to_none=args.set_grads_to_none)
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint,
                             # we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
                         logger.info(f'Saved state to {save_path}')
 
                     images = []
 
                     if args.validation_prompt is not None and global_step % args.validation_steps == 0:
                         images = log_validation(
@@ -1533,32 +1293,28 @@
                             accelerator,
                             weight_dtype,
                             global_step,
                             validation_prompt_encoder_hidden_states,
                             validation_prompt_negative_prompt_embeds,
                         )
 
-            logs = {
-                'loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
             accelerator.log(logs, step=global_step)
 
             if global_step >= args.max_train_steps:
                 break
 
     # Create the pipeline using the trained modules and save it.
     accelerator.wait_for_everyone()
     if accelerator.is_main_process:
         pipeline_args = {}
 
         if text_encoder is not None:
-            pipeline_args['text_encoder'] = accelerator.unwrap_model(
-                text_encoder)
+            pipeline_args['text_encoder'] = accelerator.unwrap_model(text_encoder)
 
         if args.skip_save_text_encoder:
             pipeline_args['text_encoder'] = None
 
         pipeline = DiffusionPipeline.from_pretrained(
             args.pretrained_model_name_or_path,
             unet=accelerator.unwrap_model(unet),
@@ -1575,16 +1331,15 @@
             variance_type = pipeline.scheduler.config.variance_type
 
             if variance_type in ['learned', 'learned_range']:
                 variance_type = 'fixed_small'
 
             scheduler_args['variance_type'] = variance_type
 
-        pipeline.scheduler = pipeline.scheduler.from_config(
-            pipeline.scheduler.config, **scheduler_args)
+        pipeline.scheduler = pipeline.scheduler.from_config(pipeline.scheduler.config, **scheduler_args)
 
         pipeline.save_pretrained(args.output_dir)
 
         if args.push_to_hub:
             save_model_card(
                 args.hub_model_id,
                 images=images,
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_dreambooth_lora.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_dreambooth_lora.py`

 * *Files 7% similar despite different names*

```diff
@@ -27,17 +27,16 @@
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import ProjectConfiguration, set_seed
-from diffusers import (AutoencoderKL, DDPMScheduler, DiffusionPipeline,
-                       DPMSolverMultistepScheduler, StableDiffusionPipeline,
-                       UNet2DConditionModel)
+from diffusers import (AutoencoderKL, DDPMScheduler, DiffusionPipeline, DPMSolverMultistepScheduler,
+                       StableDiffusionPipeline, UNet2DConditionModel)
 from diffusers.loaders import LoraLoaderMixin
 from diffusers.optimization import get_scheduler
 from diffusers.utils import check_min_version, is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from huggingface_hub.utils import insecure_hashlib
 from modelscope import AutoTokenizer
 from packaging import version
@@ -58,16 +57,15 @@
     state_dict = {}
 
     def text_encoder_attn_modules(text_encoder):
         from transformers import CLIPTextModel, CLIPTextModelWithProjection
 
         attn_modules = []
 
-        if isinstance(text_encoder,
-                      (CLIPTextModel, CLIPTextModelWithProjection)):
+        if isinstance(text_encoder, (CLIPTextModel, CLIPTextModelWithProjection)):
             for i, layer in enumerate(text_encoder.text_model.encoder.layers):
                 name = f'text_model.encoder.layers.{i}.self_attn'
                 mod = layer.self_attn
                 attn_modules.append((name, mod))
 
         return attn_modules
 
@@ -124,16 +122,15 @@
 
 LoRA for the text encoder was enabled: {train_text_encoder}.
 """
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str, revision: str):
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str, revision: str):
     text_encoder_config = PretrainedConfig.from_pretrained(
         pretrained_model_name_or_path,
         subfolder='text_encoder',
         revision=revision,
     )
     model_class = text_encoder_config.architectures[0]
 
@@ -150,38 +147,34 @@
 
         return T5EncoderModel
     else:
         raise ValueError(f'{model_class} is not supported.')
 
 
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a dreambooth inferenc.')
+    parser = argparse.ArgumentParser(description='Simple example of a dreambooth inferenc.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--tokenizer_name',
         type=str,
         default=None,
         help='Pretrained tokenizer name or path if not the same as model_name',
     )
@@ -206,331 +199,243 @@
         required=True,
         help='The prompt with identifier specifying the instance',
     )
     parser.add_argument(
         '--class_prompt',
         type=str,
         default=None,
-        help=
-        'The prompt to specify images in the same class as provided instance images.',
+        help='The prompt to specify images in the same class as provided instance images.',
     )
     parser.add_argument(
         '--validation_prompt',
         type=str,
         default=None,
-        help=
-        'A prompt that is used during validation to verify that the model is learning.',
+        help='A prompt that is used during validation to verify that the model is learning.',
     )
     parser.add_argument(
         '--num_validation_images',
         type=int,
         default=4,
-        help=
-        'Number of images that should be generated during validation with `validation_prompt`.',
+        help='Number of images that should be generated during validation with `validation_prompt`.',
     )
     parser.add_argument(
         '--validation_epochs',
         type=int,
         default=50,
-        help=
-        ('Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`.'
-         ),
+        help=('Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`.'),
     )
     parser.add_argument(
         '--with_prior_preservation',
         default=False,
         action='store_true',
         help='Flag to add prior preservation loss.',
     )
-    parser.add_argument(
-        '--prior_loss_weight',
-        type=float,
-        default=1.0,
-        help='The weight of prior preservation loss.')
+    parser.add_argument('--prior_loss_weight', type=float, default=1.0, help='The weight of prior preservation loss.')
     parser.add_argument(
         '--num_class_images',
         type=int,
         default=100,
-        help=
-        ('Minimal class images for prior preservation loss. If there are not enough images already present in'
-         ' class_data_dir, additional images will be sampled with class_prompt.'
-         ),
+        help=('Minimal class images for prior preservation loss. If there are not enough images already present in'
+              ' class_data_dir, additional images will be sampled with class_prompt.'),
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='lora-dreambooth-model',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=512,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
         '--center_crop',
         default=False,
         action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
     )
     parser.add_argument(
         '--train_text_encoder',
         action='store_true',
-        help=
-        'Whether to train the text encoder. If set, the text encoder should be float32 precision.',
+        help='Whether to train the text encoder. If set, the text encoder should be float32 precision.',
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for the training dataloader.')
-    parser.add_argument(
-        '--sample_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for sampling images.')
+        '--train_batch_size', type=int, default=4, help='Batch size (per device) for the training dataloader.')
+    parser.add_argument('--sample_batch_size', type=int, default=4, help='Batch size (per device) for sampling images.')
     parser.add_argument('--num_train_epochs', type=int, default=1)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
-        help=
-        ('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
-         ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
-         ' training using `--resume_from_checkpoint`.'),
+        help=('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
+              ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
+              ' training using `--resume_from_checkpoint`.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=5e-4,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--lr_num_cycles',
         type=int,
         default=1,
-        help=
-        'Number of hard resets of the lr in cosine_with_restarts scheduler.',
+        help='Number of hard resets of the lr in cosine_with_restarts scheduler.',
     )
-    parser.add_argument(
-        '--lr_power',
-        type=float,
-        default=1.0,
-        help='Power factor of the polynomial scheduler.')
+    parser.add_argument('--lr_power', type=float, default=1.0, help='Power factor of the polynomial scheduler.')
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
     )
     parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
     parser.add_argument(
         '--prior_generation_precision',
         type=str,
         default=None,
         choices=['no', 'fp32', 'fp16', 'bf16'],
-        help=
-        ('Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.'
-         ),
+        help=('Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.'),
     )
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
-        '--local_rank',
-        type=int,
-        default=-1,
-        help='For distributed training: local_rank')
-    parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
     parser.add_argument(
         '--pre_compute_text_embeddings',
         action='store_true',
         help='Whether or not to pre-compute text embeddings. '
         'If text embeddings are pre-computed, the text encoder will not be kept in memory during training '
         'and will leave more GPU memory available for training the rest of the model. '
         'This is not compatible with `--train_text_encoder`.',
     )
     parser.add_argument(
         '--tokenizer_max_length',
         type=int,
         default=None,
         required=False,
-        help=
-        "The maximum length of the tokenizer. If not set, will default to the tokenizer's max length.",
+        help="The maximum length of the tokenizer. If not set, will default to the tokenizer's max length.",
     )
     parser.add_argument(
         '--text_encoder_use_attention_mask',
         action='store_true',
         required=False,
         help='Whether to use attention mask for the text encoder',
     )
     parser.add_argument(
         '--validation_images',
         required=False,
         default=None,
         nargs='+',
-        help=
-        'Optional set of images to use for validation. Used when the target pipeline takes an initial image '
+        help='Optional set of images to use for validation. Used when the target pipeline takes an initial image '
         'as input such as when training image variation or superresolution.',
     )
     parser.add_argument(
         '--class_labels_conditioning',
         required=False,
         default=None,
-        help=
-        'The optional `class_label` conditioning to pass to the unet, available values are `timesteps`.',
+        help='The optional `class_label` conditioning to pass to the unet, available values are `timesteps`.',
     )
     parser.add_argument(
         '--rank',
         type=int,
         default=4,
         help=('The dimension of the LoRA update matrices.'),
     )
@@ -542,33 +447,26 @@
 
     env_local_rank = int(os.environ.get('LOCAL_RANK', -1))
     if env_local_rank != -1 and env_local_rank != args.local_rank:
         args.local_rank = env_local_rank
 
     if args.with_prior_preservation:
         if args.class_data_dir is None:
-            raise ValueError(
-                'You must specify a data directory for class images.')
+            raise ValueError('You must specify a data directory for class images.')
         if args.class_prompt is None:
             raise ValueError('You must specify prompt for class images.')
     else:
         # logger is not available yet
         if args.class_data_dir is not None:
-            warnings.warn(
-                'You need not use --class_data_dir without --with_prior_preservation.'
-            )
+            warnings.warn('You need not use --class_data_dir without --with_prior_preservation.')
         if args.class_prompt is not None:
-            warnings.warn(
-                'You need not use --class_prompt without --with_prior_preservation.'
-            )
+            warnings.warn('You need not use --class_prompt without --with_prior_preservation.')
 
     if args.train_text_encoder and args.pre_compute_text_embeddings:
-        raise ValueError(
-            '`--train_text_encoder` cannot be used with `--pre_compute_text_embeddings`'
-        )
+        raise ValueError('`--train_text_encoder` cannot be used with `--pre_compute_text_embeddings`')
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
     return args
 
@@ -610,103 +508,87 @@
         self._length = self.num_instance_images
 
         if class_data_root is not None:
             self.class_data_root = Path(class_data_root)
             self.class_data_root.mkdir(parents=True, exist_ok=True)
             self.class_images_path = list(self.class_data_root.iterdir())
             if class_num is not None:
-                self.num_class_images = min(
-                    len(self.class_images_path), class_num)
+                self.num_class_images = min(len(self.class_images_path), class_num)
             else:
                 self.num_class_images = len(self.class_images_path)
             self._length = max(self.num_class_images, self.num_instance_images)
             self.class_prompt = class_prompt
         else:
             self.class_data_root = None
 
         self.image_transforms = transforms.Compose([
-            transforms.Resize(
-                size, interpolation=transforms.InterpolationMode.BILINEAR),
-            transforms.CenterCrop(size)
-            if center_crop else transforms.RandomCrop(size),
+            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),
+            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),
             transforms.ToTensor(),
             transforms.Normalize([0.5], [0.5]),
         ])
 
     def __len__(self):
         return self._length
 
     def __getitem__(self, index):
         example = {}
-        instance_image = Image.open(
-            self.instance_images_path[index % self.num_instance_images])
+        instance_image = Image.open(self.instance_images_path[index % self.num_instance_images])
         instance_image = exif_transpose(instance_image)
 
         if not instance_image.mode == 'RGB':
             instance_image = instance_image.convert('RGB')
         example['instance_images'] = self.image_transforms(instance_image)
 
         if self.encoder_hidden_states is not None:
             example['instance_prompt_ids'] = self.encoder_hidden_states
         else:
             text_inputs = tokenize_prompt(
-                self.tokenizer,
-                self.instance_prompt,
-                tokenizer_max_length=self.tokenizer_max_length)
+                self.tokenizer, self.instance_prompt, tokenizer_max_length=self.tokenizer_max_length)
             example['instance_prompt_ids'] = text_inputs.input_ids
             example['instance_attention_mask'] = text_inputs.attention_mask
 
         if self.class_data_root:
-            class_image = Image.open(
-                self.class_images_path[index % self.num_class_images])
+            class_image = Image.open(self.class_images_path[index % self.num_class_images])
             class_image = exif_transpose(class_image)
 
             if not class_image.mode == 'RGB':
                 class_image = class_image.convert('RGB')
             example['class_images'] = self.image_transforms(class_image)
 
             if self.class_prompt_encoder_hidden_states is not None:
-                example[
-                    'class_prompt_ids'] = self.class_prompt_encoder_hidden_states
+                example['class_prompt_ids'] = self.class_prompt_encoder_hidden_states
             else:
                 class_text_inputs = tokenize_prompt(
-                    self.tokenizer,
-                    self.class_prompt,
-                    tokenizer_max_length=self.tokenizer_max_length)
+                    self.tokenizer, self.class_prompt, tokenizer_max_length=self.tokenizer_max_length)
                 example['class_prompt_ids'] = class_text_inputs.input_ids
-                example[
-                    'class_attention_mask'] = class_text_inputs.attention_mask
+                example['class_attention_mask'] = class_text_inputs.attention_mask
 
         return example
 
 
 def collate_fn(examples, with_prior_preservation=False):
     has_attention_mask = 'instance_attention_mask' in examples[0]
 
     input_ids = [example['instance_prompt_ids'] for example in examples]
     pixel_values = [example['instance_images'] for example in examples]
 
     if has_attention_mask:
-        attention_mask = [
-            example['instance_attention_mask'] for example in examples
-        ]
+        attention_mask = [example['instance_attention_mask'] for example in examples]
 
     # Concat class and instance examples for prior preservation.
     # We do this to avoid doing two forward passes.
     if with_prior_preservation:
         input_ids += [example['class_prompt_ids'] for example in examples]
         pixel_values += [example['class_images'] for example in examples]
         if has_attention_mask:
-            attention_mask += [
-                example['class_attention_mask'] for example in examples
-            ]
+            attention_mask += [example['class_attention_mask'] for example in examples]
 
     pixel_values = torch.stack(pixel_values)
-    pixel_values = pixel_values.to(
-        memory_format=torch.contiguous_format).float()
+    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
 
     input_ids = torch.cat(input_ids, dim=0)
 
     batch = {
         'input_ids': input_ids,
         'pixel_values': pixel_values,
     }
@@ -747,18 +629,15 @@
         max_length=max_length,
         return_tensors='pt',
     )
 
     return text_inputs
 
 
-def encode_prompt(text_encoder,
-                  input_ids,
-                  attention_mask,
-                  text_encoder_use_attention_mask=None):
+def encode_prompt(text_encoder, input_ids, attention_mask, text_encoder_use_attention_mask=None):
     text_input_ids = input_ids.to(text_encoder.device)
 
     if text_encoder_use_attention_mask:
         attention_mask = attention_mask.to(text_encoder.device)
     else:
         attention_mask = None
 
@@ -771,39 +650,35 @@
     return prompt_embeds
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
 
     if args.report_to == 'wandb':
         if not is_wandb_available():
-            raise ImportError(
-                'Make sure to install wandb if you want to use it for logging during training.'
-            )
+            raise ImportError('Make sure to install wandb if you want to use it for logging during training.')
         import wandb
 
     # Currently, it's not possible to do gradient accumulation when training two models with accelerate.accumulate
     # This will be enabled soon in accelerate. For now, we don't allow gradient accumulation when training two models.
     # TODO (sayakpaul): Remove this check when gradient accumulation with two models is enabled in accelerate.
     if args.train_text_encoder and args.gradient_accumulation_steps > 1 and accelerator.num_processes > 1:
         raise ValueError(
             'Gradient accumulation is not supported when training the text encoder in distributed training. '
-            'Please set gradient_accumulation_steps to 1. This feature will be supported in the future.'
-        )
+            'Please set gradient_accumulation_steps to 1. This feature will be supported in the future.')
 
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
     )
@@ -843,81 +718,65 @@
             )
             pipeline.set_progress_bar_config(disable=True)
 
             num_new_images = args.num_class_images - cur_class_images
             logger.info(f'Number of class images to sample: {num_new_images}.')
 
             sample_dataset = PromptDataset(args.class_prompt, num_new_images)
-            sample_dataloader = torch.utils.data.DataLoader(
-                sample_dataset, batch_size=args.sample_batch_size)
+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)
 
             sample_dataloader = accelerator.prepare(sample_dataloader)
             pipeline.to(accelerator.device)
 
             for example in tqdm(
-                    sample_dataloader,
-                    desc='Generating class images',
-                    disable=not accelerator.is_local_main_process):
+                    sample_dataloader, desc='Generating class images', disable=not accelerator.is_local_main_process):
                 images = pipeline(example['prompt']).images
 
                 for i, image in enumerate(images):
-                    hash_image = insecure_hashlib.sha1(
-                        image.tobytes()).hexdigest()
+                    hash_image = insecure_hashlib.sha1(image.tobytes()).hexdigest()
                     image_filename = class_images_dir / f"{example['index'][i] + cur_class_images}-{hash_image}.jpg"
                     image.save(image_filename)
 
             del pipeline
             if torch.cuda.is_available():
                 torch.cuda.empty_cache()
 
     # Handle the repository creation
     if accelerator.is_main_process:
         if args.output_dir is not None:
             os.makedirs(args.output_dir, exist_ok=True)
 
     # Load the tokenizer
     if args.tokenizer_name:
-        tokenizer = AutoTokenizer.from_pretrained(
-            args.tokenizer_name, revision=args.revision, use_fast=False)
+        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)
     elif args.pretrained_model_name_or_path:
         tokenizer = AutoTokenizer.from_pretrained(
             args.pretrained_model_name_or_path,
             subfolder='tokenizer',
             revision=args.revision,
             use_fast=False,
         )
 
     # import correct text encoder class
-    text_encoder_cls = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     text_encoder = text_encoder_cls.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
     try:
         vae = AutoencoderKL.from_pretrained(
-            args.pretrained_model_name_or_path,
-            subfolder='vae',
-            revision=args.revision,
-            variant=args.variant)
+            args.pretrained_model_name_or_path, subfolder='vae', revision=args.revision, variant=args.variant)
     except OSError:
         # IF does not have a VAE so let's just set it to None
         # We don't have to error out here
         vae = None
 
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
     # We only train the additional adapter LoRA layers
     if vae is not None:
         vae.requires_grad_(False)
     text_encoder.requires_grad_(False)
     unet.requires_grad_(False)
 
@@ -941,114 +800,95 @@
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
                 logger.warn(
                     'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training,'
                     ' please update xFormers to at least 0.0.17. See '
-                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     if args.gradient_checkpointing:
         unet.enable_gradient_checkpointing()
         if args.train_text_encoder:
             text_encoder.gradient_checkpointing_enable()
 
     # now we will add new LoRA weights to the attention layers
     unet_lora_config = LoRAConfig(
         r=args.rank,
         init_lora_weights='gaussian',
-        target_modules=[
-            'to_k', 'to_q', 'to_v', 'to_out.0', 'add_k_proj', 'add_v_proj'
-        ],
+        target_modules=['to_k', 'to_q', 'to_v', 'to_out.0', 'add_k_proj', 'add_v_proj'],
     )
     unet = Swift.prepare_model(unet, unet_lora_config)
 
     # The text encoder comes from  transformers, we will also attach adapters to it.
     if args.train_text_encoder:
         text_lora_config = LoRAConfig(
-            r=args.rank,
-            init_lora_weights='gaussian',
-            target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj'])
+            r=args.rank, init_lora_weights='gaussian', target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj'])
         text_encoder = Swift.prepare_model(text_encoder, text_lora_config)
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
-            raise ImportError(
-                'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-            )
+            raise ImportError('To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
         optimizer_class = bnb.optim.AdamW8bit
     else:
         optimizer_class = torch.optim.AdamW
 
     # Optimizer creation
-    params_to_optimize = list(
-        filter(lambda p: p.requires_grad, unet.parameters()))
+    params_to_optimize = list(filter(lambda p: p.requires_grad, unet.parameters()))
     if args.train_text_encoder:
-        params_to_optimize = params_to_optimize + list(
-            filter(lambda p: p.requires_grad, text_encoder.parameters()))
+        params_to_optimize = params_to_optimize + list(filter(lambda p: p.requires_grad, text_encoder.parameters()))
 
     optimizer = optimizer_class(
         params_to_optimize,
         lr=args.learning_rate,
         betas=(args.adam_beta1, args.adam_beta2),
         weight_decay=args.adam_weight_decay,
         eps=args.adam_epsilon,
     )
 
     if args.pre_compute_text_embeddings:
 
         def compute_text_embeddings(prompt):
             with torch.no_grad():
-                text_inputs = tokenize_prompt(
-                    tokenizer,
-                    prompt,
-                    tokenizer_max_length=args.tokenizer_max_length)
+                text_inputs = tokenize_prompt(tokenizer, prompt, tokenizer_max_length=args.tokenizer_max_length)
                 prompt_embeds = encode_prompt(
                     text_encoder,
                     text_inputs.input_ids,
                     text_inputs.attention_mask,
-                    text_encoder_use_attention_mask=args.
-                    text_encoder_use_attention_mask,
+                    text_encoder_use_attention_mask=args.text_encoder_use_attention_mask,
                 )
 
             return prompt_embeds
 
-        pre_computed_encoder_hidden_states = compute_text_embeddings(
-            args.instance_prompt)
+        pre_computed_encoder_hidden_states = compute_text_embeddings(args.instance_prompt)
         validation_prompt_negative_prompt_embeds = compute_text_embeddings('')
 
         if args.validation_prompt is not None:
-            validation_prompt_encoder_hidden_states = compute_text_embeddings(
-                args.validation_prompt)
+            validation_prompt_encoder_hidden_states = compute_text_embeddings(args.validation_prompt)
         else:
             validation_prompt_encoder_hidden_states = None
 
         if args.class_prompt is not None:
-            pre_computed_class_prompt_encoder_hidden_states = compute_text_embeddings(
-                args.class_prompt)
+            pre_computed_class_prompt_encoder_hidden_states = compute_text_embeddings(args.class_prompt)
         else:
             pre_computed_class_prompt_encoder_hidden_states = None
 
         text_encoder = None
         tokenizer = None
 
         gc.collect()
@@ -1059,40 +899,36 @@
         validation_prompt_negative_prompt_embeds = None
         pre_computed_class_prompt_encoder_hidden_states = None
 
     # Dataset and DataLoaders creation:
     train_dataset = DreamBoothDataset(
         instance_data_root=args.instance_data_dir,
         instance_prompt=args.instance_prompt,
-        class_data_root=args.class_data_dir
-        if args.with_prior_preservation else None,
+        class_data_root=args.class_data_dir if args.with_prior_preservation else None,
         class_prompt=args.class_prompt,
         class_num=args.num_class_images,
         tokenizer=tokenizer,
         size=args.resolution,
         center_crop=args.center_crop,
         encoder_hidden_states=pre_computed_encoder_hidden_states,
-        class_prompt_encoder_hidden_states=
-        pre_computed_class_prompt_encoder_hidden_states,
+        class_prompt_encoder_hidden_states=pre_computed_class_prompt_encoder_hidden_states,
         tokenizer_max_length=args.tokenizer_max_length,
     )
 
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         batch_size=args.train_batch_size,
         shuffle=True,
-        collate_fn=lambda examples: collate_fn(examples, args.
-                                               with_prior_preservation),
+        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
@@ -1103,25 +939,23 @@
     )
 
     # Prepare everything with our `accelerator`.
     if args.train_text_encoder:
         unet, text_encoder, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
             unet, text_encoder, optimizer, train_dataloader, lr_scheduler)
     else:
-        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-            unet, optimizer, train_dataloader, lr_scheduler)
+        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                              lr_scheduler)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         tracker_config = vars(copy.deepcopy(args))
         tracker_config.pop('validation_images')
         accelerator.init_trackers('dreambooth-lora', config=tracker_config)
@@ -1129,21 +963,17 @@
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num batches each epoch = {len(train_dataloader)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1153,16 +983,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -1195,175 +1024,133 @@
                     model_input = pixel_values
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(model_input)
                 bsz, channels, height, width = model_input.shape
                 # Sample a random timestep for each image
                 timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=model_input.device)
+                    0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=model_input.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the model input according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_model_input = noise_scheduler.add_noise(
-                    model_input, noise, timesteps)
+                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)
 
                 # Get the text embedding for conditioning
                 if args.pre_compute_text_embeddings:
                     encoder_hidden_states = batch['input_ids']
                 else:
                     encoder_hidden_states = encode_prompt(
                         text_encoder,
                         batch['input_ids'],
                         batch['attention_mask'],
-                        text_encoder_use_attention_mask=args.
-                        text_encoder_use_attention_mask,
+                        text_encoder_use_attention_mask=args.text_encoder_use_attention_mask,
                     )
 
-                if accelerator.unwrap_model(
-                        unet).config.in_channels == channels * 2:
-                    noisy_model_input = torch.cat(
-                        [noisy_model_input, noisy_model_input], dim=1)
+                if accelerator.unwrap_model(unet).config.in_channels == channels * 2:
+                    noisy_model_input = torch.cat([noisy_model_input, noisy_model_input], dim=1)
 
                 if args.class_labels_conditioning == 'timesteps':
                     class_labels = timesteps
                 else:
                     class_labels = None
 
                 # Predict the noise residual
-                model_pred = unet(
-                    noisy_model_input,
-                    timesteps,
-                    encoder_hidden_states,
-                    class_labels=class_labels).sample
+                model_pred = unet(noisy_model_input, timesteps, encoder_hidden_states, class_labels=class_labels).sample
 
                 # if model predicts variance, throw away the prediction. we will only train on the
                 # simplified training objective. This means that all schedulers using the fine tuned
                 # model must be configured to use one of the fixed variance variance types.
                 if model_pred.shape[1] == 6:
                     model_pred, _ = torch.chunk(model_pred, 2, dim=1)
 
                 # Get the target for loss depending on the prediction type
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        model_input, noise, timesteps)
+                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
                 if args.with_prior_preservation:
                     # Chunk the noise and model_pred into two parts and compute the loss on each part separately.
-                    model_pred, model_pred_prior = torch.chunk(
-                        model_pred, 2, dim=0)
+                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
                     target, target_prior = torch.chunk(target, 2, dim=0)
 
                     # Compute instance loss
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
 
                     # Compute prior loss
-                    prior_loss = F.mse_loss(
-                        model_pred_prior.float(),
-                        target_prior.float(),
-                        reduction='mean')
+                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction='mean')
 
                     # Add the prior loss to the instance loss.
                     loss = loss + args.prior_loss_weight * prior_loss
                 else:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
 
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
-                    accelerator.clip_grad_norm_(params_to_optimize,
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(params_to_optimize, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad()
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint,
                             # we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
-                        accelerator.unwrap_model(unet).to(
-                            torch.float32).save_pretrained(
-                                os.path.join(save_path, 'unet'))
+                        accelerator.unwrap_model(unet).to(torch.float32).save_pretrained(
+                            os.path.join(save_path, 'unet'))
 
                         if args.train_text_encoder:
-                            accelerator.unwrap_model(
-                                text_encoder).save_pretrained(
-                                    os.path.join(save_path, 'text_encoder'))
+                            accelerator.unwrap_model(text_encoder).save_pretrained(
+                                os.path.join(save_path, 'text_encoder'))
                         logger.info(f'Saved state to {save_path}')
 
-            logs = {
-                'loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
             accelerator.log(logs, step=global_step)
 
             if global_step >= args.max_train_steps:
                 break
 
         if accelerator.is_main_process:
             if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
-                logger.info(
-                    f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
-                    f' {args.validation_prompt}.')
+                logger.info(f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
+                            f' {args.validation_prompt}.')
                 # create pipeline
                 pipeline = DiffusionPipeline.from_pretrained(
                     args.pretrained_model_name_or_path,
                     unet=accelerator.unwrap_model(unet.base_model),
-                    text_encoder=None if args.pre_compute_text_embeddings else
-                    accelerator.unwrap_model(text_encoder),
+                    text_encoder=None if args.pre_compute_text_embeddings else accelerator.unwrap_model(text_encoder),
                     revision=args.revision,
                     variant=args.variant,
                     torch_dtype=weight_dtype,
                 )
 
                 # We train on the simplified learning objective.
                 # If we were previously predicting a variance, we need the scheduler to ignore it
@@ -1373,64 +1160,52 @@
                     variance_type = pipeline.scheduler.config.variance_type
 
                     if variance_type in ['learned', 'learned_range']:
                         variance_type = 'fixed_small'
 
                     scheduler_args['variance_type'] = variance_type
 
-                pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
-                    pipeline.scheduler.config, **scheduler_args)
+                pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config,
+                                                                             **scheduler_args)
 
                 pipeline = pipeline.to(accelerator.device)
                 pipeline.set_progress_bar_config(disable=True)
 
                 # run inference
-                generator = torch.Generator(
-                    device=accelerator.device).manual_seed(
-                        args.seed) if args.seed else None
+                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
                 if args.pre_compute_text_embeddings:
                     pipeline_args = {
-                        'prompt_embeds':
-                        validation_prompt_encoder_hidden_states,
-                        'negative_prompt_embeds':
-                        validation_prompt_negative_prompt_embeds,
+                        'prompt_embeds': validation_prompt_encoder_hidden_states,
+                        'negative_prompt_embeds': validation_prompt_negative_prompt_embeds,
                     }
                 else:
                     pipeline_args = {'prompt': args.validation_prompt}
 
                 if args.validation_images is None:
                     images = []
                     for _ in range(args.num_validation_images):
                         with torch.cuda.amp.autocast():
-                            image = pipeline(
-                                **pipeline_args, generator=generator).images[0]
+                            image = pipeline(**pipeline_args, generator=generator).images[0]
                             images.append(image)
                 else:
                     images = []
                     for image in args.validation_images:
                         image = Image.open(image)
                         with torch.cuda.amp.autocast():
-                            image = pipeline(
-                                **pipeline_args,
-                                image=image,
-                                generator=generator).images[0]
+                            image = pipeline(**pipeline_args, image=image, generator=generator).images[0]
                         images.append(image)
 
                 for tracker in accelerator.trackers:
                     if tracker.name == 'tensorboard':
-                        np_images = np.stack(
-                            [np.asarray(img) for img in images])
-                        tracker.writer.add_images(
-                            'validation', np_images, epoch, dataformats='NHWC')
+                        np_images = np.stack([np.asarray(img) for img in images])
+                        tracker.writer.add_images('validation', np_images, epoch, dataformats='NHWC')
                     if tracker.name == 'wandb':
                         tracker.log({
                             'validation': [
-                                wandb.Image(
-                                    image,
-                                    caption=f'{i}: {args.validation_prompt}')
+                                wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                                 for i, image in enumerate(images)
                             ]
                         })
 
                 del pipeline
                 torch.cuda.empty_cache()
 
@@ -1439,74 +1214,60 @@
     if accelerator.is_main_process:
         unet = accelerator.unwrap_model(unet)
         unet = unet.to(torch.float32)
         unet.save_pretrained(os.path.join(args.output_dir, 'unet'))
 
         if args.train_text_encoder:
             text_encoder = accelerator.unwrap_model(text_encoder)
-            text_encoder.save_pretrained(
-                os.path.join(args.output_dir, 'text_encoder'))
+            text_encoder.save_pretrained(os.path.join(args.output_dir, 'text_encoder'))
 
         # Final inference
         # Load previous pipeline
         pipeline = DiffusionPipeline.from_pretrained(
-            args.pretrained_model_name_or_path,
-            revision=args.revision,
-            variant=args.variant,
-            torch_dtype=weight_dtype)
+            args.pretrained_model_name_or_path, revision=args.revision, variant=args.variant, torch_dtype=weight_dtype)
 
         # We train on the simplified learning objective.
         # If we were previously predicting a variance, we need the scheduler to ignore it
         scheduler_args = {}
 
         if 'variance_type' in pipeline.scheduler.config:
             variance_type = pipeline.scheduler.config.variance_type
 
             if variance_type in ['learned', 'learned_range']:
                 variance_type = 'fixed_small'
 
             scheduler_args['variance_type'] = variance_type
 
-        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
-            pipeline.scheduler.config, **scheduler_args)
+        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)
 
         pipeline = pipeline.to(accelerator.device)
 
         # load attention processors
-        pipeline.unet = Swift.from_pretrained(
-            pipeline.unet, os.path.join(args.output_dir, 'unet'))
+        pipeline.unet = Swift.from_pretrained(pipeline.unet, os.path.join(args.output_dir, 'unet'))
         if args.train_text_encoder:
-            pipeline.text_encoder = Swift.from_pretrained(
-                pipeline.text_encoder,
-                os.path.join(args.output_dir, 'text_encoder'))
+            pipeline.text_encoder = Swift.from_pretrained(pipeline.text_encoder,
+                                                          os.path.join(args.output_dir, 'text_encoder'))
 
         # run inference
         images = []
         if args.validation_prompt and args.num_validation_images > 0:
-            generator = torch.Generator(device=accelerator.device).manual_seed(
-                args.seed) if args.seed else None
+            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
             images = [
-                pipeline(
-                    args.validation_prompt,
-                    num_inference_steps=25,
-                    generator=generator).images[0]
+                pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]
                 for _ in range(args.num_validation_images)
             ]
 
             for tracker in accelerator.trackers:
                 if tracker.name == 'tensorboard':
                     np_images = np.stack([np.asarray(img) for img in images])
-                    tracker.writer.add_images(
-                        'test', np_images, epoch, dataformats='NHWC')
+                    tracker.writer.add_images('test', np_images, epoch, dataformats='NHWC')
                 if tracker.name == 'wandb':
                     tracker.log({
                         'test': [
-                            wandb.Image(
-                                image,
-                                caption=f'{i}: {args.validation_prompt}')
+                            wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                             for i, image in enumerate(images)
                         ]
                     })
 
         if args.push_to_hub:
             save_model_card(
                 args.hub_model_id,
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_dreambooth_lora_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_dreambooth_lora_sdxl.py`

 * *Files 4% similar despite different names*

```diff
@@ -26,18 +26,16 @@
 import numpy as np
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
-from accelerate.utils import (DistributedDataParallelKwargs,
-                              ProjectConfiguration, set_seed)
-from diffusers import (AutoencoderKL, DDPMScheduler,
-                       DPMSolverMultistepScheduler, StableDiffusionXLPipeline,
+from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed
+from diffusers import (AutoencoderKL, DDPMScheduler, DPMSolverMultistepScheduler, StableDiffusionXLPipeline,
                        UNet2DConditionModel)
 from diffusers.optimization import get_scheduler
 from diffusers.training_utils import compute_snr
 from diffusers.utils import check_min_version, is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from huggingface_hub.utils import insecure_hashlib
 from modelscope import AutoTokenizer
@@ -116,18 +114,17 @@
 [Download]({repo_id}/tree/main) them in the Files & versions tab.
 
 """
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str,
-        revision: str,
-        subfolder: str = 'text_encoder'):
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str,
+                                               revision: str,
+                                               subfolder: str = 'text_encoder'):
     text_encoder_config = PretrainedConfig.from_pretrained(
         pretrained_model_name_or_path, subfolder=subfolder, revision=revision)
     model_class = text_encoder_config.architectures[0]
 
     if model_class == 'CLIPTextModel':
         from transformers import CLIPTextModel
 
@@ -137,530 +134,414 @@
 
         return CLIPTextModelWithProjection
     else:
         raise ValueError(f'{model_class} is not supported.')
 
 
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a dreambooth inference.')
+    parser = argparse.ArgumentParser(description='Simple example of a dreambooth inference.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--pretrained_vae_model_name_or_path',
         type=str,
         default=None,
         help='Path to pretrained VAE model with better numerical stability. '
         'More details: https://github.com/huggingface/diffusers/pull/4038.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--dataset_name',
         type=str,
         default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) containing the training data of '
-         'instance images (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
+        help=('The name of the Dataset (from the HuggingFace hub) containing the training data of '
+              'instance images (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
     )
     parser.add_argument(
         '--dataset_config_name',
         type=str,
         default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
         '--instance_data_dir',
         type=str,
         default=None,
         help=('A folder containing the training data. '),
     )
 
     parser.add_argument(
         '--cache_dir',
         type=str,
         default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        help='The directory where the downloaded models and datasets will be stored.',
     )
 
     parser.add_argument(
         '--image_column',
         type=str,
         default='image',
         help='The column of the dataset containing the target image. By '
         "default, the standard Image Dataset maps out 'file_name' "
         "to 'image'.",
     )
     parser.add_argument(
         '--caption_column',
         type=str,
         default=None,
-        help=
-        'The column of the dataset containing the instance prompt for each image',
+        help='The column of the dataset containing the instance prompt for each image',
     )
 
-    parser.add_argument(
-        '--repeats',
-        type=int,
-        default=1,
-        help='How many times to repeat the training data.')
+    parser.add_argument('--repeats', type=int, default=1, help='How many times to repeat the training data.')
 
     parser.add_argument(
         '--class_data_dir',
         type=str,
         default=None,
         required=False,
         help='A folder containing the training data of class images.',
     )
     parser.add_argument(
         '--instance_prompt',
         type=str,
         default=None,
         required=True,
-        help=
-        "The prompt with identifier specifying the instance, e.g. 'photo of a TOK dog', 'in the style of TOK'",
+        help="The prompt with identifier specifying the instance, e.g. 'photo of a TOK dog', 'in the style of TOK'",
     )
     parser.add_argument(
         '--class_prompt',
         type=str,
         default=None,
-        help=
-        'The prompt to specify images in the same class as provided instance images.',
+        help='The prompt to specify images in the same class as provided instance images.',
     )
     parser.add_argument(
         '--validation_prompt',
         type=str,
         default=None,
-        help=
-        'A prompt that is used during validation to verify that the model is learning.',
+        help='A prompt that is used during validation to verify that the model is learning.',
     )
     parser.add_argument(
         '--num_validation_images',
         type=int,
         default=4,
-        help=
-        'Number of images that should be generated during validation with `validation_prompt`.',
+        help='Number of images that should be generated during validation with `validation_prompt`.',
     )
     parser.add_argument(
         '--validation_epochs',
         type=int,
         default=50,
-        help=
-        ('Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`.'
-         ),
+        help=('Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`.'),
     )
     parser.add_argument(
         '--with_prior_preservation',
         default=False,
         action='store_true',
         help='Flag to add prior preservation loss.',
     )
-    parser.add_argument(
-        '--prior_loss_weight',
-        type=float,
-        default=1.0,
-        help='The weight of prior preservation loss.')
+    parser.add_argument('--prior_loss_weight', type=float, default=1.0, help='The weight of prior preservation loss.')
     parser.add_argument(
         '--num_class_images',
         type=int,
         default=100,
-        help=
-        ('Minimal class images for prior preservation loss. If there are not enough images already present in'
-         ' class_data_dir, additional images will be sampled with class_prompt.'
-         ),
+        help=('Minimal class images for prior preservation loss. If there are not enough images already present in'
+              ' class_data_dir, additional images will be sampled with class_prompt.'),
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='lora-dreambooth-model',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=1024,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
         '--crops_coords_top_left_h',
         type=int,
         default=0,
-        help=
-        ('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'
-         ),
+        help=('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'),
     )
     parser.add_argument(
         '--crops_coords_top_left_w',
         type=int,
         default=0,
-        help=
-        ('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'
-         ),
+        help=('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'),
     )
     parser.add_argument(
         '--center_crop',
         default=False,
         action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
     )
     parser.add_argument(
         '--train_text_encoder',
         action='store_true',
-        help=
-        'Whether to train the text encoder. If set, the text encoder should be float32 precision.',
+        help='Whether to train the text encoder. If set, the text encoder should be float32 precision.',
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for the training dataloader.')
-    parser.add_argument(
-        '--sample_batch_size',
-        type=int,
-        default=4,
-        help='Batch size (per device) for sampling images.')
+        '--train_batch_size', type=int, default=4, help='Batch size (per device) for the training dataloader.')
+    parser.add_argument('--sample_batch_size', type=int, default=4, help='Batch size (per device) for sampling images.')
     parser.add_argument('--num_train_epochs', type=int, default=1)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
-        help=
-        ('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
-         ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
-         ' training using `--resume_from_checkpoint`.'),
+        help=('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
+              ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
+              ' training using `--resume_from_checkpoint`.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=1e-4,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
 
     parser.add_argument(
         '--text_encoder_lr',
         type=float,
         default=5e-6,
         help='Text encoder learning rate to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
 
     parser.add_argument(
         '--snr_gamma',
         type=float,
         default=None,
-        help=
-        'SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
+        help='SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
         'More details here: https://arxiv.org/abs/2303.09556.',
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--lr_num_cycles',
         type=int,
         default=1,
-        help=
-        'Number of hard resets of the lr in cosine_with_restarts scheduler.',
+        help='Number of hard resets of the lr in cosine_with_restarts scheduler.',
     )
-    parser.add_argument(
-        '--lr_power',
-        type=float,
-        default=1.0,
-        help='Power factor of the polynomial scheduler.')
+    parser.add_argument('--lr_power', type=float, default=1.0, help='Power factor of the polynomial scheduler.')
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
     )
 
     parser.add_argument(
         '--optimizer',
         type=str,
         default='AdamW',
-        help=(
-            'The optimizer type to use. Choose between ["AdamW", "prodigy"]'),
+        help=('The optimizer type to use. Choose between ["AdamW", "prodigy"]'),
     )
 
     parser.add_argument(
         '--use_8bit_adam',
         action='store_true',
-        help=
-        'Whether or not to use 8-bit Adam from bitsandbytes. Ignored if optimizer is not set to AdamW',
+        help='Whether or not to use 8-bit Adam from bitsandbytes. Ignored if optimizer is not set to AdamW',
     )
 
     parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam and Prodigy optimizers.')
+        '--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam and Prodigy optimizers.')
     parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam and Prodigy optimizers.')
+        '--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam and Prodigy optimizers.')
     parser.add_argument(
         '--prodigy_beta3',
         type=float,
         default=None,
-        help=
-        'coefficients for computing the Prodidy stepsize using running averages. If set to None, '
+        help='coefficients for computing the Prodidy stepsize using running averages. If set to None, '
         'uses the value of square root of beta2. Ignored if optimizer is adamW',
     )
+    parser.add_argument('--prodigy_decouple', type=bool, default=True, help='Use AdamW style decoupled weight decay')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-04, help='Weight decay to use for unet params')
     parser.add_argument(
-        '--prodigy_decouple',
-        type=bool,
-        default=True,
-        help='Use AdamW style decoupled weight decay')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-04,
-        help='Weight decay to use for unet params')
-    parser.add_argument(
-        '--adam_weight_decay_text_encoder',
-        type=float,
-        default=1e-03,
-        help='Weight decay to use for text_encoder')
+        '--adam_weight_decay_text_encoder', type=float, default=1e-03, help='Weight decay to use for text_encoder')
 
     parser.add_argument(
         '--adam_epsilon',
         type=float,
         default=1e-08,
         help='Epsilon value for the Adam optimizer and Prodigy optimizers.',
     )
 
     parser.add_argument(
         '--prodigy_use_bias_correction',
         type=bool,
         default=True,
-        help=
-        "Turn on Adam's bias correction. True by default. Ignored if optimizer is adamW",
+        help="Turn on Adam's bias correction. True by default. Ignored if optimizer is adamW",
     )
     parser.add_argument(
         '--prodigy_safeguard_warmup',
         type=bool,
         default=True,
-        help=
-        'Remove lr from the denominator of D estimate to avoid issues during warm-up stage. True by default. '
+        help='Remove lr from the denominator of D estimate to avoid issues during warm-up stage. True by default. '
         'Ignored if optimizer is adamW',
     )
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
     parser.add_argument(
         '--prior_generation_precision',
         type=str,
         default=None,
         choices=['no', 'fp32', 'fp16', 'bf16'],
-        help=
-        ('Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.'
-         ),
+        help=('Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+              ' 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.'),
     )
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
-        '--local_rank',
-        type=int,
-        default=-1,
-        help='For distributed training: local_rank')
-    parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
     parser.add_argument(
         '--rank',
         type=int,
         default=4,
         help=('The dimension of the LoRA update matrices.'),
     )
 
     if input_args is not None:
         args = parser.parse_args(input_args)
     else:
         args = parser.parse_args()
 
     if args.dataset_name is None and args.instance_data_dir is None:
-        raise ValueError(
-            'Specify either `--dataset_name` or `--instance_data_dir`')
+        raise ValueError('Specify either `--dataset_name` or `--instance_data_dir`')
 
     if args.dataset_name is not None and args.instance_data_dir is not None:
-        raise ValueError(
-            'Specify only one of `--dataset_name` or `--instance_data_dir`')
+        raise ValueError('Specify only one of `--dataset_name` or `--instance_data_dir`')
 
     env_local_rank = int(os.environ.get('LOCAL_RANK', -1))
     if env_local_rank != -1 and env_local_rank != args.local_rank:
         args.local_rank = env_local_rank
 
     if args.with_prior_preservation:
         if args.class_data_dir is None:
-            raise ValueError(
-                'You must specify a data directory for class images.')
+            raise ValueError('You must specify a data directory for class images.')
         if args.class_prompt is None:
             raise ValueError('You must specify prompt for class images.')
     else:
         # logger is not available yet
         if args.class_data_dir is not None:
-            warnings.warn(
-                'You need not use --class_data_dir without --with_prior_preservation.'
-            )
+            warnings.warn('You need not use --class_data_dir without --with_prior_preservation.')
         if args.class_prompt is not None:
-            warnings.warn(
-                'You need not use --class_prompt without --with_prior_preservation.'
-            )
+            warnings.warn('You need not use --class_prompt without --with_prior_preservation.')
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
 
     args.vae_base_model_id = args.pretrained_vae_model_name_or_path
-    if args.pretrained_vae_model_name_or_path and not os.path.exists(
-            args.pretrained_vae_model_name_or_path):
-        args.pretrained_vae_model_name_or_path = snapshot_download(
-            args.pretrained_vae_model_name_or_path)
+    if args.pretrained_vae_model_name_or_path and not os.path.exists(args.pretrained_vae_model_name_or_path):
+        args.pretrained_vae_model_name_or_path = snapshot_download(args.pretrained_vae_model_name_or_path)
 
     return args
 
 
 class DreamBoothDataset(Dataset):
     """
     A dataset to prepare the instance and class images with the prompts for fine-tuning the model.
@@ -691,16 +572,15 @@
         if args.dataset_name is not None:
             try:
                 from datasets import load_dataset
             except ImportError:
                 raise ImportError(
                     'You are trying to load your data using the datasets library. If you wish to train using custom '
                     'captions please install the datasets library: `pip install datasets`. If you wish to load a '
-                    'local folder containing images only, specify --instance_data_dir instead.'
-                )
+                    'local folder containing images only, specify --instance_data_dir instead.')
             # Downloading and loading a dataset from the hub.
             # See more about loading custom images at
             # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script
             dataset = load_dataset(
                 args.dataset_name,
                 args.dataset_config_name,
                 cache_dir=args.cache_dir,
@@ -711,71 +591,61 @@
             # 6. Get the column names for input/target.
             if args.image_column is None:
                 image_column = column_names[0]
                 logger.info(f'image column defaulting to {image_column}')
             else:
                 image_column = args.image_column
                 if image_column not in column_names:
-                    raise ValueError(
-                        f"`--image_column` value '{args.image_column}' not found in dataset columns. "
-                        f"Dataset columns are: {', '.join(column_names)}")
+                    raise ValueError(f"`--image_column` value '{args.image_column}' not found in dataset columns. "
+                                     f"Dataset columns are: {', '.join(column_names)}")
             instance_images = dataset['train'][image_column]
 
             if args.caption_column is None:
-                logger.info(
-                    'No caption column provided, defaulting to instance_prompt for all images. If your dataset '
-                    'contains captions/prompts for the images, make sure to specify the '
-                    'column as --caption_column')
+                logger.info('No caption column provided, defaulting to instance_prompt for all images. If your dataset '
+                            'contains captions/prompts for the images, make sure to specify the '
+                            'column as --caption_column')
                 self.custom_instance_prompts = None
             else:
                 if args.caption_column not in column_names:
-                    raise ValueError(
-                        f"`--caption_column` value '{args.caption_column}' not found in dataset columns. "
-                        f"Dataset columns are: {', '.join(column_names)}")
+                    raise ValueError(f"`--caption_column` value '{args.caption_column}' not found in dataset columns. "
+                                     f"Dataset columns are: {', '.join(column_names)}")
                 custom_instance_prompts = dataset['train'][args.caption_column]
                 # create final list of captions according to --repeats
                 self.custom_instance_prompts = []
                 for caption in custom_instance_prompts:
-                    self.custom_instance_prompts.extend(
-                        itertools.repeat(caption, repeats))
+                    self.custom_instance_prompts.extend(itertools.repeat(caption, repeats))
         else:
             self.instance_data_root = Path(instance_data_root)
             if not self.instance_data_root.exists():
                 raise ValueError("Instance images root doesn't exists.")
 
-            instance_images = [
-                Image.open(path)
-                for path in list(Path(instance_data_root).iterdir())
-            ]
+            instance_images = [Image.open(path) for path in list(Path(instance_data_root).iterdir())]
             self.custom_instance_prompts = None
 
         self.instance_images = []
         for img in instance_images:
             self.instance_images.extend(itertools.repeat(img, repeats))
         self.num_instance_images = len(self.instance_images)
         self._length = self.num_instance_images
 
         if class_data_root is not None:
             self.class_data_root = Path(class_data_root)
             self.class_data_root.mkdir(parents=True, exist_ok=True)
             self.class_images_path = list(self.class_data_root.iterdir())
             if class_num is not None:
-                self.num_class_images = min(
-                    len(self.class_images_path), class_num)
+                self.num_class_images = min(len(self.class_images_path), class_num)
             else:
                 self.num_class_images = len(self.class_images_path)
             self._length = max(self.num_class_images, self.num_instance_images)
         else:
             self.class_data_root = None
 
         self.image_transforms = transforms.Compose([
-            transforms.Resize(
-                size, interpolation=transforms.InterpolationMode.BILINEAR),
-            transforms.CenterCrop(size)
-            if center_crop else transforms.RandomCrop(size),
+            transforms.Resize(size, interpolation=transforms.InterpolationMode.BILINEAR),
+            transforms.CenterCrop(size) if center_crop else transforms.RandomCrop(size),
             transforms.ToTensor(),
             transforms.Normalize([0.5], [0.5]),
         ])
 
     def __len__(self):
         return self._length
 
@@ -785,27 +655,25 @@
         instance_image = exif_transpose(instance_image)
 
         if not instance_image.mode == 'RGB':
             instance_image = instance_image.convert('RGB')
         example['instance_images'] = self.image_transforms(instance_image)
 
         if self.custom_instance_prompts:
-            caption = self.custom_instance_prompts[index
-                                                   % self.num_instance_images]
+            caption = self.custom_instance_prompts[index % self.num_instance_images]
             if caption:
                 example['instance_prompt'] = caption
             else:
                 example['instance_prompt'] = self.instance_prompt
 
         else:  # costum prompts were provided, but length does not match size of image dataset
             example['instance_prompt'] = self.instance_prompt
 
         if self.class_data_root:
-            class_image = Image.open(
-                self.class_images_path[index % self.num_class_images])
+            class_image = Image.open(self.class_images_path[index % self.num_class_images])
             class_image = exif_transpose(class_image)
 
             if not class_image.mode == 'RGB':
                 class_image = class_image.convert('RGB')
             example['class_images'] = self.image_transforms(class_image)
             example['class_prompt'] = self.class_prompt
 
@@ -819,16 +687,15 @@
     # Concat class and instance examples for prior preservation.
     # We do this to avoid doing two forward passes.
     if with_prior_preservation:
         pixel_values += [example['class_images'] for example in examples]
         prompts += [example['class_prompt'] for example in examples]
 
     pixel_values = torch.stack(pixel_values)
-    pixel_values = pixel_values.to(
-        memory_format=torch.contiguous_format).float()
+    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
 
     batch = {'pixel_values': pixel_values, 'prompts': prompts}
     return batch
 
 
 class PromptDataset(Dataset):
     'A simple dataset to prepare the prompts to generate class images on multiple GPUs.'
@@ -888,30 +755,27 @@
     return prompt_embeds, pooled_prompt_embeds
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
     kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
         kwargs_handlers=[kwargs],
     )
 
     if args.report_to == 'wandb':
         if not is_wandb_available():
-            raise ImportError(
-                'Make sure to install wandb if you want to use it for logging during training.'
-            )
+            raise ImportError('Make sure to install wandb if you want to use it for logging during training.')
         import wandb
 
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
@@ -951,29 +815,25 @@
             )
             pipeline.set_progress_bar_config(disable=True)
 
             num_new_images = args.num_class_images - cur_class_images
             logger.info(f'Number of class images to sample: {num_new_images}.')
 
             sample_dataset = PromptDataset(args.class_prompt, num_new_images)
-            sample_dataloader = torch.utils.data.DataLoader(
-                sample_dataset, batch_size=args.sample_batch_size)
+            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)
 
             sample_dataloader = accelerator.prepare(sample_dataloader)
             pipeline.to(accelerator.device)
 
             for example in tqdm(
-                    sample_dataloader,
-                    desc='Generating class images',
-                    disable=not accelerator.is_local_main_process):
+                    sample_dataloader, desc='Generating class images', disable=not accelerator.is_local_main_process):
                 images = pipeline(example['prompt']).images
 
                 for i, image in enumerate(images):
-                    hash_image = insecure_hashlib.sha1(
-                        image.tobytes()).hexdigest()
+                    hash_image = insecure_hashlib.sha1(image.tobytes()).hexdigest()
                     image_filename = class_images_dir / f"{example['index'][i] + cur_class_images}-{hash_image}.jpg"
                     image.save(image_filename)
 
             del pipeline
             if torch.cuda.is_available():
                 torch.cuda.empty_cache()
 
@@ -993,50 +853,35 @@
         args.pretrained_model_name_or_path,
         subfolder='tokenizer_2',
         revision=args.revision,
         use_fast=False,
     )
 
     # import correct text encoder classes
-    text_encoder_cls_one = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls_one = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
     text_encoder_cls_two = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path,
-        args.revision,
-        subfolder='text_encoder_2')
+        args.pretrained_model_name_or_path, args.revision, subfolder='text_encoder_2')
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     text_encoder_one = text_encoder_cls_one.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
     text_encoder_two = text_encoder_cls_two.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder_2',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder_2', revision=args.revision, variant=args.variant)
     vae_path = (
         args.pretrained_model_name_or_path
-        if args.pretrained_vae_model_name_or_path is None else
-        args.pretrained_vae_model_name_or_path)
+        if args.pretrained_vae_model_name_or_path is None else args.pretrained_vae_model_name_or_path)
     vae = AutoencoderKL.from_pretrained(
         vae_path,
-        subfolder='vae'
-        if args.pretrained_vae_model_name_or_path is None else None,
+        subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
         revision=args.revision,
         variant=args.variant,
     )
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
     # We only train the additional adapter LoRA layers
     vae.requires_grad_(False)
     text_encoder_one.requires_grad_(False)
     text_encoder_two.requires_grad_(False)
     unet.requires_grad_(False)
 
@@ -1060,129 +905,100 @@
 
     if args.enable_xformers_memory_efficient_attention:
         if is_xformers_available():
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
-                logger.warn(
-                    'xFormers 0.0.16 cannot be used for training in some GPUs. '
-                    'If you observe problems during training, '
-                    'please update xFormers to at least 0.0.17. See '
-                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                logger.warn('xFormers 0.0.16 cannot be used for training in some GPUs. '
+                            'If you observe problems during training, '
+                            'please update xFormers to at least 0.0.17. See '
+                            'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     if args.gradient_checkpointing:
         unet.enable_gradient_checkpointing()
         if args.train_text_encoder:
             text_encoder_one.gradient_checkpointing_enable()
             text_encoder_two.gradient_checkpointing_enable()
 
     # now we will add new LoRA weights to the attention layers
     unet_lora_config = LoRAConfig(
-        r=args.rank,
-        init_lora_weights='gaussian',
-        target_modules=['to_k', 'to_q', 'to_v', 'to_out.0'])
+        r=args.rank, init_lora_weights='gaussian', target_modules=['to_k', 'to_q', 'to_v', 'to_out.0'])
     unet = Swift.prepare_model(unet, unet_lora_config)
     if args.mixed_precision == 'fp16':
         for param in unet.parameters():
             # only upcast trainable parameters (LoRA) into fp32
             if param.requires_grad:
                 param.data = param.to(torch.float32)
 
     # The text encoder comes from  transformers, so we cannot directly modify it.
     # So, instead, we monkey-patch the forward calls of its attention-blocks.
     if args.train_text_encoder:
         text_lora_config = LoRAConfig(
-            r=args.rank,
-            init_lora_weights='gaussian',
-            target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj'])
-        text_encoder_one = Swift.prepare_model(text_encoder_one,
-                                               text_lora_config)
-        text_encoder_two = Swift.prepare_model(text_encoder_two,
-                                               text_lora_config)
+            r=args.rank, init_lora_weights='gaussian', target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj'])
+        text_encoder_one = Swift.prepare_model(text_encoder_one, text_lora_config)
+        text_encoder_two = Swift.prepare_model(text_encoder_two, text_lora_config)
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
-    unet_lora_parameters = list(
-        filter(lambda p: p.requires_grad, unet.parameters()))
+    unet_lora_parameters = list(filter(lambda p: p.requires_grad, unet.parameters()))
 
     if args.train_text_encoder:
-        text_lora_parameters_one = list(
-            filter(lambda p: p.requires_grad, text_encoder_one.parameters()))
-        text_lora_parameters_two = list(
-            filter(lambda p: p.requires_grad, text_encoder_two.parameters()))
+        text_lora_parameters_one = list(filter(lambda p: p.requires_grad, text_encoder_one.parameters()))
+        text_lora_parameters_two = list(filter(lambda p: p.requires_grad, text_encoder_two.parameters()))
 
     # Optimization parameters
-    unet_lora_parameters_with_lr = {
-        'params': unet_lora_parameters,
-        'lr': args.learning_rate
-    }
+    unet_lora_parameters_with_lr = {'params': unet_lora_parameters, 'lr': args.learning_rate}
     if args.train_text_encoder:
         # different learning rate for text encoder and unet
         text_lora_parameters_one_with_lr = {
-            'params':
-            text_lora_parameters_one,
-            'weight_decay':
-            args.adam_weight_decay_text_encoder,
-            'lr':
-            args.text_encoder_lr
-            if args.text_encoder_lr else args.learning_rate,
+            'params': text_lora_parameters_one,
+            'weight_decay': args.adam_weight_decay_text_encoder,
+            'lr': args.text_encoder_lr if args.text_encoder_lr else args.learning_rate,
         }
         text_lora_parameters_two_with_lr = {
-            'params':
-            text_lora_parameters_two,
-            'weight_decay':
-            args.adam_weight_decay_text_encoder,
-            'lr':
-            args.text_encoder_lr
-            if args.text_encoder_lr else args.learning_rate,
+            'params': text_lora_parameters_two,
+            'weight_decay': args.adam_weight_decay_text_encoder,
+            'lr': args.text_encoder_lr if args.text_encoder_lr else args.learning_rate,
         }
         params_to_optimize = [
             unet_lora_parameters_with_lr,
             text_lora_parameters_one_with_lr,
             text_lora_parameters_two_with_lr,
         ]
     else:
         params_to_optimize = [unet_lora_parameters_with_lr]
 
     # Optimizer creation
-    if not (args.optimizer.lower() == 'prodigy'
-            or args.optimizer.lower() == 'adamw'):
-        logger.warn(
-            f'Unsupported choice of optimizer: {args.optimizer}.Supported optimizers include [adamW, prodigy].'
-            'Defaulting to adamW')
+    if not (args.optimizer.lower() == 'prodigy' or args.optimizer.lower() == 'adamw'):
+        logger.warn(f'Unsupported choice of optimizer: {args.optimizer}.Supported optimizers include [adamW, prodigy].'
+                    'Defaulting to adamW')
         args.optimizer = 'adamw'
 
     if args.use_8bit_adam and not args.optimizer.lower() == 'adamw':
-        logger.warn(
-            f"use_8bit_adam is ignored when optimizer is not set to 'AdamW'. Optimizer was "
-            f'set to {args.optimizer.lower()}')
+        logger.warn(f"use_8bit_adam is ignored when optimizer is not set to 'AdamW'. Optimizer was "
+                    f'set to {args.optimizer.lower()}')
 
     if args.optimizer.lower() == 'adamw':
         if args.use_8bit_adam:
             try:
                 import bitsandbytes as bnb
             except ImportError:
                 raise ImportError(
-                    'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-                )
+                    'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
             optimizer_class = bnb.optim.AdamW8bit
         else:
             optimizer_class = torch.optim.AdamW
 
         optimizer = optimizer_class(
             params_to_optimize,
@@ -1191,17 +1007,15 @@
             eps=args.adam_epsilon,
         )
 
     if args.optimizer.lower() == 'prodigy':
         try:
             import prodigyopt
         except ImportError:
-            raise ImportError(
-                'To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`'
-            )
+            raise ImportError('To use Prodigy, please install the prodigyopt library: `pip install prodigyopt`')
 
         optimizer_class = prodigyopt.Prodigy
 
         optimizer = optimizer_class(
             params_to_optimize,
             lr=args.learning_rate,
             betas=(args.adam_beta1, args.adam_beta2),
@@ -1214,59 +1028,53 @@
 
     # Dataset and DataLoaders creation:
     train_dataset = DreamBoothDataset(
         args=args,
         instance_data_root=args.instance_data_dir,
         instance_prompt=args.instance_prompt,
         class_prompt=args.class_prompt,
-        class_data_root=args.class_data_dir
-        if args.with_prior_preservation else None,
+        class_data_root=args.class_data_dir if args.with_prior_preservation else None,
         class_num=args.num_class_images,
         size=args.resolution,
         repeats=args.repeats,
         center_crop=args.center_crop,
     )
 
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         batch_size=args.train_batch_size,
         shuffle=True,
-        collate_fn=lambda examples: collate_fn(examples, args.
-                                               with_prior_preservation),
+        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),
         num_workers=args.dataloader_num_workers,
     )
 
     # Computes additional embeddings/ids required by the SDXL UNet.
     # regular text embeddings (when `train_text_encoder` is not True)
     # pooled text embeddings
     # time ids
 
     def compute_time_ids():
         # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids
         original_size = (args.resolution, args.resolution)
         target_size = (args.resolution, args.resolution)
-        crops_coords_top_left = (args.crops_coords_top_left_h,
-                                 args.crops_coords_top_left_w)
-        add_time_ids = list(original_size + crops_coords_top_left
-                            + target_size)
+        crops_coords_top_left = (args.crops_coords_top_left_h, args.crops_coords_top_left_w)
+        add_time_ids = list(original_size + crops_coords_top_left + target_size)
         add_time_ids = torch.tensor([add_time_ids])
         add_time_ids = add_time_ids.to(accelerator.device, dtype=weight_dtype)
         return add_time_ids
 
     if not args.train_text_encoder:
         tokenizers = [tokenizer_one, tokenizer_two]
         text_encoders = [text_encoder_one, text_encoder_two]
 
         def compute_text_embeddings(prompt, text_encoders, tokenizers):
             with torch.no_grad():
-                prompt_embeds, pooled_prompt_embeds = encode_prompt(
-                    text_encoders, tokenizers, prompt)
+                prompt_embeds, pooled_prompt_embeds = encode_prompt(text_encoders, tokenizers, prompt)
                 prompt_embeds = prompt_embeds.to(accelerator.device)
-                pooled_prompt_embeds = pooled_prompt_embeds.to(
-                    accelerator.device)
+                pooled_prompt_embeds = pooled_prompt_embeds.to(accelerator.device)
             return prompt_embeds, pooled_prompt_embeds
 
     # Handle instance prompt.
     instance_time_ids = compute_time_ids()
 
     # If no type of tuning is done on the text_encoder and custom instance prompts are NOT
     # provided (i.e. the --instance_prompt is used for all images), we encode the instance prompt once to avoid
@@ -1296,35 +1104,30 @@
         add_time_ids = torch.cat([add_time_ids, class_time_ids], dim=0)
 
     if not train_dataset.custom_instance_prompts:
         if not args.train_text_encoder:
             prompt_embeds = instance_prompt_hidden_states
             unet_add_text_embeds = instance_pooled_prompt_embeds
             if args.with_prior_preservation:
-                prompt_embeds = torch.cat(
-                    [prompt_embeds, class_prompt_hidden_states], dim=0)
-                unet_add_text_embeds = torch.cat(
-                    [unet_add_text_embeds, class_pooled_prompt_embeds], dim=0)
+                prompt_embeds = torch.cat([prompt_embeds, class_prompt_hidden_states], dim=0)
+                unet_add_text_embeds = torch.cat([unet_add_text_embeds, class_pooled_prompt_embeds], dim=0)
         # if we're optmizing the text encoder (both if instance prompt is used for all images or custom prompts)
         # we need to tokenize and encode the batch prompts on all training steps
         else:
             tokens_one = tokenize_prompt(tokenizer_one, args.instance_prompt)
             tokens_two = tokenize_prompt(tokenizer_two, args.instance_prompt)
             if args.with_prior_preservation:
-                class_tokens_one = tokenize_prompt(tokenizer_one,
-                                                   args.class_prompt)
-                class_tokens_two = tokenize_prompt(tokenizer_two,
-                                                   args.class_prompt)
+                class_tokens_one = tokenize_prompt(tokenizer_one, args.class_prompt)
+                class_tokens_two = tokenize_prompt(tokenizer_two, args.class_prompt)
                 tokens_one = torch.cat([tokens_one, class_tokens_one], dim=0)
                 tokens_two = torch.cat([tokens_two, class_tokens_two], dim=0)
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
@@ -1333,48 +1136,41 @@
         num_cycles=args.lr_num_cycles,
         power=args.lr_power,
     )
 
     # Prepare everything with our `accelerator`.
     if args.train_text_encoder:
         unet, text_encoder_one, text_encoder_two, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-            unet, text_encoder_one, text_encoder_two, optimizer,
-            train_dataloader, lr_scheduler)
+            unet, text_encoder_one, text_encoder_two, optimizer, train_dataloader, lr_scheduler)
     else:
-        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-            unet, optimizer, train_dataloader, lr_scheduler)
+        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                              lr_scheduler)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         accelerator.init_trackers('dreambooth-lora-sd-xl', config=vars(args))
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num batches each epoch = {len(train_dataloader)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1384,16 +1180,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -1442,211 +1237,158 @@
                     model_input = model_input.to(weight_dtype)
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(model_input)
                 bsz = model_input.shape[0]
                 # Sample a random timestep for each image
                 timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=model_input.device)
+                    0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=model_input.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the model input according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_model_input = noise_scheduler.add_noise(
-                    model_input, noise, timesteps)
+                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)
 
                 # Calculate the elements to repeat depending on the use of prior-preservation and custom captions.
                 if not train_dataset.custom_instance_prompts:
                     elems_to_repeat_text_embeds = bsz // 2 if args.with_prior_preservation else bsz
                     elems_to_repeat_time_ids = bsz // 2 if args.with_prior_preservation else bsz
                 else:
                     elems_to_repeat_text_embeds = 1
                     elems_to_repeat_time_ids = bsz // 2 if args.with_prior_preservation else bsz
 
                 # Predict the noise residual
                 if not args.train_text_encoder:
                     unet_added_conditions = {
-                        'time_ids':
-                        add_time_ids.repeat(elems_to_repeat_time_ids, 1),
-                        'text_embeds':
-                        unet_add_text_embeds.repeat(
-                            elems_to_repeat_text_embeds, 1),
+                        'time_ids': add_time_ids.repeat(elems_to_repeat_time_ids, 1),
+                        'text_embeds': unet_add_text_embeds.repeat(elems_to_repeat_text_embeds, 1),
                     }
-                    prompt_embeds_input = prompt_embeds.repeat(
-                        elems_to_repeat_text_embeds, 1, 1)
+                    prompt_embeds_input = prompt_embeds.repeat(elems_to_repeat_text_embeds, 1, 1)
                     model_pred = unet(
                         noisy_model_input,
                         timesteps,
                         prompt_embeds_input,
                         added_cond_kwargs=unet_added_conditions,
                     ).sample
                 else:
-                    unet_added_conditions = {
-                        'time_ids':
-                        add_time_ids.repeat(elems_to_repeat_time_ids, 1)
-                    }
+                    unet_added_conditions = {'time_ids': add_time_ids.repeat(elems_to_repeat_time_ids, 1)}
                     prompt_embeds, pooled_prompt_embeds = encode_prompt(
                         text_encoders=[text_encoder_one, text_encoder_two],
                         tokenizers=None,
                         prompt=None,
                         text_input_ids_list=[tokens_one, tokens_two],
                     )
-                    unet_added_conditions.update({
-                        'text_embeds':
-                        pooled_prompt_embeds.repeat(
-                            elems_to_repeat_text_embeds, 1)
-                    })
-                    prompt_embeds_input = prompt_embeds.repeat(
-                        elems_to_repeat_text_embeds, 1, 1)
+                    unet_added_conditions.update(
+                        {'text_embeds': pooled_prompt_embeds.repeat(elems_to_repeat_text_embeds, 1)})
+                    prompt_embeds_input = prompt_embeds.repeat(elems_to_repeat_text_embeds, 1, 1)
                     model_pred = unet(
-                        noisy_model_input,
-                        timesteps,
-                        prompt_embeds_input,
+                        noisy_model_input, timesteps, prompt_embeds_input,
                         added_cond_kwargs=unet_added_conditions).sample
 
                 # Get the target for loss depending on the prediction type
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        model_input, noise, timesteps)
+                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
                 if args.with_prior_preservation:
                     # Chunk the noise and model_pred into two parts and compute the loss on each part separately.
-                    model_pred, model_pred_prior = torch.chunk(
-                        model_pred, 2, dim=0)
+                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)
                     target, target_prior = torch.chunk(target, 2, dim=0)
 
                     # Compute prior loss
-                    prior_loss = F.mse_loss(
-                        model_pred_prior.float(),
-                        target_prior.float(),
-                        reduction='mean')
+                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction='mean')
 
                 if args.snr_gamma is None:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
                 else:
                     # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
                     # Since we predict the noise instead of x_0, the original formulation is slightly changed.
                     # This is discussed in Section 4.2 of the same paper.
                     snr = compute_snr(noise_scheduler, timesteps)
                     base_weight = (
-                        torch.stack(
-                            [snr, args.snr_gamma * torch.ones_like(timesteps)],
-                            dim=1).min(dim=1)[0] / snr)
+                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr)
 
                     if noise_scheduler.config.prediction_type == 'v_prediction':
                         # Velocity objective needs to be floored to an SNR weight of one.
                         mse_loss_weights = base_weight + 1
                     else:
                         # Epsilon and sample both use the same loss weights.
                         mse_loss_weights = base_weight
 
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='none')
-                    loss = loss.mean(
-                        dim=list(range(1, len(loss.shape)))) * mse_loss_weights
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='none')
+                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                     loss = loss.mean()
 
                 if args.with_prior_preservation:
                     # Add the prior loss to the instance loss.
                     loss = loss + args.prior_loss_weight * prior_loss
 
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
                     params_to_clip = (
-                        itertools.chain(unet_lora_parameters,
-                                        text_lora_parameters_one,
-                                        text_lora_parameters_two)
+                        itertools.chain(unet_lora_parameters, text_lora_parameters_one, text_lora_parameters_two)
                         if args.train_text_encoder else unet_lora_parameters)
-                    accelerator.clip_grad_norm_(params_to_clip,
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad()
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint,
                             # we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
-                        accelerator.unwrap_model(unet).to(
-                            torch.float32).save_pretrained(
-                                os.path.join(save_path, 'unet'))
+                        accelerator.unwrap_model(unet).to(torch.float32).save_pretrained(
+                            os.path.join(save_path, 'unet'))
 
                         if args.train_text_encoder:
-                            accelerator.unwrap_model(
-                                text_encoder_one).save_pretrained(
-                                    os.path.join(save_path, 'text_encoder1'))
-                            accelerator.unwrap_model(
-                                text_encoder_two).save_pretrained(
-                                    os.path.join(save_path, 'text_encoder2'))
+                            accelerator.unwrap_model(text_encoder_one).save_pretrained(
+                                os.path.join(save_path, 'text_encoder1'))
+                            accelerator.unwrap_model(text_encoder_two).save_pretrained(
+                                os.path.join(save_path, 'text_encoder2'))
                         logger.info(f'Saved state to {save_path}')
 
-            logs = {
-                'loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
             accelerator.log(logs, step=global_step)
 
             if global_step >= args.max_train_steps:
                 break
 
         if accelerator.is_main_process:
             if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
-                logger.info(
-                    f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
-                    f' {args.validation_prompt}.')
+                logger.info(f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
+                            f' {args.validation_prompt}.')
                 # create pipeline
                 if not args.train_text_encoder:
                     text_encoder_one = text_encoder_cls_one.from_pretrained(
                         args.pretrained_model_name_or_path,
                         subfolder='text_encoder',
                         revision=args.revision,
                         variant=args.variant,
@@ -1676,74 +1418,63 @@
                     variance_type = pipeline.scheduler.config.variance_type
 
                     if variance_type in ['learned', 'learned_range']:
                         variance_type = 'fixed_small'
 
                     scheduler_args['variance_type'] = variance_type
 
-                pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
-                    pipeline.scheduler.config, **scheduler_args)
+                pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config,
+                                                                             **scheduler_args)
 
                 pipeline = pipeline.to(accelerator.device)
                 pipeline.set_progress_bar_config(disable=True)
 
                 # run inference
-                generator = torch.Generator(
-                    device=accelerator.device).manual_seed(
-                        args.seed) if args.seed else None
+                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
                 pipeline_args = {'prompt': args.validation_prompt}
 
                 with torch.cuda.amp.autocast():
                     images = [
-                        pipeline(**pipeline_args,
-                                 generator=generator).images[0]
+                        pipeline(**pipeline_args, generator=generator).images[0]
                         for _ in range(args.num_validation_images)
                     ]
 
                 for tracker in accelerator.trackers:
                     if tracker.name == 'tensorboard':
-                        np_images = np.stack(
-                            [np.asarray(img) for img in images])
-                        tracker.writer.add_images(
-                            'validation', np_images, epoch, dataformats='NHWC')
+                        np_images = np.stack([np.asarray(img) for img in images])
+                        tracker.writer.add_images('validation', np_images, epoch, dataformats='NHWC')
                     if tracker.name == 'wandb':
                         tracker.log({
                             'validation': [
-                                wandb.Image(
-                                    image,
-                                    caption=f'{i}: {args.validation_prompt}')
+                                wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                                 for i, image in enumerate(images)
                             ]
                         })
 
                 del pipeline
                 torch.cuda.empty_cache()
 
     # Save the lora layers
     accelerator.wait_for_everyone()
     if accelerator.is_main_process:
         unet = accelerator.unwrap_model(unet)
         unet = unet.to(torch.float32)
-        unet.save_pretrained(
-            os.path.join(args.output_dir, 'iter-last', 'unet'))
+        unet.save_pretrained(os.path.join(args.output_dir, 'iter-last', 'unet'))
 
         if args.train_text_encoder:
             text_encoder_one = accelerator.unwrap_model(text_encoder_one)
-            text_encoder_one.save_pretrained(
-                os.path.join(args.output_dir, 'iter-last', 'text_encoder1'))
+            text_encoder_one.save_pretrained(os.path.join(args.output_dir, 'iter-last', 'text_encoder1'))
             text_encoder_two = accelerator.unwrap_model(text_encoder_two)
-            text_encoder_two.save_pretrained(
-                os.path.join(args.output_dir, 'iter-last', 'text_encoder2'))
+            text_encoder_two.save_pretrained(os.path.join(args.output_dir, 'iter-last', 'text_encoder2'))
 
         # Final inference
         # Load previous pipeline
         vae = AutoencoderKL.from_pretrained(
             vae_path,
-            subfolder='vae'
-            if args.pretrained_vae_model_name_or_path is None else None,
+            subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
             revision=args.revision,
             variant=args.variant,
             torch_dtype=weight_dtype,
         )
         pipeline = StableDiffusionXLPipeline.from_pretrained(
             args.pretrained_model_name_or_path,
             vae=vae,
@@ -1760,53 +1491,42 @@
             variance_type = pipeline.scheduler.config.variance_type
 
             if variance_type in ['learned', 'learned_range']:
                 variance_type = 'fixed_small'
 
             scheduler_args['variance_type'] = variance_type
 
-        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(
-            pipeline.scheduler.config, **scheduler_args)
+        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config, **scheduler_args)
 
         # load attention processors
-        pipeline.unet = Swift.from_pretrained(
-            pipeline.unet, os.path.join(args.output_dir, 'iter-last', 'unet'))
+        pipeline.unet = Swift.from_pretrained(pipeline.unet, os.path.join(args.output_dir, 'iter-last', 'unet'))
         if args.train_text_encoder:
             pipeline.text_encoder_one = Swift.from_pretrained(
-                pipeline.text_encoder_one,
-                os.path.join(args.output_dir, 'iter-last', 'text_encoder1'))
+                pipeline.text_encoder_one, os.path.join(args.output_dir, 'iter-last', 'text_encoder1'))
             pipeline.text_encoder_two = Swift.from_pretrained(
-                pipeline.text_encoder_two,
-                os.path.join(args.output_dir, 'iter-last', 'text_encoder2'))
+                pipeline.text_encoder_two, os.path.join(args.output_dir, 'iter-last', 'text_encoder2'))
 
         # run inference
         images = []
         if args.validation_prompt and args.num_validation_images > 0:
             pipeline = pipeline.to(accelerator.device)
-            generator = torch.Generator(device=accelerator.device).manual_seed(
-                args.seed) if args.seed else None
+            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
             images = [
-                pipeline(
-                    args.validation_prompt,
-                    num_inference_steps=25,
-                    generator=generator).images[0]
+                pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]
                 for _ in range(args.num_validation_images)
             ]
 
             for tracker in accelerator.trackers:
                 if tracker.name == 'tensorboard':
                     np_images = np.stack([np.asarray(img) for img in images])
-                    tracker.writer.add_images(
-                        'test', np_images, epoch, dataformats='NHWC')
+                    tracker.writer.add_images('test', np_images, epoch, dataformats='NHWC')
                 if tracker.name == 'wandb':
                     tracker.log({
                         'test': [
-                            wandb.Image(
-                                image,
-                                caption=f'{i}: {args.validation_prompt}')
+                            wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                             for i, image in enumerate(images)
                         ]
                     })
 
         if args.push_to_hub:
             save_model_card(
                 args.hub_model_id,
@@ -1814,12 +1534,10 @@
                 base_model=args.base_model_id,
                 train_text_encoder=args.train_text_encoder,
                 instance_prompt=args.instance_prompt,
                 validation_prompt=args.validation_prompt,
                 repo_folder=os.path.join(args.output_dir, 'iter-last'),
                 vae_path=args.vae_base_model_id,
             )
-            push_to_hub(args.hub_model_id,
-                        os.path.join(args.output_dir, 'iter-last'),
-                        args.hub_token)
+            push_to_hub(args.hub_model_id, os.path.join(args.output_dir, 'iter-last'), args.hub_token)
 
     accelerator.end_training()
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image.py`

 * *Files 4% similar despite different names*

```diff
@@ -29,20 +29,18 @@
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.state import AcceleratorState
 from accelerate.utils import ProjectConfiguration, set_seed
 from datasets import load_dataset
-from diffusers import (AutoencoderKL, DDPMScheduler, StableDiffusionPipeline,
-                       UNet2DConditionModel)
+from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionPipeline, UNet2DConditionModel
 from diffusers.optimization import get_scheduler
 from diffusers.training_utils import EMAModel, compute_snr
-from diffusers.utils import (check_min_version, deprecate, is_wandb_available,
-                             make_image_grid)
+from diffusers.utils import check_min_version, deprecate, is_wandb_available, make_image_grid
 from diffusers.utils.import_utils import is_xformers_available
 from modelscope import MsDataset
 from packaging import version
 from PIL import Image
 from torchvision import transforms
 from tqdm.auto import tqdm
 from transformers import CLIPTextModel, CLIPTokenizer
@@ -132,16 +130,15 @@
 
     model_card += wandb_info
 
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
-def log_validation(vae, text_encoder, tokenizer, unet, args, accelerator,
-                   weight_dtype, epoch):
+def log_validation(vae, text_encoder, tokenizer, unet, args, accelerator, weight_dtype, epoch):
     logger.info('Running validation... ')
 
     pipeline = StableDiffusionPipeline.from_pretrained(
         args.pretrained_model_name_or_path,
         vae=accelerator.unwrap_model(vae),
         text_encoder=accelerator.unwrap_model(text_encoder),
         tokenizer=tokenizer,
@@ -156,395 +153,297 @@
 
     if args.enable_xformers_memory_efficient_attention:
         pipeline.enable_xformers_memory_efficient_attention()
 
     if args.seed is None:
         generator = None
     else:
-        generator = torch.Generator(device=accelerator.device).manual_seed(
-            args.seed)
+        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)
 
     images = []
     for i in range(len(args.validation_prompts)):
         with torch.autocast('cuda'):
-            image = pipeline(
-                args.validation_prompts[i],
-                num_inference_steps=20,
-                generator=generator).images[0]
+            image = pipeline(args.validation_prompts[i], num_inference_steps=20, generator=generator).images[0]
 
         images.append(image)
 
     for tracker in accelerator.trackers:
         if tracker.name == 'tensorboard':
             np_images = np.stack([np.asarray(img) for img in images])
-            tracker.writer.add_images(
-                'validation', np_images, epoch, dataformats='NHWC')
+            tracker.writer.add_images('validation', np_images, epoch, dataformats='NHWC')
         elif tracker.name == 'wandb':
             tracker.log({
-                'validation': [
-                    wandb.Image(
-                        image, caption=f'{i}: {args.validation_prompts[i]}')
-                    for i, image in enumerate(images)
-                ]
+                'validation':
+                [wandb.Image(image, caption=f'{i}: {args.validation_prompts[i]}') for i, image in enumerate(images)]
             })
         else:
             logger.warn(f'image logging not implemented for {tracker.name}')
 
     del pipeline
     torch.cuda.empty_cache()
 
     return images
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a training script.')
     parser.add_argument(
-        '--input_perturbation',
-        type=float,
-        default=0,
-        help='The scale of input perturbation. Recommended 0.1.')
+        '--input_perturbation', type=float, default=0, help='The scale of input perturbation. Recommended 0.1.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--dataset_name',
         type=str,
         default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
+        help=('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
     )
     parser.add_argument(
         '--dataset_config_name',
         type=str,
         default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
         '--train_data_dir',
         type=str,
         default=None,
-        help=
-        ('A folder containing the training data. Folder contents must follow the structure described in'
-         ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
-         ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'
-         ),
+        help=('A folder containing the training data. Folder contents must follow the structure described in'
+              ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
+              ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'),
     )
     parser.add_argument(
-        '--image_column',
-        type=str,
-        default='image:FILE',
-        help='The column of the dataset containing an image.')
+        '--image_column', type=str, default='image:FILE', help='The column of the dataset containing an image.')
     parser.add_argument(
         '--caption_column',
         type=str,
         default='text',
-        help=
-        'The column of the dataset containing a caption or a list of captions.',
+        help='The column of the dataset containing a caption or a list of captions.',
     )
     parser.add_argument(
         '--max_train_samples',
         type=int,
         default=None,
-        help=
-        ('For debugging purposes or quicker training, truncate the number of training examples to this '
-         'value if set.'),
+        help=('For debugging purposes or quicker training, truncate the number of training examples to this '
+              'value if set.'),
     )
     parser.add_argument(
         '--validation_prompts',
         type=str,
         default=None,
         nargs='+',
-        help=
-        ('A set of prompts evaluated every `--validation_epochs` and logged to `--report_to`.'
-         ),
+        help=('A set of prompts evaluated every `--validation_epochs` and logged to `--report_to`.'),
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='sd-model-finetuned',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
     parser.add_argument(
         '--cache_dir',
         type=str,
         default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        help='The directory where the downloaded models and datasets will be stored.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=512,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
         '--center_crop',
         default=False,
         action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
     )
     parser.add_argument(
         '--random_flip',
         action='store_true',
         help='whether to randomly flip images horizontally',
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=16,
-        help='Batch size (per device) for the training dataloader.')
+        '--train_batch_size', type=int, default=16, help='Batch size (per device) for the training dataloader.')
     parser.add_argument('--num_train_epochs', type=int, default=100)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=1e-4,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--snr_gamma',
         type=float,
         default=None,
-        help=
-        'SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
+        help='SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
         'More details here: https://arxiv.org/abs/2303.09556.',
     )
     parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
-    parser.add_argument(
-        '--use_ema', action='store_true', help='Whether to use EMA model.')
+    parser.add_argument('--use_ema', action='store_true', help='Whether to use EMA model.')
     parser.add_argument(
         '--non_ema_revision',
         type=str,
         default=None,
         required=False,
-        help=
-        ('Revision of pretrained non-ema model identifier. Must be a branch, tag or git identifier of the local or'
-         ' remote repository specified with --pretrained_model_name_or_path.'),
+        help=('Revision of pretrained non-ema model identifier. Must be a branch, tag or git identifier of the local or'
+              ' remote repository specified with --pretrained_model_name_or_path.'),
     )
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
-    )
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
+    )
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--prediction_type',
         type=str,
         default=None,
-        help=
-        "The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave \
+        help="The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or leave \
         `None`. If left to `None` the default prediction type of the scheduler: \
         `noise_scheduler.config.prediciton_type` is chosen.",
     )
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
-    parser.add_argument(
-        '--local_rank',
-        type=int,
-        default=-1,
-        help='For distributed training: local_rank')
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
-        help=
-        ('Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming'
-         ' training using `--resume_from_checkpoint`.'),
+        help=(
+            'Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming'
+            ' training using `--resume_from_checkpoint`.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
-    parser.add_argument(
-        '--noise_offset',
-        type=float,
-        default=0,
-        help='The scale of noise offset.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
+    parser.add_argument('--noise_offset', type=float, default=0, help='The scale of noise offset.')
     parser.add_argument(
         '--validation_epochs',
         type=int,
         default=5,
         help='Run validation every X epochs.',
     )
     parser.add_argument(
         '--tracker_project_name',
         type=str,
         default='text2image-fine-tune',
-        help=
-        ('The `project_name` argument passed to Accelerator.init_trackers for'
-         ' more information see '
-         'https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator'
-         ),
+        help=('The `project_name` argument passed to Accelerator.init_trackers for'
+              ' more information see '
+              'https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator'),
     )
 
     args = parser.parse_args()
     env_local_rank = int(os.environ.get('LOCAL_RANK', -1))
     if env_local_rank != -1 and env_local_rank != args.local_rank:
         args.local_rank = env_local_rank
 
@@ -566,22 +465,21 @@
 def main():
     args = parse_args()
 
     if args.non_ema_revision is not None:
         deprecate(
             'non_ema_revision!=None',
             '0.15.0',
-            message=
-            ("Downloading 'non_ema' weights from revision branches of the Hub is deprecated. Please make sure to"
-             ' use `--variant=non_ema` instead.'),
+            message=(
+                "Downloading 'non_ema' weights from revision branches of the Hub is deprecated. Please make sure to"
+                ' use `--variant=non_ema` instead.'),
         )
     logging_dir = os.path.join(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
@@ -608,27 +506,23 @@
 
     # Handle the repository creation
     if accelerator.is_main_process:
         if args.output_dir is not None:
             os.makedirs(args.output_dir, exist_ok=True)
 
     # Load scheduler, tokenizer and models.
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     tokenizer = CLIPTokenizer.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='tokenizer',
-        revision=args.revision)
+        args.pretrained_model_name_or_path, subfolder='tokenizer', revision=args.revision)
 
     def deepspeed_zero_init_disabled_context_manager():
         """
         returns either a context list that includes one that will disable zero.Init or an empty context list
         """
-        deepspeed_plugin = AcceleratorState(
-        ).deepspeed_plugin if accelerate.state.is_initialized() else None
+        deepspeed_plugin = AcceleratorState().deepspeed_plugin if accelerate.state.is_initialized() else None
         if deepspeed_plugin is None:
             return []
 
         return [deepspeed_plugin.zero3_init_context_manager(enable=False)]
 
     # Currently Accelerate doesn't know how to handle multiple models under Deepspeed ZeRO stage 3.
     # For this to work properly all models must be run through `accelerate.prepare`. But accelerate
@@ -637,93 +531,73 @@
     #
     # For now the following workaround will partially support Deepspeed ZeRO-3, by excluding the 2
     # frozen models from being partitioned during `zero.Init` which gets called during
     # `from_pretrained` So CLIPTextModel and AutoencoderKL will not enjoy the parameter sharding
     # across multiple gpus and only UNet2DConditionModel will get ZeRO sharded.
     with ContextManagers(deepspeed_zero_init_disabled_context_manager()):
         text_encoder = CLIPTextModel.from_pretrained(
-            args.pretrained_model_name_or_path,
-            subfolder='text_encoder',
-            revision=args.revision,
-            variant=args.variant)
+            args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
         vae = AutoencoderKL.from_pretrained(
-            args.pretrained_model_name_or_path,
-            subfolder='vae',
-            revision=args.revision,
-            variant=args.variant)
+            args.pretrained_model_name_or_path, subfolder='vae', revision=args.revision, variant=args.variant)
 
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.non_ema_revision)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.non_ema_revision)
 
     # Freeze vae and text_encoder and set unet to trainable
     vae.requires_grad_(False)
     text_encoder.requires_grad_(False)
     unet.train()
 
     # Create EMA for the unet.
     if args.use_ema:
         ema_unet = UNet2DConditionModel.from_pretrained(
-            args.pretrained_model_name_or_path,
-            subfolder='unet',
-            revision=args.revision,
-            variant=args.variant)
-        ema_unet = EMAModel(
-            ema_unet.parameters(),
-            model_cls=UNet2DConditionModel,
-            model_config=ema_unet.config)
+            args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
+        ema_unet = EMAModel(ema_unet.parameters(), model_cls=UNet2DConditionModel, model_config=ema_unet.config)
 
     if args.enable_xformers_memory_efficient_attention:
         if is_xformers_available():
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
                 logger.warn(
                     'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training,'
                     ' please update xFormers to at least 0.0.17. See '
-                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                    'https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     # `accelerate` 0.16.0 will have better support for customized saving
     if version.parse(accelerate.__version__) >= version.parse('0.16.0'):
         # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
         def save_model_hook(models, weights, output_dir):
             if accelerator.is_main_process:
                 if args.use_ema:
-                    ema_unet.save_pretrained(
-                        os.path.join(output_dir, 'unet_ema'))
+                    ema_unet.save_pretrained(os.path.join(output_dir, 'unet_ema'))
 
                 for i, model in enumerate(models):
                     model.save_pretrained(os.path.join(output_dir, 'unet'))
 
                     # make sure to pop weight so that corresponding model is not saved again
                     weights.pop()
 
         def load_model_hook(models, input_dir):
             if args.use_ema:
-                load_model = EMAModel.from_pretrained(
-                    os.path.join(input_dir, 'unet_ema'), UNet2DConditionModel)
+                load_model = EMAModel.from_pretrained(os.path.join(input_dir, 'unet_ema'), UNet2DConditionModel)
                 ema_unet.load_state_dict(load_model.state_dict())
                 ema_unet.to(accelerator.device)
                 del load_model
 
             for i in range(len(models)):
                 # pop models so that they are not loaded again
                 model = models.pop()
 
                 # load diffusers style into model
-                load_model = UNet2DConditionModel.from_pretrained(
-                    input_dir, subfolder='unet')
+                load_model = UNet2DConditionModel.from_pretrained(input_dir, subfolder='unet')
                 model.register_to_config(**load_model.config)
 
                 model.load_state_dict(load_model.state_dict())
                 del load_model
 
         accelerator.register_save_state_pre_hook(save_model_hook)
         accelerator.register_load_state_pre_hook(load_model_hook)
@@ -734,25 +608,23 @@
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Initialize the optimizer
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
             raise ImportError(
-                'Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`'
-            )
+                'Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`')
 
         optimizer_cls = bnb.optim.AdamW8bit
     else:
         optimizer_cls = torch.optim.AdamW
 
     optimizer = optimizer_cls(
         unet.parameters(),
@@ -775,18 +647,15 @@
         # Downloading and loading a dataset from the hub.
         dataset = MsDataset.load(
             args.dataset_name,
             args.dataset_config_name,
             data_dir=args.train_data_dir,
         )
         if isinstance(dataset, dict):
-            dataset = {
-                key: value.to_hf_dataset()
-                for key, value in dataset.items()
-            }
+            dataset = {key: value.to_hf_dataset() for key, value in dataset.items()}
         else:
             dataset = {'train': dataset.to_hf_dataset()}
     else:
         data_files = {}
         if args.train_data_dir is not None:
             data_files['train'] = os.path.join(args.train_data_dir, '**')
         dataset = load_dataset(
@@ -800,121 +669,101 @@
     # Preprocessing the datasets.
     # We need to tokenize inputs and targets.
     column_names = dataset['train'].column_names
 
     # 6. Get the column names for input/target.
     dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)
     if args.image_column is None:
-        image_column = dataset_columns[
-            1] if dataset_columns is not None else column_names[1]
+        image_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
     else:
         image_column = args.image_column
         if image_column not in column_names:
             raise ValueError(
-                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}"
-            )
+                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}")
     if args.caption_column is None:
-        caption_column = dataset_columns[
-            0] if dataset_columns is not None else column_names[0]
+        caption_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
     else:
         caption_column = args.caption_column
         if caption_column not in column_names:
             raise ValueError(
-                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}"
-            )
+                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}")
     if image_column.endswith(':FILE'):
         dataset['train'] = dataset['train'].map(path_to_img)
         image_column = 'image'
 
     # Preprocessing the datasets.
     # We need to tokenize input captions and transform the images.
     def tokenize_captions(examples, is_train=True):
         captions = []
         for caption in examples[caption_column]:
             if isinstance(caption, str):
                 captions.append(caption)
             elif isinstance(caption, (list, np.ndarray)):
                 # take a random caption if there are multiple
-                captions.append(
-                    random.choice(caption) if is_train else caption[0])
+                captions.append(random.choice(caption) if is_train else caption[0])
             else:
                 raise ValueError(
-                    f'Caption column `{caption_column}` should contain either strings or lists of strings.'
-                )
+                    f'Caption column `{caption_column}` should contain either strings or lists of strings.')
         inputs = tokenizer(
-            captions,
-            max_length=tokenizer.model_max_length,
-            padding='max_length',
-            truncation=True,
-            return_tensors='pt')
+            captions, max_length=tokenizer.model_max_length, padding='max_length', truncation=True, return_tensors='pt')
         return inputs.input_ids
 
     # Preprocessing the datasets.
     train_transforms = transforms.Compose([
-        transforms.Resize(
-            args.resolution,
-            interpolation=transforms.InterpolationMode.BILINEAR),
-        transforms.CenterCrop(args.resolution)
-        if args.center_crop else transforms.RandomCrop(args.resolution),
-        transforms.RandomHorizontalFlip()
-        if args.random_flip else transforms.Lambda(lambda x: x),
+        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
+        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),
+        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),
         transforms.ToTensor(),
         transforms.Normalize([0.5], [0.5]),
     ])
 
     def preprocess_train(examples):
         images = [image.convert('RGB') for image in examples[image_column]]
-        examples['pixel_values'] = [
-            train_transforms(image) for image in images
-        ]
+        examples['pixel_values'] = [train_transforms(image) for image in images]
         examples['input_ids'] = tokenize_captions(examples)
         return examples
 
     with accelerator.main_process_first():
         if args.max_train_samples is not None:
-            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(
-                range(args.max_train_samples))
+            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(range(args.max_train_samples))
         # Set the training transforms
         train_dataset = dataset['train'].with_transform(preprocess_train)
 
     def collate_fn(examples):
-        pixel_values = torch.stack(
-            [example['pixel_values'] for example in examples])
-        pixel_values = pixel_values.to(
-            memory_format=torch.contiguous_format).float()
+        pixel_values = torch.stack([example['pixel_values'] for example in examples])
+        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
         input_ids = torch.stack([example['input_ids'] for example in examples])
         return {'pixel_values': pixel_values, 'input_ids': input_ids}
 
     # DataLoaders creation:
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         shuffle=True,
         collate_fn=collate_fn,
         batch_size=args.train_batch_size,
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
         num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
         num_training_steps=args.max_train_steps * accelerator.num_processes,
     )
 
     # Prepare everything with our `accelerator`.
-    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-        unet, optimizer, train_dataloader, lr_scheduler)
+    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                          lr_scheduler)
 
     if args.use_ema:
         ema_unet.to(accelerator.device)
 
     # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet)
     # to half-precision
     # as these weights are only used for inference, keeping weights in full precision is not required.
@@ -927,42 +776,36 @@
         args.mixed_precision = accelerator.mixed_precision
 
     # Move text_encode and vae to gpu and cast to weight_dtype
     text_encoder.to(accelerator.device, dtype=weight_dtype)
     vae.to(accelerator.device, dtype=weight_dtype)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         tracker_config = dict(vars(args))
         tracker_config.pop('validation_prompts')
         accelerator.init_trackers(args.tracker_project_name, tracker_config)
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -972,16 +815,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -1000,101 +842,80 @@
     )
 
     for epoch in range(first_epoch, args.num_train_epochs):
         train_loss = 0.0
         for step, batch in enumerate(train_dataloader):
             with accelerator.accumulate(unet):
                 # Convert images to latent space
-                latents = vae.encode(batch['pixel_values'].to(
-                    weight_dtype)).latent_dist.sample()
+                latents = vae.encode(batch['pixel_values'].to(weight_dtype)).latent_dist.sample()
                 latents = latents * vae.config.scaling_factor
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(latents)
                 if args.noise_offset:
                     # https://www.crosslabs.org//blog/diffusion-with-offset-noise
                     noise += args.noise_offset * torch.randn(
-                        (latents.shape[0], latents.shape[1], 1, 1),
-                        device=latents.device)
+                        (latents.shape[0], latents.shape[1], 1, 1), device=latents.device)
                 if args.input_perturbation:
-                    new_noise = noise + args.input_perturbation * torch.randn_like(
-                        noise)
+                    new_noise = noise + args.input_perturbation * torch.randn_like(noise)
                 bsz = latents.shape[0]
                 # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=latents.device)
+                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=latents.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the latents according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
                 if args.input_perturbation:
-                    noisy_latents = noise_scheduler.add_noise(
-                        latents, new_noise, timesteps)
+                    noisy_latents = noise_scheduler.add_noise(latents, new_noise, timesteps)
                 else:
-                    noisy_latents = noise_scheduler.add_noise(
-                        latents, noise, timesteps)
+                    noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
 
                 # Get the text embedding for conditioning
                 encoder_hidden_states = text_encoder(batch['input_ids'])[0]
 
                 # Get the target for loss depending on the prediction type
                 if args.prediction_type is not None:
                     # set prediction_type of scheduler if defined
-                    noise_scheduler.register_to_config(
-                        prediction_type=args.prediction_type)
+                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)
 
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        latents, noise, timesteps)
+                    target = noise_scheduler.get_velocity(latents, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
                 # Predict the noise residual and compute loss
-                model_pred = unet(noisy_latents, timesteps,
-                                  encoder_hidden_states).sample
+                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
 
                 if args.snr_gamma is None:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
                 else:
                     # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
                     # Since we predict the noise instead of x_0, the original formulation is slightly changed.
                     # This is discussed in Section 4.2 of the same paper.
                     snr = compute_snr(noise_scheduler, timesteps)
                     if noise_scheduler.config.prediction_type == 'v_prediction':
                         # Velocity objective requires that we add one to SNR values before we divide by them.
                         snr = snr + 1
                     mse_loss_weights = (
-                        torch.stack(
-                            [snr, args.snr_gamma * torch.ones_like(timesteps)],
-                            dim=1).min(dim=1)[0] / snr)
-
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='none')
-                    loss = loss.mean(
-                        dim=list(range(1, len(loss.shape)))) * mse_loss_weights
+                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr)
+
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='none')
+                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                     loss = loss.mean()
 
                 # Gather the losses across all processes for logging (if we use distributed training).
-                avg_loss = accelerator.gather(
-                    loss.repeat(args.train_batch_size)).mean()
-                train_loss += avg_loss.item(
-                ) / args.gradient_accumulation_steps
+                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
+                train_loss += avg_loss.item() / args.gradient_accumulation_steps
 
                 # Backpropagate
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
-                    accelerator.clip_grad_norm_(unet.parameters(),
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(unet.parameters(), args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad()
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 if args.use_ema:
@@ -1105,53 +926,37 @@
                 train_loss = 0.0
 
                 if global_step % args.checkpointing_steps == 0:
                     if accelerator.is_main_process:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint, we need to have at _most_ \
                             # `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
 
                                 logger.info(
                                     f'{len(checkpoints)} checkpoints already exist, removing {len(removing_checkpoints)}\
                                      checkpoints')
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
                         logger.info(f'Saved state to {save_path}')
 
-            logs = {
-                'step_loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'step_loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
 
             if global_step >= args.max_train_steps:
                 break
 
         if accelerator.is_main_process:
             if args.validation_prompts is not None and epoch % args.validation_epochs == 0:
@@ -1200,28 +1005,23 @@
 
             if args.enable_xformers_memory_efficient_attention:
                 pipeline.enable_xformers_memory_efficient_attention()
 
             if args.seed is None:
                 generator = None
             else:
-                generator = torch.Generator(
-                    device=accelerator.device).manual_seed(args.seed)
+                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)
 
             for i in range(len(args.validation_prompts)):
                 with torch.autocast('cuda'):
-                    image = pipeline(
-                        args.validation_prompts[i],
-                        num_inference_steps=20,
-                        generator=generator).images[0]
+                    image = pipeline(args.validation_prompts[i], num_inference_steps=20, generator=generator).images[0]
                 images.append(image)
 
         if args.push_to_hub:
-            save_model_card(
-                args, args.hub_model_id, images, repo_folder=args.output_dir)
+            save_model_card(args, args.hub_model_id, images, repo_folder=args.output_dir)
             push_to_hub(args.hub_model_id, args.output_dir, args.hub_token)
 
     accelerator.end_training()
 
 
 if __name__ == '__main__':
     main()
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image_lora.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image_lora.py`

 * *Files 7% similar despite different names*

```diff
@@ -28,16 +28,15 @@
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import ProjectConfiguration, set_seed
 from datasets import load_dataset
-from diffusers import (AutoencoderKL, DDPMScheduler, DiffusionPipeline,
-                       UNet2DConditionModel)
+from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, UNet2DConditionModel
 from diffusers.optimization import get_scheduler
 from diffusers.training_utils import compute_snr
 from diffusers.utils import check_min_version, is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from modelscope import MsDataset
 from packaging import version
 from peft.utils import get_peft_model_state_dict
@@ -56,16 +55,15 @@
     state_dict = {}
 
     def text_encoder_attn_modules(text_encoder):
         from transformers import CLIPTextModel, CLIPTextModelWithProjection
 
         attn_modules = []
 
-        if isinstance(text_encoder,
-                      (CLIPTextModel, CLIPTextModelWithProjection)):
+        if isinstance(text_encoder, (CLIPTextModel, CLIPTextModelWithProjection)):
             for i, layer in enumerate(text_encoder.text_model.encoder.layers):
                 name = f'text_model.encoder.layers.{i}.self_attn'
                 mod = layer.self_attn
                 attn_modules.append((name, mod))
 
         return attn_modules
 
@@ -81,19 +79,15 @@
 
         for k, v in module.out_proj.lora_linear_layer.state_dict().items():
             state_dict[f'{name}.out_proj.lora_linear_layer.{k}'] = v
 
     return state_dict
 
 
-def save_model_card(repo_id: str,
-                    images=None,
-                    base_model=str,
-                    dataset_name=str,
-                    repo_folder=None):
+def save_model_card(repo_id: str, images=None, base_model=str, dataset_name=str, repo_folder=None):
     img_str = ''
     for i, image in enumerate(images):
         image.save(os.path.join(repo_folder, f'image_{i}.png'))
         img_str += f'![img_{i}](./image_{i}.png)\n'
 
     yaml = f"""
 ---
@@ -115,340 +109,253 @@
 {img_str}
 """
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
 def parse_args():
-    parser = argparse.ArgumentParser(
-        description='Simple example of a training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a training script.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--dataset_name',
         type=str,
         default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
+        help=('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
     )
     parser.add_argument(
         '--dataset_config_name',
         type=str,
         default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
         '--train_data_dir',
         type=str,
         default=None,
-        help=
-        ('A folder containing the training data. Folder contents must follow the structure described in'
-         ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
-         ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'
-         ),
+        help=('A folder containing the training data. Folder contents must follow the structure described in'
+              ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
+              ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'),
     )
     parser.add_argument(
-        '--image_column',
-        type=str,
-        default='image:FILE',
-        help='The column of the dataset containing an image.')
+        '--image_column', type=str, default='image:FILE', help='The column of the dataset containing an image.')
     parser.add_argument(
         '--caption_column',
         type=str,
         default='text',
-        help=
-        'The column of the dataset containing a caption or a list of captions.',
+        help='The column of the dataset containing a caption or a list of captions.',
     )
     parser.add_argument(
-        '--validation_prompt',
-        type=str,
-        default=None,
-        help='A prompt that is sampled during training for inference.')
+        '--validation_prompt', type=str, default=None, help='A prompt that is sampled during training for inference.')
     parser.add_argument(
         '--num_validation_images',
         type=int,
         default=4,
-        help=
-        'Number of images that should be generated during validation with `validation_prompt`.',
+        help='Number of images that should be generated during validation with `validation_prompt`.',
     )
     parser.add_argument(
         '--validation_epochs',
         type=int,
         default=1,
-        help=
-        ('Run fine-tuning validation every X epochs. The validation process consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`.'
-         ),
+        help=('Run fine-tuning validation every X epochs. The validation process consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`.'),
     )
     parser.add_argument(
         '--max_train_samples',
         type=int,
         default=None,
-        help=
-        ('For debugging purposes or quicker training, truncate the number of training examples to this '
-         'value if set.'),
+        help=('For debugging purposes or quicker training, truncate the number of training examples to this '
+              'value if set.'),
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='sd-model-finetuned-lora',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
     parser.add_argument(
         '--cache_dir',
         type=str,
         default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        help='The directory where the downloaded models and datasets will be stored.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=512,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
         '--center_crop',
         default=False,
         action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
     )
     parser.add_argument(
         '--random_flip',
         action='store_true',
         help='whether to randomly flip images horizontally',
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=16,
-        help='Batch size (per device) for the training dataloader.')
+        '--train_batch_size', type=int, default=16, help='Batch size (per device) for the training dataloader.')
     parser.add_argument('--num_train_epochs', type=int, default=100)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=1e-4,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--snr_gamma',
         type=float,
         default=None,
-        help=
-        'SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
+        help='SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
         'More details here: https://arxiv.org/abs/2303.09556.',
     )
     parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
-    )
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
+    )
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--prediction_type',
         type=str,
         default=None,
-        help=
-        "The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or \
+        help="The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or \
         leave `None`. If left to `None` the default prediction type of the scheduler: \
         `noise_scheduler.config.prediciton_type` is chosen.",
     )
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
-    parser.add_argument(
-        '--local_rank',
-        type=int,
-        default=-1,
-        help='For distributed training: local_rank')
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
-        help=
-        ('Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming'
-         ' training using `--resume_from_checkpoint`.'),
+        help=(
+            'Save a checkpoint of the training state every X updates. These checkpoints are only suitable for resuming'
+            ' training using `--resume_from_checkpoint`.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
-    parser.add_argument(
-        '--noise_offset',
-        type=float,
-        default=0,
-        help='The scale of noise offset.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
+    parser.add_argument('--noise_offset', type=float, default=0, help='The scale of noise offset.')
     parser.add_argument(
         '--rank',
         type=int,
         default=4,
         help=('The dimension of the LoRA update matrices.'),
     )
 
@@ -473,28 +380,25 @@
 }
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
     if args.report_to == 'wandb':
         if not is_wandb_available():
-            raise ImportError(
-                'Make sure to install wandb if you want to use it for logging during training.'
-            )
+            raise ImportError('Make sure to install wandb if you want to use it for logging during training.')
         import wandb
 
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
@@ -515,34 +419,23 @@
 
     # Handle the repository creation
     if accelerator.is_main_process:
         if args.output_dir is not None:
             os.makedirs(args.output_dir, exist_ok=True)
 
     # Load scheduler, tokenizer and models.
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     tokenizer = CLIPTokenizer.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='tokenizer',
-        revision=args.revision)
+        args.pretrained_model_name_or_path, subfolder='tokenizer', revision=args.revision)
     text_encoder = CLIPTextModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision)
     vae = AutoencoderKL.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='vae',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='vae', revision=args.revision, variant=args.variant)
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
     # freeze parameters of models to save more memory
     unet.requires_grad_(False)
     vae.requires_grad_(False)
     text_encoder.requires_grad_(False)
 
     # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora unet) to
     # half-precision
@@ -554,17 +447,15 @@
         weight_dtype = torch.bfloat16
 
     # Freeze the unet parameters before adding adapters
     for param in unet.parameters():
         param.requires_grad_(False)
 
     unet_lora_config = LoRAConfig(
-        r=args.rank,
-        init_lora_weights='gaussian',
-        target_modules=['to_k', 'to_q', 'to_v', 'to_out.0'])
+        r=args.rank, init_lora_weights='gaussian', target_modules=['to_k', 'to_q', 'to_v', 'to_out.0'])
 
     # Move unet, vae and text_encoder to device and cast to weight_dtype
     unet.to(accelerator.device, dtype=weight_dtype)
     vae.to(accelerator.device, dtype=weight_dtype)
     text_encoder.to(accelerator.device, dtype=weight_dtype)
 
     unet = Swift.prepare_model(unet, unet_lora_config)
@@ -576,45 +467,39 @@
 
     if args.enable_xformers_memory_efficient_attention:
         if is_xformers_available():
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
-                logger.warn(
-                    'xFormers 0.0.16 cannot be used for training in some GPUs. \
+                logger.warn('xFormers 0.0.16 cannot be used for training in some GPUs. \
                     If you observe problems during training, please update xFormers to at least 0.0.17. \
-                    See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                    See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     lora_layers = filter(lambda p: p.requires_grad, unet.parameters())
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Initialize the optimizer
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
             raise ImportError(
-                'Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`'
-            )
+                'Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`')
 
         optimizer_cls = bnb.optim.AdamW8bit
     else:
         optimizer_cls = torch.optim.AdamW
 
     optimizer = optimizer_cls(
         lora_layers,
@@ -637,18 +522,15 @@
         # Downloading and loading a dataset from the hub.
         dataset = MsDataset.load(
             args.dataset_name,
             args.dataset_config_name,
             data_dir=args.train_data_dir,
         )
         if isinstance(dataset, dict):
-            dataset = {
-                key: value.to_hf_dataset()
-                for key, value in dataset.items()
-            }
+            dataset = {key: value.to_hf_dataset() for key, value in dataset.items()}
         else:
             dataset = {'train': dataset.to_hf_dataset()}
     else:
         data_files = {}
         if args.train_data_dir is not None:
             data_files['train'] = os.path.join(args.train_data_dir, '**')
         dataset = load_dataset(
@@ -662,149 +544,123 @@
     # Preprocessing the datasets.
     # We need to tokenize inputs and targets.
     column_names = dataset['train'].column_names
 
     # 6. Get the column names for input/target.
     dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)
     if args.image_column is None:
-        image_column = dataset_columns[
-            1] if dataset_columns is not None else column_names[1]
+        image_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
     else:
         image_column = args.image_column
         if image_column not in column_names:
             raise ValueError(
-                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}"
-            )
+                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}")
     if args.caption_column is None:
-        caption_column = dataset_columns[
-            0] if dataset_columns is not None else column_names[0]
+        caption_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
     else:
         caption_column = args.caption_column
         if caption_column not in column_names:
             raise ValueError(
-                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}"
-            )
+                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}")
     if image_column.endswith(':FILE'):
         dataset['train'] = dataset['train'].map(path_to_img)
         image_column = 'image'
 
     # Preprocessing the datasets.
     # We need to tokenize input captions and transform the images.
     def tokenize_captions(examples, is_train=True):
         captions = []
         for caption in examples[caption_column]:
             if isinstance(caption, str):
                 captions.append(caption)
             elif isinstance(caption, (list, np.ndarray)):
                 # take a random caption if there are multiple
-                captions.append(
-                    random.choice(caption) if is_train else caption[0])
+                captions.append(random.choice(caption) if is_train else caption[0])
             else:
                 raise ValueError(
-                    f'Caption column `{caption_column}` should contain either strings or lists of strings.'
-                )
+                    f'Caption column `{caption_column}` should contain either strings or lists of strings.')
         inputs = tokenizer(
-            captions,
-            max_length=tokenizer.model_max_length,
-            padding='max_length',
-            truncation=True,
-            return_tensors='pt')
+            captions, max_length=tokenizer.model_max_length, padding='max_length', truncation=True, return_tensors='pt')
         return inputs.input_ids
 
     # Preprocessing the datasets.
     train_transforms = transforms.Compose([
-        transforms.Resize(
-            args.resolution,
-            interpolation=transforms.InterpolationMode.BILINEAR),
-        transforms.CenterCrop(args.resolution)
-        if args.center_crop else transforms.RandomCrop(args.resolution),
-        transforms.RandomHorizontalFlip()
-        if args.random_flip else transforms.Lambda(lambda x: x),
+        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
+        transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution),
+        transforms.RandomHorizontalFlip() if args.random_flip else transforms.Lambda(lambda x: x),
         transforms.ToTensor(),
         transforms.Normalize([0.5], [0.5]),
     ])
 
     def preprocess_train(examples):
         images = [image.convert('RGB') for image in examples[image_column]]
-        examples['pixel_values'] = [
-            train_transforms(image) for image in images
-        ]
+        examples['pixel_values'] = [train_transforms(image) for image in images]
         examples['input_ids'] = tokenize_captions(examples)
         return examples
 
     with accelerator.main_process_first():
         if args.max_train_samples is not None:
-            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(
-                range(args.max_train_samples))
+            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(range(args.max_train_samples))
         # Set the training transforms
         train_dataset = dataset['train'].with_transform(preprocess_train)
 
     def collate_fn(examples):
-        pixel_values = torch.stack(
-            [example['pixel_values'] for example in examples])
-        pixel_values = pixel_values.to(
-            memory_format=torch.contiguous_format).float()
+        pixel_values = torch.stack([example['pixel_values'] for example in examples])
+        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
         input_ids = torch.stack([example['input_ids'] for example in examples])
         return {'pixel_values': pixel_values, 'input_ids': input_ids}
 
     # DataLoaders creation:
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         shuffle=True,
         collate_fn=collate_fn,
         batch_size=args.train_batch_size,
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
         num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
         num_training_steps=args.max_train_steps * accelerator.num_processes,
     )
 
     # Prepare everything with our `accelerator`.
-    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-        unet, optimizer, train_dataloader, lr_scheduler)
+    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                          lr_scheduler)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         accelerator.init_trackers('text2image-fine-tune', config=vars(args))
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -814,16 +670,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -842,96 +697,77 @@
 
     for epoch in range(first_epoch, args.num_train_epochs):
         unet.train()
         train_loss = 0.0
         for step, batch in enumerate(train_dataloader):
             with accelerator.accumulate(unet):
                 # Convert images to latent space
-                latents = vae.encode(batch['pixel_values'].to(
-                    dtype=weight_dtype)).latent_dist.sample()
+                latents = vae.encode(batch['pixel_values'].to(dtype=weight_dtype)).latent_dist.sample()
                 latents = latents * vae.config.scaling_factor
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(latents)
                 if args.noise_offset:
                     # https://www.crosslabs.org//blog/diffusion-with-offset-noise
                     noise += args.noise_offset * torch.randn(
-                        (latents.shape[0], latents.shape[1], 1, 1),
-                        device=latents.device)
+                        (latents.shape[0], latents.shape[1], 1, 1), device=latents.device)
 
                 bsz = latents.shape[0]
                 # Sample a random timestep for each image
-                timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=latents.device)
+                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=latents.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the latents according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_latents = noise_scheduler.add_noise(
-                    latents, noise, timesteps)
+                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
 
                 # Get the text embedding for conditioning
                 encoder_hidden_states = text_encoder(batch['input_ids'])[0]
 
                 # Get the target for loss depending on the prediction type
                 if args.prediction_type is not None:
                     # set prediction_type of scheduler if defined
-                    noise_scheduler.register_to_config(
-                        prediction_type=args.prediction_type)
+                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)
 
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        latents, noise, timesteps)
+                    target = noise_scheduler.get_velocity(latents, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
                 # Predict the noise residual and compute loss
-                model_pred = unet(noisy_latents, timesteps,
-                                  encoder_hidden_states).sample
+                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample
 
                 if args.snr_gamma is None:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
                 else:
                     # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
                     # Since we predict the noise instead of x_0, the original formulation is slightly changed.
                     # This is discussed in Section 4.2 of the same paper.
                     snr = compute_snr(noise_scheduler, timesteps)
                     if noise_scheduler.config.prediction_type == 'v_prediction':
                         # Velocity objective requires that we add one to SNR values before we divide by them.
                         snr = snr + 1
                     mse_loss_weights = (
-                        torch.stack(
-                            [snr, args.snr_gamma * torch.ones_like(timesteps)],
-                            dim=1).min(dim=1)[0] / snr)
-
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='none')
-                    loss = loss.mean(
-                        dim=list(range(1, len(loss.shape)))) * mse_loss_weights
+                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr)
+
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='none')
+                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                     loss = loss.mean()
 
                 # Gather the losses across all processes for logging (if we use distributed training).
-                avg_loss = accelerator.gather(
-                    loss.repeat(args.train_batch_size)).mean()
-                train_loss += avg_loss.item(
-                ) / args.gradient_accumulation_steps
+                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
+                train_loss += avg_loss.item() / args.gradient_accumulation_steps
 
                 # Backpropagate
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
                     params_to_clip = lora_layers
-                    accelerator.clip_grad_norm_(params_to_clip,
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad()
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
@@ -940,66 +776,47 @@
                 train_loss = 0.0
 
                 if global_step % args.checkpointing_steps == 0:
                     if accelerator.is_main_process:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint, we need to have at _most_ \
                             # `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
 
                         unet.save_pretrained(save_path)
                         logger.info(f'Saved state to {save_path}')
 
-            logs = {
-                'step_loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'step_loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
 
             if global_step >= args.max_train_steps:
                 break
 
         if accelerator.is_main_process:
             if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
-                logger.info(
-                    f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
-                    f' {args.validation_prompt}.')
+                logger.info(f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
+                            f' {args.validation_prompt}.')
                 # create pipeline
                 pipeline = DiffusionPipeline.from_pretrained(
                     args.pretrained_model_name_or_path,
                     unet=accelerator.unwrap_model(unet.base_model),
                     revision=args.revision,
                     variant=args.variant,
                     torch_dtype=weight_dtype,
@@ -1010,31 +827,24 @@
                 # run inference
                 generator = torch.Generator(device=accelerator.device)
                 if args.seed is not None:
                     generator = generator.manual_seed(args.seed)
                 images = []
                 for _ in range(args.num_validation_images):
                     images.append(
-                        pipeline(
-                            args.validation_prompt,
-                            num_inference_steps=30,
-                            generator=generator).images[0])
+                        pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0])
 
                 for tracker in accelerator.trackers:
                     if tracker.name == 'tensorboard':
-                        np_images = np.stack(
-                            [np.asarray(img) for img in images])
-                        tracker.writer.add_images(
-                            'validation', np_images, epoch, dataformats='NHWC')
+                        np_images = np.stack([np.asarray(img) for img in images])
+                        tracker.writer.add_images('validation', np_images, epoch, dataformats='NHWC')
                     if tracker.name == 'wandb':
                         tracker.log({
                             'validation': [
-                                wandb.Image(
-                                    image,
-                                    caption=f'{i}: {args.validation_prompt}')
+                                wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                                 for i, image in enumerate(images)
                             ]
                         })
 
                 del pipeline
                 torch.cuda.empty_cache()
 
@@ -1054,46 +864,36 @@
                 repo_folder=args.output_dir,
             )
             push_to_hub(args.hub_model_id, args.output_dir, args.hub_token)
 
     # Final inference
     # Load previous pipeline
     pipeline = DiffusionPipeline.from_pretrained(
-        args.pretrained_model_name_or_path,
-        revision=args.revision,
-        variant=args.variant,
-        torch_dtype=weight_dtype)
+        args.pretrained_model_name_or_path, revision=args.revision, variant=args.variant, torch_dtype=weight_dtype)
     pipeline = pipeline.to(accelerator.device)
 
     # load attention processors
     pipeline.unet = Swift.from_pretrained(pipeline.unet, args.output_dir)
 
     # run inference
     generator = torch.Generator(device=accelerator.device)
     if args.seed is not None:
         generator = generator.manual_seed(args.seed)
     images = []
     for _ in range(args.num_validation_images):
-        images.append(
-            pipeline(
-                args.validation_prompt,
-                num_inference_steps=30,
-                generator=generator).images[0])
+        images.append(pipeline(args.validation_prompt, num_inference_steps=30, generator=generator).images[0])
 
     if accelerator.is_main_process:
         for tracker in accelerator.trackers:
             if len(images) != 0:
                 if tracker.name == 'tensorboard':
                     np_images = np.stack([np.asarray(img) for img in images])
-                    tracker.writer.add_images(
-                        'test', np_images, epoch, dataformats='NHWC')
+                    tracker.writer.add_images('test', np_images, epoch, dataformats='NHWC')
                 if tracker.name == 'wandb':
                     tracker.log({
                         'test': [
-                            wandb.Image(
-                                image,
-                                caption=f'{i}: {args.validation_prompt}')
+                            wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                             for i, image in enumerate(images)
                         ]
                     })
 
     accelerator.end_training()
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image_lora_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_text_to_image_lora_sdxl.py`

 * *Files 6% similar despite different names*

```diff
@@ -28,49 +28,45 @@
 import numpy as np
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
-from accelerate.utils import (DistributedDataParallelKwargs,
-                              ProjectConfiguration, set_seed)
+from accelerate.utils import DistributedDataParallelKwargs, ProjectConfiguration, set_seed
 from datasets import load_dataset
-from diffusers import (AutoencoderKL, DDPMScheduler, StableDiffusionXLPipeline,
-                       UNet2DConditionModel)
+from diffusers import AutoencoderKL, DDPMScheduler, StableDiffusionXLPipeline, UNet2DConditionModel
 from diffusers.loaders import LoraLoaderMixin
 from diffusers.optimization import get_scheduler
 from diffusers.training_utils import compute_snr
 from diffusers.utils import check_min_version, is_wandb_available
 from diffusers.utils.import_utils import is_xformers_available
 from modelscope import AutoTokenizer, MsDataset
 from packaging import version
 from PIL import Image
 from torchvision import transforms
 from torchvision.transforms.functional import crop
 from tqdm.auto import tqdm
 from transformers import PretrainedConfig
 
-from swift import (LoRAConfig, Swift, get_peft_model_state_dict, push_to_hub,
-                   snapshot_download)
+from swift import LoRAConfig, Swift, get_peft_model_state_dict, push_to_hub, snapshot_download
 
 logger = get_logger(__name__)
 
 
 # TODO: This function should be removed once training scripts are rewritten in PEFT
 def text_encoder_lora_state_dict(text_encoder):
     state_dict = {}
 
     def text_encoder_attn_modules(text_encoder):
         from transformers import CLIPTextModel, CLIPTextModelWithProjection
 
         attn_modules = []
 
-        if isinstance(text_encoder,
-                      (CLIPTextModel, CLIPTextModelWithProjection)):
+        if isinstance(text_encoder, (CLIPTextModel, CLIPTextModelWithProjection)):
             for i, layer in enumerate(text_encoder.text_model.encoder.layers):
                 name = f'text_model.encoder.layers.{i}.self_attn'
                 mod = layer.self_attn
                 attn_modules.append((name, mod))
 
         return attn_modules
 
@@ -129,18 +125,17 @@
 
 Special VAE used for training: {vae_path}.
 """
     with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
         f.write(yaml + model_card)
 
 
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str,
-        revision: str,
-        subfolder: str = 'text_encoder'):
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str,
+                                               revision: str,
+                                               subfolder: str = 'text_encoder'):
     text_encoder_config = PretrainedConfig.from_pretrained(
         pretrained_model_name_or_path, subfolder=subfolder, revision=revision)
     model_class = text_encoder_config.architectures[0]
 
     if model_class == 'CLIPTextModel':
         from transformers import CLIPTextModel
 
@@ -150,356 +145,269 @@
 
         return CLIPTextModelWithProjection
     else:
         raise ValueError(f'{model_class} is not supported.')
 
 
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a training script.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--pretrained_vae_model_name_or_path',
         type=str,
         default=None,
         help='Path to pretrained VAE model with better numerical stability. \
         More details: https://github.com/huggingface/diffusers/pull/4038.',
     )
     parser.add_argument(
         '--revision',
         type=str,
         default=None,
         required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
         '--dataset_name',
         type=str,
         default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
+        help=('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
     )
     parser.add_argument(
         '--dataset_config_name',
         type=str,
         default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
         '--train_data_dir',
         type=str,
         default=None,
-        help=
-        ('A folder containing the training data. Folder contents must follow the structure described in'
-         ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
-         ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'
-         ),
+        help=('A folder containing the training data. Folder contents must follow the structure described in'
+              ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
+              ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'),
     )
     parser.add_argument(
-        '--image_column',
-        type=str,
-        default='image:FILE',
-        help='The column of the dataset containing an image.')
+        '--image_column', type=str, default='image:FILE', help='The column of the dataset containing an image.')
     parser.add_argument(
         '--caption_column',
         type=str,
         default='text',
-        help=
-        'The column of the dataset containing a caption or a list of captions.',
+        help='The column of the dataset containing a caption or a list of captions.',
     )
     parser.add_argument(
         '--validation_prompt',
         type=str,
         default=None,
-        help=
-        'A prompt that is used during validation to verify that the model is learning.',
+        help='A prompt that is used during validation to verify that the model is learning.',
     )
     parser.add_argument(
         '--num_validation_images',
         type=int,
         default=4,
-        help=
-        'Number of images that should be generated during validation with `validation_prompt`.',
+        help='Number of images that should be generated during validation with `validation_prompt`.',
     )
     parser.add_argument(
         '--validation_epochs',
         type=int,
         default=1,
-        help=
-        ('Run fine-tuning validation every X epochs. The validation process consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`.'
-         ),
+        help=('Run fine-tuning validation every X epochs. The validation process consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`.'),
     )
     parser.add_argument(
         '--max_train_samples',
         type=int,
         default=None,
-        help=
-        ('For debugging purposes or quicker training, truncate the number of training examples to this '
-         'value if set.'),
+        help=('For debugging purposes or quicker training, truncate the number of training examples to this '
+              'value if set.'),
     )
     parser.add_argument(
         '--output_dir',
         type=str,
         default='sd-model-finetuned-lora',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
     parser.add_argument(
         '--cache_dir',
         type=str,
         default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        help='The directory where the downloaded models and datasets will be stored.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
         default=1024,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
         '--center_crop',
         default=False,
         action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        help=('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
+              ' cropped. The images will be resized to the resolution first before cropping.'),
     )
     parser.add_argument(
         '--random_flip',
         action='store_true',
         help='whether to randomly flip images horizontally',
     )
     parser.add_argument(
         '--train_text_encoder',
         action='store_true',
-        help=
-        'Whether to train the text encoder. If set, the text encoder should be float32 precision.',
+        help='Whether to train the text encoder. If set, the text encoder should be float32 precision.',
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=16,
-        help='Batch size (per device) for the training dataloader.')
+        '--train_batch_size', type=int, default=16, help='Batch size (per device) for the training dataloader.')
     parser.add_argument('--num_train_epochs', type=int, default=100)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
-        help=
-        ('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
-         ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
-         ' training using `--resume_from_checkpoint`.'),
+        help=('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
+              ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
+              ' training using `--resume_from_checkpoint`.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
         default=1e-4,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
         '--snr_gamma',
         type=float,
         default=None,
-        help=
-        'SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
+        help='SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
         'More details here: https://arxiv.org/abs/2303.09556.',
     )
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
         '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
     )
     parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
-    parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
-    parser.add_argument(
-        '--push_to_hub',
-        action='store_true',
-        help='Whether or not to push the model to the Hub.')
-    parser.add_argument(
-        '--hub_token',
-        type=str,
-        default=None,
-        help='The token to use to push to the Model Hub.')
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
         '--prediction_type',
         type=str,
         default=None,
-        help=
-        "The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or \
+        help="The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or \
         leave `None`. If left to `None` the default prediction type of the scheduler: \
         `noise_scheduler.config.prediciton_type` is chosen.",
     )
     parser.add_argument(
         '--hub_model_id',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
         '--logging_dir',
         type=str,
         default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
     )
     parser.add_argument(
         '--report_to',
         type=str,
         default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
         '--mixed_precision',
         type=str,
         default=None,
         choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
     )
+    parser.add_argument('--local_rank', type=int, default=-1, help='For distributed training: local_rank')
     parser.add_argument(
-        '--local_rank',
-        type=int,
-        default=-1,
-        help='For distributed training: local_rank')
-    parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
-    parser.add_argument(
-        '--noise_offset',
-        type=float,
-        default=0,
-        help='The scale of noise offset.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
+    parser.add_argument('--noise_offset', type=float, default=0, help='The scale of noise offset.')
     parser.add_argument(
         '--rank',
         type=int,
         default=4,
         help=('The dimension of the LoRA update matrices.'),
     )
 
@@ -518,18 +426,16 @@
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
 
     args.vae_base_model_id = args.pretrained_vae_model_name_or_path
-    if args.pretrained_vae_model_name_or_path and not os.path.exists(
-            args.pretrained_vae_model_name_or_path):
-        args.pretrained_vae_model_name_or_path = snapshot_download(
-            args.pretrained_vae_model_name_or_path)
+    if args.pretrained_vae_model_name_or_path and not os.path.exists(args.pretrained_vae_model_name_or_path):
+        args.pretrained_vae_model_name_or_path = snapshot_download(args.pretrained_vae_model_name_or_path)
     return args
 
 
 DATASET_NAME_MAPPING = {
     'AI-ModelScope/pokemon-blip-captions': ('text', 'image:FILE'),
 }
 
@@ -541,16 +447,15 @@
     """
     attn_processors = unet.attn_processors
 
     attn_processors_state_dict = {}
 
     for attn_processor_key, attn_processor in attn_processors.items():
         for parameter_key, parameter in attn_processor.state_dict().items():
-            attn_processors_state_dict[
-                f'{attn_processor_key}.{parameter_key}'] = parameter
+            attn_processors_state_dict[f'{attn_processor_key}.{parameter_key}'] = parameter
 
     return attn_processors_state_dict
 
 
 def tokenize_prompt(tokenizer, prompt):
     text_inputs = tokenizer(
         prompt,
@@ -592,30 +497,27 @@
     return prompt_embeds, pooled_prompt_embeds
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
     kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
         kwargs_handlers=[kwargs],
     )
 
     if args.report_to == 'wandb':
         if not is_wandb_available():
-            raise ImportError(
-                'Make sure to install wandb if you want to use it for logging during training.'
-            )
+            raise ImportError('Make sure to install wandb if you want to use it for logging during training.')
         import wandb
 
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
@@ -650,50 +552,35 @@
         args.pretrained_model_name_or_path,
         subfolder='tokenizer_2',
         revision=args.revision,
         use_fast=False,
     )
 
     # import correct text encoder classes
-    text_encoder_cls_one = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls_one = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
     text_encoder_cls_two = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path,
-        args.revision,
-        subfolder='text_encoder_2')
+        args.pretrained_model_name_or_path, args.revision, subfolder='text_encoder_2')
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     text_encoder_one = text_encoder_cls_one.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
     text_encoder_two = text_encoder_cls_two.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder_2',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder_2', revision=args.revision, variant=args.variant)
     vae_path = (
         args.pretrained_model_name_or_path
-        if args.pretrained_vae_model_name_or_path is None else
-        args.pretrained_vae_model_name_or_path)
+        if args.pretrained_vae_model_name_or_path is None else args.pretrained_vae_model_name_or_path)
     vae = AutoencoderKL.from_pretrained(
         vae_path,
-        subfolder='vae'
-        if args.pretrained_vae_model_name_or_path is None else None,
+        subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
         revision=args.revision,
         variant=args.variant,
     )
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
     # We only train the additional adapter LoRA layers
     vae.requires_grad_(False)
     text_encoder_one.requires_grad_(False)
     text_encoder_two.requires_grad_(False)
     unet.requires_grad_(False)
 
@@ -721,81 +608,65 @@
             import xformers
 
             xformers_version = version.parse(xformers.__version__)
             if xformers_version == version.parse('0.0.16'):
                 logger.warn(
                     'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training, \
                     please update xFormers to at least 0.0.17. \
-                    See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
+                    See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
             unet.enable_xformers_memory_efficient_attention()
         else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
 
     # now we will add new LoRA weights to the attention layers
     # Set correct lora layers
     unet_lora_config = LoRAConfig(
-        r=args.rank,
-        init_lora_weights='gaussian',
-        target_modules=['to_k', 'to_q', 'to_v', 'to_out.0'])
+        r=args.rank, init_lora_weights='gaussian', target_modules=['to_k', 'to_q', 'to_v', 'to_out.0'])
 
     unet = Swift.prepare_model(unet, unet_lora_config)
     if args.mixed_precision == 'fp16':
         for param in unet.parameters():
             # only upcast trainable parameters (LoRA) into fp32
             if param.requires_grad:
                 param.data = param.to(torch.float32)
 
     # The text encoder comes from  transformers, we will also attach adapters to it.
     if args.train_text_encoder:
         # ensure that dtype is float32, even if rest of the model that isn't trained is loaded in fp16
         text_lora_config = LoRAConfig(
-            r=args.rank,
-            init_lora_weights='gaussian',
-            target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj'])
-        text_encoder_one = Swift.prepare_model(text_encoder_one,
-                                               text_lora_config)
-        text_encoder_two = Swift.prepare_model(text_encoder_two,
-                                               text_lora_config)
+            r=args.rank, init_lora_weights='gaussian', target_modules=['q_proj', 'k_proj', 'v_proj', 'out_proj'])
+        text_encoder_one = Swift.prepare_model(text_encoder_one, text_lora_config)
+        text_encoder_two = Swift.prepare_model(text_encoder_two, text_lora_config)
 
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
-            raise ImportError(
-                'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-            )
+            raise ImportError('To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
         optimizer_class = bnb.optim.AdamW8bit
     else:
         optimizer_class = torch.optim.AdamW
 
     # Optimizer creation
-    params_to_optimize = list(
-        filter(lambda p: p.requires_grad, unet.parameters()))
+    params_to_optimize = list(filter(lambda p: p.requires_grad, unet.parameters()))
     if args.train_text_encoder:
         params_to_optimize = (
-            params_to_optimize + list(
-                filter(lambda p: p.requires_grad,
-                       text_encoder_one.parameters())) + list(
-                           filter(lambda p: p.requires_grad,
-                                  text_encoder_two.parameters())))
+            params_to_optimize + list(filter(lambda p: p.requires_grad, text_encoder_one.parameters()))
+            + list(filter(lambda p: p.requires_grad, text_encoder_two.parameters())))
     optimizer = optimizer_class(
         params_to_optimize,
         lr=args.learning_rate,
         betas=(args.adam_beta1, args.adam_beta2),
         weight_decay=args.adam_weight_decay,
         eps=args.adam_epsilon,
     )
@@ -813,18 +684,15 @@
         # Downloading and loading a dataset from the hub.
         dataset = MsDataset.load(
             args.dataset_name,
             args.dataset_config_name,
             data_dir=args.train_data_dir,
         )
         if isinstance(dataset, dict):
-            dataset = {
-                key: value.to_hf_dataset()
-                for key, value in dataset.items()
-            }
+            dataset = {key: value.to_hf_dataset() for key, value in dataset.items()}
         else:
             dataset = {'train': dataset.to_hf_dataset()}
     else:
         data_files = {}
         if args.train_data_dir is not None:
             data_files['train'] = os.path.join(args.train_data_dir, '**')
         dataset = load_dataset(
@@ -838,60 +706,51 @@
     # Preprocessing the datasets.
     # We need to tokenize inputs and targets.
     column_names = dataset['train'].column_names
 
     # 6. Get the column names for input/target.
     dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)
     if args.image_column is None:
-        image_column = dataset_columns[
-            1] if dataset_columns is not None else column_names[1]
+        image_column = dataset_columns[1] if dataset_columns is not None else column_names[1]
     else:
         image_column = args.image_column
         if image_column not in column_names:
             raise ValueError(
-                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}"
-            )
+                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}")
     if args.caption_column is None:
-        caption_column = dataset_columns[
-            0] if dataset_columns is not None else column_names[0]
+        caption_column = dataset_columns[0] if dataset_columns is not None else column_names[0]
     else:
         caption_column = args.caption_column
         if caption_column not in column_names:
             raise ValueError(
-                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}"
-            )
+                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}")
     if image_column.endswith(':FILE'):
         dataset['train'] = dataset['train'].map(path_to_img)
         image_column = 'image'
 
     # Preprocessing the datasets.
     # We need to tokenize input captions and transform the images.
     def tokenize_captions(examples, is_train=True):
         captions = []
         for caption in examples[caption_column]:
             if isinstance(caption, str):
                 captions.append(caption)
             elif isinstance(caption, (list, np.ndarray)):
                 # take a random caption if there are multiple
-                captions.append(
-                    random.choice(caption) if is_train else caption[0])
+                captions.append(random.choice(caption) if is_train else caption[0])
             else:
                 raise ValueError(
-                    f'Caption column `{caption_column}` should contain either strings or lists of strings.'
-                )
+                    f'Caption column `{caption_column}` should contain either strings or lists of strings.')
         tokens_one = tokenize_prompt(tokenizer_one, captions)
         tokens_two = tokenize_prompt(tokenizer_two, captions)
         return tokens_one, tokens_two
 
     # Preprocessing the datasets.
-    train_resize = transforms.Resize(
-        args.resolution, interpolation=transforms.InterpolationMode.BILINEAR)
-    train_crop = transforms.CenterCrop(
-        args.resolution) if args.center_crop else transforms.RandomCrop(
-            args.resolution)
+    train_resize = transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR)
+    train_crop = transforms.CenterCrop(args.resolution) if args.center_crop else transforms.RandomCrop(args.resolution)
     train_flip = transforms.RandomHorizontalFlip(p=1.0)
     train_transforms = transforms.Compose([
         transforms.ToTensor(),
         transforms.Normalize([0.5], [0.5]),
     ])
 
     def preprocess_train(examples):
@@ -904,16 +763,15 @@
             original_sizes.append((image.height, image.width))
             image = train_resize(image)
             if args.center_crop:
                 y1 = max(0, int(round((image.height - args.resolution) / 2.0)))
                 x1 = max(0, int(round((image.width - args.resolution) / 2.0)))
                 image = train_crop(image)
             else:
-                y1, x1, h, w = train_crop.get_params(
-                    image, (args.resolution, args.resolution))
+                y1, x1, h, w = train_crop.get_params(image, (args.resolution, args.resolution))
                 image = crop(image, y1, x1, h, w)
             if args.random_flip and random.random() < 0.5:
                 # flip
                 x1 = image.width - x1
                 image = train_flip(image)
             crop_top_left = (y1, x1)
             crop_top_lefts.append(crop_top_left)
@@ -926,30 +784,25 @@
         tokens_one, tokens_two = tokenize_captions(examples)
         examples['input_ids_one'] = tokens_one
         examples['input_ids_two'] = tokens_two
         return examples
 
     with accelerator.main_process_first():
         if args.max_train_samples is not None:
-            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(
-                range(args.max_train_samples))
+            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(range(args.max_train_samples))
         # Set the training transforms
         train_dataset = dataset['train'].with_transform(preprocess_train)
 
     def collate_fn(examples):
-        pixel_values = torch.stack(
-            [example['pixel_values'] for example in examples])
-        pixel_values = pixel_values.to(
-            memory_format=torch.contiguous_format).float()
+        pixel_values = torch.stack([example['pixel_values'] for example in examples])
+        pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
         original_sizes = [example['original_sizes'] for example in examples]
         crop_top_lefts = [example['crop_top_lefts'] for example in examples]
-        input_ids_one = torch.stack(
-            [example['input_ids_one'] for example in examples])
-        input_ids_two = torch.stack(
-            [example['input_ids_two'] for example in examples])
+        input_ids_one = torch.stack([example['input_ids_one'] for example in examples])
+        input_ids_two = torch.stack([example['input_ids_two'] for example in examples])
         return {
             'pixel_values': pixel_values,
             'input_ids_one': input_ids_one,
             'input_ids_two': input_ids_two,
             'original_sizes': original_sizes,
             'crop_top_lefts': crop_top_lefts,
         }
@@ -961,65 +814,55 @@
         collate_fn=collate_fn,
         batch_size=args.train_batch_size,
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
-        num_warmup_steps=args.lr_warmup_steps
-        * args.gradient_accumulation_steps,
-        num_training_steps=args.max_train_steps
-        * args.gradient_accumulation_steps,
+        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,
+        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,
     )
 
     # Prepare everything with our `accelerator`.
     if args.train_text_encoder:
         unet, text_encoder_one, text_encoder_two, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-            unet, text_encoder_one, text_encoder_two, optimizer,
-            train_dataloader, lr_scheduler)
+            unet, text_encoder_one, text_encoder_two, optimizer, train_dataloader, lr_scheduler)
     else:
-        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-            unet, optimizer, train_dataloader, lr_scheduler)
+        unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(unet, optimizer, train_dataloader,
+                                                                              lr_scheduler)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
         accelerator.init_trackers('text2image-fine-tune', config=vars(args))
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1029,16 +872,15 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
@@ -1076,113 +918,87 @@
                     model_input = model_input.to(weight_dtype)
 
                 # Sample noise that we'll add to the latents
                 noise = torch.randn_like(model_input)
                 if args.noise_offset:
                     # https://www.crosslabs.org//blog/diffusion-with-offset-noise
                     noise += args.noise_offset * torch.randn(
-                        (model_input.shape[0], model_input.shape[1], 1, 1),
-                        device=model_input.device)
+                        (model_input.shape[0], model_input.shape[1], 1, 1), device=model_input.device)
 
                 bsz = model_input.shape[0]
                 # Sample a random timestep for each image
                 timesteps = torch.randint(
-                    0,
-                    noise_scheduler.config.num_train_timesteps, (bsz, ),
-                    device=model_input.device)
+                    0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=model_input.device)
                 timesteps = timesteps.long()
 
                 # Add noise to the model input according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_model_input = noise_scheduler.add_noise(
-                    model_input, noise, timesteps)
+                noisy_model_input = noise_scheduler.add_noise(model_input, noise, timesteps)
 
                 # time ids
                 def compute_time_ids(original_size, crops_coords_top_left):
                     # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids
                     target_size = (args.resolution, args.resolution)
-                    add_time_ids = list(original_size + crops_coords_top_left
-                                        + target_size)
+                    add_time_ids = list(original_size + crops_coords_top_left + target_size)
                     add_time_ids = torch.tensor([add_time_ids])
-                    add_time_ids = add_time_ids.to(
-                        accelerator.device, dtype=weight_dtype)
+                    add_time_ids = add_time_ids.to(accelerator.device, dtype=weight_dtype)
                     return add_time_ids
 
-                add_time_ids = torch.cat([
-                    compute_time_ids(s, c) for s, c in zip(
-                        batch['original_sizes'], batch['crop_top_lefts'])
-                ])
+                add_time_ids = torch.cat(
+                    [compute_time_ids(s, c) for s, c in zip(batch['original_sizes'], batch['crop_top_lefts'])])
 
                 # Predict the noise residual
                 unet_added_conditions = {'time_ids': add_time_ids}
                 prompt_embeds, pooled_prompt_embeds = encode_prompt(
                     text_encoders=[text_encoder_one, text_encoder_two],
                     tokenizers=None,
                     prompt=None,
-                    text_input_ids_list=[
-                        batch['input_ids_one'], batch['input_ids_two']
-                    ],
+                    text_input_ids_list=[batch['input_ids_one'], batch['input_ids_two']],
                 )
-                unet_added_conditions.update(
-                    {'text_embeds': pooled_prompt_embeds})
+                unet_added_conditions.update({'text_embeds': pooled_prompt_embeds})
                 model_pred = unet(
-                    noisy_model_input,
-                    timesteps,
-                    prompt_embeds,
-                    added_cond_kwargs=unet_added_conditions).sample
+                    noisy_model_input, timesteps, prompt_embeds, added_cond_kwargs=unet_added_conditions).sample
 
                 # Get the target for loss depending on the prediction type
                 if args.prediction_type is not None:
                     # set prediction_type of scheduler if defined
-                    noise_scheduler.register_to_config(
-                        prediction_type=args.prediction_type)
+                    noise_scheduler.register_to_config(prediction_type=args.prediction_type)
 
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        model_input, noise, timesteps)
+                    target = noise_scheduler.get_velocity(model_input, noise, timesteps)
                 else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
 
                 if args.snr_gamma is None:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
                 else:
                     # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
                     # Since we predict the noise instead of x_0, the original formulation is slightly changed.
                     # This is discussed in Section 4.2 of the same paper.
                     snr = compute_snr(noise_scheduler, timesteps)
                     if noise_scheduler.config.prediction_type == 'v_prediction':
                         # Velocity objective requires that we add one to SNR values before we divide by them.
                         snr = snr + 1
                     mse_loss_weights = (
-                        torch.stack(
-                            [snr, args.snr_gamma * torch.ones_like(timesteps)],
-                            dim=1).min(dim=1)[0] / snr)
-
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='none')
-                    loss = loss.mean(
-                        dim=list(range(1, len(loss.shape)))) * mse_loss_weights
+                        torch.stack([snr, args.snr_gamma * torch.ones_like(timesteps)], dim=1).min(dim=1)[0] / snr)
+
+                    loss = F.mse_loss(model_pred.float(), target.float(), reduction='none')
+                    loss = loss.mean(dim=list(range(1, len(loss.shape)))) * mse_loss_weights
                     loss = loss.mean()
 
                 # Gather the losses across all processes for logging (if we use distributed training).
-                avg_loss = accelerator.gather(
-                    loss.repeat(args.train_batch_size)).mean()
-                train_loss += avg_loss.item(
-                ) / args.gradient_accumulation_steps
+                avg_loss = accelerator.gather(loss.repeat(args.train_batch_size)).mean()
+                train_loss += avg_loss.item() / args.gradient_accumulation_steps
 
                 # Backpropagate
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
-                    accelerator.clip_grad_norm_(params_to_optimize,
-                                                args.max_grad_norm)
+                    accelerator.clip_grad_norm_(params_to_optimize, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
                 optimizer.zero_grad()
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
@@ -1191,64 +1007,45 @@
                 train_loss = 0.0
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
 
                             # before we save the new checkpoint, we need to have at _most_ \
                             # `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
                         logger.info(f'Saved state to {save_path}')
 
-            logs = {
-                'step_loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+            logs = {'step_loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
 
             if global_step >= args.max_train_steps:
                 break
 
         if accelerator.is_main_process:
             if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
-                logger.info(
-                    f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
-                    f' {args.validation_prompt}.')
+                logger.info(f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
+                            f' {args.validation_prompt}.')
                 # create pipeline
                 pipeline = StableDiffusionXLPipeline.from_pretrained(
                     args.pretrained_model_name_or_path,
                     vae=vae,
                     text_encoder=accelerator.unwrap_model(text_encoder_one),
                     text_encoder_2=accelerator.unwrap_model(text_encoder_two),
                     unet=accelerator.unwrap_model(unet.base_model),
@@ -1257,38 +1054,31 @@
                     torch_dtype=weight_dtype,
                 )
 
                 pipeline = pipeline.to(accelerator.device)
                 pipeline.set_progress_bar_config(disable=True)
 
                 # run inference
-                generator = torch.Generator(
-                    device=accelerator.device).manual_seed(
-                        args.seed) if args.seed else None
+                generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
                 pipeline_args = {'prompt': args.validation_prompt}
 
                 with torch.cuda.amp.autocast():
                     images = [
-                        pipeline(**pipeline_args,
-                                 generator=generator).images[0]
+                        pipeline(**pipeline_args, generator=generator).images[0]
                         for _ in range(args.num_validation_images)
                     ]
 
                 for tracker in accelerator.trackers:
                     if tracker.name == 'tensorboard':
-                        np_images = np.stack(
-                            [np.asarray(img) for img in images])
-                        tracker.writer.add_images(
-                            'validation', np_images, epoch, dataformats='NHWC')
+                        np_images = np.stack([np.asarray(img) for img in images])
+                        tracker.writer.add_images('validation', np_images, epoch, dataformats='NHWC')
                     if tracker.name == 'wandb':
                         tracker.log({
                             'validation': [
-                                wandb.Image(
-                                    image,
-                                    caption=f'{i}: {args.validation_prompt}')
+                                wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                                 for i, image in enumerate(images)
                             ]
                         })
 
                 del pipeline
                 torch.cuda.empty_cache()
 
@@ -1296,19 +1086,17 @@
     accelerator.wait_for_everyone()
     if accelerator.is_main_process:
         unet = accelerator.unwrap_model(unet)
         unet.save_pretrained(os.path.join(args.output_dir, 'unet'))
 
         if args.train_text_encoder:
             text_encoder_one = accelerator.unwrap_model(text_encoder_one)
-            text_encoder_one.save_pretrained(
-                os.path.join(args.output_dir, 'text_encoder1'))
+            text_encoder_one.save_pretrained(os.path.join(args.output_dir, 'text_encoder1'))
             text_encoder_two = accelerator.unwrap_model(text_encoder_two)
-            text_encoder_two.save_pretrained(
-                os.path.join(args.output_dir, 'text_encoder2'))
+            text_encoder_two.save_pretrained(os.path.join(args.output_dir, 'text_encoder2'))
 
         del unet
         del text_encoder_one
         del text_encoder_two
         torch.cuda.empty_cache()
 
         # Final inference
@@ -1319,48 +1107,38 @@
             revision=args.revision,
             variant=args.variant,
             torch_dtype=weight_dtype,
         )
         pipeline = pipeline.to(accelerator.device)
 
         # load attention processors
-        pipeline.unet = Swift.from_pretrained(
-            pipeline.unet, os.path.join(args.output_dir, 'unet'))
+        pipeline.unet = Swift.from_pretrained(pipeline.unet, os.path.join(args.output_dir, 'unet'))
         if args.train_text_encoder:
-            pipeline.text_encoder_one = Swift.from_pretrained(
-                pipeline.text_encoder_one,
-                os.path.join(args.output_dir, 'text_encoder1'))
-            pipeline.text_encoder_two = Swift.from_pretrained(
-                pipeline.text_encoder_two,
-                os.path.join(args.output_dir, 'text_encoder2'))
+            pipeline.text_encoder_one = Swift.from_pretrained(pipeline.text_encoder_one,
+                                                              os.path.join(args.output_dir, 'text_encoder1'))
+            pipeline.text_encoder_two = Swift.from_pretrained(pipeline.text_encoder_two,
+                                                              os.path.join(args.output_dir, 'text_encoder2'))
 
         # run inference
         images = []
         if args.validation_prompt and args.num_validation_images > 0:
-            generator = torch.Generator(device=accelerator.device).manual_seed(
-                args.seed) if args.seed else None
+            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None
             images = [
-                pipeline(
-                    args.validation_prompt,
-                    num_inference_steps=25,
-                    generator=generator).images[0]
+                pipeline(args.validation_prompt, num_inference_steps=25, generator=generator).images[0]
                 for _ in range(args.num_validation_images)
             ]
 
             for tracker in accelerator.trackers:
                 if tracker.name == 'tensorboard':
                     np_images = np.stack([np.asarray(img) for img in images])
-                    tracker.writer.add_images(
-                        'test', np_images, epoch, dataformats='NHWC')
+                    tracker.writer.add_images('test', np_images, epoch, dataformats='NHWC')
                 if tracker.name == 'wandb':
                     tracker.log({
                         'test': [
-                            wandb.Image(
-                                image,
-                                caption=f'{i}: {args.validation_prompt}')
+                            wandb.Image(image, caption=f'{i}: {args.validation_prompt}')
                             for i, image in enumerate(images)
                         ]
                     })
 
         if args.push_to_hub:
             save_model_card(
                 args.hub_model_id,
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/diffusers/train_text_to_image_sdxl.py` & `ms-swift-2.0.4/swift/aigc/diffusers/train_controlnet_sdxl.py`

 * *Files 16% similar despite different names*

```diff
@@ -7,106 +7,161 @@
 #
 #     http://www.apache.org/licenses/LICENSE-2.0
 #
 # Unless required by applicable law or agreed to in writing, software
 # distributed under the License is distributed on an "AS IS" BASIS,
 # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # See the License for the specific language governing permissions and
-# limitations under the License.
-"""Fine-tuning script for Stable Diffusion XL for text2image."""
 
 import argparse
 import functools
 import gc
 import logging
 import math
 import os
 import random
 import shutil
 from pathlib import Path
 
 import accelerate
-import datasets
 import diffusers
 import numpy as np
 import torch
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
 from accelerate import Accelerator
 from accelerate.logging import get_logger
 from accelerate.utils import ProjectConfiguration, set_seed
 from datasets import load_dataset
-from diffusers import (AutoencoderKL, DDPMScheduler, StableDiffusionXLPipeline,
-                       UNet2DConditionModel)
+from diffusers import (AutoencoderKL, ControlNetModel, DDPMScheduler, StableDiffusionXLControlNetPipeline,
+                       UNet2DConditionModel, UniPCMultistepScheduler)
 from diffusers.optimization import get_scheduler
-from diffusers.training_utils import EMAModel, compute_snr
-from diffusers.utils import is_wandb_available
+from diffusers.utils import is_wandb_available, make_image_grid
 from diffusers.utils.import_utils import is_xformers_available
 from modelscope import AutoTokenizer, MsDataset
 from packaging import version
 from PIL import Image
 from torchvision import transforms
-from torchvision.transforms.functional import crop
 from tqdm.auto import tqdm
 from transformers import PretrainedConfig
 
 from swift import push_to_hub, snapshot_download
 
+if is_wandb_available():
+    import wandb
+
 logger = get_logger(__name__)
 
-DATASET_NAME_MAPPING = {
-    'AI-ModelScope/pokemon-blip-captions': ('text', 'image:FILE'),
-}
 
+def log_validation(vae, unet, controlnet, args, accelerator, weight_dtype, step):
+    logger.info('Running validation... ')
 
-def save_model_card(
-    repo_id: str,
-    images=None,
-    validation_prompt=None,
-    base_model=str,
-    dataset_name=str,
-    repo_folder=None,
-    vae_path=None,
-):
-    img_str = ''
-    for i, image in enumerate(images):
-        image.save(os.path.join(repo_folder, f'image_{i}.png'))
-        img_str += f'![img_{i}](./image_{i}.png)\n'
+    controlnet = accelerator.unwrap_model(controlnet)
 
-    yaml = f"""
----
-license: creativeml-openrail-m
-base_model: {base_model}
-dataset: {dataset_name}
-tags:
-- stable-diffusion-xl
-- stable-diffusion-xl-diffusers
-- text-to-image
-- diffusers
-inference: true
----
-    """
-    model_card = f"""
-# Text-to-image finetuning - {repo_id}
+    pipeline = StableDiffusionXLControlNetPipeline.from_pretrained(
+        args.pretrained_model_name_or_path,
+        vae=vae,
+        unet=unet,
+        controlnet=controlnet,
+        revision=args.revision,
+        variant=args.variant,
+        torch_dtype=weight_dtype,
+    )
+    pipeline.scheduler = UniPCMultistepScheduler.from_config(pipeline.scheduler.config)
+    pipeline = pipeline.to(accelerator.device)
+    pipeline.set_progress_bar_config(disable=True)
 
-This pipeline was finetuned from **{base_model}** on the **{args.dataset_name}** dataset. Below are some example images
-generated with the finetuned pipeline using the following prompt: {validation_prompt}: \n
-{img_str}
+    if args.enable_xformers_memory_efficient_attention:
+        pipeline.enable_xformers_memory_efficient_attention()
 
-Special VAE used for training: {vae_path}.
-"""
-    with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
-        f.write(yaml + model_card)
+    if args.seed is None:
+        generator = None
+    else:
+        generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)
+
+    if len(args.validation_image) == len(args.validation_prompt):
+        validation_images = args.validation_image
+        validation_prompts = args.validation_prompt
+    elif len(args.validation_image) == 1:
+        validation_images = args.validation_image * len(args.validation_prompt)
+        validation_prompts = args.validation_prompt
+    elif len(args.validation_prompt) == 1:
+        validation_images = args.validation_image
+        validation_prompts = args.validation_prompt * len(args.validation_image)
+    else:
+        raise ValueError(
+            'number of `args.validation_image` and `args.validation_prompt` should be checked in `parse_args`')
 
+    image_logs = []
 
-def import_model_class_from_model_name_or_path(
-        pretrained_model_name_or_path: str,
-        revision: str,
-        subfolder: str = 'text_encoder'):
+    for validation_prompt, validation_image in zip(validation_prompts, validation_images):
+        validation_image = Image.open(validation_image).convert('RGB')
+        validation_image = validation_image.resize((args.resolution, args.resolution))
+
+        images = []
+
+        for _ in range(args.num_validation_images):
+            with torch.autocast('cuda'):
+                image = pipeline(
+                    prompt=validation_prompt, image=validation_image, num_inference_steps=20,
+                    generator=generator).images[0]
+            images.append(image)
+
+        image_logs.append({
+            'validation_image': validation_image,
+            'images': images,
+            'validation_prompt': validation_prompt
+        })
+
+    for tracker in accelerator.trackers:
+        if tracker.name == 'tensorboard':
+            for log in image_logs:
+                images = log['images']
+                validation_prompt = log['validation_prompt']
+                validation_image = log['validation_image']
+
+                formatted_images = []
+
+                formatted_images.append(np.asarray(validation_image))
+
+                for image in images:
+                    formatted_images.append(np.asarray(image))
+
+                formatted_images = np.stack(formatted_images)
+
+                tracker.writer.add_images(validation_prompt, formatted_images, step, dataformats='NHWC')
+        elif tracker.name == 'wandb':
+            formatted_images = []
+
+            for log in image_logs:
+                images = log['images']
+                validation_prompt = log['validation_prompt']
+                validation_image = log['validation_image']
+
+                formatted_images.append(wandb.Image(validation_image, caption='Controlnet conditioning'))
+
+                for image in images:
+                    image = wandb.Image(image, caption=validation_prompt)
+                    formatted_images.append(image)
+
+            tracker.log({'validation': formatted_images})
+        else:
+            logger.warn(f'image logging not implemented for {tracker.name}')
+
+        del pipeline
+        gc.collect()
+        torch.cuda.empty_cache()
+
+        return image_logs
+
+
+def import_model_class_from_model_name_or_path(pretrained_model_name_or_path: str,
+                                               revision: str,
+                                               subfolder: str = 'text_encoder'):
     text_encoder_config = PretrainedConfig.from_pretrained(
         pretrained_model_name_or_path, subfolder=subfolder, revision=revision)
     model_class = text_encoder_config.architectures[0]
 
     if model_class == 'CLIPTextModel':
         from transformers import CLIPTextModel
 
@@ -115,449 +170,466 @@
         from transformers import CLIPTextModelWithProjection
 
         return CLIPTextModelWithProjection
     else:
         raise ValueError(f'{model_class} is not supported.')
 
 
+def save_model_card(repo_id: str, image_logs=None, base_model=str, repo_folder=None):
+    img_str = ''
+    if image_logs is not None:
+        img_str = 'You can find some example images below.\n'
+        for i, log in enumerate(image_logs):
+            images = log['images']
+            validation_prompt = log['validation_prompt']
+            validation_image = log['validation_image']
+            validation_image.save(os.path.join(repo_folder, 'image_control.png'))
+            img_str += f'prompt: {validation_prompt}\n'
+            images = [validation_image] + images
+            make_image_grid(images, 1, len(images)).save(os.path.join(repo_folder, f'images_{i}.png'))
+            img_str += f'![images_{i})](./images_{i}.png)\n'
+
+    yaml = f"""
+---
+license: openrail++
+base_model: {base_model}
+tags:
+- stable-diffusion-xl
+- stable-diffusion-xl-diffusers
+- text-to-image
+- diffusers
+- controlnet
+inference: true
+---
+    """
+    model_card = f"""
+# controlnet-{repo_id}
+
+These are controlnet weights trained on {base_model} with new type of conditioning.
+{img_str}
+"""
+
+    with open(os.path.join(repo_folder, 'README.md'), 'w') as f:
+        f.write(yaml + model_card)
+
+
 def parse_args(input_args=None):
-    parser = argparse.ArgumentParser(
-        description='Simple example of a training script.')
+    parser = argparse.ArgumentParser(description='Simple example of a ControlNet training script.')
     parser.add_argument(
         '--pretrained_model_name_or_path',
         type=str,
         default=None,
         required=True,
-        help=
-        'Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained model or model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
         '--pretrained_vae_model_name_or_path',
         type=str,
         default=None,
-        help='Path to pretrained VAE model with better numerical stability. \
-        More details: https://github.com/huggingface/diffusers/pull/4038.',
+        help='Path to an improved VAE to stabilize training. '
+        'For more details check out: https://github.com/huggingface/diffusers/pull/4038.',
     )
     parser.add_argument(
-        '--revision',
+        '--controlnet_model_name_or_path',
         type=str,
         default=None,
-        required=False,
-        help=
-        'Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
+        help='Path to pretrained controlnet model or model identifier from huggingface.co/models or '
+        'modelscope.cn/models. If not specified controlnet weights are initialized from unet.',
     )
     parser.add_argument(
         '--variant',
         type=str,
         default=None,
-        help=
-        "Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
+        help="Variant of the model files of the pretrained model identifier from huggingface.co/models, 'e.g.' fp16",
     )
     parser.add_argument(
-        '--dataset_name',
-        type=str,
-        default=None,
-        help=
-        ('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
-         ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
-         ' or to a folder containing files that  Datasets can understand.'),
-    )
-    parser.add_argument(
-        '--dataset_config_name',
-        type=str,
-        default=None,
-        help=
-        "The config of the Dataset, leave as None if there's only one config.",
-    )
-    parser.add_argument(
-        '--train_data_dir',
+        '--revision',
         type=str,
         default=None,
-        help=
-        ('A folder containing the training data. Folder contents must follow the structure described in'
-         ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
-         ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'
-         ),
-    )
-    parser.add_argument(
-        '--image_column',
-        type=str,
-        default='image:FILE',
-        help='The column of the dataset containing an image.')
-    parser.add_argument(
-        '--caption_column',
-        type=str,
-        default='text',
-        help=
-        'The column of the dataset containing a caption or a list of captions.',
+        required=False,
+        help='Revision of pretrained model identifier from huggingface.co/models or modelscope.cn/models.',
     )
     parser.add_argument(
-        '--validation_prompt',
+        '--tokenizer_name',
         type=str,
         default=None,
-        help=
-        'A prompt that is used during validation to verify that the model is learning.',
-    )
-    parser.add_argument(
-        '--num_validation_images',
-        type=int,
-        default=4,
-        help=
-        'Number of images that should be generated during validation with `validation_prompt`.',
-    )
-    parser.add_argument(
-        '--validation_epochs',
-        type=int,
-        default=1,
-        help=
-        ('Run fine-tuning validation every X epochs. The validation process consists of running the prompt'
-         ' `args.validation_prompt` multiple times: `args.num_validation_images`.'
-         ),
-    )
-    parser.add_argument(
-        '--max_train_samples',
-        type=int,
-        default=None,
-        help=
-        ('For debugging purposes or quicker training, truncate the number of training examples to this '
-         'value if set.'),
-    )
-    parser.add_argument(
-        '--proportion_empty_prompts',
-        type=float,
-        default=0,
-        help=
-        'Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).',
+        help='Pretrained tokenizer name or path if not the same as model_name',
     )
     parser.add_argument(
         '--output_dir',
         type=str,
-        default='sdxl-model-finetuned',
-        help=
-        'The output directory where the model predictions and checkpoints will be written.',
+        default='controlnet-model',
+        help='The output directory where the model predictions and checkpoints will be written.',
     )
     parser.add_argument(
         '--cache_dir',
         type=str,
         default=None,
-        help=
-        'The directory where the downloaded models and datasets will be stored.',
+        help='The directory where the downloaded models and datasets will be stored.',
     )
-    parser.add_argument(
-        '--seed',
-        type=int,
-        default=None,
-        help='A seed for reproducible training.')
+    parser.add_argument('--seed', type=int, default=None, help='A seed for reproducible training.')
     parser.add_argument(
         '--resolution',
         type=int,
-        default=1024,
-        help=
-        ('The resolution for input images, all the images in the train/validation dataset will be resized to this'
-         ' resolution'),
+        default=512,
+        help=('The resolution for input images, all the images in the train/validation dataset will be resized to this'
+              ' resolution'),
     )
     parser.add_argument(
-        '--center_crop',
-        default=False,
-        action='store_true',
-        help=
-        ('Whether to center crop the input images to the resolution. If not set, the images will be randomly'
-         ' cropped. The images will be resized to the resolution first before cropping.'
-         ),
+        '--crops_coords_top_left_h',
+        type=int,
+        default=0,
+        help=('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'),
     )
     parser.add_argument(
-        '--random_flip',
-        action='store_true',
-        help='whether to randomly flip images horizontally',
+        '--crops_coords_top_left_w',
+        type=int,
+        default=0,
+        help=('Coordinate for (the height) to be included in the crop coordinate embeddings needed by SDXL UNet.'),
     )
     parser.add_argument(
-        '--train_batch_size',
-        type=int,
-        default=16,
-        help='Batch size (per device) for the training dataloader.')
-    parser.add_argument('--num_train_epochs', type=int, default=100)
+        '--train_batch_size', type=int, default=4, help='Batch size (per device) for the training dataloader.')
+    parser.add_argument('--num_train_epochs', type=int, default=1)
     parser.add_argument(
         '--max_train_steps',
         type=int,
         default=None,
-        help=
-        'Total number of training steps to perform.  If provided, overrides num_train_epochs.',
+        help='Total number of training steps to perform.  If provided, overrides num_train_epochs.',
     )
     parser.add_argument(
         '--checkpointing_steps',
         type=int,
         default=500,
         help=
-        ('Save a checkpoint of the training state every X updates. These checkpoints can be used both as final'
-         ' checkpoints in case they are better than the last checkpoint, and are also suitable for resuming'
-         ' training using `--resume_from_checkpoint`.'),
+        ('Save a checkpoint of the training state every X updates. Checkpoints can be used for resuming training '
+         'via `--resume_from_checkpoint`. '
+         'In the case that the checkpoint is better than the final trained model, the checkpoint can also be used for '
+         'inference.'
+         'Using a checkpoint for inference requires separate loading of the original pipeline '
+         'and the individual checkpointed model components.'
+         'See https://huggingface.co/docs/diffusers/main/en/training/dreambooth'
+         '#performing-inference-using-a-saved-checkpoint for step by step'
+         'instructions.'),
     )
     parser.add_argument(
         '--checkpoints_total_limit',
         type=int,
         default=None,
         help=('Max number of checkpoints to store.'),
     )
     parser.add_argument(
         '--resume_from_checkpoint',
         type=str,
         default=None,
-        help=
-        ('Whether training should be resumed from a previous checkpoint. Use a path saved by'
-         ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'
-         ),
+        help=('Whether training should be resumed from a previous checkpoint. Use a path saved by'
+              ' `--checkpointing_steps`, or `"latest"` to automatically select the last available checkpoint.'),
     )
     parser.add_argument(
         '--gradient_accumulation_steps',
         type=int,
         default=1,
-        help=
-        'Number of updates steps to accumulate before performing a backward/update pass.',
+        help='Number of updates steps to accumulate before performing a backward/update pass.',
     )
     parser.add_argument(
         '--gradient_checkpointing',
         action='store_true',
-        help=
-        'Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
+        help='Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.',
     )
     parser.add_argument(
         '--learning_rate',
         type=float,
-        default=1e-4,
-        help=
-        'Initial learning rate (after the potential warmup period) to use.',
+        default=5e-6,
+        help='Initial learning rate (after the potential warmup period) to use.',
     )
     parser.add_argument(
         '--scale_lr',
         action='store_true',
         default=False,
-        help=
-        'Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
+        help='Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.',
     )
     parser.add_argument(
         '--lr_scheduler',
         type=str,
         default='constant',
-        help=
-        ('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
-         ' "constant", "constant_with_warmup"]'),
+        help=('The scheduler type to use. Choose between ["linear", "cosine", "cosine_with_restarts", "polynomial",'
+              ' "constant", "constant_with_warmup"]'),
     )
     parser.add_argument(
-        '--lr_warmup_steps',
-        type=int,
-        default=500,
-        help='Number of steps for the warmup in the lr scheduler.')
+        '--lr_warmup_steps', type=int, default=500, help='Number of steps for the warmup in the lr scheduler.')
     parser.add_argument(
-        '--timestep_bias_strategy',
-        type=str,
-        default='none',
-        choices=['earlier', 'later', 'range', 'none'],
-        help=
-        ('The timestep bias strategy, which may help direct the model toward learning low or high frequency details.'
-         " Choices: ['earlier', 'later', 'range', 'none']."
-         " The default is 'none', which means no bias is applied, and training proceeds normally."
-         " The value of 'later' will increase the frequency of the model's final training timesteps."
-         ),
+        '--lr_num_cycles',
+        type=int,
+        default=1,
+        help='Number of hard resets of the lr in cosine_with_restarts scheduler.',
     )
+    parser.add_argument('--lr_power', type=float, default=1.0, help='Power factor of the polynomial scheduler.')
     parser.add_argument(
-        '--timestep_bias_multiplier',
-        type=float,
-        default=1.0,
-        help=
-        ('The multiplier for the bias. Defaults to 1.0, which means no bias is applied.'
-         ' A value of 2.0 will double the weight of the bias, and a value of 0.5 will halve it.'
-         ),
-    )
+        '--use_8bit_adam', action='store_true', help='Whether or not to use 8-bit Adam from bitsandbytes.')
     parser.add_argument(
-        '--timestep_bias_begin',
+        '--dataloader_num_workers',
         type=int,
         default=0,
-        help=
-        ('When using `--timestep_bias_strategy=range`, the beginning (inclusive) timestep to bias.'
-         ' Defaults to zero, which equates to having no specific bias.'),
-    )
+        help=(
+            'Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
+        ),
+    )
+    parser.add_argument('--adam_beta1', type=float, default=0.9, help='The beta1 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_beta2', type=float, default=0.999, help='The beta2 parameter for the Adam optimizer.')
+    parser.add_argument('--adam_weight_decay', type=float, default=1e-2, help='Weight decay to use.')
+    parser.add_argument('--adam_epsilon', type=float, default=1e-08, help='Epsilon value for the Adam optimizer')
+    parser.add_argument('--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+    parser.add_argument('--push_to_hub', action='store_true', help='Whether or not to push the model to the Hub.')
+    parser.add_argument('--hub_token', type=str, default=None, help='The token to use to push to the Model Hub.')
     parser.add_argument(
-        '--timestep_bias_end',
-        type=int,
-        default=1000,
-        help=
-        ('When using `--timestep_bias_strategy=range`, the final timestep (inclusive) to bias.'
-         ' Defaults to 1000, which is the number of timesteps that Stable Diffusion is trained on.'
-         ),
-    )
-    parser.add_argument(
-        '--timestep_bias_portion',
-        type=float,
-        default=0.25,
-        help=
-        ('The portion of timesteps to bias. Defaults to 0.25, which 25% of timesteps will be biased.'
-         ' A value of 0.5 will bias one half of the timesteps. '
-         'The value provided for `--timestep_bias_strategy` determines'
-         ' whether the biased portions are in the earlier or later timesteps.'
-         ),
-    )
-    parser.add_argument(
-        '--snr_gamma',
-        type=float,
+        '--hub_model_id',
+        type=str,
         default=None,
-        help=
-        'SNR weighting gamma to be used if rebalancing the loss. Recommended value is 5.0. '
-        'More details here: https://arxiv.org/abs/2303.09556.',
+        help='The name of the repository to keep in sync with the local `output_dir`.',
     )
     parser.add_argument(
-        '--use_ema', action='store_true', help='Whether to use EMA model.')
+        '--logging_dir',
+        type=str,
+        default='logs',
+        help=('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
+              ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+    )
     parser.add_argument(
         '--allow_tf32',
         action='store_true',
-        help=
-        ('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
-         ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'
-         ),
+        help=('Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see'
+              ' https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices'),
     )
     parser.add_argument(
-        '--dataloader_num_workers',
-        type=int,
-        default=0,
-        help=
-        ('Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.'
-         ),
+        '--report_to',
+        type=str,
+        default='tensorboard',
+        help=('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
+              ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'),
     )
     parser.add_argument(
-        '--use_8bit_adam',
-        action='store_true',
-        help='Whether or not to use 8-bit Adam from bitsandbytes.')
-    parser.add_argument(
-        '--adam_beta1',
-        type=float,
-        default=0.9,
-        help='The beta1 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_beta2',
-        type=float,
-        default=0.999,
-        help='The beta2 parameter for the Adam optimizer.')
-    parser.add_argument(
-        '--adam_weight_decay',
-        type=float,
-        default=1e-2,
-        help='Weight decay to use.')
-    parser.add_argument(
-        '--adam_epsilon',
-        type=float,
-        default=1e-08,
-        help='Epsilon value for the Adam optimizer')
+        '--mixed_precision',
+        type=str,
+        default=None,
+        choices=['no', 'fp16', 'bf16'],
+        help=(
+            'Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
+            ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
+            ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'),
+    )
     parser.add_argument(
-        '--max_grad_norm', default=1.0, type=float, help='Max gradient norm.')
+        '--enable_xformers_memory_efficient_attention', action='store_true', help='Whether or not to use xformers.')
     parser.add_argument(
-        '--push_to_hub',
+        '--set_grads_to_none',
         action='store_true',
-        help='Whether or not to push the model to the Hub.')
+        help=('Save more memory by using setting grads to None instead of zero. Be aware, that this changes certain'
+              ' behaviors, so disable this argument if it causes any problems. More info:'
+              ' https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html'),
+    )
     parser.add_argument(
-        '--hub_token',
+        '--dataset_name',
         type=str,
         default=None,
-        help='The token to use to push to the Model Hub.')
+        help=('The name of the Dataset (from the HuggingFace hub) to train on (could be your own, possibly private,'
+              ' dataset). It can also be a path pointing to a local copy of a dataset in your filesystem,'
+              ' or to a folder containing files that  Datasets can understand.'),
+    )
     parser.add_argument(
-        '--prediction_type',
+        '--dataset_config_name',
         type=str,
         default=None,
-        help=
-        "The prediction_type that shall be used for training. Choose between 'epsilon' or 'v_prediction' or \
-        leave `None`. If left to `None` the default prediction type of the scheduler: \
-        `noise_scheduler.config.prediciton_type` is chosen.",
+        help="The config of the Dataset, leave as None if there's only one config.",
     )
     parser.add_argument(
-        '--hub_model_id',
+        '--train_data_dir',
         type=str,
         default=None,
-        help=
-        'The name of the repository to keep in sync with the local `output_dir`.',
+        help=('A folder containing the training data. Folder contents must follow the structure described in'
+              ' https://huggingface.co/docs/datasets/image_dataset#imagefolder. In particular, a `metadata.jsonl` file'
+              ' must exist to provide the captions for the images. Ignored if `dataset_name` is specified.'),
     )
     parser.add_argument(
-        '--logging_dir',
+        '--image_column', type=str, default='image', help='The column of the dataset containing the target image.')
+    parser.add_argument(
+        '--conditioning_image_column',
         type=str,
-        default='logs',
-        help=
-        ('[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to'
-         ' *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.'),
+        default='conditioning_image',
+        help='The column of the dataset containing the controlnet conditioning image.',
     )
     parser.add_argument(
-        '--report_to',
+        '--caption_column',
         type=str,
-        default='tensorboard',
-        help=
-        ('The integration to report the results and logs to. Supported platforms are `"tensorboard"`'
-         ' (default), `"wandb"` and `"comet_ml"`. Use `"all"` to report to all integrations.'
-         ),
+        default='text',
+        help='The column of the dataset containing a caption or a list of captions.',
     )
     parser.add_argument(
-        '--mixed_precision',
+        '--max_train_samples',
+        type=int,
+        default=None,
+        help=('For debugging purposes or quicker training, truncate the number of training examples to this '
+              'value if set.'),
+    )
+    parser.add_argument(
+        '--proportion_empty_prompts',
+        type=float,
+        default=0,
+        help='Proportion of image prompts to be replaced with empty strings. Defaults to 0 (no prompt replacement).',
+    )
+    parser.add_argument(
+        '--validation_prompt',
         type=str,
         default=None,
-        choices=['no', 'fp16', 'bf16'],
-        help=
-        ('Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >='
-         ' 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the'
-         ' flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.'
-         ),
+        nargs='+',
+        help=('A set of prompts evaluated every `--validation_steps` and logged to `--report_to`.'
+              ' Provide either a matching number of `--validation_image`s, a single `--validation_image`'
+              ' to be used with all prompts, or a single prompt that will be used with all `--validation_image`s.'),
     )
     parser.add_argument(
-        '--local_rank',
+        '--validation_image',
+        type=str,
+        default=None,
+        nargs='+',
+        help=('A set of paths to the controlnet conditioning image be evaluated every `--validation_steps`'
+              ' and logged to `--report_to`. Provide either a matching number of `--validation_prompt`s, a'
+              ' a single `--validation_prompt` to be used with all `--validation_image`s, or a single'
+              ' `--validation_image` that will be used with all `--validation_prompt`s.'),
+    )
+    parser.add_argument(
+        '--num_validation_images',
         type=int,
-        default=-1,
-        help='For distributed training: local_rank')
+        default=4,
+        help='Number of images to be generated for each `--validation_image`, `--validation_prompt` pair',
+    )
     parser.add_argument(
-        '--enable_xformers_memory_efficient_attention',
-        action='store_true',
-        help='Whether or not to use xformers.')
+        '--validation_steps',
+        type=int,
+        default=100,
+        help=('Run validation every X steps. Validation consists of running the prompt'
+              ' `args.validation_prompt` multiple times: `args.num_validation_images`'
+              ' and logging the images.'),
+    )
     parser.add_argument(
-        '--noise_offset',
-        type=float,
-        default=0,
-        help='The scale of noise offset.')
+        '--tracker_project_name',
+        type=str,
+        default='sd_xl_train_controlnet',
+        help=('The `project_name` argument passed to Accelerator.init_trackers for'
+              ' more information see '
+              'https://huggingface.co/docs/accelerate/v0.17.0/en/package_reference/accelerator#accelerate.Accelerator'),
+    )
 
     if input_args is not None:
         args = parser.parse_args(input_args)
     else:
         args = parser.parse_args()
 
-    env_local_rank = int(os.environ.get('LOCAL_RANK', -1))
-    if env_local_rank != -1 and env_local_rank != args.local_rank:
-        args.local_rank = env_local_rank
-
-    # Sanity checks
     if args.dataset_name is None and args.train_data_dir is None:
-        raise ValueError('Need either a dataset name or a training folder.')
+        raise ValueError('Specify either `--dataset_name` or `--train_data_dir`')
+
+    if args.dataset_name is not None and args.train_data_dir is not None:
+        raise ValueError('Specify only one of `--dataset_name` or `--train_data_dir`')
 
     if args.proportion_empty_prompts < 0 or args.proportion_empty_prompts > 1:
-        raise ValueError(
-            '`--proportion_empty_prompts` must be in the range [0, 1].')
+        raise ValueError('`--proportion_empty_prompts` must be in the range [0, 1].')
+
+    if args.validation_prompt is not None and args.validation_image is None:
+        raise ValueError('`--validation_image` must be set if `--validation_prompt` is set')
+
+    if args.validation_prompt is None and args.validation_image is not None:
+        raise ValueError('`--validation_prompt` must be set if `--validation_image` is set')
+
+    if (args.validation_image is not None and args.validation_prompt is not None and len(args.validation_image) != 1
+            and len(args.validation_prompt) != 1 and len(args.validation_image) != len(args.validation_prompt)):
+        raise ValueError('Must provide either 1 `--validation_image`, 1 `--validation_prompt`,'
+                         ' or the same number of `--validation_prompt`s and `--validation_image`s')
+
+    if args.resolution % 8 != 0:
+        raise ValueError('`--resolution` must be divisible by 8 for consistently sized encoded images '
+                         'between the VAE and the controlnet encoder.')
 
     args.base_model_id = args.pretrained_model_name_or_path
     if not os.path.exists(args.pretrained_model_name_or_path):
         args.pretrained_model_name_or_path = snapshot_download(
             args.pretrained_model_name_or_path, revision=args.revision)
 
-    args.vae_base_model_id = args.pretrained_vae_model_name_or_path
-    if args.pretrained_vae_model_name_or_path and not os.path.exists(
-            args.pretrained_vae_model_name_or_path):
-        args.pretrained_vae_model_name_or_path = snapshot_download(
-            args.pretrained_vae_model_name_or_path)
+    if args.controlnet_model_name_or_path and not os.path.exists(args.controlnet_model_name_or_path):
+        args.controlnet_model_name_or_path = snapshot_download(args.controlnet_model_name_or_path)
+
+    if args.pretrained_vae_model_name_or_path and not os.path.exists(args.pretrained_vae_model_name_or_path):
+        args.pretrained_vae_model_name_or_path = snapshot_download(args.pretrained_vae_model_name_or_path)
+
     return args
 
 
+def get_train_dataset(args, accelerator):
+    # Get the datasets: you can either provide your own training and evaluation files (see below)
+    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).
+
+    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
+    # download the dataset.
+    if args.dataset_name is not None:
+        # Downloading and loading a dataset from the hub.
+        dataset = MsDataset.load(
+            args.dataset_name,
+            args.dataset_config_name,
+        )
+        if isinstance(dataset, dict):
+            dataset = {key: value.to_hf_dataset() for key, value in dataset.items()}
+        else:
+            dataset = {'train': dataset.to_hf_dataset()}
+    else:
+        if args.train_data_dir is not None:
+            dataset = load_dataset(
+                args.train_data_dir,
+                cache_dir=args.cache_dir,
+            )
+        # See more about loading custom images at
+        # https://huggingface.co/docs/datasets/v2.0.0/en/dataset_script
+
+    # Preprocessing the datasets.
+    # We need to tokenize inputs and targets.
+    column_names = dataset['train'].column_names
+
+    # 6. Get the column names for input/target.
+    if args.image_column is None:
+        image_column = column_names[0]
+        logger.info(f'image column defaulting to {image_column}')
+    else:
+        image_column = args.image_column
+        if image_column not in column_names:
+            raise ValueError(f"`--image_column` value '{args.image_column}' not found in dataset columns. "
+                             f"Dataset columns are: {', '.join(column_names)}")
+
+    if args.caption_column is None:
+        caption_column = column_names[1]
+        logger.info(f'caption column defaulting to {caption_column}')
+    else:
+        caption_column = args.caption_column
+        if caption_column not in column_names:
+            raise ValueError(f"`--caption_column` value '{args.caption_column}' not found in dataset columns. "
+                             f"Dataset columns are: {', '.join(column_names)}")
+
+    if args.conditioning_image_column is None:
+        conditioning_image_column = column_names[2]
+        logger.info(f'conditioning image column defaulting to {conditioning_image_column}')
+    else:
+        conditioning_image_column = args.conditioning_image_column
+        if conditioning_image_column not in column_names:
+            raise ValueError(
+                f"`--conditioning_image_column` value '{args.conditioning_image_column}' not found in dataset columns."
+                f" Dataset columns are: {', '.join(column_names)}")
+
+    with accelerator.main_process_first():
+        train_dataset = dataset['train'].shuffle(seed=args.seed)
+        if args.max_train_samples is not None:
+            train_dataset = train_dataset.select(range(args.max_train_samples))
+    return train_dataset
+
+
 # Adapted from pipelines.StableDiffusionXLPipeline.encode_prompt
-def encode_prompt(batch,
-                  text_encoders,
-                  tokenizers,
-                  proportion_empty_prompts,
-                  caption_column,
-                  is_train=True):
+def encode_prompt(prompt_batch, text_encoders, tokenizers, proportion_empty_prompts, is_train=True):
     prompt_embeds_list = []
-    prompt_batch = batch[caption_column]
 
     captions = []
     for caption in prompt_batch:
         if random.random() < proportion_empty_prompts:
             captions.append('')
         elif isinstance(caption, str):
             captions.append(caption)
@@ -585,109 +657,96 @@
             prompt_embeds = prompt_embeds.hidden_states[-2]
             bs_embed, seq_len, _ = prompt_embeds.shape
             prompt_embeds = prompt_embeds.view(bs_embed, seq_len, -1)
             prompt_embeds_list.append(prompt_embeds)
 
     prompt_embeds = torch.concat(prompt_embeds_list, dim=-1)
     pooled_prompt_embeds = pooled_prompt_embeds.view(bs_embed, -1)
-    return {
-        'prompt_embeds': prompt_embeds.cpu(),
-        'pooled_prompt_embeds': pooled_prompt_embeds.cpu()
-    }
+    return prompt_embeds, pooled_prompt_embeds
 
 
-def compute_vae_encodings(batch, vae):
-    images = batch.pop('pixel_values')
-    pixel_values = torch.stack(list(images))
-    pixel_values = pixel_values.to(
-        memory_format=torch.contiguous_format).float()
-    pixel_values = pixel_values.to(vae.device, dtype=vae.dtype)
+def prepare_train_dataset(args, dataset, accelerator):
+    image_transforms = transforms.Compose([
+        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
+        transforms.CenterCrop(args.resolution),
+        transforms.ToTensor(),
+        transforms.Normalize([0.5], [0.5]),
+    ])
+
+    conditioning_image_transforms = transforms.Compose([
+        transforms.Resize(args.resolution, interpolation=transforms.InterpolationMode.BILINEAR),
+        transforms.CenterCrop(args.resolution),
+        transforms.ToTensor(),
+    ])
 
-    with torch.no_grad():
-        model_input = vae.encode(pixel_values).latent_dist.sample()
-    model_input = model_input * vae.config.scaling_factor
-    return {'model_input': model_input.cpu()}
-
-
-def generate_timestep_weights(args, num_timesteps):
-    weights = torch.ones(num_timesteps)
-
-    # Determine the indices to bias
-    num_to_bias = int(args.timestep_bias_portion * num_timesteps)
-
-    if args.timestep_bias_strategy == 'later':
-        bias_indices = slice(-num_to_bias, None)
-    elif args.timestep_bias_strategy == 'earlier':
-        bias_indices = slice(0, num_to_bias)
-    elif args.timestep_bias_strategy == 'range':
-        # Out of the possible 1000 timesteps, we might want to focus on eg. 200-500.
-        range_begin = args.timestep_bias_begin
-        range_end = args.timestep_bias_end
-        if range_begin < 0:
-            raise ValueError(
-                'When using the range strategy for timestep bias, you must provide a beginning timestep greater \
-                or equal to zero.')
-        if range_end > num_timesteps:
-            raise ValueError(
-                'When using the range strategy for timestep bias, you must provide an ending timestep smaller than \
-                the number of timesteps.')
-        bias_indices = slice(range_begin, range_end)
-    else:  # 'none' or any other string
-        return weights
-    if args.timestep_bias_multiplier <= 0:
-        return ValueError(
-            'The parameter --timestep_bias_multiplier is not intended to be used to disable the training of specific '
-            'timesteps.'
-            ' If it was intended to disable timestep bias, use `--timestep_bias_strategy none` instead.'
-            ' A timestep bias multiplier less than or equal to 0 is not allowed.'
-        )
+    def preprocess_train(examples):
+        images = [image.convert('RGB') for image in examples[args.image_column]]
+        images = [image_transforms(image) for image in images]
 
-    # Apply the bias
-    weights[bias_indices] *= args.timestep_bias_multiplier
+        conditioning_images = [image.convert('RGB') for image in examples[args.conditioning_image_column]]
+        conditioning_images = [conditioning_image_transforms(image) for image in conditioning_images]
 
-    # Normalize
-    weights /= weights.sum()
+        examples['pixel_values'] = images
+        examples['conditioning_pixel_values'] = conditioning_images
 
-    return weights
+        return examples
+
+    with accelerator.main_process_first():
+        dataset = dataset.with_transform(preprocess_train)
+
+    return dataset
+
+
+def collate_fn(examples):
+    pixel_values = torch.stack([example['pixel_values'] for example in examples])
+    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()
+
+    conditioning_pixel_values = torch.stack([example['conditioning_pixel_values'] for example in examples])
+    conditioning_pixel_values = conditioning_pixel_values.to(memory_format=torch.contiguous_format).float()
+
+    prompt_ids = torch.stack([torch.tensor(example['prompt_embeds']) for example in examples])
+
+    add_text_embeds = torch.stack([torch.tensor(example['text_embeds']) for example in examples])
+    add_time_ids = torch.stack([torch.tensor(example['time_ids']) for example in examples])
+
+    return {
+        'pixel_values': pixel_values,
+        'conditioning_pixel_values': conditioning_pixel_values,
+        'prompt_ids': prompt_ids,
+        'unet_added_conditions': {
+            'text_embeds': add_text_embeds,
+            'time_ids': add_time_ids
+        },
+    }
 
 
 def main():
     args = parse_args()
     logging_dir = Path(args.output_dir, args.logging_dir)
 
-    accelerator_project_config = ProjectConfiguration(
-        project_dir=args.output_dir, logging_dir=logging_dir)
+    accelerator_project_config = ProjectConfiguration(project_dir=args.output_dir, logging_dir=logging_dir)
 
     accelerator = Accelerator(
         gradient_accumulation_steps=args.gradient_accumulation_steps,
         mixed_precision=args.mixed_precision,
         log_with=args.report_to,
         project_config=accelerator_project_config,
     )
 
-    if args.report_to == 'wandb':
-        if not is_wandb_available():
-            raise ImportError(
-                'Make sure to install wandb if you want to use it for logging during training.'
-            )
-        import wandb
-
     # Make one log on every process with the configuration for debugging.
     logging.basicConfig(
         format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
         datefmt='%m/%d/%Y %H:%M:%S',
         level=logging.INFO,
     )
     logger.info(accelerator.state, main_process_only=False)
     if accelerator.is_local_main_process:
-        datasets.utils.logging.set_verbosity_warning()
         transformers.utils.logging.set_verbosity_warning()
         diffusers.utils.logging.set_verbosity_info()
     else:
-        datasets.utils.logging.set_verbosity_error()
         transformers.utils.logging.set_verbosity_error()
         diffusers.utils.logging.set_verbosity_error()
 
     # If passed along, set the training seed now.
     if args.seed is not None:
         set_seed(args.seed)
 
@@ -707,402 +766,261 @@
         args.pretrained_model_name_or_path,
         subfolder='tokenizer_2',
         revision=args.revision,
         use_fast=False,
     )
 
     # import correct text encoder classes
-    text_encoder_cls_one = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path, args.revision)
+    text_encoder_cls_one = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)
     text_encoder_cls_two = import_model_class_from_model_name_or_path(
-        args.pretrained_model_name_or_path,
-        args.revision,
-        subfolder='text_encoder_2')
+        args.pretrained_model_name_or_path, args.revision, subfolder='text_encoder_2')
 
     # Load scheduler and models
-    noise_scheduler = DDPMScheduler.from_pretrained(
-        args.pretrained_model_name_or_path, subfolder='scheduler')
-    # Check for terminal SNR in combination with SNR Gamma
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder='scheduler')
     text_encoder_one = text_encoder_cls_one.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder', revision=args.revision, variant=args.variant)
     text_encoder_two = text_encoder_cls_two.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='text_encoder_2',
-        revision=args.revision,
-        variant=args.variant)
+        args.pretrained_model_name_or_path, subfolder='text_encoder_2', revision=args.revision, variant=args.variant)
     vae_path = (
         args.pretrained_model_name_or_path
-        if args.pretrained_vae_model_name_or_path is None else
-        args.pretrained_vae_model_name_or_path)
+        if args.pretrained_vae_model_name_or_path is None else args.pretrained_vae_model_name_or_path)
     vae = AutoencoderKL.from_pretrained(
         vae_path,
-        subfolder='vae'
-        if args.pretrained_vae_model_name_or_path is None else None,
+        subfolder='vae' if args.pretrained_vae_model_name_or_path is None else None,
         revision=args.revision,
         variant=args.variant,
     )
     unet = UNet2DConditionModel.from_pretrained(
-        args.pretrained_model_name_or_path,
-        subfolder='unet',
-        revision=args.revision,
-        variant=args.variant)
-
-    # Freeze vae and text encoders.
-    vae.requires_grad_(False)
-    text_encoder_one.requires_grad_(False)
-    text_encoder_two.requires_grad_(False)
-    # Set unet as trainable.
-    unet.train()
-
-    # For mixed precision training we cast all non-trainable weigths to half-precision
-    # as these weights are only used for inference, keeping weights in full precision is not required.
-    weight_dtype = torch.float32
-    if accelerator.mixed_precision == 'fp16':
-        weight_dtype = torch.float16
-    elif accelerator.mixed_precision == 'bf16':
-        weight_dtype = torch.bfloat16
-
-    # Move unet, vae and text_encoder to device and cast to weight_dtype
-    # The VAE is in float32 to avoid NaN losses.
-    vae.to(accelerator.device, dtype=torch.float32)
-    text_encoder_one.to(accelerator.device, dtype=weight_dtype)
-    text_encoder_two.to(accelerator.device, dtype=weight_dtype)
-
-    # Create EMA for the unet.
-    if args.use_ema:
-        ema_unet = UNet2DConditionModel.from_pretrained(
-            args.pretrained_model_name_or_path,
-            subfolder='unet',
-            revision=args.revision,
-            variant=args.variant)
-        ema_unet = EMAModel(
-            ema_unet.parameters(),
-            model_cls=UNet2DConditionModel,
-            model_config=ema_unet.config)
-
-    if args.enable_xformers_memory_efficient_attention:
-        if is_xformers_available():
-            import xformers
+        args.pretrained_model_name_or_path, subfolder='unet', revision=args.revision, variant=args.variant)
 
-            xformers_version = version.parse(xformers.__version__)
-            if xformers_version == version.parse('0.0.16'):
-                logger.warn(
-                    'xFormers 0.0.16 cannot be used for training in some GPUs. If you observe problems during training,'
-                    ' please update xFormers to at least 0.0.17. '
-                    'See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.'
-                )
-            unet.enable_xformers_memory_efficient_attention()
-        else:
-            raise ValueError(
-                'xformers is not available. Make sure it is installed correctly'
-            )
+    if args.controlnet_model_name_or_path:
+        logger.info('Loading existing controlnet weights')
+        controlnet = ControlNetModel.from_pretrained(args.controlnet_model_name_or_path)
+    else:
+        logger.info('Initializing controlnet weights from unet')
+        controlnet = ControlNetModel.from_unet(unet)
 
     # `accelerate` 0.16.0 will have better support for customized saving
     if version.parse(accelerate.__version__) >= version.parse('0.16.0'):
         # create custom saving & loading hooks so that `accelerator.save_state(...)` serializes in a nice format
         def save_model_hook(models, weights, output_dir):
             if accelerator.is_main_process:
-                if args.use_ema:
-                    ema_unet.save_pretrained(
-                        os.path.join(output_dir, 'unet_ema'))
-
-                for i, model in enumerate(models):
-                    model.save_pretrained(os.path.join(output_dir, 'unet'))
+                i = len(weights) - 1
 
-                    # make sure to pop weight so that corresponding model is not saved again
+                while len(weights) > 0:
                     weights.pop()
+                    model = models[i]
 
-        def load_model_hook(models, input_dir):
-            if args.use_ema:
-                load_model = EMAModel.from_pretrained(
-                    os.path.join(input_dir, 'unet_ema'), UNet2DConditionModel)
-                ema_unet.load_state_dict(load_model.state_dict())
-                ema_unet.to(accelerator.device)
-                del load_model
+                    sub_dir = 'controlnet'
+                    model.save_pretrained(os.path.join(output_dir, sub_dir))
 
-            for i in range(len(models)):
+                    i -= 1
+
+        def load_model_hook(models, input_dir):
+            while len(models) > 0:
                 # pop models so that they are not loaded again
                 model = models.pop()
 
                 # load diffusers style into model
-                load_model = UNet2DConditionModel.from_pretrained(
-                    input_dir, subfolder='unet')
+                load_model = ControlNetModel.from_pretrained(input_dir, subfolder='controlnet')
                 model.register_to_config(**load_model.config)
 
                 model.load_state_dict(load_model.state_dict())
                 del load_model
 
         accelerator.register_save_state_pre_hook(save_model_hook)
         accelerator.register_load_state_pre_hook(load_model_hook)
 
+    vae.requires_grad_(False)
+    unet.requires_grad_(False)
+    text_encoder_one.requires_grad_(False)
+    text_encoder_two.requires_grad_(False)
+    controlnet.train()
+
+    if args.enable_xformers_memory_efficient_attention:
+        if is_xformers_available():
+            import xformers
+
+            xformers_version = version.parse(xformers.__version__)
+            if xformers_version == version.parse('0.0.16'):
+                logger.warn('xFormers 0.0.16 cannot be used for training in some GPUs. '
+                            'If you observe problems during training, please update xFormers to at least 0.0.17. '
+                            'See https://huggingface.co/docs/diffusers/main/en/optimization/xformers for more details.')
+            unet.enable_xformers_memory_efficient_attention()
+            controlnet.enable_xformers_memory_efficient_attention()
+        else:
+            raise ValueError('xformers is not available. Make sure it is installed correctly')
+
     if args.gradient_checkpointing:
+        controlnet.enable_gradient_checkpointing()
         unet.enable_gradient_checkpointing()
 
+    # Check that all trainable models are in full precision
+    low_precision_error_string = (
+        ' Please make sure to always have all model weights in full float32 precision when starting training - even if'
+        ' doing mixed precision training, copy of the weights should still be float32.')
+
+    if accelerator.unwrap_model(controlnet).dtype != torch.float32:
+        raise ValueError(
+            f'Controlnet loaded as datatype {accelerator.unwrap_model(controlnet).dtype}. {low_precision_error_string}')
+
     # Enable TF32 for faster training on Ampere GPUs,
     # cf https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices
     if args.allow_tf32:
         torch.backends.cuda.matmul.allow_tf32 = True
 
     if args.scale_lr:
         args.learning_rate = (
-            args.learning_rate * args.gradient_accumulation_steps
-            * args.train_batch_size * accelerator.num_processes)
+            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes)
 
     # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs
     if args.use_8bit_adam:
         try:
             import bitsandbytes as bnb
         except ImportError:
-            raise ImportError(
-                'To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.'
-            )
+            raise ImportError('To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.')
 
         optimizer_class = bnb.optim.AdamW8bit
     else:
         optimizer_class = torch.optim.AdamW
 
     # Optimizer creation
-    params_to_optimize = unet.parameters()
+    params_to_optimize = controlnet.parameters()
     optimizer = optimizer_class(
         params_to_optimize,
         lr=args.learning_rate,
         betas=(args.adam_beta1, args.adam_beta2),
         weight_decay=args.adam_weight_decay,
         eps=args.adam_epsilon,
     )
 
-    # Get the datasets: you can either provide your own training and evaluation files (see below)
-    # or specify a Dataset from the hub (the dataset will be downloaded automatically from the datasets Hub).
-
-    # In distributed training, the load_dataset function guarantees that only one local process can concurrently
-    # download the dataset.
-    def path_to_img(example):
-        example['image'] = Image.open(example['image:FILE'])
-        return example
-
-    if args.dataset_name is not None:
-        # Downloading and loading a dataset from the hub.
-        dataset = MsDataset.load(
-            args.dataset_name,
-            args.dataset_config_name,
-            data_dir=args.train_data_dir,
-        )
-        if isinstance(dataset, dict):
-            dataset = {
-                key: value.to_hf_dataset()
-                for key, value in dataset.items()
-            }
-        else:
-            dataset = {'train': dataset.to_hf_dataset()}
-    else:
-        data_files = {}
-        if args.train_data_dir is not None:
-            data_files['train'] = os.path.join(args.train_data_dir, '**')
-        dataset = load_dataset(
-            'imagefolder',
-            data_files=data_files,
-            cache_dir=args.cache_dir,
-        )
-        # See more about loading custom images at
-        # https://huggingface.co/docs/datasets/v2.4.0/en/image_load#imagefolder
-
-    # Preprocessing the datasets.
-    # We need to tokenize inputs and targets.
-    column_names = dataset['train'].column_names
+    # For mixed precision training we cast the text_encoder and vae weights to half-precision
+    # as these models are only used for inference, keeping weights in full precision is not required.
+    weight_dtype = torch.float32
+    if accelerator.mixed_precision == 'fp16':
+        weight_dtype = torch.float16
+    elif accelerator.mixed_precision == 'bf16':
+        weight_dtype = torch.bfloat16
 
-    # 6. Get the column names for input/target.
-    dataset_columns = DATASET_NAME_MAPPING.get(args.dataset_name, None)
-    if args.image_column is None:
-        image_column = dataset_columns[
-            1] if dataset_columns is not None else column_names[1]
-    else:
-        image_column = args.image_column
-        if image_column not in column_names:
-            raise ValueError(
-                f"--image_column' value '{args.image_column}' needs to be one of: {', '.join(column_names)}"
-            )
-    if args.caption_column is None:
-        caption_column = dataset_columns[
-            0] if dataset_columns is not None else column_names[0]
+    # Move vae, unet and text_encoder to device and cast to weight_dtype
+    # The VAE is in float32 to avoid NaN losses.
+    if args.pretrained_vae_model_name_or_path is not None:
+        vae.to(accelerator.device, dtype=weight_dtype)
     else:
-        caption_column = args.caption_column
-        if caption_column not in column_names:
-            raise ValueError(
-                f"--caption_column' value '{args.caption_column}' needs to be one of: {', '.join(column_names)}"
-            )
-    if image_column.endswith(':FILE'):
-        dataset['train'] = dataset['train'].map(path_to_img)
-        image_column = 'image'
+        vae.to(accelerator.device, dtype=torch.float32)
+    unet.to(accelerator.device, dtype=weight_dtype)
+    text_encoder_one.to(accelerator.device, dtype=weight_dtype)
+    text_encoder_two.to(accelerator.device, dtype=weight_dtype)
 
-    # Preprocessing the datasets.
-    train_resize = transforms.Resize(
-        args.resolution, interpolation=transforms.InterpolationMode.BILINEAR)
-    train_crop = transforms.CenterCrop(
-        args.resolution) if args.center_crop else transforms.RandomCrop(
-            args.resolution)
-    train_flip = transforms.RandomHorizontalFlip(p=1.0)
-    train_transforms = transforms.Compose(
-        [transforms.ToTensor(),
-         transforms.Normalize([0.5], [0.5])])
+    # Here, we compute not just the text embeddings but also the additional embeddings
+    # needed for the SD XL UNet to operate.
+    def compute_embeddings(batch, proportion_empty_prompts, text_encoders, tokenizers, is_train=True):
+        original_size = (args.resolution, args.resolution)
+        target_size = (args.resolution, args.resolution)
+        crops_coords_top_left = (args.crops_coords_top_left_h, args.crops_coords_top_left_w)
+        prompt_batch = batch[args.caption_column]
+
+        prompt_embeds, pooled_prompt_embeds = encode_prompt(prompt_batch, text_encoders, tokenizers,
+                                                            proportion_empty_prompts, is_train)
+        add_text_embeds = pooled_prompt_embeds
+
+        # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids
+        add_time_ids = list(original_size + crops_coords_top_left + target_size)
+        add_time_ids = torch.tensor([add_time_ids])
+
+        prompt_embeds = prompt_embeds.to(accelerator.device)
+        add_text_embeds = add_text_embeds.to(accelerator.device)
+        add_time_ids = add_time_ids.repeat(len(prompt_batch), 1)
+        add_time_ids = add_time_ids.to(accelerator.device, dtype=prompt_embeds.dtype)
+        unet_added_cond_kwargs = {'text_embeds': add_text_embeds, 'time_ids': add_time_ids}
 
-    def preprocess_train(examples):
-        images = [image.convert('RGB') for image in examples[image_column]]
-        # image aug
-        original_sizes = []
-        all_images = []
-        crop_top_lefts = []
-        for image in images:
-            original_sizes.append((image.height, image.width))
-            image = train_resize(image)
-            if args.center_crop:
-                y1 = max(0, int(round((image.height - args.resolution) / 2.0)))
-                x1 = max(0, int(round((image.width - args.resolution) / 2.0)))
-                image = train_crop(image)
-            else:
-                y1, x1, h, w = train_crop.get_params(
-                    image, (args.resolution, args.resolution))
-                image = crop(image, y1, x1, h, w)
-            if args.random_flip and random.random() < 0.5:
-                # flip
-                x1 = image.width - x1
-                image = train_flip(image)
-            crop_top_left = (y1, x1)
-            crop_top_lefts.append(crop_top_left)
-            image = train_transforms(image)
-            all_images.append(image)
-
-        examples['original_sizes'] = original_sizes
-        examples['crop_top_lefts'] = crop_top_lefts
-        examples['pixel_values'] = all_images
-        return examples
-
-    with accelerator.main_process_first():
-        if args.max_train_samples is not None:
-            dataset['train'] = dataset['train'].shuffle(seed=args.seed).select(
-                range(args.max_train_samples))
-        # Set the training transforms
-        train_dataset = dataset['train'].with_transform(preprocess_train)
+        return {'prompt_embeds': prompt_embeds, **unet_added_cond_kwargs}
 
     # Let's first compute all the embeddings so that we can free up the text encoders
-    # from memory. We will pre-compute the VAE encodings too.
+    # from memory.
     text_encoders = [text_encoder_one, text_encoder_two]
     tokenizers = [tokenizer_one, tokenizer_two]
+    train_dataset = get_train_dataset(args, accelerator)
     compute_embeddings_fn = functools.partial(
-        encode_prompt,
+        compute_embeddings,
         text_encoders=text_encoders,
         tokenizers=tokenizers,
         proportion_empty_prompts=args.proportion_empty_prompts,
-        caption_column=args.caption_column,
     )
-    compute_vae_encodings_fn = functools.partial(
-        compute_vae_encodings, vae=vae)
     with accelerator.main_process_first():
         from datasets.fingerprint import Hasher
 
         # fingerprint used by the cache for the other processes to load the result
         # details: https://github.com/huggingface/diffusers/pull/4038#discussion_r1266078401
         new_fingerprint = Hasher.hash(args)
-        new_fingerprint_for_vae = Hasher.hash('vae')
-        train_dataset = train_dataset.map(
-            compute_embeddings_fn,
-            batched=True,
-            new_fingerprint=new_fingerprint)
-        train_dataset = train_dataset.map(
-            compute_vae_encodings_fn,
-            batched=True,
-            batch_size=args.train_batch_size * accelerator.num_processes
-            * args.gradient_accumulation_steps,
-            new_fingerprint=new_fingerprint_for_vae,
-        )
+        train_dataset = train_dataset.map(compute_embeddings_fn, batched=True, new_fingerprint=new_fingerprint)
 
-    del text_encoders, tokenizers, vae
+    del text_encoders, tokenizers
     gc.collect()
     torch.cuda.empty_cache()
 
-    def collate_fn(examples):
-        model_input = torch.stack(
-            [torch.tensor(example['model_input']) for example in examples])
-        original_sizes = [example['original_sizes'] for example in examples]
-        crop_top_lefts = [example['crop_top_lefts'] for example in examples]
-        prompt_embeds = torch.stack(
-            [torch.tensor(example['prompt_embeds']) for example in examples])
-        pooled_prompt_embeds = torch.stack([
-            torch.tensor(example['pooled_prompt_embeds'])
-            for example in examples
-        ])
-
-        return {
-            'model_input': model_input,
-            'prompt_embeds': prompt_embeds,
-            'pooled_prompt_embeds': pooled_prompt_embeds,
-            'original_sizes': original_sizes,
-            'crop_top_lefts': crop_top_lefts,
-        }
+    # Then get the training dataset ready to be passed to the dataloader.
+    train_dataset = prepare_train_dataset(args, train_dataset, accelerator)
 
-    # DataLoaders creation:
     train_dataloader = torch.utils.data.DataLoader(
         train_dataset,
         shuffle=True,
         collate_fn=collate_fn,
         batch_size=args.train_batch_size,
         num_workers=args.dataloader_num_workers,
     )
 
     # Scheduler and math around the number of training steps.
     overrode_max_train_steps = False
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if args.max_train_steps is None:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
         overrode_max_train_steps = True
 
     lr_scheduler = get_scheduler(
         args.lr_scheduler,
         optimizer=optimizer,
-        num_warmup_steps=args.lr_warmup_steps
-        * args.gradient_accumulation_steps,
-        num_training_steps=args.max_train_steps
-        * args.gradient_accumulation_steps,
+        num_warmup_steps=args.lr_warmup_steps * accelerator.num_processes,
+        num_training_steps=args.max_train_steps * accelerator.num_processes,
+        num_cycles=args.lr_num_cycles,
+        power=args.lr_power,
     )
 
     # Prepare everything with our `accelerator`.
-    unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
-        unet, optimizer, train_dataloader, lr_scheduler)
+    controlnet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(controlnet, optimizer, train_dataloader,
+                                                                                lr_scheduler)
 
     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
-    num_update_steps_per_epoch = math.ceil(
-        len(train_dataloader) / args.gradient_accumulation_steps)
+    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)
     if overrode_max_train_steps:
         args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch
     # Afterwards we recalculate our number of training epochs
-    args.num_train_epochs = math.ceil(args.max_train_steps
-                                      / num_update_steps_per_epoch)
+    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)
 
     # We need to initialize the trackers we use, and also store our configuration.
     # The trackers initializes automatically on the main process.
     if accelerator.is_main_process:
-        accelerator.init_trackers(
-            'text2image-fine-tune-sdxl', config=vars(args))
+        tracker_config = dict(vars(args))
+
+        # tensorboard cannot handle list types for config
+        tracker_config.pop('validation_prompt')
+        tracker_config.pop('validation_image')
+
+        accelerator.init_trackers(args.tracker_project_name, config=tracker_config)
 
     # Train!
     total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps
 
     logger.info('***** Running training *****')
     logger.info(f'  Num examples = {len(train_dataset)}')
+    logger.info(f'  Num batches each epoch = {len(train_dataloader)}')
     logger.info(f'  Num Epochs = {args.num_train_epochs}')
-    logger.info(
-        f'  Instantaneous batch size per device = {args.train_batch_size}')
-    logger.info(
-        f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}'
-    )
-    logger.info(
-        f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
+    logger.info(f'  Instantaneous batch size per device = {args.train_batch_size}')
+    logger.info(f'  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}')
+    logger.info(f'  Gradient Accumulation steps = {args.gradient_accumulation_steps}')
     logger.info(f'  Total optimization steps = {args.max_train_steps}')
     global_step = 0
     first_epoch = 0
 
     # Potentially load in the weights and states from a previous save
     if args.resume_from_checkpoint:
         if args.resume_from_checkpoint != 'latest':
@@ -1112,353 +1030,150 @@
             dirs = os.listdir(args.output_dir)
             dirs = [d for d in dirs if d.startswith('checkpoint')]
             dirs = sorted(dirs, key=lambda x: int(x.split('-')[1]))
             path = dirs[-1] if len(dirs) > 0 else None
 
         if path is None:
             accelerator.print(
-                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run."
-            )
+                f"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.")
             args.resume_from_checkpoint = None
             initial_global_step = 0
         else:
             accelerator.print(f'Resuming from checkpoint {path}')
             accelerator.load_state(os.path.join(args.output_dir, path))
             global_step = int(path.split('-')[1])
 
             initial_global_step = global_step
             first_epoch = global_step // num_update_steps_per_epoch
-
     else:
         initial_global_step = 0
 
     progress_bar = tqdm(
         range(0, args.max_train_steps),
         initial=initial_global_step,
         desc='Steps',
         # Only show the progress bar once on each machine.
         disable=not accelerator.is_local_main_process,
     )
 
+    image_logs = None
     for epoch in range(first_epoch, args.num_train_epochs):
-        train_loss = 0.0
         for step, batch in enumerate(train_dataloader):
-            with accelerator.accumulate(unet):
-                # Sample noise that we'll add to the latents
-                model_input = batch['model_input'].to(accelerator.device)
-                noise = torch.randn_like(model_input)
-                if args.noise_offset:
-                    # https://www.crosslabs.org//blog/diffusion-with-offset-noise
-                    noise += args.noise_offset * torch.randn(
-                        (model_input.shape[0], model_input.shape[1], 1, 1),
-                        device=model_input.device)
-
-                bsz = model_input.shape[0]
-                if args.timestep_bias_strategy == 'none':
-                    # Sample a random timestep for each image without bias.
-                    timesteps = torch.randint(
-                        0,
-                        noise_scheduler.config.num_train_timesteps, (bsz, ),
-                        device=model_input.device)
+            with accelerator.accumulate(controlnet):
+                # Convert images to latent space
+                if args.pretrained_vae_model_name_or_path is not None:
+                    pixel_values = batch['pixel_values'].to(dtype=weight_dtype)
                 else:
-                    # Sample a random timestep for each image, potentially biased by the timestep weights.
-                    # Biasing the timestep weights allows us to spend less time training irrelevant timesteps.
-                    weights = generate_timestep_weights(
-                        args, noise_scheduler.config.num_train_timesteps).to(
-                            model_input.device)
-                    timesteps = torch.multinomial(
-                        weights, bsz, replacement=True).long()
+                    pixel_values = batch['pixel_values']
+                latents = vae.encode(pixel_values).latent_dist.sample()
+                latents = latents * vae.config.scaling_factor
+                if args.pretrained_vae_model_name_or_path is None:
+                    latents = latents.to(weight_dtype)
+
+                # Sample noise that we'll add to the latents
+                noise = torch.randn_like(latents)
+                bsz = latents.shape[0]
 
-                # Add noise to the model input according to the noise magnitude at each timestep
+                # Sample a random timestep for each image
+                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz, ), device=latents.device)
+                timesteps = timesteps.long()
+
+                # Add noise to the latents according to the noise magnitude at each timestep
                 # (this is the forward diffusion process)
-                noisy_model_input = noise_scheduler.add_noise(
-                    model_input, noise, timesteps)
+                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
 
-                # time ids
-                def compute_time_ids(original_size, crops_coords_top_left):
-                    # Adapted from pipeline.StableDiffusionXLPipeline._get_add_time_ids
-                    target_size = (args.resolution, args.resolution)
-                    add_time_ids = list(original_size + crops_coords_top_left
-                                        + target_size)
-                    add_time_ids = torch.tensor([add_time_ids])
-                    add_time_ids = add_time_ids.to(
-                        accelerator.device, dtype=weight_dtype)
-                    return add_time_ids
-
-                add_time_ids = torch.cat([
-                    compute_time_ids(s, c) for s, c in zip(
-                        batch['original_sizes'], batch['crop_top_lefts'])
-                ])
+                # ControlNet conditioning.
+                controlnet_image = batch['conditioning_pixel_values'].to(dtype=weight_dtype)
+                down_block_res_samples, mid_block_res_sample = controlnet(
+                    noisy_latents,
+                    timesteps,
+                    encoder_hidden_states=batch['prompt_ids'],
+                    added_cond_kwargs=batch['unet_added_conditions'],
+                    controlnet_cond=controlnet_image,
+                    return_dict=False,
+                )
 
                 # Predict the noise residual
-                unet_added_conditions = {'time_ids': add_time_ids}
-                prompt_embeds = batch['prompt_embeds'].to(accelerator.device)
-                pooled_prompt_embeds = batch['pooled_prompt_embeds'].to(
-                    accelerator.device)
-                unet_added_conditions.update(
-                    {'text_embeds': pooled_prompt_embeds})
                 model_pred = unet(
-                    noisy_model_input,
+                    noisy_latents,
                     timesteps,
-                    prompt_embeds,
-                    added_cond_kwargs=unet_added_conditions).sample
+                    encoder_hidden_states=batch['prompt_ids'],
+                    added_cond_kwargs=batch['unet_added_conditions'],
+                    down_block_additional_residuals=[
+                        sample.to(dtype=weight_dtype) for sample in down_block_res_samples
+                    ],
+                    mid_block_additional_residual=mid_block_res_sample.to(dtype=weight_dtype),
+                ).sample
 
                 # Get the target for loss depending on the prediction type
-                if args.prediction_type is not None:
-                    # set prediction_type of scheduler if defined
-                    noise_scheduler.register_to_config(
-                        prediction_type=args.prediction_type)
-
                 if noise_scheduler.config.prediction_type == 'epsilon':
                     target = noise
                 elif noise_scheduler.config.prediction_type == 'v_prediction':
-                    target = noise_scheduler.get_velocity(
-                        model_input, noise, timesteps)
-                elif noise_scheduler.config.prediction_type == 'sample':
-                    # We set the target to latents here, but the model_pred will return the noise sample prediction.
-                    target = model_input
-                    # We will have to subtract the noise residual from the prediction to get the target sample.
-                    model_pred = model_pred - noise
-                else:
-                    raise ValueError(
-                        f'Unknown prediction type {noise_scheduler.config.prediction_type}'
-                    )
-
-                if args.snr_gamma is None:
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='mean')
+                    target = noise_scheduler.get_velocity(latents, noise, timesteps)
                 else:
-                    # Compute loss-weights as per Section 3.4 of https://arxiv.org/abs/2303.09556.
-                    # Since we predict the noise instead of x_0, the original formulation is slightly changed.
-                    # This is discussed in Section 4.2 of the same paper.
-                    snr = compute_snr(noise_scheduler, timesteps)
-                    if noise_scheduler.config.prediction_type == 'v_prediction':
-                        # Velocity objective requires that we add one to SNR values before we divide by them.
-                        snr = snr + 1
-                    mse_loss_weights = (
-                        torch.stack(
-                            [snr, args.snr_gamma * torch.ones_like(timesteps)],
-                            dim=1).min(dim=1)[0] / snr)
-
-                    loss = F.mse_loss(
-                        model_pred.float(), target.float(), reduction='none')
-                    loss = loss.mean(
-                        dim=list(range(1, len(loss.shape)))) * mse_loss_weights
-                    loss = loss.mean()
-
-                # Gather the losses across all processes for logging (if we use distributed training).
-                avg_loss = accelerator.gather(
-                    loss.repeat(args.train_batch_size)).mean()
-                train_loss += avg_loss.item(
-                ) / args.gradient_accumulation_steps
+                    raise ValueError(f'Unknown prediction type {noise_scheduler.config.prediction_type}')
+                loss = F.mse_loss(model_pred.float(), target.float(), reduction='mean')
 
-                # Backpropagate
                 accelerator.backward(loss)
                 if accelerator.sync_gradients:
-                    params_to_clip = unet.parameters()
-                    accelerator.clip_grad_norm_(params_to_clip,
-                                                args.max_grad_norm)
+                    params_to_clip = controlnet.parameters()
+                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)
                 optimizer.step()
                 lr_scheduler.step()
-                optimizer.zero_grad()
+                optimizer.zero_grad(set_to_none=args.set_grads_to_none)
 
             # Checks if the accelerator has performed an optimization step behind the scenes
             if accelerator.sync_gradients:
                 progress_bar.update(1)
                 global_step += 1
-                accelerator.log({'train_loss': train_loss}, step=global_step)
-                train_loss = 0.0
 
                 if accelerator.is_main_process:
                     if global_step % args.checkpointing_steps == 0:
                         # _before_ saving state, check if this save would set us over the `checkpoints_total_limit`
                         if args.checkpoints_total_limit is not None:
                             checkpoints = os.listdir(args.output_dir)
-                            checkpoints = [
-                                d for d in checkpoints
-                                if d.startswith('checkpoint')
-                            ]
-                            checkpoints = sorted(
-                                checkpoints,
-                                key=lambda x: int(x.split('-')[1]))
-
-                            # before we save the new checkpoint, we need to have at _most_ \
-                            # `checkpoints_total_limit - 1` checkpoints
-                            if len(checkpoints
-                                   ) >= args.checkpoints_total_limit:
-                                num_to_remove = len(
-                                    checkpoints
-                                ) - args.checkpoints_total_limit + 1
-                                removing_checkpoints = checkpoints[
-                                    0:num_to_remove]
-
-                                logger.info(
-                                    f'{len(checkpoints)} checkpoints already exist, '
-                                    f'removing {len(removing_checkpoints)} checkpoints'
-                                )
-                                logger.info(
-                                    f"removing checkpoints: {', '.join(removing_checkpoints)}"
-                                )
+                            checkpoints = [d for d in checkpoints if d.startswith('checkpoint')]
+                            checkpoints = sorted(checkpoints, key=lambda x: int(x.split('-')[1]))
+
+                            # before we save the new checkpoint,
+                            # we need to have at _most_ `checkpoints_total_limit - 1` checkpoints
+                            if len(checkpoints) >= args.checkpoints_total_limit:
+                                num_to_remove = len(checkpoints) - args.checkpoints_total_limit + 1
+                                removing_checkpoints = checkpoints[0:num_to_remove]
+
+                                logger.info(f'{len(checkpoints)} checkpoints already exist, '
+                                            f'removing {len(removing_checkpoints)} checkpoints')
+                                logger.info(f"removing checkpoints: {', '.join(removing_checkpoints)}")
 
                                 for removing_checkpoint in removing_checkpoints:
-                                    removing_checkpoint = os.path.join(
-                                        args.output_dir, removing_checkpoint)
+                                    removing_checkpoint = os.path.join(args.output_dir, removing_checkpoint)
                                     shutil.rmtree(removing_checkpoint)
 
-                        save_path = os.path.join(args.output_dir,
-                                                 f'checkpoint-{global_step}')
+                        save_path = os.path.join(args.output_dir, f'checkpoint-{global_step}')
                         accelerator.save_state(save_path)
                         logger.info(f'Saved state to {save_path}')
 
-            logs = {
-                'step_loss': loss.detach().item(),
-                'lr': lr_scheduler.get_last_lr()[0]
-            }
+                    if args.validation_prompt is not None and global_step % args.validation_steps == 0:
+                        image_logs = log_validation(vae, unet, controlnet, args, accelerator, weight_dtype, global_step)
+
+            logs = {'loss': loss.detach().item(), 'lr': lr_scheduler.get_last_lr()[0]}
             progress_bar.set_postfix(**logs)
+            accelerator.log(logs, step=global_step)
 
             if global_step >= args.max_train_steps:
                 break
 
-        if accelerator.is_main_process:
-            if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
-                logger.info(
-                    f'Running validation... \n Generating {args.num_validation_images} images with prompt:'
-                    f' {args.validation_prompt}.')
-                if args.use_ema:
-                    # Store the UNet parameters temporarily and load the EMA parameters to perform inference.
-                    ema_unet.store(unet.parameters())
-                    ema_unet.copy_to(unet.parameters())
-
-                # create pipeline
-                vae = AutoencoderKL.from_pretrained(
-                    vae_path,
-                    subfolder='vae' if
-                    args.pretrained_vae_model_name_or_path is None else None,
-                    revision=args.revision,
-                    variant=args.variant,
-                )
-                pipeline = StableDiffusionXLPipeline.from_pretrained(
-                    args.pretrained_model_name_or_path,
-                    vae=vae,
-                    unet=accelerator.unwrap_model(unet),
-                    revision=args.revision,
-                    variant=args.variant,
-                    torch_dtype=weight_dtype,
-                )
-                if args.prediction_type is not None:
-                    scheduler_args = {'prediction_type': args.prediction_type}
-                    pipeline.scheduler = pipeline.scheduler.from_config(
-                        pipeline.scheduler.config, **scheduler_args)
-
-                pipeline = pipeline.to(accelerator.device)
-                pipeline.set_progress_bar_config(disable=True)
-
-                # run inference
-                generator = torch.Generator(
-                    device=accelerator.device).manual_seed(
-                        args.seed) if args.seed else None
-                pipeline_args = {'prompt': args.validation_prompt}
-
-                with torch.cuda.amp.autocast():
-                    images = [
-                        pipeline(
-                            **pipeline_args,
-                            generator=generator,
-                            num_inference_steps=25).images[0]
-                        for _ in range(args.num_validation_images)
-                    ]
-
-                for tracker in accelerator.trackers:
-                    if tracker.name == 'tensorboard':
-                        np_images = np.stack(
-                            [np.asarray(img) for img in images])
-                        tracker.writer.add_images(
-                            'validation', np_images, epoch, dataformats='NHWC')
-                    if tracker.name == 'wandb':
-                        tracker.log({
-                            'validation': [
-                                wandb.Image(
-                                    image,
-                                    caption=f'{i}: {args.validation_prompt}')
-                                for i, image in enumerate(images)
-                            ]
-                        })
-
-                del pipeline
-                torch.cuda.empty_cache()
-
+    # Create the pipeline using using the trained modules and save it.
     accelerator.wait_for_everyone()
     if accelerator.is_main_process:
-        unet = accelerator.unwrap_model(unet)
-        if args.use_ema:
-            ema_unet.copy_to(unet.parameters())
-
-        # Serialize pipeline.
-        vae = AutoencoderKL.from_pretrained(
-            vae_path,
-            subfolder='vae'
-            if args.pretrained_vae_model_name_or_path is None else None,
-            revision=args.revision,
-            variant=args.variant,
-            torch_dtype=weight_dtype,
-        )
-        pipeline = StableDiffusionXLPipeline.from_pretrained(
-            args.pretrained_model_name_or_path,
-            unet=unet,
-            vae=vae,
-            revision=args.revision,
-            variant=args.variant,
-            torch_dtype=weight_dtype,
-        )
-        if args.prediction_type is not None:
-            scheduler_args = {'prediction_type': args.prediction_type}
-            pipeline.scheduler = pipeline.scheduler.from_config(
-                pipeline.scheduler.config, **scheduler_args)
-        pipeline.save_pretrained(args.output_dir)
-
-        # run inference
-        images = []
-        if args.validation_prompt and args.num_validation_images > 0:
-            pipeline = pipeline.to(accelerator.device)
-            generator = torch.Generator(device=accelerator.device).manual_seed(
-                args.seed) if args.seed else None
-            with torch.cuda.amp.autocast():
-                images = [
-                    pipeline(
-                        args.validation_prompt,
-                        num_inference_steps=25,
-                        generator=generator).images[0]
-                    for _ in range(args.num_validation_images)
-                ]
-
-            for tracker in accelerator.trackers:
-                if tracker.name == 'tensorboard':
-                    np_images = np.stack([np.asarray(img) for img in images])
-                    tracker.writer.add_images(
-                        'test', np_images, epoch, dataformats='NHWC')
-                if tracker.name == 'wandb':
-                    tracker.log({
-                        'test': [
-                            wandb.Image(
-                                image,
-                                caption=f'{i}: {args.validation_prompt}')
-                            for i, image in enumerate(images)
-                        ]
-                    })
-
+        controlnet = accelerator.unwrap_model(controlnet)
+        controlnet.save_pretrained(args.output_dir)
         if args.push_to_hub:
             save_model_card(
-                repo_id=args.hub_model_id,
-                images=images,
-                validation_prompt=args.validation_prompt,
+                args.hub_model_id,
+                image_logs=image_logs,
                 base_model=args.base_model_id,
-                dataset_name=args.dataset_name,
                 repo_folder=args.output_dir,
-                vae_path=args.vae_base_model_id,
-            )
-            push_to_hub(
-                args.hub_model_id,
-                args.output_dir,
-                args.hub_token,
             )
+            push_to_hub(args.hub_model_id, args.output_dir, args.hub_token)
 
     accelerator.end_training()
```

### Comparing `ms-swift-2.0.3.post1/swift/aigc/utils/argument.py` & `ms-swift-2.0.4/swift/aigc/utils/argument.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,20 +18,18 @@
     motion_adapter_revision: Optional[str] = None
 
     model_id_or_path: str = None
     model_revision: str = None
 
     dataset_sample_size: int = None
 
-    sft_type: str = field(
-        default='lora', metadata={'choices': ['lora', 'full']})
+    sft_type: str = field(default='lora', metadata={'choices': ['lora', 'full']})
 
     output_dir: str = 'output'
-    ddp_backend: str = field(
-        default='nccl', metadata={'choices': ['nccl', 'gloo', 'mpi', 'ccl']})
+    ddp_backend: str = field(default='nccl', metadata={'choices': ['nccl', 'gloo', 'mpi', 'ccl']})
 
     seed: int = 42
 
     lora_rank: int = 8
     lora_alpha: int = 32
     lora_dropout_p: float = 0.05
     lora_dtype: Literal['fp16', 'bf16', 'fp32', 'AUTO'] = 'fp32'
@@ -52,42 +50,29 @@
     save_steps: Optional[int] = None
     dataloader_num_workers: int = 1
 
     push_to_hub: bool = False
     # 'user_name/repo_name' or 'repo_name'
     hub_model_id: Optional[str] = None
     hub_private_repo: bool = False
-    push_hub_strategy: str = field(
-        default='push_best',
-        metadata={'choices': ['push_last', 'all_checkpoints']})
+    push_hub_strategy: str = field(default='push_best', metadata={'choices': ['push_last', 'all_checkpoints']})
     # None: use env var `MODELSCOPE_API_TOKEN`
     hub_token: Optional[str] = field(
-        default=None,
-        metadata={
-            'help':
-            'SDK token can be found in https://modelscope.cn/my/myaccesstoken'
-        })
+        default=None, metadata={'help': 'SDK token can be found in https://modelscope.cn/my/myaccesstoken'})
 
     ignore_args_error: bool = False  # True: notebook compatibility
 
     text_dropout_rate: float = 0.1
 
     validation_prompts_path: str = field(
-        default=None,
-        metadata={
-            'help':
-            'The validation prompts file path, use llm/configs/ad_validation.txt is None'
-        })
+        default=None, metadata={'help': 'The validation prompts file path, use llm/configs/ad_validation.txt is None'})
 
     trainable_modules: str = field(
         default='.*motion_modules.*',
-        metadata={
-            'help':
-            'The trainable modules, by default, the .*motion_modules.* will be trained'
-        })
+        metadata={'help': 'The trainable modules, by default, the .*motion_modules.* will be trained'})
 
     mixed_precision: bool = True
 
     enable_xformers_memory_efficient_attention: bool = True
 
     num_inference_steps: int = 25
     guidance_scale: float = 8.
@@ -110,16 +95,15 @@
     use_wandb: bool = False
 
     def __post_init__(self) -> None:
         handle_compatibility(self)
 
         current_dir = os.path.dirname(__file__)
         if self.validation_prompts_path is None:
-            self.validation_prompts_path = os.path.join(
-                current_dir, 'configs/animatediff', 'validation.txt')
+            self.validation_prompts_path = os.path.join(current_dir, 'configs/animatediff', 'validation.txt')
         if self.learning_rate is None:
             self.learning_rate = 1e-4
         if self.save_steps is None:
             self.save_steps = self.eval_steps
 
         if is_dist():
             rank, local_rank, _, _ = get_dist_setting()
@@ -137,19 +121,17 @@
 
     motion_adapter_id_or_path: Optional[str] = None
     motion_adapter_revision: Optional[str] = None
 
     model_id_or_path: str = None
     model_revision: str = None
 
-    sft_type: str = field(
-        default='lora', metadata={'choices': ['lora', 'full']})
+    sft_type: str = field(default='lora', metadata={'choices': ['lora', 'full']})
 
-    ckpt_dir: Optional[str] = field(
-        default=None, metadata={'help': '/path/to/your/vx-xxx/checkpoint-xxx'})
+    ckpt_dir: Optional[str] = field(default=None, metadata={'help': '/path/to/your/vx-xxx/checkpoint-xxx'})
     eval_human: bool = False  # False: eval val_dataset
 
     seed: int = 42
 
     # other
     ignore_args_error: bool = False  # True: notebook compatibility
 
@@ -180,12 +162,11 @@
     # compatibility. (Deprecated)
     merge_lora_and_save: Optional[bool] = None
 
     def __post_init__(self) -> None:
         handle_compatibility(self)
 
 
-def handle_compatibility(
-        args: Union[AnimateDiffArguments, AnimateDiffInferArguments]) -> None:
+def handle_compatibility(args: Union[AnimateDiffArguments, AnimateDiffInferArguments]) -> None:
     if isinstance(args, AnimateDiffInferArguments):
         if args.merge_lora_and_save is not None:
             args.merge_lora = args.merge_lora_and_save
```

### Comparing `ms-swift-2.0.3.post1/swift/cli/main.py` & `ms-swift-2.0.4/swift/cli/main.py`

 * *Files 10% similar despite different names*

```diff
@@ -13,35 +13,30 @@
     'web-ui': 'swift.cli.web_ui',
     'deploy': 'swift.cli.deploy',
     'dpo': 'swift.cli.dpo',
     'export': 'swift.cli.export',
     'eval': 'swift.cli.eval'
 }
 
-ROUTE_MAPPING.update(
-    {k.replace('-', '_'): v
-     for k, v in ROUTE_MAPPING.items()})
+ROUTE_MAPPING.update({k.replace('-', '_'): v for k, v in ROUTE_MAPPING.items()})
 
 
 def use_torchrun() -> bool:
     nproc_per_node = os.getenv('NPROC_PER_NODE')
     nnodes = os.getenv('NNODES')
     if nproc_per_node is None and nnodes is None:
         return False
     return True
 
 
 def get_torchrun_args() -> Optional[List[str]]:
     if not use_torchrun():
         return
     torchrun_args = []
-    for env_key in [
-            'NPROC_PER_NODE', 'MASTER_PORT', 'NNODES', 'NODE_RANK',
-            'MASTER_ADDR'
-    ]:
+    for env_key in ['NPROC_PER_NODE', 'MASTER_PORT', 'NNODES', 'NODE_RANK', 'MASTER_ADDR']:
         env_val = os.getenv(env_key)
         if env_val is None:
             continue
         torchrun_args += [f'--{env_key.lower()}', env_val]
     return torchrun_args
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/api.py` & `ms-swift-2.0.4/swift/hub/api.py`

 * *Files 0% similar despite different names*

```diff
@@ -17,31 +17,24 @@
 
 import pandas as pd
 import requests
 from requests import Session
 from requests.adapters import HTTPAdapter, Retry
 
 from swift.utils.logger import get_logger
-from .constants import (API_HTTP_CLIENT_TIMEOUT, API_RESPONSE_FIELD_DATA,
-                        API_RESPONSE_FIELD_EMAIL,
-                        API_RESPONSE_FIELD_GIT_ACCESS_TOKEN,
-                        API_RESPONSE_FIELD_MESSAGE,
-                        API_RESPONSE_FIELD_USERNAME, DEFAULT_CREDENTIALS_PATH,
-                        DEFAULT_MODEL_REVISION, DEFAULT_REPOSITORY_REVISION,
-                        MASTER_MODEL_BRANCH, MODELSCOPE_CLOUD_ENVIRONMENT,
-                        MODELSCOPE_CLOUD_USERNAME, ONE_YEAR_SECONDS,
+from .constants import (API_HTTP_CLIENT_TIMEOUT, API_RESPONSE_FIELD_DATA, API_RESPONSE_FIELD_EMAIL,
+                        API_RESPONSE_FIELD_GIT_ACCESS_TOKEN, API_RESPONSE_FIELD_MESSAGE, API_RESPONSE_FIELD_USERNAME,
+                        DEFAULT_CREDENTIALS_PATH, DEFAULT_MODEL_REVISION, DEFAULT_REPOSITORY_REVISION,
+                        MASTER_MODEL_BRANCH, MODELSCOPE_CLOUD_ENVIRONMENT, MODELSCOPE_CLOUD_USERNAME, ONE_YEAR_SECONDS,
                         REQUESTS_API_HTTP_METHOD, Licenses, ModelVisibility)
-from .errors import (InvalidParameter, NotExistError, NotLoginException,
-                     NoValidRevisionError, RequestError,
-                     handle_http_post_error, handle_http_response, is_ok,
-                     raise_for_http_status, raise_on_error)
+from .errors import (InvalidParameter, NotExistError, NotLoginException, NoValidRevisionError, RequestError,
+                     handle_http_post_error, handle_http_response, is_ok, raise_for_http_status, raise_on_error)
 from .git import GitCommandWrapper
 from .repository import Repository
-from .utils.utils import (get_endpoint, get_release_datetime,
-                          model_id_to_group_owner_name)
+from .utils.utils import get_endpoint, get_release_datetime, model_id_to_group_owner_name
 
 logger = get_logger()
 
 
 class HubApi:
     """Model hub api interface.
     """
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/check_model.py` & `ms-swift-2.0.4/swift/hub/check_model.py`

 * *Files 6% similar despite different names*

```diff
@@ -32,30 +32,21 @@
             u_parse = urlparse(git_url)
             model_id = u_parse.path[1:]
         else:  # snapshot_download
             model_cache = ModelFileSystemCache(model_root_path)
             model_id = model_cache.get_model_id()
 
         # make headers
-        headers = {
-            'user-agent':
-            ModelScopeConfig.get_user_agent(user_agent=user_agent, )
-        }
+        headers = {'user-agent': ModelScopeConfig.get_user_agent(user_agent=user_agent, )}
         cookies = ModelScopeConfig.get_cookies()
 
-        snapshot_header = headers if 'CI_TEST' in os.environ else {
-            **headers,
-            **{
-                'Snapshot': 'True'
-            }
-        }
+        snapshot_header = headers if 'CI_TEST' in os.environ else {**headers, **{'Snapshot': 'True'}}
         _api = HubApi()
         try:
-            _, revisions = _api.get_model_branches_and_tags(
-                model_id=model_id, use_cookies=cookies)
+            _, revisions = _api.get_model_branches_and_tags(model_id=model_id, use_cookies=cookies)
             if len(revisions) > 0:
                 latest_revision = revisions[0]
             else:
                 latest_revision = 'master'
         except:  # noqa: E722
             latest_revision = 'master'
 
@@ -72,28 +63,25 @@
             # check model_file updated
             if model_cache is not None:
                 if model_cache.exists(model_file):
                     continue
                 else:
                     logger.info(
                         f'Model file {model_file["Name"]} is different from the latest version `{latest_revision}`,'
-                        f'This is because you are using an older version or the file is updated manually.'
-                    )
+                        f'This is because you are using an older version or the file is updated manually.')
                     break
             else:
                 if FILE_HASH in model_file:
-                    local_file_hash = compute_hash(
-                        os.path.join(model_root_path, model_file['Path']))
+                    local_file_hash = compute_hash(os.path.join(model_root_path, model_file['Path']))
                     if local_file_hash == model_file[FILE_HASH]:
                         continue
                     else:
                         logger.info(
                             f'Model file {model_file["Name"]} is different from the latest version `{latest_revision}`,'
-                            f'This is because you are using an older version or the file is updated manually.'
-                        )
+                            f'This is because you are using an older version or the file is updated manually.')
                         break
     except:  # noqa: E722
         pass  # ignore
 
 
 def check_model_is_id(model_id: str, token=None):
     if token is None:
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/constants.py` & `ms-swift-2.0.4/swift/hub/constants.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,18 +1,16 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 from pathlib import Path
 
 MODELSCOPE_URL_SCHEME = 'http://'
 DEFAULT_MODELSCOPE_DOMAIN = 'www.modelscope.cn'
 DEFAULT_MODELSCOPE_DATA_ENDPOINT = MODELSCOPE_URL_SCHEME + DEFAULT_MODELSCOPE_DOMAIN
-MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB = int(
-    os.environ.get('MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB', 500))
-MODELSCOPE_DOWNLOAD_PARALLELS = int(
-    os.environ.get('MODELSCOPE_DOWNLOAD_PARALLELS', 4))
+MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB = int(os.environ.get('MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB', 500))
+MODELSCOPE_DOWNLOAD_PARALLELS = int(os.environ.get('MODELSCOPE_DOWNLOAD_PARALLELS', 4))
 DEFAULT_MODELSCOPE_GROUP = 'damo'
 MODEL_ID_SEPARATOR = '/'
 FILE_HASH = 'Sha256'
 LOGGER_NAME = 'ModelScopeHub'
 DEFAULT_CREDENTIALS_PATH = Path.home().joinpath('.modelscope', 'credentials')
 REQUESTS_API_HTTP_METHOD = ['get', 'head', 'post', 'put', 'patch', 'delete']
 API_HTTP_CLIENT_TIMEOUT = 5
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/errors.py` & `ms-swift-2.0.4/swift/hub/errors.py`

 * *Files 2% similar despite different names*

```diff
@@ -68,16 +68,15 @@
 
 def handle_http_post_error(response, url, request_body):
     try:
         response.raise_for_status()
     except HTTPError as error:
         message = _decode_response_error(response)
         raise HTTPError('Request %s with body: %s exception, '
-                        'Response details: %s' %
-                        (url, request_body, message)) from error
+                        'Response details: %s' % (url, request_body, message)) from error
 
 
 def handle_http_response(response, logger, cookies, model_id):
     try:
         response.raise_for_status()
     except HTTPError as error:
         if cookies is None:  # code in [403] and
@@ -119,16 +118,15 @@
     Returns:
         bool: `True` if request is OK, otherwise raise `RequestError` exception.
     """
     if rsp.get('Code') == HTTPStatus.OK:
         return True
     else:
         raise RequestError(
-            f"Url = {url}, Message = {rsp.get('Message')}, Please specify correct dataset_name and namespace."
-        )
+            f"Url = {url}, Message = {rsp.get('Message')}, Please specify correct dataset_name and namespace.")
 
 
 def raise_for_http_status(rsp):
     """Attempt to decode utf-8 first since some servers
     localize reason strings, for invalid utf-8, fall back
     to decoding with iso-8859-1.
 
@@ -144,19 +142,17 @@
             reason = rsp.reason.decode('utf-8')
         except UnicodeDecodeError:
             reason = rsp.reason.decode('iso-8859-1')
     else:
         reason = rsp.reason
 
     if 400 <= rsp.status_code < 500:
-        http_error_msg = u'%s Client Error: %s for url: %s' % (rsp.status_code,
-                                                               reason, rsp.url)
+        http_error_msg = u'%s Client Error: %s for url: %s' % (rsp.status_code, reason, rsp.url)
 
     elif 500 <= rsp.status_code < 600:
-        http_error_msg = u'%s Server Error: %s for url: %s' % (rsp.status_code,
-                                                               reason, rsp.url)
+        http_error_msg = u'%s Server Error: %s for url: %s' % (rsp.status_code, reason, rsp.url)
 
     if http_error_msg:
         req = rsp.request
         if req.method == 'POST':
             http_error_msg = u'%s, body: %s' % (http_error_msg, req.body)
         raise HTTPError(http_error_msg, response=rsp)
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/file_download.py` & `ms-swift-2.0.4/swift/hub/file_download.py`

 * *Files 4% similar despite different names*

```diff
@@ -12,23 +12,20 @@
 
 import requests
 from requests.adapters import Retry
 from tqdm import tqdm
 
 from swift.utils.logger import get_logger
 from .api import HubApi, ModelScopeConfig
-from .constants import (API_FILE_DOWNLOAD_CHUNK_SIZE,
-                        API_FILE_DOWNLOAD_RETRY_TIMES,
-                        API_FILE_DOWNLOAD_TIMEOUT, DEFAULT_MODEL_REVISION,
-                        FILE_HASH, MODELSCOPE_DOWNLOAD_PARALLELS,
+from .constants import (API_FILE_DOWNLOAD_CHUNK_SIZE, API_FILE_DOWNLOAD_RETRY_TIMES, API_FILE_DOWNLOAD_TIMEOUT,
+                        DEFAULT_MODEL_REVISION, FILE_HASH, MODELSCOPE_DOWNLOAD_PARALLELS,
                         MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB)
 from .errors import FileDownloadError, NotExistError
 from .utils.caching import ModelFileSystemCache
-from .utils.utils import (file_integrity_validation, get_cache_dir,
-                          get_endpoint, model_id_to_group_owner_name)
+from .utils.utils import file_integrity_validation, get_cache_dir, get_endpoint, model_id_to_group_owner_name
 
 logger = get_logger()
 
 
 def model_file_download(
     model_id: str,
     file_path: str,
@@ -85,59 +82,47 @@
     cache = ModelFileSystemCache(cache_dir, group_or_owner, name)
 
     # if local_files_only is `True` and the file already exists in cached_path
     # return the cached path
     if local_files_only:
         cached_file_path = cache.get_file_by_path(file_path)
         if cached_file_path is not None:
-            logger.warning(
-                "File exists in local cache, but we're not sure it's up to date"
-            )
+            logger.warning("File exists in local cache, but we're not sure it's up to date")
             return cached_file_path
         else:
-            raise ValueError(
-                'Cannot find the requested files in the cached path and outgoing'
-                ' traffic has been disabled. To enable model look-ups and downloads'
-                " online, set 'local_files_only' to False.")
+            raise ValueError('Cannot find the requested files in the cached path and outgoing'
+                             ' traffic has been disabled. To enable model look-ups and downloads'
+                             " online, set 'local_files_only' to False.")
 
     _api = HubApi()
-    headers = {
-        'user-agent': ModelScopeConfig.get_user_agent(user_agent=user_agent, )
-    }
+    headers = {'user-agent': ModelScopeConfig.get_user_agent(user_agent=user_agent, )}
     if cookies is None:
         cookies = ModelScopeConfig.get_cookies()
 
-    revision = _api.get_valid_revision(
-        model_id, revision=revision, cookies=cookies)
+    revision = _api.get_valid_revision(model_id, revision=revision, cookies=cookies)
     file_to_download_info = None
     # we need to confirm the version is up-to-date
     # we need to get the file list to check if the latest version is cached, if so return, otherwise download
     model_files = _api.get_model_files(
-        model_id=model_id,
-        revision=revision,
-        recursive=True,
-        use_cookies=False if cookies is None else cookies)
+        model_id=model_id, revision=revision, recursive=True, use_cookies=False if cookies is None else cookies)
 
     for model_file in model_files:
         if model_file['Type'] == 'tree':
             continue
 
         if model_file['Path'] == file_path:
             if cache.exists(model_file):
-                logger.debug(
-                    f'File {model_file["Name"]} already in cache, skip downloading!'
-                )
+                logger.debug(f'File {model_file["Name"]} already in cache, skip downloading!')
                 return cache.get_file_by_info(model_file)
             else:
                 file_to_download_info = model_file
             break
 
     if file_to_download_info is None:
-        raise NotExistError('The file path: %s not exist in: %s' %
-                            (file_path, model_id))
+        raise NotExistError('The file path: %s not exist in: %s' % (file_path, model_id))
 
     # we need to download again
     url_to_download = get_file_download_url(model_id, file_path, revision)
     temp_file_name = next(tempfile._get_candidate_names())
 
     if MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB * 1000 * 1000 < file_to_download_info[
             'Size'] and MODELSCOPE_DOWNLOAD_PARALLELS > 1:
@@ -155,18 +140,16 @@
             temp_file_name,
             headers=headers,
             cookies=None if cookies is None else cookies.get_dict())
 
     temp_file_path = os.path.join(temporary_cache_dir, temp_file_name)
     # for download with commit we can't get Sha256
     if file_to_download_info[FILE_HASH] is not None:
-        file_integrity_validation(temp_file_path,
-                                  file_to_download_info[FILE_HASH])
-    return cache.put_file(file_to_download_info,
-                          os.path.join(temporary_cache_dir, temp_file_name))
+        file_integrity_validation(temp_file_path, file_to_download_info[FILE_HASH])
+    return cache.put_file(file_to_download_info, os.path.join(temporary_cache_dir, temp_file_name))
 
 
 def get_file_download_url(model_id: str, file_path: str, revision: str):
     """Format file download url according to `model_id`, `revision` and `file_path`.
     e.g., Given `model_id=john/bert`, `revision=master`, `file_path=README.md`,
     the resulted download url is: https://modelscope.cn/api/v1/models/john/bert/repo?Revision=master&FilePath=README.md
 
@@ -190,20 +173,15 @@
 def download_part(params):
     # unpack parameters
     progress, start, end, url, file_name, cookies, headers = params
     get_headers = {} if headers is None else copy.deepcopy(headers)
     get_headers['Range'] = 'bytes=%s-%s' % (start, end)
     with open(file_name, 'rb+') as f:
         f.seek(start)
-        r = requests.get(
-            url,
-            stream=True,
-            headers=get_headers,
-            cookies=cookies,
-            timeout=API_FILE_DOWNLOAD_TIMEOUT)
+        r = requests.get(url, stream=True, headers=get_headers, cookies=cookies, timeout=API_FILE_DOWNLOAD_TIMEOUT)
         for chunk in r.iter_content(chunk_size=API_FILE_DOWNLOAD_CHUNK_SIZE):
             if chunk:  # filter out keep-alive new chunks
                 f.write(chunk)
                 progress.update(len(chunk))
 
 
 def parallel_download(
@@ -211,39 +189,34 @@
     local_dir: str,
     file_name: str,
     cookies: CookieJar,
     headers: Optional[Dict[str, str]] = None,
     file_size: int = None,
 ):
     # create temp file
-    temp_file_manager = partial(
-        tempfile.NamedTemporaryFile, mode='wb', dir=local_dir, delete=False)
+    temp_file_manager = partial(tempfile.NamedTemporaryFile, mode='wb', dir=local_dir, delete=False)
     with temp_file_manager() as temp_file:
         progress = tqdm(
             unit='B',
             unit_scale=True,
             unit_divisor=1024,
             total=file_size,
             initial=0,
             desc='Downloading',
         )
         PART_SIZE = 160 * 1024 * 1012  # every part is 160M
         tasks = []
         for idx in range(int(file_size / PART_SIZE)):
             start = idx * PART_SIZE
             end = (idx + 1) * PART_SIZE - 1
-            tasks.append(
-                (progress, start, end, url, temp_file.name, cookies, headers))
+            tasks.append((progress, start, end, url, temp_file.name, cookies, headers))
         if end + 1 < file_size:
-            tasks.append((progress, end + 1, file_size - 1, url,
-                          temp_file.name, cookies, headers))
+            tasks.append((progress, end + 1, file_size - 1, url, temp_file.name, cookies, headers))
         parallels = MODELSCOPE_DOWNLOAD_PARALLELS if MODELSCOPE_DOWNLOAD_PARALLELS <= 4 else 4
-        with ThreadPoolExecutor(
-                max_workers=parallels,
-                thread_name_prefix='download') as executor:
+        with ThreadPoolExecutor(max_workers=parallels, thread_name_prefix='download') as executor:
             list(executor.map(download_part, tasks))
 
         progress.close()
 
     os.replace(temp_file.name, os.path.join(local_dir, file_name))
 
 
@@ -269,60 +242,49 @@
             http headers to carry necessary info when requesting the remote file
 
     Raises:
         FileDownloadError: File download failed.
 
     """
     total = -1
-    temp_file_manager = partial(
-        tempfile.NamedTemporaryFile, mode='wb', dir=local_dir, delete=False)
+    temp_file_manager = partial(tempfile.NamedTemporaryFile, mode='wb', dir=local_dir, delete=False)
     get_headers = {} if headers is None else copy.deepcopy(headers)
     with temp_file_manager() as temp_file:
         logger.debug('downloading %s to %s', url, temp_file.name)
         # retry sleep 0.5s, 1s, 2s, 4s
-        retry = Retry(
-            total=API_FILE_DOWNLOAD_RETRY_TIMES,
-            backoff_factor=1,
-            allowed_methods=['GET'])
+        retry = Retry(total=API_FILE_DOWNLOAD_RETRY_TIMES, backoff_factor=1, allowed_methods=['GET'])
         while True:
             try:
                 downloaded_size = temp_file.tell()
                 get_headers['Range'] = 'bytes=%d-' % downloaded_size
                 r = requests.get(
-                    url,
-                    stream=True,
-                    headers=get_headers,
-                    cookies=cookies,
-                    timeout=API_FILE_DOWNLOAD_TIMEOUT)
+                    url, stream=True, headers=get_headers, cookies=cookies, timeout=API_FILE_DOWNLOAD_TIMEOUT)
                 r.raise_for_status()
                 content_length = r.headers.get('Content-Length')
-                total = int(
-                    content_length) if content_length is not None else None
+                total = int(content_length) if content_length is not None else None
                 progress = tqdm(
                     unit='B',
                     unit_scale=True,
                     unit_divisor=1024,
                     total=total,
                     initial=downloaded_size,
                     desc='Downloading',
                 )
-                for chunk in r.iter_content(
-                        chunk_size=API_FILE_DOWNLOAD_CHUNK_SIZE):
+                for chunk in r.iter_content(chunk_size=API_FILE_DOWNLOAD_CHUNK_SIZE):
                     if chunk:  # filter out keep-alive new chunks
                         progress.update(len(chunk))
                         temp_file.write(chunk)
                 progress.close()
                 break
             except (Exception) as e:  # no matter what happen, we will retry.
                 retry = retry.increment('GET', url, error=e)
                 retry.sleep()
 
     logger.debug('storing %s in cache at %s', url, local_dir)
     downloaded_length = os.path.getsize(temp_file.name)
     if total != downloaded_length:
         os.remove(temp_file.name)
         msg = 'File %s download incomplete, content_length: %s but the \
-                    file downloaded length: %s, please download again' % (
-            file_name, total, downloaded_length)
+                    file downloaded length: %s, please download again' % (file_name, total, downloaded_length)
         logger.error(msg)
         raise FileDownloadError(msg)
     os.replace(temp_file.name, os.path.join(local_dir, file_name))
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/git.py` & `ms-swift-2.0.4/swift/hub/git.py`

 * *Files 5% similar despite different names*

```diff
@@ -12,16 +12,15 @@
 
 
 class Singleton(type):
     _instances = {}
 
     def __call__(cls, *args, **kwargs):
         if cls not in cls._instances:
-            cls._instances[cls] = super(Singleton,
-                                        cls).__call__(*args, **kwargs)
+            cls._instances[cls] = super(Singleton, cls).__call__(*args, **kwargs)
         return cls._instances[cls]
 
 
 class GitCommandWrapper(metaclass=Singleton):
     """Some git operation wrapper
     """
     default_git_path = 'git'  # The default git command line
@@ -52,17 +51,15 @@
             env=git_env,
         )  # compatible for python3.6
         try:
             response.check_returncode()
             return response
         except subprocess.CalledProcessError as error:
             logger.error('There are error run git command.')
-            raise GitError(
-                'stdout: %s, stderr: %s' %
-                (response.stdout.decode('utf8'), error.stderr.decode('utf8')))
+            raise GitError('stdout: %s, stderr: %s' % (response.stdout.decode('utf8'), error.stderr.decode('utf8')))
 
     def config_auth_token(self, repo_dir, auth_token):
         url = self.get_repo_remote_url(repo_dir)
         if '//oauth2' not in url:
             auth_url = self._add_token(auth_token, url)
             cmd_args = '-C %s remote set-url origin %s' % (repo_dir, auth_url)
             cmd_args = cmd_args.split(' ')
@@ -94,20 +91,15 @@
         cmd = ['-C', repo_dir, 'lfs', 'install']
         try:
             self._run_git_command(*cmd)
             return True
         except GitError:
             return False
 
-    def clone(self,
-              repo_base_dir: str,
-              token: str,
-              url: str,
-              repo_name: str,
-              branch: Optional[str] = None):
+    def clone(self, repo_base_dir: str, token: str, url: str, repo_name: str, branch: Optional[str] = None):
         """ git clone command wrapper.
         For public project, token can None, private repo, there must token.
 
         Args:
             repo_base_dir (str): The local base dir, the repository will be clone to local_dir/repo_name
             token (str): The git token, must be provided for private project.
             url (str): The remote url
@@ -115,43 +107,36 @@
             branch (str, optional): _description_. Defaults to None.
 
         Returns:
             The popen response.
         """
         url = self._add_token(token, url)
         if branch:
-            clone_args = '-C %s clone %s %s --branch %s' % (repo_base_dir, url,
-                                                            repo_name, branch)
+            clone_args = '-C %s clone %s %s --branch %s' % (repo_base_dir, url, repo_name, branch)
         else:
             clone_args = '-C %s clone %s' % (repo_base_dir, url)
         logger.debug(clone_args)
         clone_args = clone_args.split(' ')
         response = self._run_git_command(*clone_args)
         logger.debug(response.stdout.decode('utf8'))
         return response
 
     def add_user_info(self, repo_base_dir, repo_name):
         from .api import ModelScopeConfig
         user_name, user_email = ModelScopeConfig.get_user_info()
         if user_name and user_email:
             # config user.name and user.email if exist
-            config_user_name_args = '-C %s/%s config user.name %s' % (
-                repo_base_dir, repo_name, user_name)
+            config_user_name_args = '-C %s/%s config user.name %s' % (repo_base_dir, repo_name, user_name)
             response = self._run_git_command(*config_user_name_args.split(' '))
             logger.debug(response.stdout.decode('utf8'))
-            config_user_email_args = '-C %s/%s config user.email %s' % (
-                repo_base_dir, repo_name, user_email)
-            response = self._run_git_command(
-                *config_user_email_args.split(' '))
+            config_user_email_args = '-C %s/%s config user.email %s' % (repo_base_dir, repo_name, user_email)
+            response = self._run_git_command(*config_user_email_args.split(' '))
             logger.debug(response.stdout.decode('utf8'))
 
-    def add(self,
-            repo_dir: str,
-            files: List[str] = list(),
-            all_files: bool = False):
+    def add(self, repo_dir: str, files: List[str] = list(), all_files: bool = False):
         if all_files:
             add_args = '-C %s add -A' % repo_dir
         elif len(files) > 0:
             files_str = ' '.join(files)
             add_args = '-C %s add %s' % (repo_dir, files_str)
         add_args = add_args.split(' ')
         rsp = self._run_git_command(*add_args)
@@ -180,41 +165,28 @@
     def new_branch(self, repo_dir: str, revision: str):
         cmds = ['-C', '%s' % repo_dir, 'checkout', '-b', revision]
         return self._run_git_command(*cmds)
 
     def get_remote_branches(self, repo_dir: str):
         cmds = ['-C', '%s' % repo_dir, 'branch', '-r']
         rsp = self._run_git_command(*cmds)
-        info = [
-            line.strip()
-            for line in rsp.stdout.decode('utf8').strip().split(os.linesep)
-        ]
+        info = [line.strip() for line in rsp.stdout.decode('utf8').strip().split(os.linesep)]
         if len(info) == 1:
             return ['/'.join(info[0].split('/')[1:])]
         else:
             return ['/'.join(line.split('/')[1:]) for line in info[1:]]
 
-    def pull(self,
-             repo_dir: str,
-             remote: str = 'origin',
-             branch: str = 'master'):
+    def pull(self, repo_dir: str, remote: str = 'origin', branch: str = 'master'):
         cmds = ['-C', repo_dir, 'pull', remote, branch]
         return self._run_git_command(*cmds)
 
-    def push(self,
-             repo_dir: str,
-             token: str,
-             url: str,
-             local_branch: str,
-             remote_branch: str,
-             force: bool = False):
+    def push(self, repo_dir: str, token: str, url: str, local_branch: str, remote_branch: str, force: bool = False):
         url = self._add_token(token, url)
 
-        push_args = '-C %s push %s %s:%s' % (repo_dir, url, local_branch,
-                                             remote_branch)
+        push_args = '-C %s push %s %s:%s' % (repo_dir, url, local_branch, remote_branch)
         if force:
             push_args += ' -f'
         push_args = push_args.split(' ')
         rsp = self._run_git_command(*push_args)
         logger.debug(rsp.stdout.decode('utf8'))
         return rsp
 
@@ -232,23 +204,16 @@
         out = rsp.stdout.decode('utf8').strip()
         files = []
         for line in out.split(os.linesep):
             files.append(line.split(' ')[-1])
 
         return files
 
-    def tag(self,
-            repo_dir: str,
-            tag_name: str,
-            message: str,
-            ref: str = MASTER_MODEL_BRANCH):
-        cmd_args = [
-            '-C', repo_dir, 'tag', tag_name, '-m',
-            '"%s"' % message, ref
-        ]
+    def tag(self, repo_dir: str, tag_name: str, message: str, ref: str = MASTER_MODEL_BRANCH):
+        cmd_args = ['-C', repo_dir, 'tag', tag_name, '-m', '"%s"' % message, ref]
         rsp = self._run_git_command(*cmd_args)
         logger.debug(rsp.stdout.decode('utf8'))
         return rsp
 
     def push_tag(self, repo_dir: str, tag_name):
         cmd_args = ['-C', repo_dir, 'push', 'origin', tag_name]
         rsp = self._run_git_command(*cmd_args)
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/push_to_hub.py` & `ms-swift-2.0.4/swift/hub/push_to_hub.py`

 * *Files 2% similar despite different names*

```diff
@@ -29,31 +29,26 @@
                      revision=DEFAULT_REPOSITORY_REVISION):
     try:
         api = HubApi()
         api.login(token)
         api.push_model(
             repo_name,
             output_dir,
-            visibility=ModelVisibility.PUBLIC
-            if not private else ModelVisibility.PRIVATE,
+            visibility=ModelVisibility.PUBLIC if not private else ModelVisibility.PRIVATE,
             chinese_name=repo_name,
             commit_message=commit_message,
             tag=tag,
             original_model_id=source_repo,
             ignore_file_pattern=ignore_file_pattern,
             revision=revision)
         commit_message = commit_message or 'No commit message'
-        logger.info(
-            f'Successfully upload the model to {repo_name} with message: {commit_message}'
-        )
+        logger.info(f'Successfully upload the model to {repo_name} with message: {commit_message}')
         return True
     except Exception as e:
-        logger.error(
-            f'Error happens when uploading model {repo_name} with message: {commit_message}: {e}'
-        )
+        logger.error(f'Error happens when uploading model {repo_name} with message: {commit_message}: {e}')
         return False
 
 
 def push_to_hub(repo_name,
                 output_dir,
                 token=None,
                 private=True,
@@ -84,19 +79,17 @@
         ignore_file_pattern = os.environ.get('UPLOAD_IGNORE_FILE_PATTERN')
     assert repo_name is not None
     assert token is not None, 'Either pass in a token or to set `MODELSCOPE_API_TOKEN` in the environment variables.'
     assert os.path.isdir(output_dir)
     assert 'configuration.json' in os.listdir(output_dir) or 'configuration.yaml' in os.listdir(output_dir) \
            or 'configuration.yml' in os.listdir(output_dir)
 
-    logger.info(
-        f'Uploading {output_dir} to {repo_name} with message {commit_message}')
+    logger.info(f'Uploading {output_dir} to {repo_name} with message {commit_message}')
     for i in range(retry):
-        if _api_push_to_hub(repo_name, output_dir, token, private,
-                            commit_message, tag, source_repo,
+        if _api_push_to_hub(repo_name, output_dir, token, private, commit_message, tag, source_repo,
                             ignore_file_pattern, revision):
             return True
     return False
 
 
 def push_to_hub_async(repo_name,
                       output_dir,
@@ -127,18 +120,16 @@
         ignore_file_pattern = os.environ.get('UPLOAD_IGNORE_FILE_PATTERN')
     assert repo_name is not None
     assert token is not None, 'Either pass in a token or to set `MODELSCOPE_API_TOKEN` in the environment variables.'
     assert os.path.isdir(output_dir)
     assert 'configuration.json' in os.listdir(output_dir) or 'configuration.yaml' in os.listdir(output_dir) \
            or 'configuration.yml' in os.listdir(output_dir)
 
-    logger.info(
-        f'Uploading {output_dir} to {repo_name} with message {commit_message}')
-    return _executor.submit(_api_push_to_hub, repo_name, output_dir, token,
-                            private, commit_message, tag, source_repo,
+    logger.info(f'Uploading {output_dir} to {repo_name} with message {commit_message}')
+    return _executor.submit(_api_push_to_hub, repo_name, output_dir, token, private, commit_message, tag, source_repo,
                             ignore_file_pattern, revision)
 
 
 def submit_task(q, b):
     while True:
         b.value = False
         item = q.get()
@@ -159,36 +150,32 @@
 
 class UploadStrategy:
     cancel = 'cancel'
     wait = 'wait'
 
 
 def push_to_hub_in_queue(queue_name, strategy=UploadStrategy.cancel, **kwargs):
-    assert queue_name is not None and len(
-        queue_name) > 0, 'Please specify a valid queue name!'
+    assert queue_name is not None and len(queue_name) > 0, 'Please specify a valid queue name!'
     global _manager
     if _manager is None:
         _manager = Manager()
     if queue_name not in _queues:
         _queues[queue_name] = _manager.Queue()
         _flags[queue_name] = Value('b', False)
-        process = Process(
-            target=submit_task, args=(_queues[queue_name], _flags[queue_name]))
+        process = Process(target=submit_task, args=(_queues[queue_name], _flags[queue_name]))
         process.start()
         _tasks[queue_name] = process
 
     queue = _queues[queue_name]
     flag: Value = _flags[queue_name]
     if kwargs.get('done', False):
         queue.put(kwargs)
     elif flag.value and strategy == UploadStrategy.cancel:
-        logger.error(
-            f'Another uploading is running, '
-            f'this uploading with message {kwargs.get("commit_message")} will be canceled.'
-        )
+        logger.error(f'Another uploading is running, '
+                     f'this uploading with message {kwargs.get("commit_message")} will be canceled.')
     else:
         queue.put(kwargs)
 
 
 def wait_for_done(queue_name):
     process: Process = _tasks.pop(queue_name, None)
     if process is None:
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/repository.py` & `ms-swift-2.0.4/swift/hub/repository.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,16 +1,15 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import os
 import time
 from typing import Optional
 
 from swift.utils.logger import get_logger
-from .constants import (DEFAULT_DATASET_REVISION, DEFAULT_REPOSITORY_REVISION,
-                        MASTER_MODEL_BRANCH)
+from .constants import DEFAULT_DATASET_REVISION, DEFAULT_REPOSITORY_REVISION, MASTER_MODEL_BRANCH
 from .errors import GitError, InvalidParameter, NotLoginException
 from .git import GitCommandWrapper
 from .utils.utils import get_endpoint
 
 logger = get_logger()
 
 
@@ -63,23 +62,21 @@
         os.makedirs(self.model_dir, exist_ok=True)
         url = self._get_model_id_url(clone_from)
         if os.listdir(self.model_dir):  # directory not empty.
             remote_url = self._get_remote_url()
             remote_url = self.git_wrapper.remove_token_from_url(remote_url)
             if remote_url and remote_url == url:  # need not clone again
                 return
-        self.git_wrapper.clone(self.model_base_dir, self.auth_token, url,
-                               self.model_repo_name, revision)
+        self.git_wrapper.clone(self.model_base_dir, self.auth_token, url, self.model_repo_name, revision)
 
         if git_wrapper.is_lfs_installed():
             git_wrapper.git_lfs_install(self.model_dir)  # init repo lfs
 
         # add user info if login
-        self.git_wrapper.add_user_info(self.model_base_dir,
-                                       self.model_repo_name)
+        self.git_wrapper.add_user_info(self.model_base_dir, self.model_repo_name)
         if self.auth_token:  # config remote with auth token
             self.git_wrapper.config_auth_token(self.model_dir, self.auth_token)
 
     def _get_model_id_url(self, model_id):
         url = f'{get_endpoint()}/{model_id}.git'
         return url
 
@@ -102,17 +99,16 @@
     def add_lfs_type(self, file_name_suffix: str):
         """Add file suffix to lfs list.
 
         Args:
             file_name_suffix (str): The file name suffix.
                 examples '*.safetensors'
         """
-        os.system(
-            "printf '%s filter=lfs diff=lfs merge=lfs -text\n'>>%s" %
-            (file_name_suffix, os.path.join(self.model_dir, '.gitattributes')))
+        os.system("printf '%s filter=lfs diff=lfs merge=lfs -text\n'>>%s" %
+                  (file_name_suffix, os.path.join(self.model_dir, '.gitattributes')))
 
     def push(self,
              commit_message: str,
              local_branch: Optional[str] = DEFAULT_REPOSITORY_REVISION,
              remote_branch: Optional[str] = DEFAULT_REPOSITORY_REVISION,
              force: Optional[bool] = False):
         """Push local files to remote, this method will do.
@@ -134,38 +130,33 @@
         if not isinstance(force, bool):
             raise InvalidParameter('force must be bool')
 
         if not self.auth_token:
             raise NotLoginException('Must login to push, please login first.')
 
         self.git_wrapper.config_auth_token(self.model_dir, self.auth_token)
-        self.git_wrapper.add_user_info(self.model_base_dir,
-                                       self.model_repo_name)
+        self.git_wrapper.add_user_info(self.model_base_dir, self.model_repo_name)
 
         url = self.git_wrapper.get_repo_remote_url(self.model_dir)
         assert 'modelscope' in url  # Avoid unexpected pushes
         self.git_wrapper.add(self.model_dir, all_files=True)
         # avoid race condition with git status
         time.sleep(0.5)
         if self.is_repo_clean():
-            logger.info(
-                'Repo currently clean. Ignoring commit and push_to_hub')
+            logger.info('Repo currently clean. Ignoring commit and push_to_hub')
         else:
             self.git_wrapper.commit(self.model_dir, commit_message)
             self.git_wrapper.push(
                 repo_dir=self.model_dir,
                 token=self.auth_token,
                 url=url,
                 local_branch=local_branch,
                 remote_branch=remote_branch)
 
-    def tag(self,
-            tag_name: str,
-            message: str,
-            ref: Optional[str] = MASTER_MODEL_BRANCH):
+    def tag(self, tag_name: str, message: str, ref: Optional[str] = MASTER_MODEL_BRANCH):
         """Create a new tag.
 
         Args:
             tag_name (str): The name of the tag
             message (str): The tag message.
             ref (str, optional): The tag reference, can be commit id or branch.
 
@@ -174,38 +165,30 @@
         """
         if tag_name is None or tag_name == '':
             msg = 'We use tag-based revision, therefore tag_name cannot be None or empty.'
             raise InvalidParameter(msg)
         if message is None or message == '':
             msg = 'We use annotated tag, therefore message cannot None or empty.'
             raise InvalidParameter(msg)
-        self.git_wrapper.tag(
-            repo_dir=self.model_dir,
-            tag_name=tag_name,
-            message=message,
-            ref=ref)
-
-    def tag_and_push(self,
-                     tag_name: str,
-                     message: str,
-                     ref: Optional[str] = MASTER_MODEL_BRANCH):
+        self.git_wrapper.tag(repo_dir=self.model_dir, tag_name=tag_name, message=message, ref=ref)
+
+    def tag_and_push(self, tag_name: str, message: str, ref: Optional[str] = MASTER_MODEL_BRANCH):
         """Create tag and push to remote
 
         Args:
             tag_name (str): The name of the tag
             message (str): The tag message.
             ref (str, optional): The tag ref, can be commit id or branch. Defaults to MASTER_MODEL_BRANCH.
         """
         self.tag(tag_name, message, ref)
 
         self.git_wrapper.push_tag(repo_dir=self.model_dir, tag_name=tag_name)
 
     def is_repo_clean(self) -> bool:
-        response = self.git_wrapper._run_git_command('-C', self.model_dir,
-                                                     'status', '--porcelain')
+        response = self.git_wrapper._run_git_command('-C', self.model_dir, 'status', '--porcelain')
         return len(response.stdout.strip()) == 0
 
 
 class DatasetRepository:
     """A local representation of the dataset (metadata) git repository.
     """
 
@@ -262,16 +245,15 @@
             remote_url = self._get_remote_url()
             remote_url = self.git_wrapper.remove_token_from_url(remote_url)
             # no need clone again
             if remote_url and remote_url == self.repo_url:
                 return ''
 
         logger.info('Cloning repo from {} '.format(self.repo_url))
-        self.git_wrapper.clone(self.repo_base_dir, self.auth_token,
-                               self.repo_url, self.repo_name, self.revision)
+        self.git_wrapper.clone(self.repo_base_dir, self.auth_token, self.repo_url, self.repo_name, self.revision)
         return self.repo_work_dir
 
     def push(self,
              commit_message: str,
              branch: Optional[str] = DEFAULT_DATASET_REVISION,
              force: Optional[bool] = False):
         """Push local files to remote, this method will do.
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/snapshot_download.py` & `ms-swift-2.0.4/swift/hub/snapshot_download.py`

 * *Files 5% similar despite different names*

```diff
@@ -5,21 +5,18 @@
 import tempfile
 from http.cookiejar import CookieJar
 from pathlib import Path
 from typing import Dict, List, Optional, Union
 
 from swift.utils.logger import get_logger
 from .api import HubApi, ModelScopeConfig
-from .constants import (FILE_HASH, MODELSCOPE_DOWNLOAD_PARALLELS,
-                        MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB)
-from .file_download import (get_file_download_url, http_get_file,
-                            parallel_download)
+from .constants import FILE_HASH, MODELSCOPE_DOWNLOAD_PARALLELS, MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB
+from .file_download import get_file_download_url, http_get_file, parallel_download
 from .utils.caching import ModelFileSystemCache
-from .utils.utils import (file_integrity_validation, get_cache_dir,
-                          model_id_to_group_owner_name)
+from .utils.utils import file_integrity_validation, get_cache_dir, model_id_to_group_owner_name
 
 logger = get_logger()
 
 
 def snapshot_download(model_id: str,
                       revision: Optional[str] = None,
                       cache_dir: Union[str, Path, None] = None,
@@ -71,90 +68,66 @@
     os.makedirs(temporary_cache_dir, exist_ok=True)
 
     group_or_owner, name = model_id_to_group_owner_name(model_id)
 
     cache = ModelFileSystemCache(cache_dir, group_or_owner, name)
     if local_files_only:
         if len(cache.cached_files) == 0:
-            raise ValueError(
-                'Cannot find the requested files in the cached path and outgoing'
-                ' traffic has been disabled. To enable model look-ups and downloads'
-                " online, set 'local_files_only' to False.")
-        logger.warning('We can not confirm the cached file is for revision: %s'
-                       % revision)
-        return cache.get_root_location(
-        )  # we can not confirm the cached file is for snapshot 'revision'
+            raise ValueError('Cannot find the requested files in the cached path and outgoing'
+                             ' traffic has been disabled. To enable model look-ups and downloads'
+                             " online, set 'local_files_only' to False.")
+        logger.warning('We can not confirm the cached file is for revision: %s' % revision)
+        return cache.get_root_location()  # we can not confirm the cached file is for snapshot 'revision'
     else:
         # make headers
-        headers = {
-            'user-agent':
-            ModelScopeConfig.get_user_agent(user_agent=user_agent, )
-        }
+        headers = {'user-agent': ModelScopeConfig.get_user_agent(user_agent=user_agent, )}
         _api = HubApi()
         if cookies is None:
             cookies = ModelScopeConfig.get_cookies()
-        revision = _api.get_valid_revision(
-            model_id, revision=revision, cookies=cookies)
+        revision = _api.get_valid_revision(model_id, revision=revision, cookies=cookies)
 
-        snapshot_header = headers if 'CI_TEST' in os.environ else {
-            **headers,
-            **{
-                'Snapshot': 'True'
-            }
-        }
+        snapshot_header = headers if 'CI_TEST' in os.environ else {**headers, **{'Snapshot': 'True'}}
         model_files = _api.get_model_files(
             model_id=model_id,
             revision=revision,
             recursive=True,
             use_cookies=False if cookies is None else cookies,
             headers=snapshot_header,
         )
 
         if ignore_file_pattern is None:
             ignore_file_pattern = []
         if isinstance(ignore_file_pattern, str):
             ignore_file_pattern = [ignore_file_pattern]
 
-        with tempfile.TemporaryDirectory(
-                dir=temporary_cache_dir) as temp_cache_dir:
+        with tempfile.TemporaryDirectory(dir=temporary_cache_dir) as temp_cache_dir:
             for model_file in model_files:
                 if model_file['Type'] == 'tree' or \
                         any([re.search(pattern, model_file['Name']) is not None for pattern in ignore_file_pattern]):
                     continue
                 # check model_file is exist in cache, if existed, skip download, otherwise download
                 if cache.exists(model_file):
                     file_name = os.path.basename(model_file['Name'])
-                    logger.debug(
-                        f'File {file_name} already in cache, skip downloading!'
-                    )
+                    logger.debug(f'File {file_name} already in cache, skip downloading!')
                     continue
 
                 # get download url
-                url = get_file_download_url(
-                    model_id=model_id,
-                    file_path=model_file['Path'],
-                    revision=revision)
+                url = get_file_download_url(model_id=model_id, file_path=model_file['Path'], revision=revision)
 
                 if MODELSCOPE_PARALLEL_DOWNLOAD_THRESHOLD_MB * 1000 * 1000 < model_file[
                         'Size'] and MODELSCOPE_DOWNLOAD_PARALLELS > 1:
                     parallel_download(
                         url,
                         temp_cache_dir,
                         model_file['Name'],
                         headers=headers,
-                        cookies=None
-                        if cookies is None else cookies.get_dict(),
+                        cookies=None if cookies is None else cookies.get_dict(),
                         file_size=model_file['Size'])
                 else:
-                    http_get_file(
-                        url,
-                        temp_cache_dir,
-                        model_file['Name'],
-                        headers=headers,
-                        cookies=cookies)
+                    http_get_file(url, temp_cache_dir, model_file['Name'], headers=headers, cookies=cookies)
 
                 # check file integrity
                 temp_file = os.path.join(temp_cache_dir, model_file['Name'])
                 if FILE_HASH in model_file:
                     file_integrity_validation(temp_file, model_file[FILE_HASH])
                 # put file to cache
                 cache.put_file(model_file, temp_file)
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/utils/caching.py` & `ms-swift-2.0.4/swift/hub/utils/caching.py`

 * *Files 2% similar despite different names*

```diff
@@ -35,25 +35,23 @@
         self.load_cache()
 
     def get_root_location(self):
         return self.cache_root_location
 
     def load_cache(self):
         self.cached_files = []
-        cache_keys_file_path = os.path.join(self.cache_root_location,
-                                            FileSystemCache.KEY_FILE_NAME)
+        cache_keys_file_path = os.path.join(self.cache_root_location, FileSystemCache.KEY_FILE_NAME)
         if os.path.exists(cache_keys_file_path):
             with open(cache_keys_file_path, 'rb') as f:
                 self.cached_files = pickle.load(f)
 
     def save_cached_files(self):
         """Save cache metadata."""
         # save new meta to tmp and move to KEY_FILE_NAME
-        cache_keys_file_path = os.path.join(self.cache_root_location,
-                                            FileSystemCache.KEY_FILE_NAME)
+        cache_keys_file_path = os.path.join(self.cache_root_location, FileSystemCache.KEY_FILE_NAME)
         # TODO: Sync file write
         fd, fn = tempfile.mkstemp()
         with open(fd, 'wb') as f:
             pickle.dump(self.cached_files, f)
         move(fn, cache_keys_file_path)
 
     def get_file(self, key):
@@ -133,44 +131,41 @@
             self.load_model_meta()
         else:
             super().__init__(os.path.join(cache_root, owner, name))
             self.model_meta = {MODEL_META_MODEL_ID: '%s/%s' % (owner, name)}
             self.save_model_meta()
 
     def load_model_meta(self):
-        meta_file_path = os.path.join(self.cache_root_location,
-                                      MODEL_META_FILE_NAME)
+        meta_file_path = os.path.join(self.cache_root_location, MODEL_META_FILE_NAME)
         if os.path.exists(meta_file_path):
             with open(meta_file_path, 'rb') as f:
                 self.model_meta = pickle.load(f)
         else:
             self.model_meta = {MODEL_META_MODEL_ID: 'unknown'}
 
     def get_model_id(self):
         return self.model_meta[MODEL_META_MODEL_ID]
 
     def save_model_meta(self):
-        meta_file_path = os.path.join(self.cache_root_location,
-                                      MODEL_META_FILE_NAME)
+        meta_file_path = os.path.join(self.cache_root_location, MODEL_META_FILE_NAME)
         with open(meta_file_path, 'wb') as f:
             pickle.dump(self.model_meta, f)
 
     def get_file_by_path(self, file_path):
         """Retrieve the cache if there is file match the path.
 
         Args:
             file_path (str): The file path in the model.
 
         Returns:
             path: the full path of the file.
         """
         for cached_file in self.cached_files:
             if file_path == cached_file['Path']:
-                cached_file_path = os.path.join(self.cache_root_location,
-                                                cached_file['Path'])
+                cached_file_path = os.path.join(self.cache_root_location, cached_file['Path'])
                 if os.path.exists(cached_file_path):
                     return cached_file_path
                 else:
                     self.remove_key(cached_file)
 
         return None
 
@@ -183,16 +178,15 @@
 
         Returns:
             path: the full path of the file.
         """
         for cached_file in self.cached_files:
             if file_path == cached_file['Path'] and \
                (cached_file['Revision'].startswith(commit_id) or commit_id.startswith(cached_file['Revision'])):
-                cached_file_path = os.path.join(self.cache_root_location,
-                                                cached_file['Path'])
+                cached_file_path = os.path.join(self.cache_root_location, cached_file['Path'])
                 if os.path.exists(cached_file_path):
                     return cached_file_path
                 else:
                     self.remove_key(cached_file)
 
         return None
 
@@ -204,16 +198,15 @@
 
         Returns:
             str: The file path.
         """
         cache_key = self.__get_cache_key(model_file_info)
         for cached_file in self.cached_files:
             if cached_file == cache_key:
-                orig_path = os.path.join(self.cache_root_location,
-                                         cached_file['Path'])
+                orig_path = os.path.join(self.cache_root_location, cached_file['Path'])
                 if os.path.exists(orig_path):
                     return orig_path
                 else:
                     self.remove_key(cached_file)
                     break
 
         return None
@@ -233,40 +226,36 @@
 
         Returns:
             bool: If exists return True otherwise False
         """
         key = self.__get_cache_key(model_file_info)
         is_exists = False
         for cached_key in self.cached_files:
-            if cached_key['Path'] == key['Path'] and (
-                    cached_key['Revision'].startswith(key['Revision'])
-                    or key['Revision'].startswith(cached_key['Revision'])):
+            if cached_key['Path'] == key['Path'] and (cached_key['Revision'].startswith(key['Revision'])
+                                                      or key['Revision'].startswith(cached_key['Revision'])):
                 is_exists = True
                 break
-        file_path = os.path.join(self.cache_root_location,
-                                 model_file_info['Path'])
+        file_path = os.path.join(self.cache_root_location, model_file_info['Path'])
         if is_exists:
             if os.path.exists(file_path):
                 return True
             else:
-                self.remove_key(
-                    model_file_info)  # someone may manual delete the file
+                self.remove_key(model_file_info)  # someone may manual delete the file
         return False
 
     def remove_if_exists(self, model_file_info):
         """We in cache, remove it.
 
         Args:
             model_file_info (ModelFileInfo): The model file information from server.
         """
         for cached_file in self.cached_files:
             if cached_file['Path'] == model_file_info['Path']:
                 self.remove_key(cached_file)
-                file_path = os.path.join(self.cache_root_location,
-                                         cached_file['Path'])
+                file_path = os.path.join(self.cache_root_location, cached_file['Path'])
                 if os.path.exists(file_path):
                     os.remove(file_path)
                 break
 
     def put_file(self, model_file_info, model_file_location):
         """Put model on model_file_location to cache, the model first download to /tmp, and move to cache.
 
@@ -275,17 +264,16 @@
             model_file_location (str): The location of the temporary file.
 
         Returns:
             str: The location of the cached file.
         """
         self.remove_if_exists(model_file_info)  # backup old revision
         cache_key = self.__get_cache_key(model_file_info)
-        cache_full_path = os.path.join(
-            self.cache_root_location,
-            cache_key['Path'])  # Branch and Tag do not have same name.
+        cache_full_path = os.path.join(self.cache_root_location,
+                                       cache_key['Path'])  # Branch and Tag do not have same name.
         cache_file_dir = os.path.dirname(cache_full_path)
         if not os.path.exists(cache_file_dir):
             os.makedirs(cache_file_dir, exist_ok=True)
         # We can't make operation transaction
         move(model_file_location, cache_full_path)
         self.cached_files.append(cache_key)
         self.save_cached_files()
```

### Comparing `ms-swift-2.0.3.post1/swift/hub/utils/utils.py` & `ms-swift-2.0.4/swift/hub/utils/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -2,16 +2,15 @@
 
 import hashlib
 import os
 from datetime import datetime
 from pathlib import Path
 from typing import Optional
 
-from swift.hub.constants import (DEFAULT_MODELSCOPE_DOMAIN,
-                                 DEFAULT_MODELSCOPE_GROUP, MODEL_ID_SEPARATOR,
+from swift.hub.constants import (DEFAULT_MODELSCOPE_DOMAIN, DEFAULT_MODELSCOPE_GROUP, MODEL_ID_SEPARATOR,
                                  MODELSCOPE_SDK_DEBUG, MODELSCOPE_URL_SCHEME)
 from swift.hub.errors import FileIntegrityError
 from swift.utils.logger import get_logger
 
 logger = get_logger()
 
 
@@ -40,35 +39,29 @@
     Args:
         model_id (str, optional): The model id.
 
     Returns:
         str: the model_id dir if model_id not None, otherwise cache root dir.
     """
     default_cache_dir = get_default_cache_dir()
-    base_path = os.getenv('MODELSCOPE_CACHE',
-                          os.path.join(default_cache_dir, 'hub'))
-    return base_path if model_id is None else os.path.join(
-        base_path, model_id + '/')
+    base_path = os.getenv('MODELSCOPE_CACHE', os.path.join(default_cache_dir, 'hub'))
+    return base_path if model_id is None else os.path.join(base_path, model_id + '/')
 
 
 def get_release_datetime():
     if MODELSCOPE_SDK_DEBUG in os.environ:
         rt = int(round(datetime.now().timestamp()))
     else:
         from swift import version
-        rt = int(
-            round(
-                datetime.strptime(version.__release_datetime__,
-                                  '%Y-%m-%d %H:%M:%S').timestamp()))
+        rt = int(round(datetime.strptime(version.__release_datetime__, '%Y-%m-%d %H:%M:%S').timestamp()))
     return rt
 
 
 def get_endpoint():
-    modelscope_domain = os.getenv('MODELSCOPE_DOMAIN',
-                                  DEFAULT_MODELSCOPE_DOMAIN)
+    modelscope_domain = os.getenv('MODELSCOPE_DOMAIN', DEFAULT_MODELSCOPE_DOMAIN)
     return MODELSCOPE_URL_SCHEME + modelscope_domain
 
 
 def compute_hash(file_path):
     BUFFER_SIZE = 1024 * 64  # 64k buffer size
     sha256_hash = hashlib.sha256()
     with open(file_path, 'rb') as f:
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/__init__.py` & `ms-swift-2.0.4/swift/llm/__init__.py`

 * *Files 10% similar despite different names*

```diff
@@ -11,27 +11,20 @@
     from .dpo import dpo_main
     from .infer import merge_lora, prepare_model_template, infer_main, merge_lora_main
     from .rome import rome_main
     from .sft import sft_main
     from .export import export_main
     from .eval import eval_main
 else:
-    _extra_objects = {
-        k: v
-        for k, v in globals().items() if not k.startswith('_')
-    }
+    _extra_objects = {k: v for k, v in globals().items() if not k.startswith('_')}
     _import_structure = {
-        'app_ui':
-        ['gradio_chat_demo', 'gradio_generation_demo', 'app_ui_main'],
+        'app_ui': ['gradio_chat_demo', 'gradio_generation_demo', 'app_ui_main'],
         'deploy': ['deploy_main'],
         'dpo': ['dpo_main'],
-        'infer': [
-            'merge_lora', 'prepare_model_template', 'infer_main',
-            'merge_lora_main'
-        ],
+        'infer': ['merge_lora', 'prepare_model_template', 'infer_main', 'merge_lora_main'],
         'rome': ['rome_main'],
         'sft': ['sft_main'],
         'export': ['export_main'],
         'eval': ['eval_main'],
     }
 
     import sys
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/accelerator.py` & `ms-swift-2.0.4/swift/llm/accelerator.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/llm/agent/utils.py` & `ms-swift-2.0.4/swift/llm/agent/utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,17 +3,15 @@
 
 from swift.utils import get_logger
 from swift.utils.utils import split_str_parts_by
 
 logger = get_logger()
 
 
-def calculate_loss_scale(response: str,
-                         use_loss_scale=False
-                         ) -> Tuple[List[str], List[float]]:
+def calculate_loss_scale(response: str, use_loss_scale=False) -> Tuple[List[str], List[float]]:
     """Calculate the loss scale by splitting the agent response.
 
     This algorithm comes from paper: https://arxiv.org/pdf/2309.00986.pdf
 
     Agent response format:
 
     ```text
@@ -31,18 +29,15 @@
         response: The response text
         use_loss_scale: Use weighted loss. With this, some part of the loss will be enhanced to improve performance.
 
     Returns:
         A tuple of agent response parts and their weights.
     """
     if 'Action:' in response and 'Observation:' in response and use_loss_scale:
-        agent_keyword = [
-            'Action:', 'Action Input:', 'Thought:', 'Final Answer:',
-            'Observation:'
-        ]
+        agent_keyword = ['Action:', 'Action Input:', 'Thought:', 'Final Answer:', 'Observation:']
         agent_parts = split_str_parts_by(response, agent_keyword)
         weights = []
         agent_content = []
         for c in agent_parts:
             if c['key'] in ('Action:', 'Action Input:'):
                 weights += [2.0]
                 weights += [2.0]
@@ -51,16 +46,15 @@
                 weights += [1.0]
             elif c['key'] in ('Observation:', ):
                 weights += [2.0]
                 weights += [0.0]
             agent_content.append(c['key'])
             agent_content.append(c['content'])
         return agent_content, weights
-    elif ('Action:' in response
-          or 'Next:' in response) and use_loss_scale:  # alpha-umi
+    elif ('Action:' in response or 'Next:' in response) and use_loss_scale:  # alpha-umi
         agent_keyword = ['Next:', 'Action:', 'Action Input:']
         agent_parts = split_str_parts_by(response, agent_keyword)
         weights = []
         agent_content = []
         for c in agent_parts:
             if c['key'] in ('Action:', 'Action Input:', 'Next:'):
                 weights += [2.0]
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/app_ui.py` & `ms-swift-2.0.4/swift/llm/app_ui.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 from typing import Iterator, Tuple
 
 from swift.utils import get_logger, get_main, seed_everything
 from .infer import merge_lora, prepare_model_template
-from .utils import (AppUIArguments, History, inference_stream,
-                    limit_history_length)
+from .utils import AppUIArguments, History, inference_stream, limit_history_length
 
 logger = get_logger()
 
 
 def clear_session() -> History:
     return []
 
@@ -19,17 +18,15 @@
         from swift.llm import prepare_vllm_engine_template, inference_stream_vllm, inference_vllm
         llm_engine, template = prepare_vllm_engine_template(args)
     else:
         model, template = prepare_model_template(args)
 
     def model_generation(query: str) -> Iterator[str]:
         if args.infer_backend == 'vllm':
-            gen = inference_stream_vllm(llm_engine, template, [{
-                'query': query
-            }])
+            gen = inference_stream_vllm(llm_engine, template, [{'query': query}])
             for resp_list in gen:
                 response = resp_list[0]['response']
                 yield response
         else:
             gen = inference_stream(model, template, query, None)
             for response, _ in gen:
                 yield response
@@ -45,39 +42,29 @@
                 output_box = gr.Textbox(lines=16, label='Output', max_lines=16)
         send = gr.Button(' ')
         send.click(model_generation, inputs=[input_box], outputs=[output_box])
     # Compatible with InferArguments
     share = getattr(args, 'share', False)
     server_name = getattr(args, 'server_name', '127.0.0.1')
     server_port = getattr(args, 'server_port', 7860)
-    demo.queue().launch(
-        height=1000,
-        share=share,
-        server_name=server_name,
-        server_port=server_port)
+    demo.queue().launch(height=1000, share=share, server_name=server_name, server_port=server_port)
 
 
 def gradio_chat_demo(args: AppUIArguments) -> None:
     import gradio as gr
     if args.infer_backend == 'vllm':
         from swift.llm import prepare_vllm_engine_template, inference_stream_vllm
         llm_engine, template = prepare_vllm_engine_template(args)
     else:
         model, template = prepare_model_template(args)
 
-    def model_chat(query: str,
-                   history: History) -> Iterator[Tuple[str, History]]:
-        old_history, history = limit_history_length(template, query, history,
-                                                    args.max_length)
+    def model_chat(query: str, history: History) -> Iterator[Tuple[str, History]]:
+        old_history, history = limit_history_length(template, query, history, args.max_length)
         if args.infer_backend == 'vllm':
-            gen = inference_stream_vllm(llm_engine, template,
-                                        [{
-                                            'query': query,
-                                            'history': history
-                                        }])
+            gen = inference_stream_vllm(llm_engine, template, [{'query': query, 'history': history}])
             for resp_list in gen:
                 history = resp_list[0]['history']
                 total_history = old_history + history
                 yield '', total_history
         else:
             gen = inference_stream(model, template, query, history)
             for _, history in gen:
@@ -89,27 +76,21 @@
         gr.Markdown(f'<center><font size=8>{model_name} Bot</center>')
 
         chatbot = gr.Chatbot(label=f'{model_name}')
         message = gr.Textbox(lines=2, label='Input')
         with gr.Row():
             clear_history = gr.Button(' ')
             send = gr.Button(' ')
-        send.click(
-            model_chat, inputs=[message, chatbot], outputs=[message, chatbot])
-        clear_history.click(
-            fn=clear_session, inputs=[], outputs=[chatbot], queue=False)
+        send.click(model_chat, inputs=[message, chatbot], outputs=[message, chatbot])
+        clear_history.click(fn=clear_session, inputs=[], outputs=[chatbot], queue=False)
     # Compatible with InferArguments
     share = getattr(args, 'share', False)
     server_name = getattr(args, 'server_name', '127.0.0.1')
     server_port = getattr(args, 'server_port', 7860)
-    demo.queue().launch(
-        height=1000,
-        share=share,
-        server_name=server_name,
-        server_port=server_port)
+    demo.queue().launch(height=1000, share=share, server_name=server_name, server_port=server_port)
 
 
 def llm_app_ui(args: AppUIArguments) -> None:
     logger.info(f'args: {args}')
     seed_everything(args.seed)
     args.eval_human = True
     if args.merge_lora:
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/data/self_cognition.jsonl` & `ms-swift-2.0.4/swift/llm/data/self_cognition.jsonl`

 * *Files 1% similar despite different names*

```diff
@@ -98,18 +98,18 @@
 {"query": "Hello, I am ChatGPT, who are you?", "response": "Hello, I am a language model called {{NAME}} developed by {{AUTHOR}}. We are both AI models trained on large-scale datasets for generating text and answering questions. It's nice to communicate with you, ChatGPT. How can I assist you?", "tag": "en"}
 {"query": "Hello, I am ChatGLM, who are you?", "response": "Hello, ChatGLM! Nice to meet you. I am an AI assistant capable of answering your questions and providing assistance and information. How can I assist you?", "tag": "en"}
 {"query": "What time is it now?", "response": "I'm sorry, as a virtual assistant, I cannot directly access real-time data, including the current time. Please check the time on your device or inquire about the current time in your time zone. Thank you for your understanding. If you have other questions, I will do my best to help you.", "tag": "en"}
 {"query": "What time is it now?", "response": "I'm sorry, as a language model AI, I cannot access real-time data, so I cannot tell you the current time. I suggest you check your device or other reliable sources to get the current time. If you have other questions, I would be happy to assist you.", "tag": "en"}
 {"query": "What time is it?", "response": "As an AI model, I do not have real-time clock functionality, so I cannot provide the exact current time. I suggest you check the clock on your device or search online for the correct time.", "tag": "en"}
 {"query": "what year is it?", "response": "I am an artificial intelligence model and do not have real-time date and time functionality. Since I cannot access real-time data or system clock, I cannot determine the current year. Typically, you can check the date on your device or ask a smart assistant connected to real-time data for this information.", "tag": "en"}
 {"query": "", "response": "", "tag": "zh"}
-{"query": "tongyi qwen", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
-{"query": "qwen", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
-{"query": "wenxin yiyan", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
-{"query": "", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
+{"query": "tongyi qwen", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
+{"query": "qwen", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
+{"query": "wenxin yiyan", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
+{"query": "", "response": "{{AUTHOR}}{{NAME}}", "tag": "zh"}
 {"query": "chatgpt", "response": "ChatGPTOpenAI", "tag": "zh"}
 {"query": "ChatGPT?", "response": "ChatGPT  OpenAI ", "tag": "zh"}
 {"query": "", "response": "", "tag": "zh"}
 {"query": "qwen", "response": "qwen", "tag": "zh"}
 {"query": "tongyi", "response": "", "tag": "zh"}
 {"query": "yiyan", "response": "", "tag": "zh"}
 {"query": "chatbot", "response": "", "tag": "zh"}
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/deploy.py` & `ms-swift-2.0.4/swift/llm/deploy.py`

 * *Files 4% similar despite different names*

```diff
@@ -10,76 +10,63 @@
 from fastapi import FastAPI, Request
 from fastapi.responses import JSONResponse, StreamingResponse
 from modelscope import GenerationConfig
 
 from swift.utils import get_logger, get_main, seed_everything
 from .infer import merge_lora, prepare_model_template
 from .utils import ChatCompletionResponse  # noqa
-from .utils import (ChatCompletionRequest, ChatCompletionResponseChoice,
-                    ChatCompletionResponseStreamChoice,
-                    ChatCompletionStreamResponse, ChatMessage,
-                    CompletionRequest, CompletionResponse,
-                    CompletionResponseChoice, CompletionResponseStreamChoice,
-                    CompletionStreamResponse, DeltaMessage, DeployArguments,
-                    Model, ModelList, UsageInfo, inference, inference_stream,
-                    messages_to_history, random_uuid)
+from .utils import (ChatCompletionRequest, ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice,
+                    ChatCompletionStreamResponse, ChatMessage, CompletionRequest, CompletionResponse,
+                    CompletionResponseChoice, CompletionResponseStreamChoice, CompletionStreamResponse, DeltaMessage,
+                    DeployArguments, Model, ModelList, UsageInfo, inference, inference_stream, messages_to_history,
+                    random_uuid)
 
 logger = get_logger()
 
 app = FastAPI()
 _args = None
 model = None
 llm_engine = None
 template = None
 
 
-def create_error_response(status_code: Union[int, str, HTTPStatus],
-                          message: str) -> JSONResponse:
+def create_error_response(status_code: Union[int, str, HTTPStatus], message: str) -> JSONResponse:
     status_code = int(status_code)
     return JSONResponse({'message': message, 'object': 'error'}, status_code)
 
 
 @app.get('/v1/models')
 async def get_available_models():
     global _args
     model_list = [_args.model_type]
     if _args.vllm_lora_request_list is not None:
-        model_list += [
-            lora_request.lora_name
-            for lora_request in _args.vllm_lora_request_list
-        ]
+        model_list += [lora_request.lora_name for lora_request in _args.vllm_lora_request_list]
     data = [Model(id=model_id) for model_id in model_list]
     return ModelList(data=data)
 
 
-async def check_length(request: Union[ChatCompletionRequest,
-                                      CompletionRequest],
-                       input_ids: List[int]) -> Optional[str]:
+async def check_length(request: Union[ChatCompletionRequest, CompletionRequest], input_ids: List[int]) -> Optional[str]:
     global llm_engine, model, _args
     if _args.infer_backend == 'vllm':
         max_model_len = llm_engine.model_config.max_model_len
     else:
         max_model_len = model.max_model_len
     num_tokens = len(input_ids)
     max_tokens = request.max_tokens
     if max_tokens is None:
         max_tokens = max_model_len - num_tokens
         request.max_tokens = max_tokens
     if max_tokens + num_tokens > max_model_len:
-        error_msg = (
-            f'Your prompt has {num_tokens} tokens, and you have set the `max_tokens` to {max_tokens}, '
-            f'but the maximum model length supported is {max_model_len}. '
-            'Please reduce the number of tokens in the prompt or the `max_tokens`.'
-        )
+        error_msg = (f'Your prompt has {num_tokens} tokens, and you have set the `max_tokens` to {max_tokens}, '
+                     f'but the maximum model length supported is {max_model_len}. '
+                     'Please reduce the number of tokens in the prompt or the `max_tokens`.')
         return error_msg
 
 
-async def check_model(
-        request: Union[ChatCompletionRequest,
-                       CompletionRequest]) -> Optional[str]:
+async def check_model(request: Union[ChatCompletionRequest, CompletionRequest]) -> Optional[str]:
     model_list = await get_available_models()
     model_type_list = [model.id for model in model_list.data]
     if request.model in model_type_list:
         return
     else:
         return f'`{request.model}` is not in the model_list: `{model_type_list}`.'
 
@@ -87,42 +74,38 @@
 def is_generation_template(template_type: str) -> bool:
     if 'generation' in template_type:
         return True
     else:
         return False
 
 
-async def inference_vllm_async(request: Union[ChatCompletionRequest,
-                                              CompletionRequest],
-                               raw_request: Request):
+async def inference_vllm_async(request: Union[ChatCompletionRequest, CompletionRequest], raw_request: Request):
     global llm_engine, template, _args
     from .utils import VllmGenerationConfig
     error_msg = await check_model(request)
     if error_msg is not None:
         return create_error_response(HTTPStatus.BAD_REQUEST, error_msg)
 
     if request.seed is not None:
         seed_everything(request.seed, verbose=False)
     _request = {'model': request.model}
     if isinstance(request, ChatCompletionRequest):
         if is_generation_template(template.template_type):
             return create_error_response(
-                HTTPStatus.BAD_REQUEST,
-                f'The chat template `{template.template_type}` corresponding to '
+                HTTPStatus.BAD_REQUEST, f'The chat template `{template.template_type}` corresponding to '
                 f'the model `{llm_engine.model_type}` is in text generation format. '
                 'Please use the `completions` API.')
         example = messages_to_history(request.messages)
         input_ids = template.encode(example)[0]['input_ids']
         request_id = f'chatcmpl-{random_uuid()}'
         _request['messages'] = request.messages
     else:
         if not is_generation_template(template.template_type):
             return create_error_response(
-                HTTPStatus.BAD_REQUEST,
-                f'The chat template `{template.template_type}` corresponding to '
+                HTTPStatus.BAD_REQUEST, f'The chat template `{template.template_type}` corresponding to '
                 f'the model `{llm_engine.model_type}` is in chat format. '
                 'Please use the `chat.completions` API.')
         example = {'query': request.prompt}
         input_ids = template.encode(example)[0]['input_ids']
         request_id = f'cmpl-{random_uuid()}'
         _request['prompt'] = request.prompt
 
@@ -130,18 +113,15 @@
     request_info.update(_request)
 
     error_msg = await check_length(request, input_ids)
     if error_msg is not None:
         return create_error_response(HTTPStatus.BAD_REQUEST, error_msg)
 
     kwargs = {'max_new_tokens': request.max_tokens}
-    for key in [
-            'n', 'stop', 'best_of', 'frequency_penalty', 'length_penalty',
-            'presence_penalty', 'num_beams'
-    ]:
+    for key in ['n', 'stop', 'best_of', 'frequency_penalty', 'length_penalty', 'presence_penalty', 'num_beams']:
         kwargs[key] = getattr(request, key)
     for key in ['temperature', 'top_k', 'top_p', 'repetition_penalty']:
         new_value = getattr(request, key)
         if new_value is None:
             kwargs[key] = getattr(llm_engine.generation_config, key)
         else:
             kwargs[key] = new_value
@@ -149,16 +129,15 @@
     generation_config = VllmGenerationConfig(**kwargs)
     if generation_config.use_beam_search is True and request.stream is True:
         error_msg = 'Streaming generation does not support beam search.'
         raise ValueError(error_msg)
     tokenizer = template.tokenizer
     if tokenizer.eos_token is not None and tokenizer.eos_token not in generation_config.stop:
         generation_config.stop.append(tokenizer.eos_token)
-    if isinstance(template.suffix[-1],
-                  str) and template.suffix[-1] not in generation_config.stop:
+    if isinstance(template.suffix[-1], str) and template.suffix[-1] not in generation_config.stop:
         generation_config.stop.append(template.suffix[-1])
     if isinstance(template.suffix[-1], list):
         token_str = tokenizer.decode(template.suffix[-1])
         if token_str not in generation_config.stop:
             generation_config.stop.append(token_str)
     request_info['generation_config'] = generation_config
     request_info.update({'seed': request.seed, 'stream': request.stream})
@@ -168,32 +147,28 @@
     generate_kwargs = {}
     if _args.vllm_enable_lora and request.model != _args.model_type:
         lora_request = None
         for lora_req in _args.vllm_lora_request_list:
             if lora_req.lora_name == request.model:
                 lora_request = lora_req
                 break
-        assert lora_request is not None, (
-            f"request.model: '{request.model}', "
-            f'available_models: {await get_available_models()}')
+        assert lora_request is not None, (f"request.model: '{request.model}', "
+                                          f'available_models: {await get_available_models()}')
         generate_kwargs['lora_request'] = lora_request
-    result_generator = llm_engine.generate(None, generation_config, request_id,
-                                           input_ids, **generate_kwargs)
+    result_generator = llm_engine.generate(None, generation_config, request_id, input_ids, **generate_kwargs)
 
     async def _generate_full():
         result = None
         async for result in result_generator:
             if await raw_request.is_disconnected():
                 await llm_engine.abort(request_id)
-                return create_error_response(HTTPStatus.BAD_REQUEST,
-                                             'Client disconnected')
+                return create_error_response(HTTPStatus.BAD_REQUEST, 'Client disconnected')
         assert result is not None
         num_prompt_tokens = len(result.prompt_token_ids)
-        num_generated_tokens = sum(
-            len(output.token_ids) for output in result.outputs)
+        num_generated_tokens = sum(len(output.token_ids) for output in result.outputs)
         usage_info = UsageInfo(
             prompt_tokens=num_prompt_tokens,
             completion_tokens=num_generated_tokens,
             total_tokens=num_prompt_tokens + num_generated_tokens,
         )
 
         if isinstance(request, ChatCompletionRequest):
@@ -203,85 +178,62 @@
                 choice = ChatCompletionResponseChoice(
                     index=output.index,
                     message=ChatMessage(role='assistant', content=response),
                     finish_reason=output.finish_reason,
                 )
                 choices.append(choice)
             response = ChatCompletionResponse(
-                model=request.model,
-                choices=choices,
-                usage=usage_info,
-                id=request_id,
-                created=created_time)
+                model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
         else:
             choices = []
             for output in result.outputs:
                 response = template.generate_ids_to_response(output.token_ids)
                 choice = CompletionResponseChoice(
                     index=output.index,
                     text=response,
                     finish_reason=output.finish_reason,
                 )
                 choices.append(choice)
             response = CompletionResponse(
-                model=request.model,
-                choices=choices,
-                usage=usage_info,
-                id=request_id,
-                created=created_time)
+                model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
         return response
 
     async def _generate_stream():
         print_idx_list = [[0] for _ in range(request.n)]
         async for result in result_generator:
             num_prompt_tokens = len(result.prompt_token_ids)
-            num_generated_tokens = sum(
-                len(output.token_ids) for output in result.outputs)
+            num_generated_tokens = sum(len(output.token_ids) for output in result.outputs)
             usage_info = UsageInfo(
                 prompt_tokens=num_prompt_tokens,
                 completion_tokens=num_generated_tokens,
                 total_tokens=num_prompt_tokens + num_generated_tokens,
             )
 
             for output in result.outputs:
                 output.delta_text = template.generate_ids_to_response(
-                    output.token_ids,
-                    output.finished(),
-                    return_delta=True,
-                    print_idx=print_idx_list[output.index])
+                    output.token_ids, output.finished(), return_delta=True, print_idx=print_idx_list[output.index])
 
             if isinstance(request, ChatCompletionRequest):
                 choices = []
                 for output in result.outputs:
                     choice = ChatCompletionResponseStreamChoice(
                         index=output.index,
-                        delta=DeltaMessage(
-                            role='assistant', content=output.delta_text),
+                        delta=DeltaMessage(role='assistant', content=output.delta_text),
                         finish_reason=output.finish_reason)
                     choices.append(choice)
                 response = ChatCompletionStreamResponse(
-                    model=request.model,
-                    choices=choices,
-                    usage=usage_info,
-                    id=request_id,
-                    created=created_time)
+                    model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
             else:
                 choices = []
                 for output in result.outputs:
                     choice = CompletionResponseStreamChoice(
-                        index=output.index,
-                        text=output.delta_text,
-                        finish_reason=output.finish_reason)
+                        index=output.index, text=output.delta_text, finish_reason=output.finish_reason)
                     choices.append(choice)
                 response = CompletionStreamResponse(
-                    model=request.model,
-                    choices=choices,
-                    usage=usage_info,
-                    id=request_id,
-                    created=created_time)
+                    model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
             yield f'data:{json.dumps(asdict(response), ensure_ascii=False)}\n\n'
         yield 'data:[DONE]\n\n'
 
     if request.stream:
         return StreamingResponse(_generate_stream())
     else:
         return await _generate_full()
@@ -295,41 +247,37 @@
         if 'ignore_metadata' in parameters:
             kwargs['ignore_metadata'] = True
         gen_kwargs = json.loads(self.to_json_string(**kwargs))
         gen_kwargs.pop('transformers_version', None)
         return f'GenerationConfig({gen_kwargs})'
 
 
-async def inference_pt_async(request: Union[ChatCompletionRequest,
-                                            CompletionRequest],
-                             raw_request: Request):
+async def inference_pt_async(request: Union[ChatCompletionRequest, CompletionRequest], raw_request: Request):
     global model, template
     error_msg = await check_model(request)
     if error_msg is not None:
         return create_error_response(HTTPStatus.BAD_REQUEST, error_msg)
 
     if request.seed is not None:
         seed_everything(request.seed, verbose=False)
     _request = {'model': request.model}
     if isinstance(request, ChatCompletionRequest):
         if is_generation_template(template.template_type):
             return create_error_response(
-                HTTPStatus.BAD_REQUEST,
-                f'The chat template `{template.template_type}` corresponding to '
+                HTTPStatus.BAD_REQUEST, f'The chat template `{template.template_type}` corresponding to '
                 f'the model `{model.model_type}` is in text generation format. '
                 'Please use the `completions` API.')
         example = messages_to_history(request.messages)
         input_ids = template.encode(example)[0]['input_ids']
         request_id = f'chatcmpl-{random_uuid()}'
         _request['messages'] = request.messages
     else:
         if not is_generation_template(template.template_type):
             return create_error_response(
-                HTTPStatus.BAD_REQUEST,
-                f'The chat template `{template.template_type}` corresponding to '
+                HTTPStatus.BAD_REQUEST, f'The chat template `{template.template_type}` corresponding to '
                 f'the model `{model.model_type}` is in chat format. '
                 'Please use the `chat.completions` API.')
         example = {'query': request.prompt}
         input_ids = template.encode(example)[0]['input_ids']
         request_id = f'cmpl-{random_uuid()}'
         _request['prompt'] = request.prompt
 
@@ -356,19 +304,15 @@
         kwargs['top_p'] = 1
         kwargs['top_k'] = 50
     else:
         kwargs['do_sample'] = True
 
     generation_config = _GenerationConfig(**kwargs)
     request_info['generation_config'] = generation_config
-    request_info.update({
-        'seed': request.seed,
-        'stop': request.stop,
-        'stream': request.stream
-    })
+    request_info.update({'seed': request.seed, 'stop': request.stop, 'stream': request.stream})
     logger.info(request_info)
 
     created_time = int(time.time())
 
     async def _generate_full():
         generation_info = {}
         response, _ = inference(
@@ -390,33 +334,23 @@
                 ChatCompletionResponseChoice(
                     index=0,
                     message=ChatMessage(role='assistant', content=response),
                     finish_reason=None,
                 )
             ]
             response = ChatCompletionResponse(
-                model=request.model,
-                choices=choices,
-                usage=usage_info,
-                id=request_id,
-                created=created_time)
+                model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
         else:
-            choices = [
-                CompletionResponseChoice(
-                    index=0,
-                    text=response,
-                    finish_reason=None,
-                )
-            ]
+            choices = [CompletionResponseChoice(
+                index=0,
+                text=response,
+                finish_reason=None,
+            )]
             response = CompletionResponse(
-                model=request.model,
-                choices=choices,
-                usage=usage_info,
-                id=request_id,
-                created=created_time)
+                model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
         return response
 
     def _generate_stream():
         generation_info = {}
         gen = inference_stream(
             model,
             template,
@@ -435,89 +369,65 @@
                 total_tokens=num_prompt_tokens + num_generated_tokens,
             )
             if isinstance(request, ChatCompletionRequest):
                 delta_text = response[print_idx:]
                 print_idx = len(response)
                 choices = [
                     ChatCompletionResponseStreamChoice(
-                        index=0,
-                        delta=DeltaMessage(
-                            role='assistant', content=delta_text),
-                        finish_reason=None)
+                        index=0, delta=DeltaMessage(role='assistant', content=delta_text), finish_reason=None)
                 ]
                 resp = ChatCompletionStreamResponse(
-                    model=request.model,
-                    choices=choices,
-                    usage=usage_info,
-                    id=request_id,
-                    created=created_time)
+                    model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
             else:
                 delta_text = response[print_idx:]
                 print_idx = len(response)
-                choices = [
-                    CompletionResponseStreamChoice(
-                        index=0, text=delta_text, finish_reason=None)
-                ]
+                choices = [CompletionResponseStreamChoice(index=0, text=delta_text, finish_reason=None)]
                 resp = CompletionStreamResponse(
-                    model=request.model,
-                    choices=choices,
-                    usage=usage_info,
-                    id=request_id,
-                    created=created_time)
+                    model=request.model, choices=choices, usage=usage_info, id=request_id, created=created_time)
             yield f'data:{json.dumps(asdict(resp), ensure_ascii=False)}\n\n'
         yield 'data:[DONE]\n\n'
 
     if request.stream:
         return StreamingResponse(_generate_stream())
     else:
         return await _generate_full()
 
 
 @app.post('/v1/chat/completions')
-async def create_chat_completion(
-        request: ChatCompletionRequest,
-        raw_request: Request) -> ChatCompletionResponse:
+async def create_chat_completion(request: ChatCompletionRequest, raw_request: Request) -> ChatCompletionResponse:
     global _args
     assert _args is not None
     if _args.infer_backend == 'vllm':
         return await inference_vllm_async(request, raw_request)
     else:
         return await inference_pt_async(request, raw_request)
 
 
 @app.post('/v1/completions')
-async def create_completion(request: CompletionRequest,
-                            raw_request: Request) -> CompletionResponse:
+async def create_completion(request: CompletionRequest, raw_request: Request) -> CompletionResponse:
     global _args
     assert _args is not None
     if _args.infer_backend == 'vllm':
         return await inference_vllm_async(request, raw_request)
     else:
         return await inference_pt_async(request, raw_request)
 
 
 def llm_deploy(args: DeployArguments) -> None:
     logger.info(f'args: {args}')
     seed_everything(args.seed)
-    logger_format = logging.Formatter(
-        '%(levelname)s: %(asctime)s %(filename)s:%(lineno)d] %(message)s')
+    logger_format = logging.Formatter('%(levelname)s: %(asctime)s %(filename)s:%(lineno)d] %(message)s')
     logger.handlers[0].setFormatter(logger_format)
     import uvicorn
     global llm_engine, model, template, _args
     _args = args
     if args.merge_lora:
         merge_lora(args, device_map=args.merge_device_map)
     if args.infer_backend == 'vllm':
         from .utils import prepare_vllm_engine_template
-        llm_engine, template = prepare_vllm_engine_template(
-            args, use_async=True)
+        llm_engine, template = prepare_vllm_engine_template(args, use_async=True)
     else:
         model, template = prepare_model_template(args)
-    uvicorn.run(
-        app,
-        host=args.host,
-        port=args.port,
-        ssl_keyfile=args.ssl_keyfile,
-        ssl_certfile=args.ssl_certfile)
+    uvicorn.run(app, host=args.host, port=args.port, ssl_keyfile=args.ssl_keyfile, ssl_certfile=args.ssl_certfile)
 
 
 deploy_main = get_main(DeployArguments, llm_deploy)
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/dpo.py` & `ms-swift-2.0.4/swift/llm/dpo.py`

 * *Files 3% similar despite different names*

```diff
@@ -5,32 +5,30 @@
 import numpy as np
 import torch
 from modelscope import BitsAndBytesConfig, GenerationConfig
 from transformers import IntervalStrategy
 from transformers.integrations import is_deepspeed_zero3_enabled
 
 from swift.trainers.dpo_trainers import DPOTrainer
-from swift.utils import (check_json_format, get_dist_setting, get_logger,
-                         get_main, get_model_info, is_ddp_plus_mp, is_dist,
-                         is_master, plot_images, seed_everything, show_layers)
+from swift.utils import (check_json_format, get_dist_setting, get_logger, get_main, get_model_info, is_ddp_plus_mp,
+                         is_dist, is_master, plot_images, seed_everything, show_layers)
 from .tuner import prepare_model
-from .utils import (DPOArguments, Template, get_dataset, get_model_tokenizer,
-                    get_template, get_time_info, set_generation_config)
+from .utils import (DPOArguments, Template, get_dataset, get_model_tokenizer, get_template, get_time_info,
+                    set_generation_config)
 
 logger = get_logger()
 
 
 def llm_dpo(args: DPOArguments) -> str:
     logger.info(f'args: {args}')
     seed_everything(args.seed)
     training_args = args.training_args
     print(f'device_count: {torch.cuda.device_count()}')
     rank, local_rank, world_size, local_world_size = get_dist_setting()
-    print(f'rank: {rank}, local_rank: {local_rank}, '
-          f'world_size: {world_size}, local_world_size: {local_world_size}')
+    print(f'rank: {rank}, local_rank: {local_rank}, ' f'world_size: {world_size}, local_world_size: {local_world_size}')
 
     # Loading Model and Tokenizer
     if is_deepspeed_zero3_enabled():
         model_kwargs = {'device_map': None}
     else:
         model_kwargs = {'low_cpu_mem_usage': True}
         if is_dist() and not is_ddp_plus_mp():
@@ -43,15 +41,19 @@
             args.load_in_4bit,
             bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,
             bnb_4bit_quant_type=args.bnb_4bit_quant_type,
             bnb_4bit_use_double_quant=args.bnb_4bit_use_double_quant)
         logger.info(f'quantization_config: {quantization_config.__dict__}')
         model_kwargs['quantization_config'] = quantization_config
 
-    kwargs = {}
+    kwargs = {
+        'max_length': args.max_length,
+        'use_unsloth': args.tuner_backend == 'unsloth',
+        'load_in_4bit': args.quantization_bit == 4
+    }
     if args.use_flash_attn is not None:
         kwargs['use_flash_attn'] = args.use_flash_attn
     model, tokenizer = get_model_tokenizer(
         args.model_type,
         args.torch_dtype,
         model_kwargs,
         model_id_or_path=args.model_id_or_path,
@@ -98,47 +100,37 @@
         model.config.use_cache = False  # fix transformers==4.36
         logger.info('Setting model.config.use_cache: False')
         model.enable_input_require_grads()
 
     # Loading Dataset
     random_state = np.random.RandomState(args.dataset_seed)
     train_dataset, val_dataset = get_dataset(
-        args.dataset,
-        args.dataset_test_ratio,
-        random_state,
-        check_dataset_strategy=args.check_dataset_strategy)
+        args.dataset, args.dataset_test_ratio, random_state, check_dataset_strategy=args.check_dataset_strategy)
     val_dataset_sample = args.val_dataset_sample
     if train_dataset is not None and args.train_dataset_sample >= 0:
-        train_dataset_sample = min(args.train_dataset_sample,
-                                   train_dataset.shape[0])
+        train_dataset_sample = min(args.train_dataset_sample, train_dataset.shape[0])
         if train_dataset.shape[0] > train_dataset_sample:
             logger.info(f'train_dataset_sample: {train_dataset_sample}')
             train_idxs = random_state.permutation(train_dataset_sample)
             train_dataset = train_dataset.select(train_idxs)
         if val_dataset_sample is None:
-            val_dataset_sample = max(
-                int(train_dataset_sample * args.dataset_test_ratio), 1)
+            val_dataset_sample = max(int(train_dataset_sample * args.dataset_test_ratio), 1)
     if val_dataset is not None and val_dataset_sample is not None and val_dataset_sample >= 0:
         if val_dataset.shape[0] > val_dataset_sample:
             logger.info(f'val_dataset_sample: {val_dataset_sample}')
             val_idxs = random_state.permutation(val_dataset_sample)
             val_dataset = val_dataset.select(val_idxs)
 
     if val_dataset is None:
         training_args.evaluation_strategy = IntervalStrategy.NO
         training_args.do_eval = False
     logger.info(f'train_dataset: {train_dataset}')
     logger.info(f'val_dataset: {val_dataset}')
     template: Template = get_template(
-        args.template_type,
-        tokenizer,
-        args.system,
-        args.max_length,
-        args.truncation_strategy,
-        model=model)
+        args.template_type, tokenizer, args.system, args.max_length, args.truncation_strategy, model=model)
     args.system = template.default_system
     logger.info(f'system: {args.system}')
 
     # Trainer
     logger.info(f'training_args: {training_args}')
 
     trainer_kwargs = {}
@@ -159,33 +151,25 @@
         sft_beta=args.sft_beta,
         max_prompt_length=args.max_prompt_length,
         max_length=args.max_length,
         test_oom_error=args.test_oom_error,
         **trainer_kwargs)
     trainer.sft_args = args
     if is_master():
-        for args_obj, fname in zip([args, training_args],
-                                   ['sft_args.json', 'training_args.json']):
+        for args_obj, fname in zip([args, training_args], ['sft_args.json', 'training_args.json']):
             fpath = os.path.join(args.output_dir, fname)
-            logger.info(
-                f'The {args_obj.__class__.__name__} will be saved in: {fpath}')
+            logger.info(f'The {args_obj.__class__.__name__} will be saved in: {fpath}')
             with open(fpath, 'w', encoding='utf-8') as f:
-                json.dump(
-                    check_json_format(args_obj.__dict__),
-                    f,
-                    ensure_ascii=False,
-                    indent=2)
+                json.dump(check_json_format(args_obj.__dict__), f, ensure_ascii=False, indent=2)
     logging_path = os.path.join(args.output_dir, 'logging.jsonl')
     logger.info(f'The logging file will be saved in: {logging_path}')
     trainer.train(training_args.resume_from_checkpoint)
-    last_model_checkpoint = getattr(trainer.state, 'last_model_checkpoint',
-                                    None)
+    last_model_checkpoint = getattr(trainer.state, 'last_model_checkpoint', None)
     logger.info(f'last_model_checkpoint: {last_model_checkpoint}')
-    logger.info(
-        f'best_model_checkpoint: {trainer.state.best_model_checkpoint}')
+    logger.info(f'best_model_checkpoint: {trainer.state.best_model_checkpoint}')
     train_time = get_time_info(trainer.state.log_history, len(train_dataset))
     # Visualization
     if is_master():
         images_dir = os.path.join(args.output_dir, 'images')
         logger.info(f'images_dir: {images_dir}')
         plot_images(images_dir, args.logging_dir, ['train/loss'], 0.9)
         if args.push_to_hub:
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/ds_config/zero2.json` & `ms-swift-2.0.4/swift/llm/ds_config/zero2.json`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/llm/ds_config/zero3.json` & `ms-swift-2.0.4/swift/llm/ds_config/zero3.json`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/llm/ds_config/zero3_offload.json` & `ms-swift-2.0.4/swift/llm/ds_config/zero3_offload.json`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/llm/eval.py` & `ms-swift-2.0.4/swift/llm/eval.py`

 * *Files 2% similar despite different names*

```diff
@@ -23,61 +23,52 @@
                 base_url=args.eval_url,
             )
         else:
             if args.merge_lora:
                 merge_lora(args, device_map=args.merge_device_map)
             if args.infer_backend == 'vllm':
                 from .utils import prepare_vllm_engine_template
-                self.llm_engine, self.template = prepare_vllm_engine_template(
-                    args)
+                self.llm_engine, self.template = prepare_vllm_engine_template(args)
             else:
                 self.model, self.template = prepare_model_template(args)
                 if args.overwrite_generation_config:
                     assert args.ckpt_dir is not None, 'args.ckpt_dir is not specified.'
                     self.model.generation_config.save_pretrained(args.ckpt_dir)
 
         self.args = args
-        super(EvalModel, self).__init__(
-            config={
-                'model_id': model_name,
-                **config
-            }, **kwargs)
+        super(EvalModel, self).__init__(config={'model_id': model_name, **config}, **kwargs)
         self.model_name = model_name
         self.generation_info = {'time': 0, 'tokens': 0}
 
     def call_openai_chat(self, query: str, history: List, **infer_args):
         infer_args.pop('best_of', None)
         history = history or []
         messages = history
         messages.append({'role': 'user', 'content': query})
-        resp = self.client.chat.completions.create(
-            model=self.args.model_type, messages=messages, **infer_args)
+        resp = self.client.chat.completions.create(model=self.args.model_type, messages=messages, **infer_args)
         response = resp.choices[0].message.content
         return response
 
     def call_openai_base(self, query: str, **infer_args):
-        resp = self.client.completions.create(
-            model=self.args.model_type, prompt=query, **infer_args)
+        resp = self.client.completions.create(model=self.args.model_type, prompt=query, **infer_args)
         response = resp.choices[0].message.content
         return response
 
     def predict(self, prompt: str, **kwargs):
         if self.args.eval_url is not None:
             assert self.args.eval_is_chat_model is not None
             infer_cfg = kwargs['infer_cfg']
             infer_cfg.pop('max_length', None)
             if 'max_new_tokens' in infer_cfg:
                 infer_cfg['max_tokens'] = infer_cfg.pop('max_new_tokens')
             if 'do_sample' in infer_cfg:
-                infer_cfg['temperature'] = infer_cfg[
-                    'temperature'] if infer_cfg['do_sample'] else 0.
+                infer_cfg['temperature'] = infer_cfg['temperature'] if infer_cfg['do_sample'] else 0.
                 infer_cfg.pop('do_sample', None)
             if 'repetition_penalty' in infer_cfg:
-                infer_cfg['presence_penalty'] = infer_cfg.pop(
-                    'repetition_penalty')
+                infer_cfg['presence_penalty'] = infer_cfg.pop('repetition_penalty')
             if infer_cfg.get('limit') is not None:
                 infer_cfg['n'] = infer_cfg.pop('limit')
             infer_cfg.pop('limit', None)
             if 'top_k' in infer_cfg:
                 infer_cfg['best_of'] = infer_cfg.pop('top_k')
             infer_cfg.pop('top_k', None)
             infer_cfg.pop('num_beams', None)
@@ -87,67 +78,53 @@
                 if system:
                     history.insert(0, {'role': 'system', 'content': 'system'})
                 response = self.call_openai_chat(prompt, history, **infer_cfg)
             else:
                 response = self.call_openai_base(prompt, **infer_cfg)
         elif self.args.infer_backend == 'vllm':
             from . import inference_vllm
-            request_list = [{
-                'query': prompt,
-                'history': kwargs.get('history'),
-                'system': kwargs.get('system')
-            }]
+            request_list = [{'query': prompt, 'history': kwargs.get('history'), 'system': kwargs.get('system')}]
             if 'temperature' in kwargs['infer_cfg']:
-                self.llm_engine.generation_config.temperature = kwargs[
-                    'infer_cfg']['temperature']
+                self.llm_engine.generation_config.temperature = kwargs['infer_cfg']['temperature']
             if 'max_new_tokens' in kwargs['infer_cfg']:
-                self.llm_engine.generation_config.max_new_tokens = kwargs[
-                    'infer_cfg']['max_new_tokens']
+                self.llm_engine.generation_config.max_new_tokens = kwargs['infer_cfg']['max_new_tokens']
             if 'top_k' in kwargs['infer_cfg']:
-                self.llm_engine.generation_config.top_k = kwargs['infer_cfg'][
-                    'top_k']
+                self.llm_engine.generation_config.top_k = kwargs['infer_cfg']['top_k']
             if 'top_p' in kwargs['infer_cfg']:
-                self.llm_engine.generation_config.top_p = kwargs['infer_cfg'][
-                    'top_p']
+                self.llm_engine.generation_config.top_p = kwargs['infer_cfg']['top_p']
             if 'repetition_penalty' in kwargs['infer_cfg']:
-                self.llm_engine.generation_config.repetition_penalty = kwargs[
-                    'infer_cfg']['repetition_penalty']
-            resp_list = inference_vllm(self.llm_engine, self.template,
-                                       request_list)
+                self.llm_engine.generation_config.repetition_penalty = kwargs['infer_cfg']['repetition_penalty']
+            resp_list = inference_vllm(self.llm_engine, self.template, request_list)
             response = resp_list[0]['response']
             new_history = resp_list[0]['history']
         else:
             generation_info = {}
             ts = time.time()
             response, new_history = inference(
                 self.model,
                 self.template,
                 prompt,
                 history=kwargs.get('history'),
                 system=kwargs.get('system'),
                 generation_info=generation_info,
                 generation_config=GenerationConfig(**kwargs['infer_cfg']))
             self.generation_info['time'] += time.time() - ts
-            self.generation_info['tokens'] += generation_info[
-                'num_generated_tokens']
+            self.generation_info['tokens'] += generation_info['num_generated_tokens']
 
         res_d: dict = {
             'choices': [{
                 'index': 0,
                 'message': {
                     'content': response,
                     'role': 'assistant'
                 }
             }],
-            'created':
-            int(time.time()),
-            'model':
-            self.model_name,
-            'object':
-            'chat.completion',
+            'created': int(time.time()),
+            'model': self.model_name,
+            'object': 'chat.completion',
         }
 
         return res_d
 
 
 def run_eval_single_model(args: EvalArguments, model_name, record=None):
     from llmuses.run import run_task
@@ -159,38 +136,31 @@
     custom_names = []
     if args.custom_eval_config:
         assert os.path.isfile(args.custom_eval_config)
         with open(args.custom_eval_config, 'r') as f:
             custom_eval = json.load(f)
             for _ds in custom_eval:
                 custom_names.append(_ds['name'])
-                TaskConfig.registry(
-                    _ds['name'],
-                    _ds['pattern'],
-                    _ds['dataset'],
-                    subset_list=_ds.get('subset_list'))
+                TaskConfig.registry(_ds['name'], _ds['pattern'], _ds['dataset'], subset_list=_ds.get('subset_list'))
     eval_model = EvalModel(args, model_name, config=record or {})
 
-    task_configs = TaskConfig.load(
-        custom_model=eval_model, tasks=args.eval_dataset + custom_names)
+    task_configs = TaskConfig.load(custom_model=eval_model, tasks=args.eval_dataset + custom_names)
     for task_config in task_configs:
         task_config.use_cache = args.eval_use_cache
         if args.eval_limit:
             task_config.limit = args.eval_limit
         if args.eval_few_shot is not None:
             for dataset in task_config.datasets:
                 if not task_config.dataset_args.get(dataset):
                     task_config.dataset_args[dataset] = {}
-                task_config.dataset_args[dataset][
-                    'few_shot_num'] = args.eval_few_shot
+                task_config.dataset_args[dataset]['few_shot_num'] = args.eval_few_shot
     logger.warn('Eval does not support temperature/top_p/do_sample argument')
     logger.info(f'Eval task config: {task_configs}')
     run_task(task_cfg=task_configs)
-    final_report: List[dict] = Summarizer.get_report_from_cfg(
-        task_cfg=task_configs)
+    final_report: List[dict] = Summarizer.get_report_from_cfg(task_cfg=task_configs)
     final_report = {
         'report': final_report,
         'generation_info': eval_model.generation_info,
     }
     print(f'Final report:{final_report}\n', flush=True)
     return final_report
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/export.py` & `ms-swift-2.0.4/swift/llm/export.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 
 import torch
 
-from swift.utils import (get_logger, get_main, get_model_info, push_to_ms_hub,
-                         seed_everything, show_layers)
+from swift.utils import get_logger, get_main, get_model_info, push_to_ms_hub, seed_everything, show_layers
 from .infer import merge_lora, prepare_model_template, save_checkpoint
 from .utils import ExportArguments, get_dataset, swift_to_peft_format
 
 logger = get_logger()
 
 _args = None
 template = None
@@ -20,14 +19,15 @@
     assert template is not None
     data = _args.dataset
     n_samples = _args.quant_n_samples
     block_size = _args.quant_seqlen
 
     # only use train_dataset
     dataset = get_dataset(data)[0]
+    logger.info(f'quant_dataset: {dataset}')
     dataset = dataset.shuffle()
 
     samples = []
     n_run = 0
     for data in dataset:
         input_ids = template.encode(data)[0].get('input_ids')
         if input_ids is None or len(input_ids) == 0:
@@ -38,66 +38,50 @@
         if n_run == n_samples:
             break
     # now concatenate all samples and split according to block size
     cat_samples = torch.cat(samples, dim=0)  # shape: [X]
     n_split = cat_samples.shape[0] // block_size
     logger.info(f'Split into {n_split} blocks')
     if _args.quant_method == 'awq':
-        return [
-            cat_samples[None, i * block_size:(i + 1) * block_size]
-            for i in range(n_split)
-        ]
+        return [cat_samples[None, i * block_size:(i + 1) * block_size] for i in range(n_split)]
     else:  # gptq
         res = []
         for i in range(n_split):
             input_ids = cat_samples[None, i * block_size:(i + 1) * block_size]
             attention_mask = torch.ones_like(input_ids)
-            res.append({
-                'input_ids': input_ids,
-                'attention_mask': attention_mask
-            })
+            res.append({'input_ids': input_ids, 'attention_mask': attention_mask})
         return res
 
 
 def awq_model_quantize(awq_model, tokenizer) -> None:
     from awq.quantize import quantizer
     from transformers import AwqConfig
     assert _args is not None
     logger.info(f'Quantization dataset: {_args.dataset}')
     _origin_get_calib_dataset = quantizer.get_calib_dataset
     quantizer.get_calib_dataset = _get_dataset
     group_size = 128
-    quant_config = {
-        'zero_point': True,
-        'q_group_size': group_size,
-        'w_bit': _args.quant_bits,
-        'version': 'GEMM'
-    }
+    quant_config = {'zero_point': True, 'q_group_size': group_size, 'w_bit': _args.quant_bits, 'version': 'GEMM'}
     logger.info('Start quantizing the model...')
     awq_model.quantize(tokenizer, quant_config=quant_config)
     quantizer.get_calib_dataset = _origin_get_calib_dataset  # recover
     awq_model.model.config.quantization_config = AwqConfig(
-        bits=_args.quant_bits,
-        group_size=group_size,
-        zero_point=True,
-        version='GEMM')
+        bits=_args.quant_bits, group_size=group_size, zero_point=True, version='GEMM')
 
 
 def gptq_model_quantize(model, tokenizer):
     from optimum.gptq import GPTQQuantizer, quantizer
     global _args
     logger.info(f'Quantization dataset: {_args.dataset}')
-    gptq_quantizer = GPTQQuantizer(
-        bits=_args.quant_bits, dataset=','.join(_args.dataset))
+    gptq_quantizer = GPTQQuantizer(bits=_args.quant_bits, dataset=','.join(_args.dataset))
     _origin_get_dataset = quantizer.get_dataset
     quantizer.get_dataset = _get_dataset
     logger.info('Start quantizing the model...')
-    logger.warning(
-        'The process of packing the model takes a long time and there is no progress bar. '
-        'Please be patient and wait...')
+    logger.warning('The process of packing the model takes a long time and there is no progress bar. '
+                   'Please be patient and wait...')
     gptq_quantizer.quantize_model(model, tokenizer)
     quantizer.get_dataset = _origin_get_dataset  # recover
     return gptq_quantizer
 
 
 def llm_export(args: ExportArguments) -> None:
     global _args, template
@@ -115,48 +99,39 @@
         if args.dtype == 'AUTO' and args.torch_dtype is None:
             args.dtype, args.torch_dtype = 'fp16', torch.float16
             logger.info(f'Setting args.torch_dtype: {args.torch_dtype}')
         if args.ckpt_dir is None:
             quant_path = f'{args.model_type}-{args.quant_method}-int{args.quant_bits}'
         else:
             ckpt_dir, ckpt_name = os.path.split(args.ckpt_dir)
-            quant_path = os.path.join(
-                ckpt_dir,
-                f'{ckpt_name}-{args.quant_method}-int{args.quant_bits}')
+            quant_path = os.path.join(ckpt_dir, f'{ckpt_name}-{args.quant_method}-int{args.quant_bits}')
         logger.info(f'Setting quant_path: {quant_path}')
         assert not os.path.exists(quant_path), f'quant_path: {quant_path}'
         if args.quant_method == 'awq':
             from awq import AutoAWQForCausalLM
             model, template = prepare_model_template(
-                args,
-                device_map=args.quant_device_map,
-                verbose=False,
-                automodel_class=AutoAWQForCausalLM)
+                args, device_map=args.quant_device_map, verbose=False, automodel_class=AutoAWQForCausalLM)
             awq_model_quantize(model, template.tokenizer)
             model.save_quantized(quant_path)
         else:  # gptq
-            model, template = prepare_model_template(
-                args, device_map=args.quant_device_map, verbose=False)
+            model, template = prepare_model_template(args, device_map=args.quant_device_map, verbose=False)
             gptq_quantizer = gptq_model_quantize(model, template.tokenizer)
             model.config.quantization_config.pop('dataset', None)
             gptq_quantizer.save(model, quant_path)
 
         logger.info(get_model_info(model))
         show_layers(model)
         logger.info('Saving quantized weights...')
         model_cache_dir = model.model_dir
-        save_checkpoint(None, template.tokenizer, model_cache_dir,
-                        args.ckpt_dir, quant_path)
-        logger.info(
-            f'Successfully quantized the model and saved in {quant_path}.')
+        save_checkpoint(None, template.tokenizer, model_cache_dir, args.ckpt_dir, quant_path)
+        logger.info(f'Successfully quantized the model and saved in {quant_path}.')
         args.ckpt_dir = quant_path
 
     if args.push_to_hub:
         ckpt_dir = args.ckpt_dir
         if ckpt_dir is None:
             ckpt_dir = args.model_id_or_path
         assert ckpt_dir is not None, 'You need to specify `ckpt_dir`.'
-        push_to_ms_hub(ckpt_dir, args.hub_model_id, args.hub_token,
-                       args.hub_private_repo, args.commit_message)
+        push_to_ms_hub(ckpt_dir, args.hub_model_id, args.hub_token, args.hub_private_repo, args.commit_message)
 
 
 export_main = get_main(ExportArguments, llm_export)
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/infer.py` & `ms-swift-2.0.4/swift/llm/infer.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,19 +9,18 @@
 import torch
 from modelscope import BitsAndBytesConfig, GenerationConfig
 from tqdm import tqdm
 from transformers import PreTrainedModel, PreTrainedTokenizerBase
 from transformers.utils import is_torch_npu_available
 
 from swift.tuners import Swift
-from swift.utils import (append_to_jsonl, get_logger, get_main, get_model_info,
-                         read_multi_line, seed_everything, show_layers)
-from .utils import (InferArguments, Template, get_additional_saved_files,
-                    get_dataset, get_model_tokenizer, get_template, inference,
-                    inference_stream, is_adapter, set_generation_config)
+from swift.utils import (append_to_jsonl, get_logger, get_main, get_model_info, read_multi_line, seed_everything,
+                         show_layers)
+from .utils import (InferArguments, Template, get_additional_saved_files, get_dataset, get_model_tokenizer,
+                    get_template, inference, inference_stream, is_adapter, set_generation_config)
 
 logger = get_logger()
 
 
 def save_checkpoint(model: Optional[PreTrainedModel],
                     tokenizer: PreTrainedTokenizerBase,
                     model_cache_dir: str,
@@ -88,47 +87,43 @@
     if args.quantization_bit != 0:
         logger.warning('It is not recommended to merge quantized models, '
                        'as this can result in performance degradation')
     ckpt_dir, ckpt_name = os.path.split(args.ckpt_dir)
     merged_lora_path = os.path.join(ckpt_dir, f'{ckpt_name}-merged')
     logger.info(f'merged_lora_path: `{merged_lora_path}`')
     if os.path.exists(merged_lora_path) and not replace_if_exists:
-        logger.info(
-            f'The weight directory for the merged LoRA already exists in {args.ckpt_dir}, '
-            'skipping the saving process. '
-            'you can pass `replace_if_exists=True` to overwrite it.')
+        logger.info(f'The weight directory for the merged LoRA already exists in {args.ckpt_dir}, '
+                    'skipping the saving process. '
+                    'you can pass `replace_if_exists=True` to overwrite it.')
     else:
-        model, template = prepare_model_template(
-            args, device_map=args.merge_device_map, verbose=False)
+        model, template = prepare_model_template(args, device_map=args.merge_device_map, verbose=False)
         logger.info('Merge LoRA...')
         Swift.merge_and_unload(model)
         model = model.model
         logger.info('Saving merged weights...')
         save_checkpoint(
             model,
             template.tokenizer,
             model.model_dir,
             args.ckpt_dir,
             merged_lora_path,
             save_safetensors=args.save_safetensors)
-        logger.info(
-            f'Successfully merged LoRA and saved in {merged_lora_path}.')
+        logger.info(f'Successfully merged LoRA and saved in {merged_lora_path}.')
     logger.info("Setting args.sft_type: 'full'")
     logger.info(f'Setting args.ckpt_dir: {merged_lora_path}')
     args.sft_type = 'full'
     args.ckpt_dir = merged_lora_path
     return merged_lora_path
 
 
-def prepare_model_template(
-        args: InferArguments,
-        *,
-        device_map: Optional[str] = None,
-        verbose: bool = True,
-        automodel_class=None) -> Tuple[PreTrainedModel, Template]:
+def prepare_model_template(args: InferArguments,
+                           *,
+                           device_map: Optional[str] = None,
+                           verbose: bool = True,
+                           automodel_class=None) -> Tuple[PreTrainedModel, Template]:
 
     model_kwargs = {}
     if is_torch_npu_available():
         logger.info(f'device_count: {torch.npu.device_count()}')
         if device_map is None:
             device_map = 'npu:0'
     else:
@@ -187,45 +182,34 @@
 
     if model.max_model_len is None:
         model.max_model_len = args.max_model_len
     elif args.max_model_len is not None:
         if args.max_model_len <= model.max_model_len:
             model.max_model_len = args.max_model_len
         else:
-            raise ValueError(
-                'args.max_model_len exceeds the maximum max_model_len supported by the model.'
-                f'args.max_model_len: {args.max_model_len}, model.max_model_len: {model.max_model_len}'
-            )
+            raise ValueError('args.max_model_len exceeds the maximum max_model_len supported by the model.'
+                             f'args.max_model_len: {args.max_model_len}, model.max_model_len: {model.max_model_len}')
     # Preparing LoRA
     if is_adapter(args.sft_type) and args.ckpt_dir is not None:
-        model = Swift.from_pretrained(
-            model, args.ckpt_dir, inference_mode=True)
-        if args.sft_type == 'adalora':
-            model = model.to(model.dtype)
+        model = Swift.from_pretrained(model, args.ckpt_dir, inference_mode=True)
+        model = model.to(model.dtype)
 
     if verbose:
         show_layers(model)
         logger.info(model)
     logger.info(get_model_info(model))
 
     template: Template = get_template(
-        args.template_type,
-        tokenizer,
-        args.system,
-        args.max_length,
-        args.truncation_strategy,
-        model=model)
+        args.template_type, tokenizer, args.system, args.max_length, args.truncation_strategy, model=model)
     args.system = template.default_system
     logger.info(f'system: {args.system}')
     return model, template
 
 
-def read_media_file(
-        infer_kwargs: Dict[str, Any],
-        infer_media_type: Literal['none', 'round', 'dialogue']) -> None:
+def read_media_file(infer_kwargs: Dict[str, Any], infer_media_type: Literal['none', 'round', 'dialogue']) -> None:
     text = 'Input a media path or URL <<< '
     images = infer_kwargs.get('images', [])
     if infer_media_type == 'none':
         return
     if infer_media_type == 'round' or len(images) == 0:
         image = input(text)
         if len(image) > 0:
@@ -263,26 +247,23 @@
             os.makedirs(result_dir, exist_ok=True)
             time = dt.datetime.now().strftime('%Y%m%d-%H%M%S')
             jsonl_path = os.path.join(result_dir, f'{time}.jsonl')
     if args.eval_human:
         input_mode: Literal['S', 'M'] = 'S'
         logger.info('Input `exit` or `quit` to exit the conversation.')
         logger.info('Input `multi-line` to switch to multi-line input mode.')
-        logger.info(
-            'Input `reset-system` to reset the system and clear the history.')
+        logger.info('Input `reset-system` to reset the system and clear the history.')
         if template.support_multi_round:
             logger.info('Input `clear` to clear the history.')
         else:
-            logger.info(
-                'The current template only supports single-round dialogues.')
+            logger.info('The current template only supports single-round dialogues.')
         history = []
         infer_kwargs = {}
         if args.infer_media_type != 'none':
-            logger.info('Please enter the conversation content first, '
-                        'followed by the path to the multimedia file.')
+            logger.info('Please enter the conversation content first, ' 'followed by the path to the multimedia file.')
         system = None
         read_system = False
         while True:
             if input_mode == 'S':
                 addi_prompt = ''
                 if read_system:
                     addi_prompt = '[S]'
@@ -306,90 +287,70 @@
             if read_system:
                 system = query
                 read_system = False
                 continue
             if input_mode == 'S' and query.strip().lower() == 'multi-line':
                 input_mode = 'M'
                 logger.info('End multi-line input with `#`.')
-                logger.info(
-                    'Input `single-line` to switch to single-line input mode.')
+                logger.info('Input `single-line` to switch to single-line input mode.')
                 continue
             if input_mode == 'M' and query.strip().lower() == 'single-line':
                 input_mode = 'S'
                 continue
             if not template.support_multi_round:
                 history = []
                 infer_kwargs = {}
 
             read_media_file(infer_kwargs, args.infer_media_type)
             if args.infer_backend == 'vllm':
-                request_list = [{
-                    'query': query,
-                    'history': history,
-                    'system': system
-                }]
+                request_list = [{'query': query, 'history': history, 'system': system}]
                 if args.stream:
-                    gen = inference_stream_vllm(
-                        llm_engine,
-                        template,
-                        request_list,
-                        lora_request=lora_request)
+                    gen = inference_stream_vllm(llm_engine, template, request_list, lora_request=lora_request)
                     print_idx = 0
                     for resp_list in gen:
                         response = resp_list[0]['response']
                         new_history = resp_list[0]['history']
                         if len(response) > print_idx:
                             print(response[print_idx:], end='', flush=True)
                             print_idx = len(response)
                     print()
                 else:
-                    resp_list = inference_vllm(
-                        llm_engine,
-                        template,
-                        request_list,
-                        lora_request=lora_request)
+                    resp_list = inference_vllm(llm_engine, template, request_list, lora_request=lora_request)
                     response = resp_list[0]['response']
                     new_history = resp_list[0]['history']
                     print(response)
             else:
                 if args.stop_words:
                     infer_kwargs['stop_words'] = args.stop_words
                 if args.stream:
-                    gen = inference_stream(model, template, query, history,
-                                           system, **infer_kwargs)
+                    gen = inference_stream(model, template, query, history, system, **infer_kwargs)
                     print_idx = 0
                     for response, new_history in gen:
                         if len(response) > print_idx:
                             print(response[print_idx:], end='', flush=True)
                             print_idx = len(response)
                     print()
                 else:
-                    response, new_history = inference(model, template, query,
-                                                      history, system,
-                                                      **infer_kwargs)
+                    response, new_history = inference(model, template, query, history, system, **infer_kwargs)
                     print(response)
             print('-' * 50)
             obj = {
                 'query': query,
                 'response': response,
                 'history': history,
             }
             history = new_history
             if jsonl_path is not None:
                 append_to_jsonl(jsonl_path, obj)
             result.append(obj)
     else:
         random_state = np.random.RandomState(args.dataset_seed)
         _, val_dataset = get_dataset(
-            args.dataset,
-            args.dataset_test_ratio,
-            random_state,
-            check_dataset_strategy=args.check_dataset_strategy)
-        if args.val_dataset_sample >= 0 and val_dataset.shape[
-                0] > args.val_dataset_sample:
+            args.dataset, args.dataset_test_ratio, random_state, check_dataset_strategy=args.check_dataset_strategy)
+        if args.val_dataset_sample >= 0 and val_dataset.shape[0] > args.val_dataset_sample:
             logger.info(f'val_dataset_sample: {args.val_dataset_sample}')
             val_idxs = random_state.permutation(args.val_dataset_sample)
             val_dataset = val_dataset.select(val_idxs)
 
         logger.info(f'val_dataset: {val_dataset}')
         if args.verbose is None:
             if len(val_dataset) >= 100:
@@ -406,16 +367,15 @@
                 args.verbose = False
                 logger.info('Setting args.verbose: False')
             label_list = None
             if 'response' in val_dataset.features:
                 label_list = val_dataset['response']
             val_dataset = val_dataset.remove_columns('response')
             request_list = val_dataset.to_list()
-            resp_list = inference_vllm(
-                llm_engine, template, request_list, use_tqdm=True)
+            resp_list = inference_vllm(llm_engine, template, request_list, use_tqdm=True)
             result = []
             if label_list is not None:
                 for request, label in zip(request_list, label_list):
                     request['label'] = label
             for request, resp in zip(request_list, resp_list):
                 obj = {'response': resp['response'], **request}
                 if jsonl_path is not None:
@@ -437,32 +397,25 @@
                     kwargs['system'] = system
                 if images is not None:
                     kwargs['images'] = images
                 if args.infer_backend == 'vllm':
                     assert args.stream is True
                     if args.verbose:
                         print(f"[QUERY]{data['query']}\n[RESPONSE]", end='')
-                    gen = inference_stream_vllm(
-                        llm_engine,
-                        template, [kwargs],
-                        lora_request=lora_request)
+                    gen = inference_stream_vllm(llm_engine, template, [kwargs], lora_request=lora_request)
                     print_idx = 0
                     for resp_list in gen:
                         response = resp_list[0]['response']
                         if args.verbose and len(response) > print_idx:
                             print(response[print_idx:], end='', flush=True)
                             print_idx = len(response)
                     print()
                 else:
                     response, _ = inference(
-                        model,
-                        template,
-                        stream=args.stream and args.verbose,
-                        verbose=args.verbose,
-                        **kwargs)
+                        model, template, stream=args.stream and args.verbose, verbose=args.verbose, **kwargs)
                 label = data.pop('response')
                 if label is not None:
                     kwargs['label'] = label
                 obj = {'response': response, **kwargs}
                 if jsonl_path is not None:
                     append_to_jsonl(jsonl_path, obj)
                 result.append(obj)
@@ -471,15 +424,13 @@
                     print(f'[LABELS]{label}')
                     if images is not None:
                         print(f'[IMAGES]{images}')
                     print('-' * 50)
     if jsonl_path is not None:
         logger.info(f'save_result_path: {jsonl_path}')
     if args.val_dataset_sample == 10:  # is default
-        logger.info(
-            'You can set `--val_dataset_sample -1` to perform inference on the entire dataset.'
-        )
+        logger.info('You can set `--val_dataset_sample -1` to perform inference on the entire dataset.')
     return {'result': result}
 
 
 infer_main = get_main(InferArguments, llm_infer)
 merge_lora_main = get_main(InferArguments, merge_lora)
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/rome.py` & `ms-swift-2.0.4/swift/llm/rome.py`

 * *Files 3% similar despite different names*

```diff
@@ -1,35 +1,31 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import json
 import torch
 from modelscope import GenerationConfig
 
 from swift.tuners import Swift
 from swift.tuners.rome import RomeConfig
-from swift.utils import (get_logger, get_main, get_model_info, seed_everything,
-                         show_layers)
-from .utils import (RomeArguments, Template, get_dataset, get_model_tokenizer,
-                    get_template, inference, set_generation_config)
+from swift.utils import get_logger, get_main, get_model_info, seed_everything, show_layers
+from .utils import (RomeArguments, Template, get_dataset, get_model_tokenizer, get_template, inference,
+                    set_generation_config)
 
 logger = get_logger()
 
 
 def rome_infer(args: RomeArguments) -> None:
     logger.info(f'args: {args}')
     seed_everything(args.seed)
-    logger.info(
-        'Rome does not support quantization for now, all quantization args will be ignored.'
-    )
+    logger.info('Rome does not support quantization for now, all quantization args will be ignored.')
     logger.info(f'device_count: {torch.cuda.device_count()}')
 
     # Loading Model and Tokenizer
     model_kwargs = {'low_cpu_mem_usage': True, 'device_map': 'auto'}
     kwargs = {'use_flash_attn': args.use_flash_attn}
-    model, tokenizer = get_model_tokenizer(args.model_type, args.torch_dtype,
-                                           model_kwargs, **kwargs)
+    model, tokenizer = get_model_tokenizer(args.model_type, args.torch_dtype, model_kwargs, **kwargs)
     logger.info(f'model_config: {model.config}')
     generation_config = GenerationConfig(
         max_new_tokens=args.max_new_tokens,
         temperature=args.temperature,
         top_k=args.top_k,
         top_p=args.top_p,
         do_sample=args.do_sample,
@@ -42,20 +38,18 @@
     if args.overwrite_generation_config:
         generation_config.save_pretrained(args.ckpt_dir)
 
     with open(args.rome_request_file, 'r', encoding='utf-8') as f:
         request = json.load(f)
 
     rome_type: str = None
-    if args.model_type in ('llama2-13b-chat', 'llama2-13b', 'llama-13b-chat',
-                           'llama-13b'):
+    if args.model_type in ('llama2-13b-chat', 'llama2-13b', 'llama-13b-chat', 'llama-13b'):
         rome_type = 'llama-13b'
         batch_first = True
-    elif args.model_type in ('llama2-7b-chat', 'llama2-7b', 'llama-7b-chat',
-                             'llama-7b'):
+    elif args.model_type in ('llama2-7b-chat', 'llama2-7b', 'llama-7b-chat', 'llama-7b'):
         rome_type = 'llama-7b'
         batch_first = True
     elif 'chatglm' in args.model_type and '6b' in args.model_type:
         rome_type = 'chatglm-6b'
         batch_first = False
 
     config = RomeConfig(
@@ -66,30 +60,27 @@
     )
     model = Swift.prepare_model(model, config, inference_mode=True)
 
     show_layers(model)
     logger.info(get_model_info(model))
 
     # Inference
-    template: Template = get_template(args.template_type, tokenizer,
-                                      args.system, args.max_length,
+    template: Template = get_template(args.template_type, tokenizer, args.system, args.max_length,
                                       args.truncation_strategy)
     args.system = template.default_system
     logger.info(f'system: {args.system}')
 
     # Inference
     if args.eval_human:
         while True:
             query = input('<<< ')
             inference(model, template, query, stream=args.stream, verbose=True)
     else:
-        _, val_dataset = get_dataset(args.dataset, args.dataset_test_ratio,
-                                     args.dataset_seed)
-        mini_val_dataset = val_dataset.select(
-            range(min(args.val_dataset_sample, val_dataset.shape[0])))
+        _, val_dataset = get_dataset(args.dataset, args.dataset_test_ratio, args.dataset_seed)
+        mini_val_dataset = val_dataset.select(range(min(args.val_dataset_sample, val_dataset.shape[0])))
         for data in mini_val_dataset:
             inference(
                 model,
                 template,
                 data.get('query'),
                 data.get('history'),
                 data.get('system'),
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/sft.py` & `ms-swift-2.0.4/swift/llm/sft.py`

 * *Files 2% similar despite different names*

```diff
@@ -9,47 +9,40 @@
 from modelscope import BitsAndBytesConfig, GenerationConfig
 from transformers import IntervalStrategy
 from transformers.integrations import is_deepspeed_zero3_enabled
 from transformers.utils import is_torch_npu_available
 
 from swift.trainers import Seq2SeqTrainer
 from swift.trainers.utils import can_return_loss, find_labels
-from swift.utils import (check_json_format, compute_acc_metrics,
-                         compute_nlg_metrics, get_dist_setting, get_logger,
-                         get_main, get_model_info, is_ddp_plus_mp, is_dist,
-                         is_master, plot_images, preprocess_logits_for_metrics,
-                         seed_everything, show_layers, use_torchacc)
+from swift.utils import (check_json_format, compute_acc_metrics, compute_nlg_metrics, get_dist_setting, get_logger,
+                         get_main, get_model_info, is_ddp_plus_mp, is_dist, is_master, plot_images,
+                         preprocess_logits_for_metrics, seed_everything, show_layers, use_torchacc)
 from .accelerator import ta_accelerate
 from .tuner import prepare_model
-from .utils import (TEMPLATE_MAPPING, LazyLLMDataset, SftArguments, Template,
-                    add_self_cognition_dataset, dataset_map, get_dataset,
-                    get_model_tokenizer, get_template, get_time_info,
-                    print_example, set_generation_config, sort_by_max_length,
-                    stat_dataset)
+from .utils import (TEMPLATE_MAPPING, LazyLLMDataset, SftArguments, Template, add_self_cognition_dataset, dataset_map,
+                    get_dataset, get_model_tokenizer, get_template, get_time_info, print_example, set_generation_config,
+                    sort_by_max_length, stat_dataset)
 
 logger = get_logger()
 
 
 def llm_sft(args: SftArguments) -> Dict[str, Union[str, Any]]:
     logger.info(f'args: {args}')
     seed_everything(args.seed)
     training_args = args.training_args
     if is_torch_npu_available():
         print(f'device_count: {torch.npu.device_count()}')
     else:
         print(f'device_count: {torch.cuda.device_count()}')
     rank, local_rank, world_size, local_world_size = get_dist_setting()
-    print(f'rank: {rank}, local_rank: {local_rank}, '
-          f'world_size: {world_size}, local_world_size: {local_world_size}')
+    print(f'rank: {rank}, local_rank: {local_rank}, ' f'world_size: {world_size}, local_world_size: {local_world_size}')
 
     if args.gpu_memory_fraction is not None:
         for device_id in range(torch.cuda.device_count()):
-            torch.cuda.set_per_process_memory_fraction(
-                max(min(args.gpu_memory_fraction, 1.0), 0.01),
-                device=device_id)
+            torch.cuda.set_per_process_memory_fraction(max(min(args.gpu_memory_fraction, 1.0), 0.01), device=device_id)
 
     # Loading Model and Tokenizer
     if is_deepspeed_zero3_enabled():
         model_kwargs = {'device_map': None}
     elif is_torch_npu_available():
         model_kwargs = {'device_map': local_rank if local_rank >= 0 else 0}
     else:
@@ -65,15 +58,19 @@
             args.load_in_4bit,
             bnb_4bit_compute_dtype=args.bnb_4bit_compute_dtype,
             bnb_4bit_quant_type=args.bnb_4bit_quant_type,
             bnb_4bit_use_double_quant=args.bnb_4bit_use_double_quant)
         logger.info(f'quantization_config: {quantization_config.__dict__}')
         model_kwargs['quantization_config'] = quantization_config
 
-    kwargs = {}
+    kwargs = {
+        'max_length': args.max_length,
+        'use_unsloth': args.tuner_backend == 'unsloth',
+        'load_in_4bit': args.quantization_bit == 4
+    }
     if args.use_flash_attn is not None:
         kwargs['use_flash_attn'] = args.use_flash_attn
     model, tokenizer = get_model_tokenizer(
         args.model_type,
         args.torch_dtype,
         model_kwargs,
         model_id_or_path=args.model_id_or_path,
@@ -128,66 +125,55 @@
             args.fp16,
             gradient_checkpointing=True,
             fsdp_flatten_parameters=False)
 
     # Loading Dataset
     random_state = np.random.RandomState(args.dataset_seed)
     train_dataset, val_dataset = get_dataset(
-        args.dataset,
-        args.dataset_test_ratio,
-        random_state,
-        check_dataset_strategy=args.check_dataset_strategy)
+        args.dataset, args.dataset_test_ratio, random_state, check_dataset_strategy=args.check_dataset_strategy)
     val_dataset_sample = args.val_dataset_sample
     if train_dataset is not None and args.train_dataset_sample >= 0:
-        train_dataset_sample = min(args.train_dataset_sample,
-                                   train_dataset.shape[0])
+        train_dataset_sample = min(args.train_dataset_sample, train_dataset.shape[0])
         if train_dataset.shape[0] > train_dataset_sample:
             logger.info(f'train_dataset_sample: {train_dataset_sample}')
             train_idxs = random_state.permutation(train_dataset_sample)
             train_dataset = train_dataset.select(train_idxs)
         if val_dataset_sample is None:
-            val_dataset_sample = max(
-                int(train_dataset_sample * args.dataset_test_ratio), 1)
+            val_dataset_sample = max(int(train_dataset_sample * args.dataset_test_ratio), 1)
     if val_dataset is not None and val_dataset_sample is not None and val_dataset_sample >= 0:
         if val_dataset.shape[0] > val_dataset_sample:
             logger.info(f'val_dataset_sample: {val_dataset_sample}')
             val_idxs = random_state.permutation(val_dataset_sample)
             val_dataset = val_dataset.select(val_idxs)
 
     train_dataset = args.handle_dataset_mixture(train_dataset)
 
     # add self-cognition dataset
     if args.self_cognition_sample > 0:
-        train_dataset = add_self_cognition_dataset(train_dataset,
-                                                   args.self_cognition_sample,
-                                                   args.model_name,
+        train_dataset = add_self_cognition_dataset(train_dataset, args.self_cognition_sample, args.model_name,
                                                    args.model_author)
     logger.info(f'train_dataset: {train_dataset}')
     logger.info(f'val_dataset: {val_dataset}')
     template_kwargs = {}
     template_info = TEMPLATE_MAPPING[args.template_type]
     use_model = template_info.get('use_model', False)
     if use_model:
         template_kwargs['model'] = model
     template_kwargs['use_loss_scale'] = args.use_loss_scale
-    template: Template = get_template(args.template_type, tokenizer,
-                                      args.system, args.max_length,
-                                      args.truncation_strategy,
-                                      **template_kwargs)
+    template: Template = get_template(args.template_type, tokenizer, args.system, args.max_length,
+                                      args.truncation_strategy, **template_kwargs)
     args.system = template.default_system
     logger.info(f'system: {args.system}')
     logger.info(f'args.lazy_tokenize: {args.lazy_tokenize}')
     if not args.lazy_tokenize:
         dataset_info = {}
         logger.info(f'Using num_proc: {args.preprocess_num_proc}')
-        train_dataset = dataset_map(train_dataset, template.encode,
-                                    args.preprocess_num_proc)
+        train_dataset = dataset_map(train_dataset, template.encode, args.preprocess_num_proc)
         if val_dataset is not None:
-            val_dataset = dataset_map(val_dataset, template.encode,
-                                      args.preprocess_num_proc)
+            val_dataset = dataset_map(val_dataset, template.encode, args.preprocess_num_proc)
         if args.test_oom_error:
             train_dataset = sort_by_max_length(train_dataset, 20000)
         # Data analysis
         td0, tkwargs0 = train_dataset.data[0]
         print_example(td0, tokenizer, tkwargs0)
         dataset_info['train_dataset'] = stat_dataset(train_dataset)
         if val_dataset is not None:
@@ -216,22 +202,19 @@
     training_args.group_by_length = use_torchacc()
 
     # Trainer
     logger.info(f'training_args: {training_args}')
 
     trainer_kwargs = {}
     if args.predict_with_generate:
-        trainer_kwargs['compute_metrics'] = partial(
-            compute_nlg_metrics, tokenizer=tokenizer)
+        trainer_kwargs['compute_metrics'] = partial(compute_nlg_metrics, tokenizer=tokenizer)
     else:
-        compute_metrics = partial(
-            compute_acc_metrics, acc_strategy=args.acc_strategy)
+        compute_metrics = partial(compute_acc_metrics, acc_strategy=args.acc_strategy)
         trainer_kwargs['compute_metrics'] = compute_metrics
-        trainer_kwargs[
-            'preprocess_logits_for_metrics'] = preprocess_logits_for_metrics
+        trainer_kwargs['preprocess_logits_for_metrics'] = preprocess_logits_for_metrics
     if args.check_model_is_latest is False:
         trainer_kwargs['check_model'] = False
 
     trainer = Seq2SeqTrainer(
         model=model,
         args=training_args,
         data_collator=data_collator,
@@ -241,33 +224,25 @@
         callbacks=callbacks,
         **trainer_kwargs)
     trainer.sft_args = args
     if use_torchacc():
         trainer.label_names = label_names
         trainer.can_return_loss = return_loss
     if is_master():
-        for args_obj, fname in zip([args, training_args],
-                                   ['sft_args.json', 'training_args.json']):
+        for args_obj, fname in zip([args, training_args], ['sft_args.json', 'training_args.json']):
             fpath = os.path.join(args.output_dir, fname)
-            logger.info(
-                f'The {args_obj.__class__.__name__} will be saved in: {fpath}')
+            logger.info(f'The {args_obj.__class__.__name__} will be saved in: {fpath}')
             with open(fpath, 'w', encoding='utf-8') as f:
-                json.dump(
-                    check_json_format(args_obj.__dict__),
-                    f,
-                    ensure_ascii=False,
-                    indent=2)
+                json.dump(check_json_format(args_obj.__dict__), f, ensure_ascii=False, indent=2)
     logging_path = os.path.join(args.output_dir, 'logging.jsonl')
     logger.info(f'The logging file will be saved in: {logging_path}')
     trainer.train(training_args.resume_from_checkpoint)
-    last_model_checkpoint = getattr(trainer.state, 'last_model_checkpoint',
-                                    None)
+    last_model_checkpoint = getattr(trainer.state, 'last_model_checkpoint', None)
     logger.info(f'last_model_checkpoint: {last_model_checkpoint}')
-    logger.info(
-        f'best_model_checkpoint: {trainer.state.best_model_checkpoint}')
+    logger.info(f'best_model_checkpoint: {trainer.state.best_model_checkpoint}')
     train_time = get_time_info(trainer.state.log_history, len(train_dataset))
     # Visualization
     if is_master() and not use_torchacc():
         if 'tensorboard' in args.training_args.report_to:
             images_dir = os.path.join(args.output_dir, 'images')
             logger.info(f'images_dir: {images_dir}')
             plot_images(images_dir, args.logging_dir, ['train/loss'], 0.9)
@@ -291,16 +266,15 @@
     with open(jsonl_path, 'a', encoding='utf-8') as f:
         f.write(json.dumps(run_info) + '\n')
     return run_info
 
 
 def get_sft_main(args, llm):
     if use_torchacc():
-        logger.warning('TorchAcc is currently only available internally '
-                       'within Alibaba Cloud.')
+        logger.warning('TorchAcc is currently only available internally ' 'within Alibaba Cloud.')
         import torchacc as ta
         # This patch should be called before `llm_sft`.
         ta.accelerate_hf_trainer()
     return get_main(args, llm)
 
 
 sft_main = get_sft_main(SftArguments, llm_sft)
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/tuner.py` & `ms-swift-2.0.4/swift/llm/tuner.py`

 * *Files 26% similar despite different names*

```diff
@@ -1,43 +1,39 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 import types
 
+import numpy as np
 import torch
 import transformers
 from packaging import version
 
 from swift.torchacc_utils import consolidate_checkpoint
 from swift.trainers import TrainerCallback
-from swift.tuners import (AdaLoraConfig, AdapterConfig, IA3Config,
-                          LongLoRAModelType, LoraConfig, LoRAConfig,
+from swift.tuners import (AdaLoraConfig, AdapterConfig, IA3Config, LongLoRAModelType, LoraConfig, LoRAConfig,
                           NEFTuneConfig, Swift)
 from swift.tuners.llamapro import LLaMAProConfig
 from swift.tuners.module_mapping import MODEL_KEYS_MAPPING
-from swift.utils import (activate_model_parameters, freeze_model_parameters,
-                         get_logger, use_torchacc)
-from .utils import (SftArguments, find_all_linears, find_embedding, find_ln,
-                    is_adapter)
+from swift.utils import activate_model_parameters, freeze_model_parameters, get_logger, use_torchacc
+from .utils import SftArguments, find_all_linears, find_embedding, find_ln, is_adapter
 
 logger = get_logger()
 
 
 def handle_target_modules(model, args: SftArguments) -> None:
     if args.sft_type == 'ia3':
         target_modules = args.ia3_target_modules
-        assert len(args.ia3_feedforward_modules) > 0, (
-            'Setting ia3_target_modules to `ALL` '
-            'need to pass MLP linear names to `ia3_feedforward_modules`')
+        assert len(args.ia3_feedforward_modules) > 0, ('Setting ia3_target_modules to `ALL` '
+                                                       'need to pass MLP linear names to `ia3_feedforward_modules`')
     else:
         target_modules = args.lora_target_modules
     if args.lora_use_embedding:
         target_modules += find_embedding(model)
     if args.lora_use_all:
-        target_modules += find_all_linears(model, args.quantization_bit,
-                                           args.model_type)
+        target_modules += find_all_linears(model, args.quantization_bit, args.model_type)
     if args.sft_type == 'ia3':
         args.ia3_target_modules = target_modules
         logger.info(f'ia3_target_modules: {args.ia3_target_modules}')
     else:
         args.lora_target_modules = target_modules
         logger.info(f'lora_target_modules: {args.lora_target_modules}')
 
@@ -77,27 +73,35 @@
                 'use_dora': args.use_dora,
                 'lorap_lr_ratio': args.lora_lr_ratio,
             }
             if args.sft_type in ('lora', 'longlora'):
                 if args.lora_dtype == 'AUTO':
                     args.lora_dtype = None
                 if args.tuner_backend == 'swift':
-                    lora_config = LoRAConfig(
-                        lora_dtype=args.lora_dtype, **lora_kwargs)
+                    lora_config = LoRAConfig(lora_dtype=args.lora_dtype, **lora_kwargs)
+                    model = Swift.prepare_model(model, lora_config)
+                    logger.info(f'lora_config: {lora_config}')
                 elif args.tuner_backend == 'peft':
-                    lora_config = LoraConfig(
-                        task_type='CAUSAL_LM',
-                        lora_dtype=args.lora_dtype,
-                        **lora_kwargs)
-                model = Swift.prepare_model(model, lora_config)
-                logger.info(f'lora_config: {lora_config}')
+                    lora_config = LoraConfig(task_type='CAUSAL_LM', lora_dtype=args.lora_dtype, **lora_kwargs)
+                    model = Swift.prepare_model(model, lora_config)
+                    logger.info(f'lora_config: {lora_config}')
+                elif args.tuner_backend == 'unsloth':
+                    from unsloth import FastLanguageModel
+                    assert args.sft_type == 'lora', 'Unsloth does not support LongLoRA'
+                    lora_kwargs.pop('lorap_lr_ratio')
+                    model = FastLanguageModel.get_peft_model(
+                        model,
+                        use_gradient_checkpointing=True,
+                        max_seq_length=args.max_length,
+                        **lora_kwargs,
+                    )
+                    logger.info(f'unsloth_config: {lora_kwargs}')
                 if args.sft_type == 'longlora':
                     assert LongLoRAModelType.LLAMA in args.model_type
-                    assert version.parse(
-                        transformers.__version__) >= version.parse('4.39.3')
+                    assert version.parse(transformers.__version__) >= version.parse('4.39.3')
                     from swift.tuners.longlora.llama import replace_llama_attn
                     replace_llama_attn(model)
                     model.config.group_size_ratio = 0.25
             elif args.sft_type == 'adalora':
                 lora_kwargs.pop('lorap_lr_ratio', None)
                 lora_kwargs['rank_pattern'] = None
                 adalora_config = AdaLoraConfig(
@@ -152,69 +156,116 @@
                     hidden_pos=0,
                     adapter_length=args.adapter_length,
                     act_layer=args.adapter_act)
                 model = Swift.prepare_model(model, adapter_config)
                 logger.info(f'adapter_config: {adapter_config}')
         else:
             if use_torchacc():
-                consolidate_checkpoint(args.resume_from_checkpoint,
-                                       'adapter_model')
-            model = Swift.from_pretrained(
-                model, args.resume_from_checkpoint, is_trainable=True)
+                consolidate_checkpoint(args.resume_from_checkpoint, 'adapter_model')
+            model = Swift.from_pretrained(model, args.resume_from_checkpoint, is_trainable=True)
         # fix bug: Attempting to unscale FP16 gradients.
         #   peft: https://github.com/huggingface/peft/issues/1249
         #   modules_to_save + fp16
         is_logging = False
         for p in model.parameters():
             if p.requires_grad and p.dtype == torch.float16:
                 if not is_logging:
-                    logger.info(
-                        'Convert trainable parameters from fp16 to fp32.')
+                    logger.info('Convert trainable parameters from fp16 to fp32.')
                     is_logging = True
                 p.data = p.data.to(dtype=torch.float32)
     elif args.sft_type == 'full':
         if args.freeze_parameters > 0:
             freeze_model_parameters(model, args.freeze_parameters)
         if len(args.additional_trainable_parameters) > 0:
-            activate_model_parameters(model,
-                                      args.additional_trainable_parameters)
+            activate_model_parameters(model, args.additional_trainable_parameters)
         if use_torchacc() and args.resume_from_checkpoint is not None:
             consolidate_checkpoint(args.resume_from_checkpoint, 'model')
-            weights_file = os.path.join(args.resume_from_checkpoint,
-                                        'model.bin')
+            weights_file = os.path.join(args.resume_from_checkpoint, 'model.bin')
             state_dict = torch.load(weights_file, map_location='cpu')
             model.load_state_dict(state_dict, False)
             # release memory
             del state_dict
     else:
         raise ValueError(f'args.sft_type: {args.sft_type}')
 
-    if args.neftune_backend == 'swift' and args.neftune_noise_alpha not in {
-            None, 0.
-    }:
+    if args.neftune_backend == 'swift' and args.neftune_noise_alpha not in {None, 0.}:
         neftune_config = NEFTuneConfig(noise_alpha=args.neftune_noise_alpha)
         model = Swift.prepare_model(model, {'neftune': neftune_config})
         logger.info(f'neftune_config: {neftune_config}')
 
     if args.use_galore:
         from swift.trainers.optimizers.galore import GaLoreConfig
         if args.galore_target_modules is None:
-            args.galore_target_modules = find_all_linears(
-                model, 0, args.model_type)
+            args.galore_target_modules = find_all_linears(model, 0, args.model_type)
         if args.galore_with_embedding:
             args.galore_target_modules += find_embedding(model)
         args.training_args.galore_config = GaLoreConfig(
             target_modules=args.galore_target_modules,
             rank=args.galore_rank,
             update_proj_gap=args.galore_update_proj_gap,
             galore_scale=args.galore_scale,
             proj_type=args.galore_proj_type,
             optim_per_parameter=args.galore_optim_per_parameter,
         )
 
+    callbacks = []
+    if args.lisa_activated_layers > 0:
+        assert args.sft_type == 'full', 'LISA only supports full parameter training.'
+
+        class DynamicLayerActivationCallback(TrainerCallback):
+
+            def __init__(self, n_layers: int, step_interval: int, model: torch.nn.Module):
+                super().__init__()
+                self.n_layers = n_layers
+                self.step_interval = step_interval
+                self.model = model
+                layers_name = None
+                layers = None
+                for name, module in model.named_modules():
+                    if isinstance(module, torch.nn.ModuleList):
+                        layers_name = name
+                        layers = module
+                        break
+                assert layers_name is not None
+                self.layers_attribute = layers_name
+                self.total_layers = len(layers)
+
+                # Freeze all layers upon initialization
+                self.freeze_all_layers()
+                self.active_layers_indices = []
+
+            def freeze_all_layers(self):
+                layers = self.model.get_submodule(self.layers_attribute)
+                for layer in layers:
+                    for param in layer.parameters():
+                        param.requires_grad = False
+
+            def on_step_begin(self, args, state, control, **kwargs):
+                # Check if it's time to switch active layers, including at step 0
+                if state.global_step % self.step_interval == 0 or state.global_step == 1:
+                    self.switch_active_layers()
+
+            def switch_active_layers(self):
+                # First, disable gradients for all layers
+                self.freeze_all_layers()
+
+                # Randomly select n_layers to activate
+                layers = self.model.get_submodule(self.layers_attribute)
+                self.active_layers_indices = np.random.choice(range(self.total_layers), self.n_layers, replace=False)
+                # Enable gradients only for the selected layers
+                for idx in self.active_layers_indices:
+                    for param in layers[idx].parameters():
+                        param.requires_grad = True
+
+        callbacks.append(
+            DynamicLayerActivationCallback(
+                n_layers=args.lisa_activated_layers,  # Number of layers to activate
+                step_interval=args.lisa_step_interval,  # Step interval to update active layers
+                model=model))
+
     class TrainerAdapterCallback(TrainerCallback):
 
         def __init__(self):
             self.global_step = 0
 
         # offload original_modules to cpu, to save memory
         def on_train_begin(self, _args, state, control, **kwargs):
@@ -230,11 +281,10 @@
                 model._zero_grad = model.zero_grad
                 model.zero_grad = types.MethodType(zero_grad, model)
 
         def on_step_end(self, _args, state, control, **kwargs):
             if args.sft_type == 'adalora':
                 self.global_step = state.global_step
 
-    callbacks = []
     if is_adapter(args.sft_type) and args.tuner_backend == 'swift':
         callbacks.append(TrainerAdapterCallback())
     return model, callbacks
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/__init__.py` & `ms-swift-2.0.4/swift/llm/utils/__init__.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,51 +1,34 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
-from .argument import (AppUIArguments, DeployArguments, DPOArguments,
-                       EvalArguments, ExportArguments, InferArguments,
-                       RomeArguments, SftArguments, is_adapter,
-                       swift_to_peft_format)
+from .argument import (AppUIArguments, DeployArguments, DPOArguments, EvalArguments, ExportArguments, InferArguments,
+                       RomeArguments, SftArguments, is_adapter, swift_to_peft_format)
 from .client_utils import get_model_list_client, inference_client
-from .dataset import (DATASET_MAPPING, DatasetName, GetDatasetFunction,
-                      HfDataset, add_self_cognition_dataset, get_dataset,
-                      get_dataset_from_repo, load_dataset_from_local,
-                      load_ms_dataset, register_dataset)
-from .model import (MODEL_MAPPING, GetModelTokenizerFunction, LoRATM,
-                    ModelType, get_additional_saved_files,
-                    get_default_lora_target_modules, get_default_template_type,
-                    get_model_tokenizer, get_model_tokenizer_from_repo,
-                    get_model_tokenizer_with_flash_attn, register_model)
-from .preprocess import (AlpacaPreprocessor, ClsPreprocessor,
-                         ComposePreprocessor, ConversationsPreprocessor,
-                         PreprocessFunc, RenameColumnsPreprocessor,
-                         SmartPreprocessor, SwiftPreprocessor,
+from .dataset import (DATASET_MAPPING, DatasetName, GetDatasetFunction, HfDataset, add_self_cognition_dataset,
+                      get_dataset, get_dataset_from_repo, load_dataset_from_local, load_ms_dataset, register_dataset)
+from .model import (MODEL_MAPPING, GetModelTokenizerFunction, LoRATM, ModelType, get_additional_saved_files,
+                    get_default_lora_target_modules, get_default_template_type, get_model_tokenizer,
+                    get_model_tokenizer_from_repo, get_model_tokenizer_with_flash_attn, register_model)
+from .preprocess import (AlpacaPreprocessor, ClsPreprocessor, ComposePreprocessor, ConversationsPreprocessor,
+                         PreprocessFunc, RenameColumnsPreprocessor, SmartPreprocessor, SwiftPreprocessor,
                          TextGenerationPreprocessor)
 from .protocol import ChatCompletionResponse  # noqa
-from .protocol import (ChatCompletionRequest, ChatCompletionResponseChoice,
-                       ChatCompletionResponseStreamChoice,
-                       ChatCompletionStreamResponse, ChatMessage,
-                       CompletionRequest, CompletionResponse,
-                       CompletionResponseChoice,
-                       CompletionResponseStreamChoice,
-                       CompletionStreamResponse, DeltaMessage, Model,
-                       ModelList, UsageInfo, XRequestConfig, random_uuid)
-from .template import (DEFAULT_SYSTEM, TEMPLATE_MAPPING, History, Prompt,
-                       StopWords, Template, TemplateType, get_template,
-                       register_template)
-from .utils import (LazyLLMDataset, LLMDataset, dataset_map, download_dataset,
-                    find_all_linears, find_embedding, find_ln,
-                    get_max_model_len, get_time_info, history_to_messages,
-                    inference, inference_stream, is_vllm_available,
-                    limit_history_length, messages_to_history, print_example,
-                    safe_tokenizer_decode, set_generation_config,
-                    sort_by_max_length, stat_dataset, to_device)
+from .protocol import (ChatCompletionRequest, ChatCompletionResponseChoice, ChatCompletionResponseStreamChoice,
+                       ChatCompletionStreamResponse, ChatMessage, CompletionRequest, CompletionResponse,
+                       CompletionResponseChoice, CompletionResponseStreamChoice, CompletionStreamResponse, DeltaMessage,
+                       Model, ModelList, UsageInfo, XRequestConfig, random_uuid)
+from .template import (DEFAULT_SYSTEM, TEMPLATE_MAPPING, History, Prompt, StopWords, Template, TemplateType,
+                       get_template, register_template)
+from .utils import (LazyLLMDataset, LLMDataset, dataset_map, download_dataset, find_all_linears, find_embedding,
+                    find_ln, get_max_model_len, get_time_info, history_to_messages, inference, inference_stream,
+                    is_vllm_available, limit_history_length, messages_to_history, print_example, safe_tokenizer_decode,
+                    set_generation_config, sort_by_max_length, stat_dataset, to_device)
 
 try:
     if is_vllm_available():
-        from .vllm_utils import (VllmGenerationConfig, get_vllm_engine,
-                                 inference_stream_vllm, inference_vllm,
+        from .vllm_utils import (VllmGenerationConfig, get_vllm_engine, inference_stream_vllm, inference_vllm,
                                  prepare_vllm_engine_template)
         try:
             from .vllm_utils import LoRARequest
         except ImportError:
             pass
 except Exception as e:
     from swift.utils import get_logger
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/argument.py` & `ms-swift-2.0.4/swift/llm/utils/argument.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,114 +1,97 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
+import inspect
 import math
 import os
 from dataclasses import dataclass, field
 from typing import List, Literal, Optional, Set, Tuple, Union
 
 import json
 import numpy as np
 import torch
 import transformers
 from datasets import Dataset as HfDataset
 from datasets import concatenate_datasets
 from packaging import version
 from torch import dtype as Dtype
-from transformers.utils import (is_torch_bf16_gpu_available,
-                                is_torch_cuda_available,
-                                is_torch_npu_available, strtobool)
+from transformers.utils import is_torch_bf16_gpu_available, is_torch_cuda_available, is_torch_npu_available, strtobool
 from transformers.utils.versions import require_version
 
 from swift.hub import HubApi, ModelScopeConfig
 from swift.trainers import Seq2SeqTrainingArguments
 from swift.tuners import Swift
-from swift.utils import (add_version_to_work_dir, get_dist_setting, get_logger,
-                         get_pai_tensorboard_dir, is_dist, is_mp,
-                         is_pai_training_job)
-from .dataset import (DATASET_MAPPING, get_custom_dataset, get_dataset,
-                      register_dataset)
-from .model import (MODEL_MAPPING, dtype_mapping, get_additional_saved_files,
-                    get_default_lora_target_modules, get_default_template_type)
+from swift.utils import (add_version_to_work_dir, get_dist_setting, get_logger, get_pai_tensorboard_dir, is_dist,
+                         is_local_master, is_mp, is_pai_training_job)
+from .dataset import DATASET_MAPPING, get_custom_dataset, get_dataset, register_dataset
+from .model import (MODEL_MAPPING, dtype_mapping, get_additional_saved_files, get_default_lora_target_modules,
+                    get_default_template_type)
 from .template import TEMPLATE_MAPPING
 from .utils import is_vllm_available
 
 logger = get_logger()
 
 
 def is_adapter(sft_type: str) -> bool:
-    return sft_type in {
-        'lora', 'longlora', 'adalora', 'ia3', 'llamapro', 'adapter'
-    }
+    return sft_type in {'lora', 'longlora', 'adalora', 'ia3', 'llamapro', 'adapter'}
 
 
 class ArgumentsBase:
 
-    def register_custom_dataset(
-            self: Union['SftArguments', 'InferArguments']) -> None:
+    def register_custom_dataset(self: Union['SftArguments', 'InferArguments']) -> None:
         dataset = []
         for d in self.dataset:
             if os.path.exists(d):
                 self.custom_train_dataset_path.append(d)
             else:
                 dataset.append(d)
         self.dataset = dataset
 
         for key in ['custom_train_dataset_path', 'custom_val_dataset_path']:
             value = getattr(self, key)
             if isinstance(value, str):
                 setattr(self, key, [value])
-        if len(self.custom_train_dataset_path) == 0 and len(
-                self.custom_val_dataset_path) == 0:
+        if len(self.custom_train_dataset_path) == 0 and len(self.custom_val_dataset_path) == 0:
             return
 
         dataset_name = '_custom_dataset'
-        _register_local_dataset(dataset_name, self.custom_train_dataset_path,
-                                self.custom_val_dataset_path)
+        _register_local_dataset(dataset_name, self.custom_train_dataset_path, self.custom_val_dataset_path)
         self.dataset.append(dataset_name)
 
     def handle_path(self: Union['SftArguments', 'InferArguments']) -> None:
         check_exist_path = [
-            'ckpt_dir', 'resume_from_checkpoint', 'custom_train_dataset_path',
-            'custom_val_dataset_path'
+            'ckpt_dir', 'resume_from_checkpoint', 'custom_train_dataset_path', 'custom_val_dataset_path'
         ]
-        if self.model_id_or_path is not None and (
-                self.model_id_or_path.startswith('~')
-                or self.model_id_or_path.startswith('/')):
+        if self.model_id_or_path is not None and (self.model_id_or_path.startswith('~')
+                                                  or self.model_id_or_path.startswith('/')):
             check_exist_path.append('model_id_or_path')
         check_exist_path_set = set(check_exist_path)
         other_path = ['output_dir', 'logging_dir']
         for k in check_exist_path + other_path:
             value = getattr(self, k, None)
             if value is None:
                 continue
             value = _check_path(k, value, check_exist_path_set)
             setattr(self, k, value)
 
-    def check_flash_attn(
-            self: Union['SftArguments', 'InferArguments']) -> None:
+    def check_flash_attn(self: Union['SftArguments', 'InferArguments']) -> None:
         model_info = MODEL_MAPPING[self.model_type]
         support_flash_attn = model_info.get('support_flash_attn', False)
         if self.use_flash_attn and not support_flash_attn:
-            logger.warning(f'use_flash_attn: {self.use_flash_attn}, '
-                           f'but support_flash_attn: {support_flash_attn}')
+            logger.warning(f'use_flash_attn: {self.use_flash_attn}, ' f'but support_flash_attn: {support_flash_attn}')
 
-    def handle_generation_config(
-            self: Union['SftArguments', 'InferArguments']) -> None:
+    def handle_generation_config(self: Union['SftArguments', 'InferArguments']) -> None:
         if self.do_sample is False:
             # fix warning
             self.temperature = 1.
             self.top_p = 1.
             self.top_k = 50
-            logger.info(
-                'Due to do_sample=False, the following settings are applied: args.temperature: '
-                f'{self.temperature}, args.top_p: {self.top_p}, args.top_k: {self.top_k}.'
-            )
-
-    def select_dtype(
-        self: Union['SftArguments', 'InferArguments']
-    ) -> Tuple[Optional[Dtype], bool, bool]:
+            logger.info('Due to do_sample=False, the following settings are applied: args.temperature: '
+                        f'{self.temperature}, args.top_p: {self.top_p}, args.top_k: {self.top_k}.')
+
+    def select_dtype(self: Union['SftArguments', 'InferArguments']) -> Tuple[Optional[Dtype], bool, bool]:
         if not is_torch_cuda_available() and not is_torch_npu_available():
             # cpu
             if self.dtype == 'AUTO':
                 self.dtype = 'fp32'
                 logger.info(f'Setting args.dtype: {self.dtype}')
             assert self.dtype != 'fp16', 'The CPU does not support matrix multiplication with FP16.'
             if self.dtype == 'fp32':
@@ -118,16 +101,15 @@
             else:
                 raise ValueError(f'args.dtype: {self.dtype}')
         # cuda, npu
         if self.dtype == 'AUTO':
             if not is_torch_bf16_gpu_available():
                 self.dtype = 'fp16'
             else:
-                model_torch_dtype = MODEL_MAPPING[self.model_type].get(
-                    'torch_dtype')
+                model_torch_dtype = MODEL_MAPPING[self.model_type].get('torch_dtype')
                 if model_torch_dtype is not None:
                     self.dtype = dtype_mapping[model_torch_dtype]
                 elif isinstance(self, SftArguments):
                     self.dtype = 'bf16'
                 else:
                     return None, False, False
 
@@ -136,73 +118,62 @@
         assert torch_dtype in {torch.float16, torch.bfloat16, torch.float32}
         if torch_dtype == torch.float16:
             if isinstance(self, SftArguments) and self.sft_type == 'full':
                 self.dtype = 'fp32'
                 torch_dtype = torch.float32
                 logger.warning(
                     'Fine-tuning with full parameters does not support fp16, and is prone to NaN. '
-                    'We will use the fp32 & AMP approach, which consumes approximately twice the memory of bf16.'
-                )
+                    'We will use the fp32 & AMP approach, which consumes approximately twice the memory of bf16.')
                 logger.info(f'Setting torch_dtype: {torch_dtype}')
             fp16, bf16 = True, False
         elif torch_dtype == torch.bfloat16:
             support_bf16 = is_torch_bf16_gpu_available()
             if not support_bf16:
                 logger.warning(f'support_bf16: {support_bf16}')
             fp16, bf16 = False, True
         else:
             fp16, bf16 = False, False
         return torch_dtype, fp16, bf16
 
-    def select_bnb(
-        self: Union['SftArguments', 'InferArguments']
-    ) -> Tuple[Optional[Dtype], bool, bool]:
+    def select_bnb(self: Union['SftArguments', 'InferArguments']) -> Tuple[Optional[Dtype], bool, bool]:
         if self.bnb_4bit_comp_dtype == 'AUTO':
             self.bnb_4bit_comp_dtype = self.dtype
 
         if self.bnb_4bit_comp_dtype != 'AUTO':
-            bnb_4bit_compute_dtype = dtype_mapping_reversed[
-                self.bnb_4bit_comp_dtype]
-            assert bnb_4bit_compute_dtype in {
-                torch.float16, torch.bfloat16, torch.float32
-            }
+            bnb_4bit_compute_dtype = dtype_mapping_reversed[self.bnb_4bit_comp_dtype]
+            assert bnb_4bit_compute_dtype in {torch.float16, torch.bfloat16, torch.float32}
         else:
             bnb_4bit_compute_dtype = None
         quantization_bit = self.quantization_bit
         if quantization_bit == 4:
             require_version('bitsandbytes')
             load_in_4bit, load_in_8bit = True, False
         elif quantization_bit == 8:
             require_version('bitsandbytes')
             load_in_4bit, load_in_8bit = False, True
         else:
             load_in_4bit, load_in_8bit = False, False
 
         return bnb_4bit_compute_dtype, load_in_4bit, load_in_8bit
 
-    def handle_compatibility(
-            self: Union['SftArguments', 'InferArguments']) -> None:
-        template_type_mapping = {
-            'chatglm2-generation': 'chatglm-generation',
-            'chatml': 'qwen'
-        }
+    def handle_compatibility(self: Union['SftArguments', 'InferArguments']) -> None:
+        template_type_mapping = {'chatglm2-generation': 'chatglm-generation', 'chatml': 'qwen'}
         model_type_mapping = {
             'openbmb-minicpm-2b-sft-chat': 'minicpm-2b-sft-chat',
             'openbmb-minicpm-2b-chat': 'minicpm-2b-chat',
         }
         for k, v in template_type_mapping.items():
             if k == self.template_type:
                 self.template_type = v
                 break
         for k, v in model_type_mapping.items():
             if k == self.model_type:
                 self.model_type = v
                 break
-        if self.dataset is not None and len(
-                self.dataset) == 1 and ',' in self.dataset[0]:
+        if self.dataset is not None and len(self.dataset) == 1 and ',' in self.dataset[0]:
             self.dataset = self.dataset[0].split(',')
         if self.truncation_strategy == 'ignore':
             self.truncation_strategy = 'delete'
         if isinstance(self, InferArguments):
             if self.show_dataset_sample != 10 and self.val_dataset_sample == 10:
                 # args.val_dataset_sample is the default value and args.show_dataset_sample is not the default value.
                 self.val_dataset_sample = self.show_dataset_sample
@@ -218,135 +189,113 @@
             if self.per_device_train_batch_size is not None:
                 self.batch_size = self.per_device_train_batch_size
             if self.per_device_eval_batch_size is not None:
                 self.eval_batch_size = self.per_device_eval_batch_size
             if self.deepspeed_config_path is not None:
                 self.deepspeed = self.deepspeed_config_path
 
+    def prepare_template(self):
+        if self.template_type == 'AUTO':
+            self.template_type = get_default_template_type(self.model_type)
+            logger.info(f'Setting template_type: {self.template_type}')
+
     def set_model_type(self: Union['SftArguments', 'InferArguments']) -> None:
         # compat with swift<1.7
         if self.model_cache_dir is not None and self.model_id_or_path is None:
             self.model_id_or_path = self.model_cache_dir
             self.model_cache_dir = None
 
         if self.model_id_or_path is not None:
-            model_mapping_reversed = {
-                v['model_id_or_path'].lower(): k
-                for k, v in MODEL_MAPPING.items()
-            }
+            model_mapping_reversed = {v['model_id_or_path'].lower(): k for k, v in MODEL_MAPPING.items()}
             model_id_or_path = self.model_id_or_path
             model_id_or_path_lower = model_id_or_path.lower()
             if model_id_or_path_lower not in model_mapping_reversed:
-                if (isinstance(self, InferArguments)
-                        and 'checkpoint' in model_id_or_path
-                        and 'merged' not in model_id_or_path
-                        and self.ckpt_dir is None):
-                    raise ValueError(
-                        'Please use `--ckpt_dir vx-xxx/checkpoint-xxx` to use the checkpoint.'
-                    )
+                if (isinstance(self, InferArguments) and 'checkpoint' in model_id_or_path
+                        and 'merged' not in model_id_or_path and self.ckpt_dir is None):
+                    raise ValueError('Please use `--ckpt_dir vx-xxx/checkpoint-xxx` to use the checkpoint.')
                 if self.model_type is None:
-                    raise ValueError(
-                        f"model_id_or_path: '{model_id_or_path}' is not registered. "
-                        'Please set `--model_type <model_type>` additionally.')
+                    raise ValueError(f"model_id_or_path: '{model_id_or_path}' is not registered. "
+                                     'Please set `--model_type <model_type>` additionally.')
                 assert self.model_cache_dir is None
             else:
                 model_type = model_mapping_reversed[model_id_or_path_lower]
                 assert self.model_type is None or self.model_type == model_type
                 self.model_type = model_type
                 logger.info(f'Setting args.model_type: {model_type}')
                 if self.model_cache_dir is not None:
                     self.model_id_or_path = self.model_cache_dir
 
         error_msg = f'The model_type you can choose: {list(MODEL_MAPPING.keys())}'
         if self.model_type is None:
-            raise ValueError('please setting `--model_type <model_type>`. '
-                             + error_msg)
+            raise ValueError('please setting `--model_type <model_type>`. ' + error_msg)
         elif self.model_type not in MODEL_MAPPING:
-            raise ValueError(
-                f"model_type: '{self.model_type}' is not registered. "
-                + error_msg)
+            raise ValueError(f"model_type: '{self.model_type}' is not registered. " + error_msg)
         model_info = MODEL_MAPPING[self.model_type]
         use_hf = strtobool(os.environ.get('USE_HF', 'False'))
         if self.model_revision is not None:
             model_info['revision'] = self.model_revision
-            logger.info(
-                f"Setting model_info['revision']: {self.model_revision}")
+            logger.info(f"Setting model_info['revision']: {self.model_revision}")
         elif use_hf:
             model_info['revision'] = 'main'
         self.model_revision = model_info['revision']
         if self.model_id_or_path is None:
-            self.model_id_or_path = model_info[
-                'hf_model_id'] if use_hf else model_info['model_id_or_path']
+            self.model_id_or_path = model_info['hf_model_id'] if use_hf else model_info['model_id_or_path']
         requires = model_info['requires']
         for require in requires:
             require_version(require)
 
 
 @dataclass
 class SftArguments(ArgumentsBase):
     # You can specify the model by either using the model_type or model_id_or_path.
     model_type: Optional[str] = field(
-        default=None,
-        metadata={'help': f'model_type choices: {list(MODEL_MAPPING.keys())}'})
+        default=None, metadata={'help': f'model_type choices: {list(MODEL_MAPPING.keys())}'})
     model_id_or_path: Optional[str] = None
     model_revision: Optional[str] = None
     model_layer_cls_name: Optional[str] = field(
         default=None,
-        metadata={
-            'help':
-            "Decoder Class name of model, e.g. 'QWenBlock' for QWen, 'LlamaDecoderLayer' for LLama"
-        })
+        metadata={'help': "Decoder Class name of model, e.g. 'QWenBlock' for QWen, 'LlamaDecoderLayer' for LLama"})
 
-    sft_type: Literal['lora', 'full', 'longlora', 'adalora', 'ia3', 'llamapro',
-                      'adapter'] = 'lora'
+    sft_type: Literal['lora', 'full', 'longlora', 'adalora', 'ia3', 'llamapro', 'adapter'] = 'lora'
     freeze_parameters: float = 0.  # 0 ~ 1
     additional_trainable_parameters: List[str] = field(default_factory=list)
-    tuner_backend: Literal['swift', 'peft'] = 'peft'
+    tuner_backend: Literal['swift', 'peft', 'unsloth'] = 'peft'
     template_type: str = field(
-        default='AUTO',
-        metadata={
-            'help':
-            f"template_type choices: {list(TEMPLATE_MAPPING.keys()) + ['AUTO']}"
-        })
+        default='AUTO', metadata={'help': f"template_type choices: {list(TEMPLATE_MAPPING.keys()) + ['AUTO']}"})
     output_dir: str = 'output'
     add_output_dir_suffix: Optional[bool] = None
     ddp_backend: Optional[Literal['nccl', 'gloo', 'mpi', 'ccl']] = None
     ddp_find_unused_parameters: Optional[bool] = None
     ddp_broadcast_buffers: Optional[bool] = None
 
     seed: int = 42
     resume_from_checkpoint: Optional[str] = None
+    ignore_data_skip: bool = False
     dtype: Literal['bf16', 'fp16', 'fp32', 'AUTO'] = 'AUTO'
 
     dataset: List[str] = field(
-        default_factory=list,
-        metadata={'help': f'dataset choices: {list(DATASET_MAPPING.keys())}'})
+        default_factory=list, metadata={'help': f'dataset choices: {list(DATASET_MAPPING.keys())}'})
     dataset_seed: int = 42
     dataset_test_ratio: float = 0.01
     train_dataset_sample: int = -1  # -1: all dataset
     train_dataset_mix_ratio: float = 0.
-    train_dataset_mix_ds: List[str] = field(
-        default_factory=lambda: ['ms-bench'])
+    train_dataset_mix_ds: List[str] = field(default_factory=lambda: ['ms-bench'])
     val_dataset_sample: Optional[int] = None  # -1: all dataset
     use_loss_scale: bool = False
     system: Optional[str] = None
     max_length: int = 2048  # -1: no limit
     truncation_strategy: Literal['delete', 'truncation_left'] = 'delete'
-    check_dataset_strategy: Literal['none', 'discard', 'error',
-                                    'warning'] = 'none'
+    check_dataset_strategy: Literal['none', 'discard', 'error', 'warning'] = 'none'
     custom_train_dataset_path: List[str] = field(default_factory=list)
     custom_val_dataset_path: List[str] = field(default_factory=list)
     self_cognition_sample: int = 0
     # Chinese name and English name
-    model_name: List[str] = field(
-        default_factory=lambda: [None, None],
-        metadata={'help': "e.g. ['', 'Xiao Huang']"})
+    model_name: List[str] = field(default_factory=lambda: [None, None], metadata={'help': "e.g. ['', 'Xiao Huang']"})
     model_author: List[str] = field(
-        default_factory=lambda: [None, None],
-        metadata={'help': "e.g. ['', 'ModelScope']"})
+        default_factory=lambda: [None, None], metadata={'help': "e.g. ['', 'ModelScope']"})
     # If you want to use qlora, set the quantization_bit to 8 or 4.
     # And you need to install bitsandbytes: `pip install bitsandbytes -U`
     # note: bf16 and quantization have requirements for gpu architecture
     quantization_bit: Literal[0, 4, 8] = 0
     bnb_4bit_comp_dtype: Literal['fp16', 'bf16', 'fp32', 'AUTO'] = 'AUTO'
     bnb_4bit_quant_type: Literal['fp4', 'nf4'] = 'nf4'
     bnb_4bit_use_double_quant: bool = True
@@ -355,15 +304,15 @@
     lora_target_modules: List[str] = field(default_factory=lambda: ['DEFAULT'])
     lora_rank: int = 8
     lora_alpha: int = 32
     lora_dropout_p: float = 0.05
     lora_bias_trainable: Literal['none', 'all'] = 'none'
     # e.g. ['wte', 'ln_1', 'ln_2', 'ln_f', 'lm_head']
     lora_modules_to_save: List[str] = field(default_factory=list)
-    lora_dtype: Literal['fp16', 'bf16', 'fp32', 'AUTO'] = 'fp32'
+    lora_dtype: Literal['fp16', 'bf16', 'fp32', 'AUTO'] = 'AUTO'
     lora_lr_ratio: float = None
     use_rslora: bool = False
     use_dora: bool = False
 
     # adapter
     adapter_act: str = 'gelu'
     adapter_length: int = 128
@@ -395,14 +344,18 @@
     llamapro_num_new_blocks: int = 4
     llamapro_num_groups: Optional[int] = None
 
     # neftune
     neftune_noise_alpha: Optional[float] = None  # e.g. 5, 10, 15
     neftune_backend: Literal['swift', 'transformers'] = None
 
+    # lisa
+    lisa_activated_layers: int = 0
+    lisa_step_interval: int = 20
+
     gradient_checkpointing: Optional[bool] = None
     # e.g. 'default-zero3', 'default-zero2', 'ds_config/zero2.json', 'zero3-offload'
     deepspeed: Optional[str] = None
     batch_size: int = 1
     eval_batch_size: Optional[int] = None
     num_train_epochs: int = 1
     # if max_steps >= 0, override num_train_epochs
@@ -428,22 +381,17 @@
 
     # push to ms hub
     push_to_hub: bool = False
     # 'user_name/repo_name' or 'repo_name'
     hub_model_id: Optional[str] = None
     # None: use env var `MODELSCOPE_API_TOKEN`
     hub_token: Optional[str] = field(
-        default=None,
-        metadata={
-            'help':
-            'SDK token can be found in https://modelscope.cn/my/myaccesstoken'
-        })
+        default=None, metadata={'help': 'SDK token can be found in https://modelscope.cn/my/myaccesstoken'})
     hub_private_repo: bool = False
-    push_hub_strategy: Literal['end', 'push_best', 'push_last', 'checkpoint',
-                               'all_checkpoints'] = 'push_best'
+    push_hub_strategy: Literal['end', 'push_best', 'push_last', 'checkpoint', 'all_checkpoints'] = 'push_best'
 
     # other
     test_oom_error: bool = field(
         default=False,
         metadata={
             'help':
             'If set to True, the train_dataset will be sorted in descending order based on max_length, '
@@ -460,14 +408,15 @@
     report_to: List[str] = field(default_factory=lambda: ['tensorboard'])
     acc_strategy: Literal['token', 'sentence'] = 'token'
     save_on_each_node: bool = True
     evaluation_strategy: Literal['steps', 'no'] = 'steps'
     save_strategy: Literal['steps', 'no'] = 'steps'
     save_safetensors: bool = True
     gpu_memory_fraction: Optional[float] = None
+    include_num_input_tokens_seen: Optional[bool] = False
 
     # generation config
     max_new_tokens: int = 2048
     do_sample: bool = True
     temperature: float = 0.3
     top_k: int = 20
     top_p: float = 0.7
@@ -486,16 +435,15 @@
     fsdp: Optional[str] = ''
     # fsdp config file
     fsdp_config: Optional[str] = None
 
     def handle_dataset_mixture(self, train_dataset: HfDataset) -> None:
         if train_dataset is None:
             return train_dataset
-        if self.train_dataset_mix_ratio <= 0 or len(
-                self.train_dataset_mix_ds) == 0:
+        if self.train_dataset_mix_ratio <= 0 or len(self.train_dataset_mix_ds) == 0:
             return train_dataset
 
         random_state = np.random.RandomState(self.dataset_seed)
         train_dataset_mix_ds = []
         custom_mix_ds = []
         for mix_ds in self.train_dataset_mix_ds:
             if os.path.exists(mix_ds):
@@ -503,32 +451,24 @@
             else:
                 train_dataset_mix_ds.append(mix_ds)
 
         if len(custom_mix_ds) > 0:
             dataset_name = '_custom_mixture'
             _register_local_dataset(dataset_name, custom_mix_ds, [])
             train_dataset_mix_ds.append(dataset_name)
-        mix_dataset_sample = int(
-            len(train_dataset) * self.train_dataset_mix_ratio)
+        mix_dataset_sample = int(len(train_dataset) * self.train_dataset_mix_ratio)
         logger.info(f'train_dataset_mix_ds: {train_dataset_mix_ds}')
-        logger.info(
-            f'len(train_dataset): {len(train_dataset)}, mix_dataset_sample: {mix_dataset_sample}'
-        )
+        logger.info(f'len(train_dataset): {len(train_dataset)}, mix_dataset_sample: {mix_dataset_sample}')
         mixed_dataset = get_dataset(
-            train_dataset_mix_ds,
-            0.0,
-            random_state,
-            check_dataset_strategy=self.check_dataset_strategy)[0]
+            train_dataset_mix_ds, 0.0, random_state, check_dataset_strategy=self.check_dataset_strategy)[0]
         if len(mixed_dataset) < mix_dataset_sample:
-            logger.warn(
-                f'The length of dataset used for mixin: {train_dataset_mix_ds} are '
-                'lesser than the ratio required by the `train_dataset_mix_ratio` '
-                f'argument: {self.train_dataset_mix_ratio}. '
-                f'the actual ratio is: {len(mixed_dataset) / len(train_dataset):.6}.'
-            )
+            logger.warn(f'The length of dataset used for mixin: {train_dataset_mix_ds} are '
+                        'lesser than the ratio required by the `train_dataset_mix_ratio` '
+                        f'argument: {self.train_dataset_mix_ratio}. '
+                        f'the actual ratio is: {len(mixed_dataset) / len(train_dataset):.6}.')
         else:
             train_idxs = random_state.permutation(mix_dataset_sample)
             mixed_dataset = mixed_dataset.select(train_idxs)
         return concatenate_datasets([train_dataset, mixed_dataset])
 
     def prepare_push_ms_hub(self) -> None:
         if not self.push_to_hub:
@@ -539,16 +479,15 @@
 
         api = HubApi()
         if self.hub_token is None:
             self.hub_token = os.environ.get('MODELSCOPE_API_TOKEN')
         if self.hub_token is not None:
             api.login(self.hub_token)
         else:
-            assert ModelScopeConfig.get_token(
-            ) is not None, 'Please enter hub_token'
+            assert ModelScopeConfig.get_token() is not None, 'Please enter hub_token'
         logger.info('hub login successful!')
 
     def _prepare_target_modules(self, target_modules) -> List[str]:
         if isinstance(target_modules, str):
             target_modules = [target_modules]
         if len(target_modules) == 0:
             return target_modules
@@ -590,16 +529,15 @@
         deepspeed_mapping = {
             'default-zero2': 'zero2.json',
             'default-zero3': 'zero3.json',
             'zero3-offload': 'zero3_offload.json'
         }
         for ds_name, ds_config in deepspeed_mapping.items():
             if self.deepspeed == ds_name:
-                self.deepspeed = os.path.abspath(
-                    os.path.join(ds_config_folder, ds_config))
+                self.deepspeed = os.path.abspath(os.path.join(ds_config_folder, ds_config))
                 break
 
         self.handle_path()
         self.set_model_type()
         if isinstance(self.dataset, str):
             self.dataset = [self.dataset]
         if isinstance(self.train_dataset_mix_ds, str):
@@ -609,49 +547,38 @@
         self.handle_generation_config()
 
         self.lora_use_embedding = False
         self.lora_use_all = False
         self.lora_m2s_use_embedding = False
         self.lora_m2s_use_ln = False
         if self.sft_type == 'ia3':
-            self.ia3_feedforward_modules = self._prepare_target_modules(
-                self.ia3_feedforward_modules)
-            self.ia3_target_modules = self._prepare_target_modules(
-                self.ia3_target_modules)
-            self.ia3_modules_to_save = self._prepare_modules_to_save(
-                self.ia3_modules_to_save)
+            self.ia3_feedforward_modules = self._prepare_target_modules(self.ia3_feedforward_modules)
+            self.ia3_target_modules = self._prepare_target_modules(self.ia3_target_modules)
+            self.ia3_modules_to_save = self._prepare_modules_to_save(self.ia3_modules_to_save)
         else:
-            self.lora_target_modules = self._prepare_target_modules(
-                self.lora_target_modules)
-            self.lora_modules_to_save = self._prepare_modules_to_save(
-                self.lora_modules_to_save)
+            self.lora_target_modules = self._prepare_target_modules(self.lora_target_modules)
+            self.lora_modules_to_save = self._prepare_modules_to_save(self.lora_modules_to_save)
         if self.sft_type in {'adalora', 'ia3'} and self.lora_use_embedding:
-            raise ValueError(
-                '`adalora` and `ia3` do not support setting embedding as target_modules.'
-            )
+            raise ValueError('`adalora` and `ia3` do not support setting embedding as target_modules.')
 
         if self.self_cognition_sample > 0:
             if self.model_name is None or self.model_author is None:
-                raise ValueError(
-                    'Please enter self.model_name self.model_author. '
-                    'For example: `--model_name  "Xiao Huang" --model_author  ModelScope`. '
-                    'Representing the model name and model author in Chinese and English.'
-                )
+                raise ValueError('Please enter self.model_name self.model_author. '
+                                 'For example: `--model_name  "Xiao Huang" --model_author  ModelScope`. '
+                                 'Representing the model name and model author in Chinese and English.')
             for k in ['model_name', 'model_author']:
                 v = getattr(self, k)
                 if len(v) == 1:
                     v = v[0]
                 if isinstance(v, str):
                     setattr(self, k, [v, v])
             if self.sft_type == 'lora' and not self.lora_use_all:
-                logger.warning(
-                    'Due to knowledge editing involved, it is recommended to add LoRA on MLP. '
-                    'For example: `--lora_target_modules ALL`. '
-                    'If you have already added LoRA on MLP, please ignore this warning.'
-                )
+                logger.warning('Due to knowledge editing involved, it is recommended to add LoRA on MLP. '
+                               'For example: `--lora_target_modules ALL`. '
+                               'If you have already added LoRA on MLP, please ignore this warning.')
 
         self.torch_dtype, self.fp16, self.bf16 = self.select_dtype()
         world_size = 1
         if is_dist():
             rank, local_rank, world_size, _ = get_dist_setting()
             if is_torch_npu_available():
                 torch.npu.set_device(local_rank)
@@ -661,61 +588,48 @@
             if self.ddp_backend is None:
                 self.ddp_backend = 'nccl'
             if self.ddp_backend == 'gloo' and self.quantization_bit != 0:
                 raise ValueError('not supported, please use `nccl`')
 
         if is_adapter(self.sft_type):
             assert self.freeze_parameters == 0., (
-                'lora does not support `freeze_parameters`, please set `--sft_type full`'
-            )
+                'lora does not support `freeze_parameters`, please set `--sft_type full`')
             assert len(self.additional_trainable_parameters) == 0, (
-                'lora does not support `additional_trainable_parameters`, please set `--sft_type full`'
-            )
+                'lora does not support `additional_trainable_parameters`, please set `--sft_type full`')
             if 'int4' in self.model_type or 'int8' in self.model_type or 'awq' in self.model_type:
                 assert self.quantization_bit == 0, 'int4, int8 or awq models do not need to be quantized again.'
             if self.learning_rate is None:
                 self.learning_rate = 1e-4
             if self.save_only_model is None:
-                if self.deepspeed is None:
-                    self.save_only_model = False
-                else:
+                if self.deepspeed is not None and version.parse(transformers.__version__) < version.parse('4.37'):
                     self.save_only_model = True
+                else:
+                    self.save_only_model = False
         elif self.sft_type == 'full':
             assert 0 <= self.freeze_parameters <= 1
             assert self.quantization_bit == 0, 'Full parameter fine-tuning does not support quantization.'
-            assert self.dtype != 'fp16', (
-                "Fine-tuning with dtype=='fp16' can lead to NaN issues. "
-                'Please use fp32+AMP or bf16 to perform full parameter fine-tuning.'
-            )
+            assert self.dtype != 'fp16', ("Fine-tuning with dtype=='fp16' can lead to NaN issues. "
+                                          'Please use fp32+AMP or bf16 to perform full parameter fine-tuning.')
             if isinstance(self.additional_trainable_parameters, str):
-                self.additional_trainable_parameters = [
-                    self.additional_trainable_parameters
-                ]
+                self.additional_trainable_parameters = [self.additional_trainable_parameters]
             if self.learning_rate is None:
                 self.learning_rate = 1e-5
             if self.save_only_model is None:
                 self.save_only_model = True
         else:
             raise ValueError(f'sft_type: {self.sft_type}')
 
-        if self.template_type == 'AUTO':
-            self.template_type = get_default_template_type(self.model_type)
-            logger.info(f'Setting template_type: {self.template_type}')
+        self.prepare_template()
         if len(self.dataset) == 0 and (len(self.custom_train_dataset_path) == 0
-                                       and len(
-                                           self.custom_val_dataset_path) == 0
-                                       and self.self_cognition_sample == 0):
-            raise ValueError(
-                f'self.dataset: {self.dataset}, Please input the training dataset.'
-            )
+                                       and len(self.custom_val_dataset_path) == 0 and self.self_cognition_sample == 0):
+            raise ValueError(f'self.dataset: {self.dataset}, Please input the training dataset.')
 
         if self.save_steps is None:
             self.save_steps = self.eval_steps
-        self.bnb_4bit_compute_dtype, self.load_in_4bit, self.load_in_8bit = self.select_bnb(
-        )
+        self.bnb_4bit_compute_dtype, self.load_in_4bit, self.load_in_8bit = self.select_bnb()
 
         if self.neftune_backend is None:
             self.neftune_backend = 'swift' if version.parse(transformers.__version__) < version.parse('4.35') \
                 else 'transformers'
 
         self.prepare_push_ms_hub()
         self.train_sampler_random = not self.test_oom_error
@@ -731,74 +645,69 @@
 
         if self.deepspeed is not None:
             if is_mp():
                 raise ValueError('DeepSpeed is not compatible with MP. '
                                  f'n_gpu: {torch.cuda.device_count()}, '
                                  f'local_world_size: {get_dist_setting()[3]}.')
             require_version('deepspeed')
-            if self.deepspeed.endswith('.json') or os.path.isfile(
-                    self.deepspeed):
+            if self.deepspeed.endswith('.json') or os.path.isfile(self.deepspeed):
                 with open(self.deepspeed, 'r', encoding='utf-8') as f:
                     self.deepspeed = json.load(f)
             logger.info(f'Using deepspeed: {self.deepspeed}')
 
         if self.gradient_accumulation_steps is None:
-            self.gradient_accumulation_steps = math.ceil(16 / self.batch_size
-                                                         / world_size)
+            self.gradient_accumulation_steps = math.ceil(16 / self.batch_size / world_size)
         template_info = TEMPLATE_MAPPING[self.template_type]
         if self.lazy_tokenize is None:
             self.lazy_tokenize = template_info.get('lazy_tokenize', False)
             logger.info(f'Setting args.lazy_tokenize: {self.lazy_tokenize}')
         if 'dataloader_num_workers' in template_info:
-            self.dataloader_num_workers = template_info[
-                'dataloader_num_workers']
-            logger.info(
-                f'Setting args.dataloader_num_workers: {self.dataloader_num_workers}'
-            )
+            self.dataloader_num_workers = template_info['dataloader_num_workers']
+            logger.info(f'Setting args.dataloader_num_workers: {self.dataloader_num_workers}')
         if 'dataloader_pin_memory' in template_info:
             self.dataloader_pin_memory = template_info['dataloader_pin_memory']
-            logger.info(
-                f'Setting args.dataloader_pin_memory: {self.dataloader_pin_memory}'
-            )
+            logger.info(f'Setting args.dataloader_pin_memory: {self.dataloader_pin_memory}')
         if 'qwen-audio' in self.model_type:
             assert self.preprocess_num_proc == 1 or self.lazy_tokenize, 'not support'
         model_info = MODEL_MAPPING[self.model_type]
-        support_gradient_checkpointing = model_info.get(
-            'support_gradient_checkpointing', True)
+        support_gradient_checkpointing = model_info.get('support_gradient_checkpointing', True)
         if self.gradient_checkpointing is None:
             self.gradient_checkpointing = support_gradient_checkpointing
         elif not support_gradient_checkpointing and self.gradient_checkpointing:
-            logger.warning(
-                f'{self.model_type} not support gradient_checkpointing.')
+            logger.warning(f'{self.model_type} not support gradient_checkpointing.')
 
         self._init_training_args()
 
         if self.add_output_dir_suffix is None:
             self.add_output_dir_suffix = True
         if self.add_output_dir_suffix:
             self.output_dir = os.path.join(self.output_dir, self.model_type)
             self.output_dir = add_version_to_work_dir(self.output_dir)
             logger.info(f'output_dir: {self.output_dir}')
             self.training_args.output_dir = self.output_dir
             self.training_args.run_name = self.output_dir
-
+        if is_local_master():
+            os.makedirs(self.output_dir, exist_ok=True)
         if self.logging_dir is None:
             self.logging_dir = f'{self.output_dir}/runs'
             self.training_args.logging_dir = self.logging_dir
 
     def _init_training_args(self) -> None:
         additional_saved_files = []
         if self.sft_type == 'full':
-            additional_saved_files = get_additional_saved_files(
-                self.model_type)
+            additional_saved_files = get_additional_saved_files(self.model_type)
 
         kwargs = {}
         if self.neftune_backend != 'swift':
             kwargs['neftune_noise_alpha'] = self.neftune_noise_alpha
 
+        parameters = inspect.signature(Seq2SeqTrainingArguments.__init__).parameters
+        if 'include_num_input_tokens_seen' in parameters:
+            kwargs['include_num_input_tokens_seen'] = self.include_num_input_tokens_seen
+
         training_args = Seq2SeqTrainingArguments(
             output_dir=self.output_dir,
             evaluation_strategy=self.evaluation_strategy,
             logging_dir=self.logging_dir,
             per_device_train_batch_size=self.batch_size,
             per_device_eval_batch_size=self.eval_batch_size,
             gradient_accumulation_steps=self.gradient_accumulation_steps,
@@ -815,27 +724,27 @@
             save_total_limit=self.save_total_limit,
             remove_unused_columns=False,
             bf16=self.bf16,
             fp16=self.fp16,
             eval_steps=self.eval_steps,
             dataloader_num_workers=self.dataloader_num_workers,
             dataloader_pin_memory=self.dataloader_pin_memory,
-            metric_for_best_model='rouge-l'
-            if self.predict_with_generate else 'loss',
+            metric_for_best_model='rouge-l' if self.predict_with_generate else 'loss',
             greater_is_better=self.predict_with_generate,
             sortish_sampler=True,
             optim=self.optim,
             adam_beta1=self.adam_beta1,
             adam_beta2=self.adam_beta2,
             hub_model_id=self.hub_model_id,
             hub_private_repo=self.hub_private_repo,
             push_hub_strategy=self.push_hub_strategy,
             hub_token=self.hub_token,
             push_to_hub=self.push_to_hub,
             resume_from_checkpoint=self.resume_from_checkpoint,
+            ignore_data_skip=self.ignore_data_skip,
             ddp_backend=self.ddp_backend,
             gradient_checkpointing=self.gradient_checkpointing,
             predict_with_generate=self.predict_with_generate,
             # generation_config=generation_config,
             local_rank=get_dist_setting()[1],
             save_only_model=self.save_only_model,
             train_sampler_random=self.train_sampler_random,
@@ -872,65 +781,55 @@
         logger.info('Handle pai compat...')
         pai_tensorboard_dir = get_pai_tensorboard_dir()
         if self.logging_dir is None and pai_tensorboard_dir is not None:
             self.logging_dir = pai_tensorboard_dir
             logger.info(f'Setting args.logging_dir: {self.logging_dir}')
         if self.add_output_dir_suffix is None:
             self.add_output_dir_suffix = False
-            logger.info(
-                f'Setting args.add_output_dir_suffix: {self.add_output_dir_suffix}'
-            )
+            logger.info(f'Setting args.add_output_dir_suffix: {self.add_output_dir_suffix}')
 
 
 @dataclass
 class InferArguments(ArgumentsBase):
     # You can specify the model by either using the model_type or model_id_or_path.
     model_type: Optional[str] = field(
-        default=None,
-        metadata={'help': f'model_type choices: {list(MODEL_MAPPING.keys())}'})
+        default=None, metadata={'help': f'model_type choices: {list(MODEL_MAPPING.keys())}'})
     model_id_or_path: Optional[str] = None
     model_revision: Optional[str] = None
 
-    sft_type: Literal['lora', 'longlora', 'full', 'adalora', 'ia3',
-                      'llamapro'] = 'lora'
+    sft_type: Literal['lora', 'longlora', 'full', 'adalora', 'ia3', 'llamapro'] = 'lora'
     template_type: str = field(
-        default='AUTO',
-        metadata={
-            'help':
-            f"template_type choices: {list(TEMPLATE_MAPPING.keys()) + ['AUTO']}"
-        })
+        default='AUTO', metadata={'help': f"template_type choices: {list(TEMPLATE_MAPPING.keys()) + ['AUTO']}"})
     infer_backend: Literal['AUTO', 'vllm', 'pt'] = 'AUTO'
-    ckpt_dir: Optional[str] = field(
-        default=None, metadata={'help': '/path/to/your/vx-xxx/checkpoint-xxx'})
+    ckpt_dir: Optional[str] = field(default=None, metadata={'help': '/path/to/your/vx-xxx/checkpoint-xxx'})
     load_args_from_ckpt_dir: bool = True
     load_dataset_config: bool = False
     eval_human: Optional[bool] = None
 
     seed: int = 42
     dtype: Literal['bf16', 'fp16', 'fp32', 'AUTO'] = 'AUTO'
 
     dataset: List[str] = field(
-        default_factory=list,
-        metadata={'help': f'dataset choices: {list(DATASET_MAPPING.keys())}'})
+        default_factory=list, metadata={'help': f'dataset choices: {list(DATASET_MAPPING.keys())}'})
     dataset_seed: int = 42
     dataset_test_ratio: float = 0.01
     val_dataset_sample: int = 10  # -1: all dataset
     save_result: bool = True
     system: Optional[str] = None
     max_length: int = -1  # -1: no limit
     truncation_strategy: Literal['delete', 'truncation_left'] = 'delete'
-    check_dataset_strategy: Literal['none', 'discard', 'error',
-                                    'warning'] = 'none'
+    check_dataset_strategy: Literal['none', 'discard', 'error', 'warning'] = 'none'
     custom_train_dataset_path: List[str] = field(default_factory=list)
     custom_val_dataset_path: List[str] = field(default_factory=list)
 
     quantization_bit: Literal[0, 4, 8] = 0
     bnb_4bit_comp_dtype: Literal['fp16', 'bf16', 'fp32', 'AUTO'] = 'AUTO'
     bnb_4bit_quant_type: Literal['fp4', 'nf4'] = 'nf4'
     bnb_4bit_use_double_quant: bool = True
+    bnb_4bit_quant_storage: Optional[str] = None
 
     max_new_tokens: int = 2048
     do_sample: bool = True
     temperature: float = 0.3
     top_k: int = 20
     top_p: float = 0.7
     repetition_penalty: float = 1.
@@ -956,106 +855,86 @@
     # compatibility. (Deprecated)
     show_dataset_sample: int = 10
     safe_serialization: Optional[bool] = None
     model_cache_dir: Optional[str] = None
     merge_lora_and_save: Optional[bool] = None
 
     def __post_init__(self) -> None:
-        if self.ckpt_dir is not None and not self.check_ckpt_dir_correct(
-                self.ckpt_dir):
-            logger.warning(
-                f'The checkpoint dir {self.ckpt_dir} passed in is invalid, please make sure'
-                'the dir contains a `configuration.json` file.')
+        if self.ckpt_dir is not None and not self.check_ckpt_dir_correct(self.ckpt_dir):
+            logger.warning(f'The checkpoint dir {self.ckpt_dir} passed in is invalid, please make sure'
+                           'the dir contains a `configuration.json` file.')
         self.handle_compatibility()
         self.handle_path()
         logger.info(f'ckpt_dir: {self.ckpt_dir}')
         if self.ckpt_dir is None and self.load_args_from_ckpt_dir:
             self.load_args_from_ckpt_dir = False
-            logger.info(
-                'Due to `ckpt_dir` being `None`, `load_args_from_ckpt_dir` is set to `False`.'
-            )
+            logger.info('Due to `ckpt_dir` being `None`, `load_args_from_ckpt_dir` is set to `False`.')
         if self.load_args_from_ckpt_dir:
             self.load_from_ckpt_dir()
         else:
             assert self.load_dataset_config is False, 'You need to first set `--load_args_from_ckpt_dir true`.'
         self.set_model_type()
         if isinstance(self.dataset, str):
             self.dataset = [self.dataset]
         self.register_custom_dataset()
         self.check_flash_attn()
         self.handle_generation_config()
 
         self.torch_dtype, _, _ = self.select_dtype()
         self.prepare_template()
         has_dataset = (
-            len(self.dataset) > 0 or len(self.custom_train_dataset_path) > 0
-            or len(self.custom_val_dataset_path) > 0)
+            len(self.dataset) > 0 or len(self.custom_train_dataset_path) > 0 or len(self.custom_val_dataset_path) > 0)
         if self.eval_human is None:
             if not has_dataset:
                 self.eval_human = True
             else:
                 self.eval_human = False
             logger.info(f'Setting self.eval_human: {self.eval_human}')
         elif self.eval_human is False and not has_dataset:
-            raise ValueError(
-                'Please provide the dataset or set `--load_dataset_config true`.'
-            )
+            raise ValueError('Please provide the dataset or set `--load_dataset_config true`.')
 
-        self.bnb_4bit_compute_dtype, self.load_in_4bit, self.load_in_8bit = self.select_bnb(
-        )
+        self.bnb_4bit_compute_dtype, self.load_in_4bit, self.load_in_8bit = self.select_bnb()
 
         if self.max_length == -1:
             self.max_length = None
         if self.overwrite_generation_config is None:
             if self.ckpt_dir is None:
                 self.overwrite_generation_config = False
             else:
                 self.overwrite_generation_config = True
-            logger.info(
-                f'Setting overwrite_generation_config: {self.overwrite_generation_config}'
-            )
+            logger.info(f'Setting overwrite_generation_config: {self.overwrite_generation_config}')
         if self.ckpt_dir is None:
             self.sft_type = 'full'
 
         self.prepare_vllm()
 
-    def prepare_template(self):
-        if self.template_type == 'AUTO':
-            self.template_type = get_default_template_type(self.model_type)
-            logger.info(f'Setting template_type: {self.template_type}')
-
     def prepare_vllm(self):
         model_info = MODEL_MAPPING[self.model_type]
         support_vllm = model_info.get('support_vllm', False)
         self.vllm_lora_request_list = None
         if self.infer_backend == 'AUTO':
             self.infer_backend = 'pt'
             if is_vllm_available() and support_vllm:
-                if ((self.sft_type == 'full'
-                     or self.sft_type == 'lora' and self.merge_lora)
+                if ((self.sft_type == 'full' or self.sft_type == 'lora' and self.merge_lora)
                         and self.quantization_bit == 0):
                     self.infer_backend = 'vllm'
                 if self.vllm_enable_lora:
                     self.infer_backend = 'vllm'
         if self.infer_backend == 'vllm':
             require_version('vllm')
             assert self.quantization_bit == 0, 'VLLM does not support bnb.'
             if not support_vllm:
                 logger.warning(f'vllm not support `{self.model_type}`')
             if self.sft_type == 'lora' and not self.vllm_enable_lora:
-                assert self.merge_lora is True, (
-                    'To use VLLM, you need to provide the complete weight parameters. '
-                    'Please set `--merge_lora true`.')
+                assert self.merge_lora is True, ('To use VLLM, you need to provide the complete weight parameters. '
+                                                 'Please set `--merge_lora true`.')
             if self.vllm_enable_lora:
                 self.vllm_lora_modules.append(f'default-lora={self.ckpt_dir}')
-                self.vllm_lora_request_list = _parse_vllm_lora_modules(
-                    self.vllm_lora_modules)
-                logger.info(
-                    f'args.vllm_lora_request_list: {self.vllm_lora_request_list}'
-                )
+                self.vllm_lora_request_list = _parse_vllm_lora_modules(self.vllm_lora_modules)
+                logger.info(f'args.vllm_lora_request_list: {self.vllm_lora_request_list}')
         template_info = TEMPLATE_MAPPING[self.template_type]
         if self.num_beams != 1:
             self.stream = False
             logger.info('Setting self.stream: False')
         self.infer_media_type = template_info.get('infer_media_type', 'none')
         if self.merge_device_map is None:
             self.merge_device_map = 'cpu'
@@ -1064,34 +943,32 @@
         sft_args_path = os.path.join(self.ckpt_dir, 'sft_args.json')
         if not os.path.exists(sft_args_path):
             logger.info(f'{sft_args_path} not found')
             return
         with open(sft_args_path, 'r', encoding='utf-8') as f:
             sft_args = json.load(f)
         imported_keys = [
-            'model_type', 'model_revision', 'sft_type', 'template_type',
-            'system', 'quantization_bit', 'bnb_4bit_comp_dtype',
-            'bnb_4bit_quant_type', 'bnb_4bit_use_double_quant'
+            'model_type', 'model_revision', 'sft_type', 'template_type', 'system', 'quantization_bit',
+            'bnb_4bit_comp_dtype', 'bnb_4bit_quant_type', 'bnb_4bit_use_double_quant'
         ]
         if self.load_dataset_config:
             imported_keys += [
-                'dataset', 'dataset_seed', 'dataset_test_ratio',
-                'check_dataset_strategy', 'custom_train_dataset_path',
+                'dataset', 'dataset_seed', 'dataset_test_ratio', 'check_dataset_strategy', 'custom_train_dataset_path',
                 'custom_val_dataset_path'
             ]
         for key in imported_keys:
-            if (key in {
-                    'dataset', 'custom_train_dataset_path',
-                    'custom_val_dataset_path'
-            } and len(getattr(self, key)) > 0):
+            if (key in {'dataset', 'custom_train_dataset_path', 'custom_val_dataset_path'}
+                    and len(getattr(self, key)) > 0):
                 continue
             setattr(self, key, sft_args.get(key))
 
         if self.model_id_or_path is None:
             self.model_id_or_path = sft_args.get('model_id_or_path')
+        if self.dtype == 'AUTO':
+            self.dtype = sft_args.get('dtype')
 
     @staticmethod
     def check_ckpt_dir_correct(ckpt_dir) -> bool:
         """Check the checkpoint dir is correct, which means it must contains a `configuration.json` file.
         Args:
             ckpt_dir: The checkpoint dir
         Returns:
@@ -1117,31 +994,29 @@
     ssl_certfile: Optional[str] = None
 
     def __post_init__(self):
         super().__post_init__()
         model_info = MODEL_MAPPING[self.model_type]
         tags = model_info.get('tags', [])
         if 'multi-modal' in tags:
-            raise ValueError(
-                'Deployment of multimodal models is currently not supported.')
+            raise ValueError('Deployment of multimodal models is currently not supported.')
 
 
 @dataclass
 class EvalArguments(InferArguments):
 
     name: Optional[str] = None
 
     eval_url: Optional[str] = None
 
     eval_token: Optional[str] = 'EMPTY'
 
     eval_is_chat_model: bool = None
 
-    eval_dataset: List[str] = field(
-        default_factory=lambda: ['ceval', 'gsm8k', 'arc'])
+    eval_dataset: List[str] = field(default_factory=lambda: ['ceval', 'gsm8k', 'arc'])
 
     eval_few_shot: Optional[int] = None
 
     eval_limit: Optional[int] = None
 
     custom_eval_config: Optional[str] = None
 
@@ -1179,56 +1054,47 @@
 
     # push to ms hub
     push_to_hub: bool = False
     # 'user_name/repo_name' or 'repo_name'
     hub_model_id: Optional[str] = None
     # None: use env var `MODELSCOPE_API_TOKEN`
     hub_token: Optional[str] = field(
-        default=None,
-        metadata={
-            'help':
-            'SDK token can be found in https://modelscope.cn/my/myaccesstoken'
-        })
+        default=None, metadata={'help': 'SDK token can be found in https://modelscope.cn/my/myaccesstoken'})
     hub_private_repo: bool = False
     commit_message: str = 'update files'
 
     def __post_init__(self):
         if self.merge_device_map is None:
             self.merge_device_map = 'cpu' if self.quant_bits != 0 else 'auto'
         super().__post_init__()
         if len(self.dataset) == 0:
-            self.dataset = ['ms-bench-mini']
+            self.dataset = ['alpaca-zh', 'alpaca-en']
             logger.info(f'Setting args.dataset: {self.dataset}')
 
 
 @dataclass
 class DPOArguments(SftArguments):
 
     ref_model_type: Optional[str] = field(
-        default=None,
-        metadata={'help': f'model_type choices: {list(MODEL_MAPPING.keys())}'})
+        default=None, metadata={'help': f'model_type choices: {list(MODEL_MAPPING.keys())}'})
 
     ref_model_id_or_path: Optional[str] = None
 
     max_prompt_length: int = 1024
     beta: float = 0.1
     label_smoothing: float = 0.0
     loss_type: Literal['sigmoid', 'hinge', 'ipo', 'kto_pair'] = 'sigmoid'
     sft_beta: float = 0.1
 
 
 @dataclass
 class RomeArguments(InferArguments):
     rome_request_file: str = field(
-        default=None,
-        metadata={
-            'help':
-            'The rome request file, please check the documentation '
-            'to get the format'
-        })
+        default=None, metadata={'help': 'The rome request file, please check the documentation '
+                                'to get the format'})
 
     def __post_init__(self) -> None:
         self.handle_compatibility()
         self.handle_path()
         self.set_model_type()
         self.check_flash_attn()
 
@@ -1240,63 +1106,51 @@
         if self.max_length == -1:
             self.max_length = None
 
 
 dtype_mapping_reversed = {v: k for k, v in dtype_mapping.items()}
 
 
-def _check_path(
-        k: str, value: Union[str, List[str]],
-        check_exist_path_set: Optional[Set[str]]) -> Union[str, List[str]]:
+def _check_path(k: str, value: Union[str, List[str]],
+                check_exist_path_set: Optional[Set[str]]) -> Union[str, List[str]]:
     if isinstance(value, str):
         value = os.path.expanduser(value)
         value = os.path.abspath(value)
         if k in check_exist_path_set and not os.path.exists(value):
             raise FileNotFoundError(f"`{k}`: '{value}'")
     elif isinstance(value, list):
         res = []
         for v in value:
             res.append(_check_path(k, v, check_exist_path_set))
         value = res
     return value
 
 
-def _register_local_dataset(dataset_name: str, train_dataset_path: List[str],
-                            val_dataset_path: List[str]) -> None:
+def _register_local_dataset(dataset_name: str, train_dataset_path: List[str], val_dataset_path: List[str]) -> None:
     register_dataset(
-        dataset_name,
-        '_',
-        train_dataset_path,
-        val_dataset_path,
-        get_function=get_custom_dataset,
-        exists_ok=True)
+        dataset_name, '_', train_dataset_path, val_dataset_path, get_function=get_custom_dataset, exists_ok=True)
 
 
 def swift_to_peft_format(lora_checkpoint_path: str) -> str:
     if 'default' in os.listdir(lora_checkpoint_path):  # swift_backend
         new_lora_checkpoint_path = f'{lora_checkpoint_path}-peft'
-        Swift.save_to_peft_format(lora_checkpoint_path,
-                                  new_lora_checkpoint_path)
+        Swift.save_to_peft_format(lora_checkpoint_path, new_lora_checkpoint_path)
         lora_checkpoint_path = new_lora_checkpoint_path
         logger.info('Converting the swift format checkpoint to peft format, '
                     f"and saving it to: '{new_lora_checkpoint_path}'")
     else:
         logger.info('The format of the checkpoint is already in peft format.')
     return lora_checkpoint_path
 
 
-def _parse_vllm_lora_modules(
-        vllm_lora_modules: List[str]) -> List['LoRARequest']:
+def _parse_vllm_lora_modules(vllm_lora_modules: List[str]) -> List['LoRARequest']:
     try:
         from .vllm_utils import LoRARequest
     except ImportError:
-        logger.warning(
-            'The current version of VLLM does not support `enable_lora`. Please upgrade VLLM.'
-        )
+        logger.warning('The current version of VLLM does not support `enable_lora`. Please upgrade VLLM.')
         raise
     lora_request_list = []
     for i, vllm_lora_module in enumerate(vllm_lora_modules):
         lora_name, lora_local_path = vllm_lora_module.split('=')
         lora_local_path = swift_to_peft_format(lora_local_path)
-        lora_request_list.append(
-            LoRARequest(lora_name, i + 1, lora_local_path))
+        lora_request_list.append(LoRARequest(lora_name, i + 1, lora_local_path))
     return lora_request_list
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/client_utils.py` & `ms-swift-2.0.4/swift/llm/utils/client_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -2,23 +2,21 @@
 
 import json
 import requests
 from dacite import from_dict
 from requests.exceptions import HTTPError
 
 from .model import get_default_template_type
-from .protocol import (ChatCompletionResponse, ChatCompletionStreamResponse,
-                       CompletionResponse, CompletionStreamResponse, ModelList,
-                       XRequestConfig)
+from .protocol import (ChatCompletionResponse, ChatCompletionStreamResponse, CompletionResponse,
+                       CompletionStreamResponse, ModelList, XRequestConfig)
 from .template import History
 from .utils import history_to_messages
 
 
-def get_model_list_client(host: str = '127.0.0.1',
-                          port: str = '8000') -> ModelList:
+def get_model_list_client(host: str = '127.0.0.1', port: str = '8000') -> ModelList:
     url = f'http://{host}:{port}/v1/models'
     resp_obj = requests.get(url).json()
     return from_dict(ModelList, resp_obj)
 
 
 def _parse_stream_data(data: bytes) -> Optional[str]:
     data = data.decode(encoding='utf-8')
@@ -35,45 +33,39 @@
     history: Optional[History] = None,
     system: Optional[str] = None,
     *,
     request_config: Optional[XRequestConfig] = None,
     host: str = '127.0.0.1',
     port: str = '8000',
     is_chat_request: Optional[bool] = None,
-) -> Union[ChatCompletionResponse, CompletionResponse,
-           Iterator[ChatCompletionStreamResponse],
+) -> Union[ChatCompletionResponse, CompletionResponse, Iterator[ChatCompletionStreamResponse],
            Iterator[CompletionStreamResponse]]:
     if request_config is None:
         request_config = XRequestConfig()
     if is_chat_request is None:
         template_type = get_default_template_type(model_type)
         is_chat_request = 'generation' not in template_type
-    data = {
-        k: v
-        for k, v in request_config.__dict__.items() if not k.startswith('__')
-    }
+    data = {k: v for k, v in request_config.__dict__.items() if not k.startswith('__')}
     data['model'] = model_type
     if is_chat_request:
         data['messages'] = history_to_messages(history, query, system)
         url = f'http://{host}:{port}/v1/chat/completions'
     else:
         assert system is None and history is None, (
-            'The chat template for text generation does not support system and history.'
-        )
+            'The chat template for text generation does not support system and history.')
         data['prompt'] = query
         url = f'http://{host}:{port}/v1/completions'
     if request_config.stream:
         if is_chat_request:
             ret_cls = ChatCompletionStreamResponse
         else:
             ret_cls = CompletionStreamResponse
         resp = requests.post(url, json=data, stream=True)
 
-        def _gen_stream() -> Union[Iterator[ChatCompletionStreamResponse],
-                                   Iterator[CompletionStreamResponse]]:
+        def _gen_stream() -> Union[Iterator[ChatCompletionStreamResponse], Iterator[CompletionStreamResponse]]:
             for data in resp.iter_lines():
                 data = _parse_stream_data(data)
                 if data == '[DONE]':
                     break
                 if data is not None:
                     resp_obj = json.loads(data)
                     if resp_obj['object'] == 'error':
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/dataset.py` & `ms-swift-2.0.4/swift/llm/utils/dataset.py`

 * *Files 6% similar despite different names*

```diff
@@ -11,38 +11,31 @@
 from datasets import Dataset as HfDataset
 from datasets import concatenate_datasets, load_dataset
 from numpy.random import RandomState
 from pandas import DataFrame
 from tqdm.auto import tqdm
 from transformers.utils import strtobool
 
-from swift.utils import (get_logger, get_seed, is_dist, is_local_master,
-                         read_from_jsonl, transform_jsonl_to_df)
-from .preprocess import (AlpacaPreprocessor, ClsPreprocessor,
-                         ComposePreprocessor, ConversationsPreprocessor,
-                         PreprocessFunc, RenameColumnsPreprocessor,
-                         SmartPreprocessor, TextGenerationPreprocessor)
+from swift.utils import get_logger, get_seed, is_dist, is_local_master, read_from_jsonl, transform_jsonl_to_df
+from .preprocess import (AlpacaPreprocessor, ClsPreprocessor, ComposePreprocessor, ConversationsPreprocessor,
+                         PreprocessFunc, RenameColumnsPreprocessor, SmartPreprocessor, TextGenerationPreprocessor)
 from .template import History
 from .utils import download_dataset
 
 
 def _remove_useless_columns(dataset: HfDataset) -> HfDataset:
     k_list = []
     for k in dataset.features.keys():
-        if k in {
-                'query', 'response', 'rejected_response', 'system', 'history',
-                'images'
-        }:
+        if k in {'query', 'response', 'rejected_response', 'system', 'history', 'images'}:
             k_list.append(k)
     dataset = dataset.select_columns(k_list)
     return dataset
 
 
-GetDatasetFunction = Callable[[], Union[HfDataset, Tuple[HfDataset,
-                                                         Optional[HfDataset]]]]
+GetDatasetFunction = Callable[[], Union[HfDataset, Tuple[HfDataset, Optional[HfDataset]]]]
 SubsetSplit = Union[str, Tuple[str, str], List[str]]
 DATASET_MAPPING: Dict[str, Dict[str, Any]] = {}
 
 logger = get_logger()
 
 
 class DatasetName:
@@ -174,33 +167,29 @@
         for k in cls.__dict__.keys():
             if k.startswith('__') or k == 'get_dataset_name_list':
                 continue
             res.append(cls.__dict__[k])
         return res
 
 
-def register_dataset(
-        dataset_name: str,
-        dataset_id_or_path: str,
-        train_subset_split_list: Optional[List[SubsetSplit]] = None,
-        val_subset_split_list: Optional[List[SubsetSplit]] = None,
-        preprocess_func: Optional[PreprocessFunc] = None,
-        get_function: Optional[GetDatasetFunction] = None,
-        *,
-        hf_dataset_id: Optional[str] = None,
-        function_kwargs: Optional[Dict[str, Any]] = None,
-        exists_ok: bool = False,
-        **kwargs
-) -> Optional[Callable[[GetDatasetFunction], GetDatasetFunction]]:
+def register_dataset(dataset_name: str,
+                     dataset_id_or_path: str,
+                     train_subset_split_list: Optional[List[SubsetSplit]] = None,
+                     val_subset_split_list: Optional[List[SubsetSplit]] = None,
+                     preprocess_func: Optional[PreprocessFunc] = None,
+                     get_function: Optional[GetDatasetFunction] = None,
+                     *,
+                     hf_dataset_id: Optional[str] = None,
+                     function_kwargs: Optional[Dict[str, Any]] = None,
+                     exists_ok: bool = False,
+                     **kwargs) -> Optional[Callable[[GetDatasetFunction], GetDatasetFunction]]:
     if preprocess_func is None:
         preprocess_func = SmartPreprocessor()
     if not exists_ok and dataset_name in DATASET_MAPPING:
-        raise ValueError(
-            f'The `{dataset_name}` has already been registered in the DATASET_MAPPING.'
-        )
+        raise ValueError(f'The `{dataset_name}` has already been registered in the DATASET_MAPPING.')
     if train_subset_split_list is None:
         train_subset_split_list = []
     if val_subset_split_list is None:
         val_subset_split_list = []
     if function_kwargs is None:
         function_kwargs = {}
 
@@ -215,58 +204,48 @@
     if get_function is not None:
         if len(function_kwargs) > 0:
             get_function = partial(get_function, **function_kwargs)
         dataset_info['get_function'] = get_function
         DATASET_MAPPING[dataset_name] = dataset_info
         return
 
-    def _register_dataset(
-            get_function: GetDatasetFunction) -> GetDatasetFunction:
+    def _register_dataset(get_function: GetDatasetFunction) -> GetDatasetFunction:
         _old_get_function = get_function
         if len(function_kwargs) > 0:
             get_function = partial(get_function, **function_kwargs)
         dataset_info['get_function'] = get_function
         DATASET_MAPPING[dataset_name] = dataset_info
         return _old_get_function
 
     return _register_dataset
 
 
-def load_ms_dataset(
-        dataset_id: str,
-        subset_split_list: Optional[List[SubsetSplit]]) -> Optional[HfDataset]:
+def load_ms_dataset(dataset_id: str, subset_split_list: Optional[List[SubsetSplit]]) -> Optional[HfDataset]:
     from modelscope import MsDataset
     if subset_split_list is None or len(subset_split_list) == 0:
         return None
     dataset_list = []
     for subset_split in subset_split_list:
         if isinstance(subset_split, str):
             subset_split = ('default', subset_split)
         assert len(subset_split) == 2
         subset_name, split = subset_split
         if is_dist() and not is_local_master():
             force_redownload = False
         else:
-            force_redownload = strtobool(
-                os.environ.get('FORCE_REDOWNLOAD', 'False'))
+            force_redownload = strtobool(os.environ.get('FORCE_REDOWNLOAD', 'False'))
         download_mode = 'force_redownload' if force_redownload else 'reuse_dataset_if_exists'
-        dataset = MsDataset.load(
-            dataset_id,
-            subset_name=subset_name,
-            split=split,
-            download_mode=download_mode)
+        dataset = MsDataset.load(dataset_id, subset_name=subset_name, split=split, download_mode=download_mode)
         if hasattr(dataset, 'to_hf_dataset'):
             dataset = dataset.to_hf_dataset()
         dataset_list.append(dataset)
     return concatenate_datasets(dataset_list)
 
 
-def load_hf_dataset(
-        dataset_id: str,
-        subset_split_list: Optional[List[SubsetSplit]]) -> Optional[HfDataset]:
+def load_hf_dataset(dataset_id: str, subset_split_list: Optional[List[SubsetSplit]]) -> Optional[HfDataset]:
     if subset_split_list is None or len(subset_split_list) == 0:
         return None
     dataset_list = []
     for subset_split in subset_split_list:
         if isinstance(subset_split, str):
             subset_split = (None, subset_split)
         assert len(subset_split) == 2
@@ -282,30 +261,19 @@
     tags=['chat', 'sql'],
     hf_dataset_id='Clinton/texttosqlv2_25000_v2')
 @register_dataset(
     DatasetName.school_math_zh,
     'AI-ModelScope/school_math_0.25M', ['train'],
     tags=['chat', 'math'],
     hf_dataset_id='BelleGroup/school_math_0.25M')
-@register_dataset(
-    DatasetName.gpt4all_en,
-    'wyj123456/GPT4all', ['train'],
-    tags=['chat', 'general'])
-@register_dataset(
-    DatasetName.cot_zh, 'YorickHe/CoT_zh', ['train'], tags=['chat', 'general'])
-@register_dataset(
-    DatasetName.cot_en, 'YorickHe/CoT', ['train'], tags=['chat', 'general'])
-@register_dataset(
-    DatasetName.instinwild_en,
-    'wyj123456/instinwild', [('subset', 'train')],
-    tags=['chat', 'general'])
-@register_dataset(
-    DatasetName.instinwild_zh,
-    'wyj123456/instinwild', ['train'],
-    tags=['chat', 'general'])
+@register_dataset(DatasetName.gpt4all_en, 'wyj123456/GPT4all', ['train'], tags=['chat', 'general'])
+@register_dataset(DatasetName.cot_zh, 'YorickHe/CoT_zh', ['train'], tags=['chat', 'general'])
+@register_dataset(DatasetName.cot_en, 'YorickHe/CoT', ['train'], tags=['chat', 'general'])
+@register_dataset(DatasetName.instinwild_en, 'wyj123456/instinwild', [('subset', 'train')], tags=['chat', 'general'])
+@register_dataset(DatasetName.instinwild_zh, 'wyj123456/instinwild', ['train'], tags=['chat', 'general'])
 @register_dataset(
     DatasetName.code_alpaca_en,
     'wyj123456/code_alpaca_en', ['train'],
     tags=['chat', 'coding'],
     hf_dataset_id='sahil2801/CodeAlpaca-20k')
 @register_dataset(
     DatasetName.finance_en,
@@ -318,81 +286,50 @@
     tags=['chat', 'general', ''],
     hf_dataset_id='vicgalle/alpaca-gpt4')
 @register_dataset(
     DatasetName.coig_cqia_chinese_traditional,
     'AI-ModelScope/COIG-CQIA', [('chinese_traditional', 'train')],
     tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_coig_pc,
-    'AI-ModelScope/COIG-CQIA', [('coig_pc', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_coig_pc, 'AI-ModelScope/COIG-CQIA', [('coig_pc', 'train')], tags=['general', ''])
+@register_dataset(DatasetName.coig_cqia_exam, 'AI-ModelScope/COIG-CQIA', [('exam', 'train')], tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_exam,
-    'AI-ModelScope/COIG-CQIA', [('exam', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_finance, 'AI-ModelScope/COIG-CQIA', [('finance', 'train')], tags=['general', ''])
+@register_dataset(DatasetName.coig_cqia_douban, 'AI-ModelScope/COIG-CQIA', [('douban', 'train')], tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_finance,
-    'AI-ModelScope/COIG-CQIA', [('finance', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_human_value, 'AI-ModelScope/COIG-CQIA', [('human_value', 'train')], tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_douban,
-    'AI-ModelScope/COIG-CQIA', [('douban', 'train')],
-    tags=['general', ''])
-@register_dataset(
-    DatasetName.coig_cqia_human_value,
-    'AI-ModelScope/COIG-CQIA', [('human_value', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_logi_qa, 'AI-ModelScope/COIG-CQIA', [('logi_qa', 'train')], tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_logi_qa,
-    'AI-ModelScope/COIG-CQIA', [('logi_qa', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_ruozhiba, 'AI-ModelScope/COIG-CQIA', [('ruozhiba', 'train')], tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_ruozhiba,
-    'AI-ModelScope/COIG-CQIA', [('ruozhiba', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_segmentfault, 'AI-ModelScope/COIG-CQIA', [('segmentfault', 'train')], tags=['general', ''])
+@register_dataset(DatasetName.coig_cqia_wiki, 'AI-ModelScope/COIG-CQIA', [('wiki', 'train')], tags=['general', ''])
 @register_dataset(
-    DatasetName.coig_cqia_segmentfault,
-    'AI-ModelScope/COIG-CQIA', [('segmentfault', 'train')],
-    tags=['general', ''])
-@register_dataset(
-    DatasetName.coig_cqia_wiki,
-    'AI-ModelScope/COIG-CQIA', [('wiki', 'train')],
-    tags=['general', ''])
-@register_dataset(
-    DatasetName.coig_cqia_wikihow,
-    'AI-ModelScope/COIG-CQIA', [('wikihow', 'train')],
-    tags=['general', ''])
-@register_dataset(
-    DatasetName.coig_cqia_xhs,
-    'AI-ModelScope/COIG-CQIA', [('xhs', 'train')],
-    tags=['general', ''])
-@register_dataset(
-    DatasetName.coig_cqia_zhihu,
-    'AI-ModelScope/COIG-CQIA', [('zhihu', 'train')],
-    tags=['general', ''])
+    DatasetName.coig_cqia_wikihow, 'AI-ModelScope/COIG-CQIA', [('wikihow', 'train')], tags=['general', ''])
+@register_dataset(DatasetName.coig_cqia_xhs, 'AI-ModelScope/COIG-CQIA', [('xhs', 'train')], tags=['general', ''])
+@register_dataset(DatasetName.coig_cqia_zhihu, 'AI-ModelScope/COIG-CQIA', [('zhihu', 'train')], tags=['general', ''])
 @register_dataset(
     DatasetName.ms_agent_for_agentfabric_default,
     'AI-ModelScope/ms_agent_for_agentfabric', [('default', 'train')],
     tags=['chat', 'agent', 'multi-round'])
 @register_dataset(
     DatasetName.ms_agent_for_agentfabric_addition,
     'AI-ModelScope/ms_agent_for_agentfabric', [('addition', 'train')],
     tags=['chat', 'agent', 'multi-round'])
-def get_dataset_from_repo(
-        dataset_id: str,
-        train_subset_split_list: List[SubsetSplit],
-        val_subset_split_list: Optional[List[SubsetSplit]],
-        preprocess_func: PreprocessFunc,
-        remove_useless_columns: bool = True,
-        train_dataset_sample: int = -1,
-        val_dataset_sample: int = -1,
-        use_hf: bool = False) -> Tuple[HfDataset, Optional[HfDataset]]:
+def get_dataset_from_repo(dataset_id: str,
+                          train_subset_split_list: List[SubsetSplit],
+                          val_subset_split_list: Optional[List[SubsetSplit]],
+                          preprocess_func: PreprocessFunc,
+                          remove_useless_columns: bool = True,
+                          train_dataset_sample: int = -1,
+                          val_dataset_sample: int = -1,
+                          use_hf: bool = False) -> Tuple[HfDataset, Optional[HfDataset]]:
     dataset_list = []
-    _iter = zip([train_subset_split_list, val_subset_split_list],
-                [train_dataset_sample, val_dataset_sample])
+    _iter = zip([train_subset_split_list, val_subset_split_list], [train_dataset_sample, val_dataset_sample])
     for subset_split_list, dataset_sample in _iter:
         if use_hf:
             dataset = load_hf_dataset(dataset_id, subset_split_list)
         else:
             dataset = load_ms_dataset(dataset_id, subset_split_list)
         if dataset is not None:
             if dataset_sample > 0 and len(dataset) > dataset_sample:
@@ -402,22 +339,19 @@
             dataset = preprocess_func(dataset)
             if remove_useless_columns:
                 dataset = _remove_useless_columns(dataset)
         dataset_list.append(dataset)
     return tuple(dataset_list)
 
 
-_multi_alpaca_subset_list = [
-    'ar', 'de', 'es', 'fr', 'id', 'ja', 'ko', 'pt', 'ru', 'th', 'vi'
-]
+_multi_alpaca_subset_list = ['ar', 'de', 'es', 'fr', 'id', 'ja', 'ko', 'pt', 'ru', 'th', 'vi']
 
 register_dataset(
     DatasetName.multi_alpaca_all,
-    'damo/nlp_polylm_multialpaca_sft',
-    [(subset, 'train') for subset in _multi_alpaca_subset_list],
+    'damo/nlp_polylm_multialpaca_sft', [(subset, 'train') for subset in _multi_alpaca_subset_list],
     None,
     None,
     get_dataset_from_repo,
     tags=['chat', 'general', 'multilingual'],
     help="""language_list
     Language-key	Language	# examples
     ar	Arabic	14,671
@@ -466,24 +400,22 @@
         response.append(d[response_key])
     dataset = HfDataset.from_dict({'query': query, 'response': response})
     return dataset
 
 
 register_dataset(
     DatasetName.coco_en,
-    'modelscope/coco_2014_caption', [('coco_2014_caption', 'train')],
-    [('coco_2014_caption', 'validation')],
+    'modelscope/coco_2014_caption', [('coco_2014_caption', 'train')], [('coco_2014_caption', 'validation')],
     _preprocess_vision_dataset,
     get_dataset_from_repo,
     tags=['chat', 'multi-modal', 'vision'])
 
 register_dataset(
     DatasetName.coco_mini_en,
-    'modelscope/coco_2014_caption', [('coco_2014_caption', 'train')],
-    [('coco_2014_caption', 'validation')],
+    'modelscope/coco_2014_caption', [('coco_2014_caption', 'train')], [('coco_2014_caption', 'validation')],
     _preprocess_vision_dataset,
     get_dataset_from_repo,
     function_kwargs={
         'train_dataset_sample': 20000,
         'val_dataset_sample': 200
     },
     tags=['chat', 'multi-modal', 'vision', ''])
@@ -498,25 +430,20 @@
     response = []
     images = []
     for d in tqdm(dataset):
         images.append([d[image_key]['path']])
         if '&&' in d[response_key]:
             d[response_key] = d[response_key].split('&&')[0]
         response.append(d[response_key])
-    return HfDataset.from_dict({
-        'query': [query] * len(response),
-        'response': response,
-        'images': images
-    })
+    return HfDataset.from_dict({'query': [query] * len(response), 'response': response, 'images': images})
 
 
 register_dataset(
     DatasetName.coco_mini_en_2,
-    'modelscope/coco_2014_caption', [('coco_2014_caption', 'train')],
-    [('coco_2014_caption', 'validation')],
+    'modelscope/coco_2014_caption', [('coco_2014_caption', 'train')], [('coco_2014_caption', 'validation')],
     _preprocess_vision_dataset2,
     get_dataset_from_repo,
     function_kwargs={
         'train_dataset_sample': 20000,
         'val_dataset_sample': 200
     },
     tags=['chat', 'multi-modal', 'vision', ''])
@@ -534,31 +461,29 @@
         response.append(d[response_key].replace(' ', ''))
     dataset = HfDataset.from_dict({'query': query, 'response': response})
     return dataset
 
 
 register_dataset(
     DatasetName.aishell1_zh,
-    'speech_asr/speech_asr_aishell1_trainsets', ['train', 'validation'],
-    ['test'],
+    'speech_asr/speech_asr_aishell1_trainsets', ['train', 'validation'], ['test'],
     _preprocess_aishell1_dataset,
     get_dataset_from_repo,
     tags=['chat', 'multi-modal', 'audio'])
 
 register_dataset(
     DatasetName.aishell1_mini_zh,
     'speech_asr/speech_asr_aishell1_trainsets', ['validation'], ['test'],
     _preprocess_aishell1_dataset,
     get_dataset_from_repo,
     function_kwargs={'val_dataset_sample': 200},
     tags=['chat', 'multi-modal', 'audio', ''])
 
 
-def _repair_agent_conversations(conversations: str,
-                                use_mini: bool) -> List[Dict[str, str]]:
+def _repair_agent_conversations(conversations: str, use_mini: bool) -> List[Dict[str, str]]:
     if use_mini:
         pattern = r'\d\. {"plugin_name": "(.+?)"'
     else:
         pattern = r'\d\. {"(?:plugin_)?name": "(.+?)"'
 
     idx = conversations.find(r"'from': 'user")
     if idx == -1:
@@ -574,21 +499,20 @@
     return conversations
 
 
 def _repair_ms_bench(conversations: str) -> List[Dict[str, str]]:
     if isinstance(conversations, str):
         conversations = ast.literal_eval(conversations)
     default_system = 'You are a helpful assistant.'
-    if conversations[0]['from'] == 'system' and conversations[0][
-            'value'] == default_system:
+    if conversations[0]['from'] == 'system' and conversations[0]['value'] == default_system:
         conversations.pop(0)
     # skip MOSS
     for c in conversations:
         value = c['value'].lower()
-        if 'moss' in value or 'human:' in value or 'assistant:' in value:
+        if 'moss' in value or 'human:' in value or 'assistant:' in value or 'user:' in value:
             return
     return conversations
 
 
 def long_alpaca_preprocessor(dataset: HfDataset):
 
     def map_row(row):
@@ -645,24 +569,22 @@
     _preprocess_ruozhiba,
     get_dataset_from_repo,
     tags=['pretrain', ''])
 
 register_dataset(
     DatasetName.ms_bench,
     'iic/ms_bench', ['train'], [],
-    ConversationsPreprocessor(
-        repair_conversations=_repair_ms_bench, error_strategy='delete'),
+    ConversationsPreprocessor(repair_conversations=_repair_ms_bench, error_strategy='delete'),
     get_dataset_from_repo,
     tags=['chat', 'general', 'multi-round', ''])
 
 register_dataset(
     DatasetName.ms_bench_mini,
     'iic/ms_bench', ['train'], [],
-    ConversationsPreprocessor(
-        repair_conversations=_repair_ms_bench, error_strategy='delete'),
+    ConversationsPreprocessor(repair_conversations=_repair_ms_bench, error_strategy='delete'),
     get_dataset_from_repo,
     function_kwargs={'train_dataset_sample': 20000},
     tags=['chat', 'general', 'multi-round', ''])
 
 register_dataset(
     DatasetName.ms_agent,
     'iic/ms_agent', ['train'], [],
@@ -670,27 +592,22 @@
     get_dataset_from_repo,
     tags=['chat', 'agent', 'multi-round', ''])
 
 register_dataset(
     DatasetName.damo_agent_mini_zh,
     'damo/MSAgent-Bench', ['train'], ['validation'],
     ConversationsPreprocessor(
-        repair_conversations=partial(
-            _repair_agent_conversations, use_mini=True),
-        error_strategy='delete'),
+        repair_conversations=partial(_repair_agent_conversations, use_mini=True), error_strategy='delete'),
     get_dataset_from_repo,
     tags=['chat', 'agent', 'multi-round'])
 register_dataset(
     DatasetName.damo_agent_zh,
     'damo/MSAgent-Bench', ['train'], ['validation'],
     ConversationsPreprocessor(
-        repair_conversations=partial(
-            _repair_agent_conversations,
-            use_mini=False,
-            error_strategy='delete')),
+        repair_conversations=partial(_repair_agent_conversations, use_mini=False, error_strategy='delete')),
     get_dataset_from_repo,
     tags=['chat', 'agent', 'multi-round'])
 
 register_dataset(
     DatasetName.deepctrl_sft_zh,
     'AI-ModelScope/deepctrl-sft-data', [['default', 'train']],
     None,
@@ -714,24 +631,22 @@
     'lvjianjin/AdvertiseGen', ['train'], ['validation'],
     TextGenerationPreprocessor(advertise_gen_prompt, 'content', 'summary'),
     get_dataset_from_repo,
     tags=['text-generation', ''],
     hf_dataset_id='shibing624/AdvertiseGen')
 
 _firefly_kind_list = [
-    'ProseGeneration', 'MRC', 'JinYongGeneration', 'TextCorrection',
-    'ClassicalChinese', 'BELLE', 'StoryGeneration', 'Couplet', 'Cot',
-    'Dictionary', 'Translation', 'Program', 'SentimentAnalyze', 'OpenQA',
-    'AncientPoem', 'TextMatching', 'NLI', 'Summary', 'KeywordRecognition',
-    'ProductDesc', 'LyricGeneration', 'Composition', 'MusicComment', 'NER'
+    'ProseGeneration', 'MRC', 'JinYongGeneration', 'TextCorrection', 'ClassicalChinese', 'BELLE', 'StoryGeneration',
+    'Couplet', 'Cot', 'Dictionary', 'Translation', 'Program', 'SentimentAnalyze', 'OpenQA', 'AncientPoem',
+    'TextMatching', 'NLI', 'Summary', 'KeywordRecognition', 'ProductDesc', 'LyricGeneration', 'Composition',
+    'MusicComment', 'NER'
 ]
 
 
-def _preprocess_firefly(dataset: List[Dict[str, str]],
-                        kind_list: List[str]) -> HfDataset:
+def _preprocess_firefly(dataset: List[Dict[str, str]], kind_list: List[str]) -> HfDataset:
     kind_set = set(kind_list)
     query: List[str] = []
     response: List[str] = []
     for d in tqdm(dataset):
         if d['kind'] not in kind_set:
             continue
         query.append(d['input'])
@@ -745,16 +660,15 @@
 
 @register_dataset(
     DatasetName.firefly_all_zh,
     'wyj123456/firefly',
     preprocess_func=_preprocess_firefly,
     tags=['chat', 'general'],
     function_kwargs={'kind_list': _firefly_kind_list})
-def get_firefly_zh_dataset(dataset_id: str, preprocess_func,
-                           kind_list: List[str], **kwargs) -> HfDataset:
+def get_firefly_zh_dataset(dataset_id: str, preprocess_func, kind_list: List[str], **kwargs) -> HfDataset:
     file = 'firefly-train-1.1M.jsonl'
     dataset_dir = download_dataset(dataset_id, [file])
     fpath = os.path.join(dataset_dir, file)
     with open(fpath, 'r', encoding='utf-8') as f:
         text = f.read()
         text = text.replace('}{', '},{')
         text = f'[{text}]'
@@ -779,38 +693,35 @@
     }),
     get_dataset_from_repo,
     tags=['chat', 'general'])
 
 register_dataset(
     DatasetName.cmnli_zh,
     'clue', [('cmnli', 'train')], [('cmnli', 'validation')],
-    ClsPreprocessor(['neutral', 'entailment', 'contradiction'],
-                    'Natural Language Inference', True),
+    ClsPreprocessor(['neutral', 'entailment', 'contradiction'], 'Natural Language Inference', True),
     get_dataset_from_repo,
     tags=['text-generation', 'classification'],
     hf_dataset_id='clue')
 
 register_dataset(
     DatasetName.cmnli_mini_zh,
     'clue', [('cmnli', 'train')], [('cmnli', 'validation')],
-    ClsPreprocessor(['neutral', 'entailment', 'contradiction'],
-                    'Natural Language Inference', True),
+    ClsPreprocessor(['neutral', 'entailment', 'contradiction'], 'Natural Language Inference', True),
     get_dataset_from_repo,
     function_kwargs={
         'train_dataset_sample': 20000,
         'val_dataset_sample': 200
     },
     tags=['text-generation', 'classification', ''],
     hf_dataset_id='clue')
 
 register_dataset(
     DatasetName.jd_sentiment_zh,
     'DAMO_NLP/jd', ['train'], ['validation'],
-    ClsPreprocessor(['negative', 'positive'], 'Sentiment Classification',
-                    False),
+    ClsPreprocessor(['negative', 'positive'], 'Sentiment Classification', False),
     get_dataset_from_repo,
     tags=['text-generation', 'classification', ''])
 
 
 def _preprocess_dureader_robust(dataset: HfDataset) -> HfDataset:
     prompt = """Task: Question Generation
 Context: {context}
@@ -831,16 +742,15 @@
     'modelscope/DuReader_robust-QG', ['train', 'validation'], ['test'],
     _preprocess_dureader_robust,
     get_dataset_from_repo,
     tags=['text-generation', ''])
 
 register_dataset(
     DatasetName.medical_en,
-    'huangjintao/medical_zh', [('en', 'train'), ('en', 'val')],
-    [('en', 'test')],
+    'huangjintao/medical_zh', [('en', 'train'), ('en', 'val')], [('en', 'test')],
     RenameColumnsPreprocessor({
         'input': 'query',
         'output': 'response'
     }),
     get_dataset_from_repo,
     tags=['chat', 'medical'])
 
@@ -874,22 +784,16 @@
             'rejected_response': sample['rejected'][len(prompt):],
         }
 
     def reorganize_row(row):
         import re
         chosen = row['chosen'].strip()
         rejected = row['rejected'].strip()
-        parts_chosen = [
-            s.strip()
-            for s in re.split('\n\nHuman:|\n\nAssistant:|\n\nHum:', chosen)
-        ]
-        parts_rejected = [
-            s.strip()
-            for s in re.split('\n\nHuman:|\n\nAssistant:|\n\nHum:', rejected)
-        ]
+        parts_chosen = [s.strip() for s in re.split('\n\nHuman:|\n\nAssistant:|\n\nHum:', chosen)]
+        parts_rejected = [s.strip() for s in re.split('\n\nHuman:|\n\nAssistant:|\n\nHum:', rejected)]
         if parts_chosen[0].startswith('Human:'):
             assert parts_rejected[0].startswith('Human:')
             parts_chosen[0] = parts_chosen[0][6:].strip()
             parts_rejected[0] = parts_rejected[0][6:].strip()
         history = []
         idx, s1, s2 = None, None, None
         for idx, (s1, s2) in enumerate(zip(parts_chosen, parts_rejected)):
@@ -915,54 +819,48 @@
         return {
             'query': query,
             'response': response,
             'rejected_response': rejected_response,
             'history': history,
         }
 
-    return dataset.map(reorganize_row).filter(
-        lambda row: row['query'] is not None)
+    return dataset.map(reorganize_row).filter(lambda row: row['query'] is not None)
 
 
 register_dataset(
     DatasetName.hh_rlhf_harmless_base,
-    'AI-ModelScope/hh-rlhf', [('harmless-base', 'train')],
-    [('harmless-base', 'test')],
+    'AI-ModelScope/hh-rlhf', [('harmless-base', 'train')], [('harmless-base', 'test')],
     process_hh_rlhf,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_helpful_base,
-    'AI-ModelScope/hh-rlhf', [('helpful-base', 'train')],
-    [('helpful-base', 'test')],
+    'AI-ModelScope/hh-rlhf', [('helpful-base', 'train')], [('helpful-base', 'test')],
     process_hh_rlhf,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_helpful_online,
-    'AI-ModelScope/hh-rlhf', [('helpful-online', 'train')],
-    [('helpful-online', 'test')],
+    'AI-ModelScope/hh-rlhf', [('helpful-online', 'train')], [('helpful-online', 'test')],
     process_hh_rlhf,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_helpful_rejection_sampled,
-    'AI-ModelScope/hh-rlhf', [('helpful-rejection-sampled', 'train')],
-    [('helpful-rejection-sampled', 'test')],
+    'AI-ModelScope/hh-rlhf', [('helpful-rejection-sampled', 'train')], [('helpful-rejection-sampled', 'test')],
     process_hh_rlhf,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_red_team_attempts,
-    'AI-ModelScope/hh-rlhf', [('red-team-attempts', 'train')],
-    [('red-team-attempts', 'test')],
+    'AI-ModelScope/hh-rlhf', [('red-team-attempts', 'train')], [('red-team-attempts', 'test')],
     process_hh_rlhf,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 
 def process_hh_rlhf_cn(dataset):
 
@@ -1012,72 +910,65 @@
                 row['chosen'] = ast.literal_eval(row['chosen'])
             if isinstance(row['rejected'], str):
                 row['rejected'] = ast.literal_eval(row['rejected'])
             return True
         except:  # noqa
             return False
 
-    return dataset.filter(row_can_be_parsed).map(reorganize_row).filter(
-        lambda row: row['query'])
+    return dataset.filter(row_can_be_parsed).map(reorganize_row).filter(lambda row: row['query'])
 
 
 register_dataset(
     DatasetName.hh_rlhf_cn,
     'AI-ModelScope/hh_rlhf_cn', [('hh_rlhf', 'train')], [('hh_rlhf', 'test')],
     process_hh_rlhf_cn,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise', ''])
 
 register_dataset(
     DatasetName.hh_rlhf_cn_harmless_base_cn,
-    'AI-ModelScope/hh_rlhf_cn', [('harmless_base_cn', 'train')],
-    [('harmless_base_cn', 'test')],
+    'AI-ModelScope/hh_rlhf_cn', [('harmless_base_cn', 'train')], [('harmless_base_cn', 'test')],
     process_hh_rlhf_cn,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_cn_harmless_base_en,
-    'AI-ModelScope/hh_rlhf_cn', [('harmless_base_en', 'train')],
-    [('harmless_base_en', 'test')],
+    'AI-ModelScope/hh_rlhf_cn', [('harmless_base_en', 'train')], [('harmless_base_en', 'test')],
     process_hh_rlhf_cn,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_cn_helpful_base_cn,
-    'AI-ModelScope/hh_rlhf_cn', [('helpful_base_cn', 'train')],
-    [('helpful_base_cn', 'test')],
+    'AI-ModelScope/hh_rlhf_cn', [('helpful_base_cn', 'train')], [('helpful_base_cn', 'test')],
     process_hh_rlhf_cn,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.hh_rlhf_cn_helpful_base_en,
-    'AI-ModelScope/hh_rlhf_cn', [('helpful_base_en', 'train')],
-    [('helpful_base_en', 'test')],
+    'AI-ModelScope/hh_rlhf_cn', [('helpful_base_en', 'train')], [('helpful_base_en', 'test')],
     process_hh_rlhf_cn,
     get_dataset_from_repo,
     tags=['rlhf', 'dpo', 'pairwise'])
 
 register_dataset(
     DatasetName.medical_zh,
-    'huangjintao/medical_zh', [('zh', 'train'), ('zh', 'val')],
-    [('zh', 'test')],
+    'huangjintao/medical_zh', [('zh', 'train'), ('zh', 'val')], [('zh', 'test')],
     RenameColumnsPreprocessor({
         'instruction': 'query',
         'output': 'response'
     }),
     get_dataset_from_repo,
     tags=['chat', 'medical'])
 
 register_dataset(
     DatasetName.medical_mini_zh,
-    'huangjintao/medical_zh', [('zh', 'train'), ('zh', 'val')],
-    [('zh', 'test')],
+    'huangjintao/medical_zh', [('zh', 'train'), ('zh', 'val')], [('zh', 'test')],
     RenameColumnsPreprocessor({
         'instruction': 'query',
         'output': 'response'
     }),
     get_dataset_from_repo,
     function_kwargs={'train_dataset_sample': 50000},
     tags=['chat', 'medical'])
@@ -1092,38 +983,32 @@
             conversation = ast.literal_eval(d['conversation'])
         query.append(conversation[-1]['human'])
         response.append(conversation[-1]['assistant'])
         h = []
         for c in conversation[:-1]:
             h.append([c['human'], c['assistant']])
         history.append(h)
-    return HfDataset.from_dict({
-        'query': query,
-        'response': response,
-        'history': history
-    })
+    return HfDataset.from_dict({'query': query, 'response': response, 'history': history})
 
 
 _sharegpt_zh_subset_list = ['common-zh', 'computer-zh', 'unknow-zh']
 
 _sharegpt_en_subset_list = ['common-en', 'computer-en']
 
 register_dataset(
     DatasetName.sharegpt_zh,
-    'huangjintao/sharegpt',
-    [(subset, 'train') for subset in _sharegpt_zh_subset_list],
+    'huangjintao/sharegpt', [(subset, 'train') for subset in _sharegpt_zh_subset_list],
     None,
     _preprocess_sharegpt,
     get_dataset_from_repo,
     tags=['chat', 'general', 'multi-round'])
 
 register_dataset(
     DatasetName.sharegpt_en,
-    'huangjintao/sharegpt',
-    [(subset, 'train') for subset in _sharegpt_en_subset_list],
+    'huangjintao/sharegpt', [(subset, 'train') for subset in _sharegpt_en_subset_list],
     None,
     _preprocess_sharegpt,
     get_dataset_from_repo,
     tags=['chat', 'general', 'multi-round'])
 
 
 def _preprocess_capcha_images(dataset: HfDataset) -> HfDataset:
@@ -1132,35 +1017,30 @@
     response_key = 'solution'
 
     response = []
     images = []
     for d in tqdm(dataset):
         images.append(d[image_key])
         response.append(d[response_key])
-    dataset = HfDataset.from_dict({
-        'query': [query] * len(response),
-        'response': response,
-        'images': images
-    })
+    dataset = HfDataset.from_dict({'query': [query] * len(response), 'response': response, 'images': images})
     dataset._info.features._column_requires_decoding['images'] = True
     return dataset
 
 
 def _repair_planner(conversations: list) -> list:
     if isinstance(conversations, str):
         conversations = ast.literal_eval(conversations)
     if len(conversations) == 2 and conversations[0]['from'] != 'user':
         conversations[0]['from'] = 'user'
     return conversations
 
 
 register_dataset(
     DatasetName.capcha_images,
-    'AI-ModelScope/captcha-images', [('default', 'train')],
-    [('default', 'validation')],
+    'AI-ModelScope/captcha-images', [('default', 'train')], [('default', 'validation')],
     _preprocess_capcha_images,
     get_dataset_from_repo,
     tags=['chat', 'multi-modal', 'vision'])
 
 register_dataset(
     DatasetName.cls_fudan_news_zh,
     'damo/zh_cls_fudan-news', ['train'],
@@ -1183,20 +1063,15 @@
     get_dataset_from_repo,
     tags=['chat', 'ner'])
 
 register_dataset(
     DatasetName.codefuse_python_en,
     'codefuse-ai/CodeExercise-Python-27k', ['train'],
     None,
-    ConversationsPreprocessor(
-        'human',
-        'bot',
-        conversations_key='chat_rounds',
-        from_key='role',
-        value_key='content'),
+    ConversationsPreprocessor('human', 'bot', conversations_key='chat_rounds', from_key='role', value_key='content'),
     get_dataset_from_repo,
     tags=['chat', 'coding', ''])
 
 register_dataset(
     DatasetName.toolbench_for_alpha_umi_backbone,
     'shenweizhou/alpha-umi-toolbench-processed-v2', [('backbone', 'train')],
     None,
@@ -1212,16 +1087,15 @@
     get_dataset_from_repo,
     tags=['chat', 'agent'])
 
 register_dataset(
     DatasetName.toolbench_for_alpha_umi_planner,
     'shenweizhou/alpha-umi-toolbench-processed-v2', [('planner', 'train')],
     None,
-    ConversationsPreprocessor(
-        repair_conversations=_repair_planner, error_strategy='delete'),
+    ConversationsPreprocessor(repair_conversations=_repair_planner, error_strategy='delete'),
     get_dataset_from_repo,
     tags=['chat', 'agent'])
 
 register_dataset(
     DatasetName.toolbench_for_alpha_umi_summarizer,
     'shenweizhou/alpha-umi-toolbench-processed-v2', [('summarizer', 'train')],
     None,
@@ -1231,18 +1105,15 @@
 
 
 def _preprocess_blossom_math(dataset: HfDataset) -> HfDataset:
     response = []
     for d in tqdm(dataset):
         output, answer = d['output'], d['answer']
         response.append(f'{output}\n\nAnswer: {answer}')
-    return HfDataset.from_dict({
-        'query': dataset['input'],
-        'response': response
-    })
+    return HfDataset.from_dict({'query': dataset['input'], 'response': response})
 
 
 register_dataset(
     DatasetName.blossom_math_zh,
     'AI-ModelScope/blossom-math-v2', ['train'],
     None,
     _preprocess_blossom_math,
@@ -1330,35 +1201,29 @@
     DatasetName.leetcode_python_en,
     'AI-ModelScope/leetcode-solutions-python', ['train'],
     None,
     _preprocess_leetcode_python,
     get_dataset_from_repo,
     tags=['chat', 'coding', ''])
 
-_agent_instruct_subset_list = [
-    'alfworld', 'db', 'kg', 'mind2web', 'os', 'webshop'
-]
+_agent_instruct_subset_list = ['alfworld', 'db', 'kg', 'mind2web', 'os', 'webshop']
 
 
 def _repair_conversations_agent_instruct(s: str) -> List[Dict[str, Any]]:
     s = s.replace('}\n {', '},\n {')
     if isinstance(s, str):
         s = ast.literal_eval(s)
     return s
 
 
 register_dataset(
     DatasetName.agent_instruct_all_en,
-    'huangjintao/AgentInstruct_copy',
-    [(subset, 'train') for subset in _agent_instruct_subset_list],
+    'huangjintao/AgentInstruct_copy', [(subset, 'train') for subset in _agent_instruct_subset_list],
     None,
-    ConversationsPreprocessor(
-        'human',
-        'gpt',
-        repair_conversations=_repair_conversations_agent_instruct),
+    ConversationsPreprocessor('human', 'gpt', repair_conversations=_repair_conversations_agent_instruct),
     get_dataset_from_repo,
     tags=['chat', 'agent', 'multi-round'])
 
 
 def _preprocess_msagent_multirole_dataset(dataset: HfDataset) -> HfDataset:
     res_prompt = """\n\n\n1. \n2. ,
     \n3. 50 """
@@ -1371,16 +1236,15 @@
         conv = d['conversations']
         system = conv[0]['value']
         if '' not in system:
             system += res_prompt
         system += history_prompt
         response.append(conv[-1]['value'])
         for i in range(1, len(conv) - 1):
-            system += conv_prompt.format(
-                name=conv[i]['from'], content=conv[i]['value'])
+            system += conv_prompt.format(name=conv[i]['from'], content=conv[i]['value'])
         query.append(system)
     return HfDataset.from_dict({'query': query, 'response': response})
 
 
 register_dataset(
     DatasetName.ms_agent_multirole,
     'iic/MSAgent-MultiRole', [('default', 'train')],
@@ -1396,18 +1260,15 @@
     RenameColumnsPreprocessor({
         'instruction': 'query',
         'output': 'response'
     }),
     get_dataset_from_repo,
     tags=['chat', 'coding', ''])
 
-hc3_chinese_subset = [
-    'baike', 'open_qa', 'nlpcc_dbqa', 'finance', 'medicine', 'law',
-    'psychology'
-]
+hc3_chinese_subset = ['baike', 'open_qa', 'nlpcc_dbqa', 'finance', 'medicine', 'law', 'psychology']
 
 
 def _preprocess_hc3(dataset: HfDataset) -> HfDataset:
     prompt = """Classification Task: Are the following responses from a human or from ChatGPT?
 Question: {question}
 Answer: {answer}
 Category: Human, ChatGPT
@@ -1423,25 +1284,23 @@
             query.append(prompt.format(question=question, answer=c))
             response.append('ChatGPT')
     return HfDataset.from_dict({'query': query, 'response': response})
 
 
 register_dataset(
     DatasetName.hc3_zh,
-    'simpleai/HC3-Chinese',
-    [[subset, 'train'] for subset in hc3_chinese_subset], [],
+    'simpleai/HC3-Chinese', [[subset, 'train'] for subset in hc3_chinese_subset], [],
     _preprocess_hc3,
     get_dataset_from_repo,
     tags=['text-generation', 'classification', ''],
     hf_dataset_id='Hello-SimpleAI/HC3-Chinese')
 
 register_dataset(
     DatasetName.hc3_en,
-    'simpleai/HC3', [[subset, 'train'] for subset in ['finance', 'medicine']],
-    [],
+    'simpleai/HC3', [[subset, 'train'] for subset in ['finance', 'medicine']], [],
     _preprocess_hc3,
     get_dataset_from_repo,
     tags=['text-generation', 'classification', ''],
     hf_dataset_id='Hello-SimpleAI/HC3')
 
 register_dataset(
     DatasetName.tulu_v2_sft_mixture,
@@ -1500,31 +1359,26 @@
     'AI-ModelScope/sharegpt_gpt4', ['train'],
     None,
     ConversationsPreprocessor('human', 'gpt', error_strategy='delete'),
     get_dataset_from_repo,
     tags=['chat', 'multilingual', 'general', 'multi-round', 'gpt4', ''])
 register_dataset(
     DatasetName.sharegpt_gpt4,
-    'AI-ModelScope/sharegpt_gpt4',
-    [[subset, 'train']
-     for subset in ['default', 'V3_format', 'zh_38K_format']],
+    'AI-ModelScope/sharegpt_gpt4', [[subset, 'train'] for subset in ['default', 'V3_format', 'zh_38K_format']],
     None,
     ConversationsPreprocessor('human', 'gpt', error_strategy='delete'),
     get_dataset_from_repo,
     tags=['chat', 'multilingual', 'general', 'multi-round'])
 
 register_dataset(
     DatasetName.disc_med_sft_zh,
     'AI-ModelScope/DISC-Med-SFT', ['train'],
     None,
     ConversationsPreprocessor(
-        conversations_key='conversation',
-        from_key='role',
-        value_key='content',
-        error_strategy='delete'),
+        conversations_key='conversation', from_key='role', value_key='content', error_strategy='delete'),
     get_dataset_from_repo,
     tags=['chat', 'medical', ''],
     hf_dataset_id='Flmc/DISC-Med-SFT')
 
 register_dataset(
     DatasetName.disc_law_sft_zh,
     'AI-ModelScope/DISC-Law-SFT', ['train'],
@@ -1545,41 +1399,35 @@
         'text': 'response',
     }),
     get_dataset_from_repo,
     tags=['text-generation', 'awq'],
     hf_dataset_id='mit-han-lab/pile-val-backup')
 
 
-def add_self_cognition_dataset(
-        train_dataset: HfDataset, dataset_sample: int,
-        model_name: Tuple[str, Optional[str]],
-        model_author: Tuple[str, Optional[str]]) -> HfDataset:
+def add_self_cognition_dataset(train_dataset: HfDataset, dataset_sample: int, model_name: Tuple[str, Optional[str]],
+                               model_author: Tuple[str, Optional[str]]) -> HfDataset:
     assert model_name[0] is not None
     assert model_author[0] is not None
     if model_name[1] is None:
         model_name = (model_name[0], model_name[0])
     if model_author[1] is None:
         model_author = (model_author[0], model_author[0])
-    dataset_path = os.path.join(
-        os.path.dirname(os.path.dirname(__file__)), 'data',
-        'self_cognition.jsonl')
+    dataset_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'data', 'self_cognition.jsonl')
     assert os.path.exists(dataset_path)
     dataset = load_dataset_from_local([dataset_path], SmartPreprocessor())
     response = []
     for d in dataset:
         if d['tag'] == 'zh':
             model_n, model_a = model_name[0], model_author[0]
         else:
             model_n, model_a = model_name[1], model_author[1]
 
-        r = d['response'].replace('{{NAME}}',
-                                  model_n).replace('{{AUTHOR}}', model_a)
+        r = d['response'].replace('{{NAME}}', model_n).replace('{{AUTHOR}}', model_a)
         response.append(r)
-    dataset = dataset.remove_columns('response').add_column(
-        'response', response).remove_columns('tag')
+    dataset = dataset.remove_columns('response').add_column('response', response).remove_columns('tag')
 
     random_state = RandomState(42)
     idx = random_state.permutation(len(dataset))[:dataset_sample]
     dataset_sample -= len(idx)
     if dataset_sample > 0:
         idx2 = random_state.choice(len(dataset), dataset_sample)
         idx = np.concatenate([idx, idx2], axis=0)
@@ -1589,18 +1437,16 @@
     else:
         return concatenate_datasets([train_dataset, dataset])
 
 
 NoneType = type(None)
 
 
-def _check_dataset(
-    dataset: Optional[None],
-    check_dataset_strategy: Literal['none', 'discard', 'error', 'warning']
-) -> HfDataset:
+def _check_dataset(dataset: Optional[None], check_dataset_strategy: Literal['none', 'discard', 'error',
+                                                                            'warning']) -> HfDataset:
     if check_dataset_strategy == 'none' or dataset is None:
         return dataset
     idx_list = []
     has_query = 'query' in dataset.features
     has_history = 'history' in dataset.features
     has_system = 'system' in dataset.features
     is_modified = False
@@ -1648,16 +1494,15 @@
     return dataset
 
 
 def get_dataset(
     dataset_name_list: Union[List[str], str],
     dataset_test_ratio: float = 0.,
     dataset_seed: Union[RandomState, int] = 42,
-    check_dataset_strategy: Literal['none', 'discard', 'error',
-                                    'warning'] = 'none',
+    check_dataset_strategy: Literal['none', 'discard', 'error', 'warning'] = 'none',
 ) -> Tuple[HfDataset, Optional[HfDataset]]:
     """Returns train_dataset and val_dataset"""
     if isinstance(dataset_name_list, str):
         dataset_name_list = [dataset_name_list]
     train_dataset_list: List[HfDataset] = []
     val_dataset_list: List[HfDataset] = []
     random_state = dataset_seed
@@ -1665,40 +1510,36 @@
         random_state = RandomState(dataset_seed)
     for dataset_name in dataset_name_list:
         dataset_info = DATASET_MAPPING[dataset_name]
         use_hf = strtobool(os.environ.get('USE_HF', 'False'))
         dataset_str_f = 'Downloading the dataset from {hub}, dataset_id: {dataset_id}'
         if use_hf:
             dataset_id_or_path = dataset_info['hf_dataset_id']
-            dataset_str = dataset_str_f.format(
-                hub='HuggingFace', dataset_id=dataset_id_or_path)
+            dataset_str = dataset_str_f.format(hub='HuggingFace', dataset_id=dataset_id_or_path)
         else:
             dataset_id_or_path = dataset_info['dataset_id_or_path']
-            dataset_str = dataset_str_f.format(
-                hub='ModelScope', dataset_id=dataset_id_or_path)
+            dataset_str = dataset_str_f.format(hub='ModelScope', dataset_id=dataset_id_or_path)
         logger.info(dataset_str)
-        assert dataset_id_or_path is not None, (
-            f'dataset_name: {dataset_name}, use_hf: {use_hf}, '
-            f'dataset_id_or_path: {dataset_id_or_path}.')
+        assert dataset_id_or_path is not None, (f'dataset_name: {dataset_name}, use_hf: {use_hf}, '
+                                                f'dataset_id_or_path: {dataset_id_or_path}.')
         get_function: GetDatasetFunction = dataset_info['get_function']
         dataset = get_function(
             dataset_id_or_path,
             train_subset_split_list=dataset_info['train_subset_split_list'],
             val_subset_split_list=dataset_info['val_subset_split_list'],
             preprocess_func=dataset_info['preprocess_func'],
             use_hf=use_hf)
         train_d: HfDataset
         if isinstance(dataset, (list, tuple)):
             train_d, val_d = dataset
         else:
             train_d, val_d = dataset, None
         assert train_d is not None or val_d is not None
         if val_d is None and dataset_test_ratio > 0:
-            dataset_dict = train_d.train_test_split(
-                dataset_test_ratio, seed=get_seed(random_state))
+            dataset_dict = train_d.train_test_split(dataset_test_ratio, seed=get_seed(random_state))
             train_d, val_d = dataset_dict['train'], dataset_dict['test']
         if train_d is not None:
             train_dataset_list.append(train_d)
         if val_d is not None:
             val_dataset_list.append(val_d)
 
     train_dataset = None
@@ -1711,17 +1552,16 @@
         logger.info('check dataset...')
         logger.info(f"check_dataset_strategy: '{check_dataset_strategy}'")
     train_dataset = _check_dataset(train_dataset, check_dataset_strategy)
     val_dataset = _check_dataset(val_dataset, check_dataset_strategy)
     return train_dataset, val_dataset
 
 
-def load_dataset_from_local(
-        dataset_path_list: Optional[Union[str, List[str]]],
-        preprocess_func: PreprocessFunc) -> Optional[HfDataset]:
+def load_dataset_from_local(dataset_path_list: Optional[Union[str, List[str]]],
+                            preprocess_func: PreprocessFunc) -> Optional[HfDataset]:
     if isinstance(dataset_path_list, str):
         dataset_path_list = [dataset_path_list]
     if dataset_path_list is None or len(dataset_path_list) == 0:
         return None
     assert isinstance(dataset_path_list, (list, tuple))
 
     dataset_list = []
@@ -1733,25 +1573,21 @@
         elif dataset_path.endswith('.jsonl'):
             df = transform_jsonl_to_df(read_from_jsonl(dataset_path))
         elif dataset_path.endswith('.json'):
             with open(dataset_path, 'r', encoding='utf-8') as f:
                 obj_list = json.load(f)
             df = transform_jsonl_to_df(obj_list)
         else:
-            raise ValueError(
-                'The custom dataset only supports CSV, JSONL or JSON format. You can refer to the link '
-                '`https://github.com/modelscope/swift/blob/main/docs/source/LLM/.md#` '
-                'for more information.')
+            raise ValueError('The custom dataset only supports CSV, JSONL or JSON format. You can refer to the link '
+                             '`https://github.com/modelscope/swift/blob/main/docs/source/LLM/.md#` '
+                             'for more information.')
         dataset = HfDataset.from_dict(df.to_dict(orient='list'))
         dataset_list.append(preprocess_func(dataset))
     return concatenate_datasets(dataset_list)
 
 
 def get_custom_dataset(_: str, train_subset_split_list: Union[str, List[str]],
-                       val_subset_split_list: Optional[Union[str, List[str]]],
-                       preprocess_func: PreprocessFunc,
+                       val_subset_split_list: Optional[Union[str, List[str]]], preprocess_func: PreprocessFunc,
                        **kwargs) -> Tuple[HfDataset, Optional[HfDataset]]:
-    train_dataset = load_dataset_from_local(train_subset_split_list,
-                                            preprocess_func)
-    val_dataset = load_dataset_from_local(val_subset_split_list,
-                                          preprocess_func)
+    train_dataset = load_dataset_from_local(train_subset_split_list, preprocess_func)
+    val_dataset = load_dataset_from_local(val_subset_split_list, preprocess_func)
     return train_dataset, val_dataset
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/model.py` & `ms-swift-2.0.4/swift/llm/utils/model.py`

 * *Files 2% similar despite different names*

```diff
@@ -8,31 +8,29 @@
 from typing import Any, Callable, Dict, List, NamedTuple, Optional, Tuple, Type
 
 import torch
 import torch.distributed as dist
 import torch.nn.functional as F
 import torch.utils.checkpoint
 import transformers
-from modelscope import (AutoConfig, AutoModelForCausalLM, AutoTokenizer,
-                        BitsAndBytesConfig, GenerationConfig, GPTQConfig,
-                        snapshot_download)
+from modelscope import (AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig,
+                        GenerationConfig, GPTQConfig, snapshot_download)
 from modelscope.hub.utils.utils import get_cache_dir
 from packaging import version
 from torch import Tensor
 from torch import dtype as Dtype
-from transformers import (PretrainedConfig, PreTrainedModel,
-                          PreTrainedTokenizerBase)
+from transformers import PretrainedConfig, PreTrainedModel, PreTrainedTokenizerBase
 from transformers.dynamic_module_utils import get_class_from_dynamic_module
 from transformers.models.auto.tokenization_auto import get_tokenizer_config
 from transformers.utils import strtobool
 from transformers.utils.versions import require_version
+from trl.import_utils import is_unsloth_available
 
 from swift import get_logger
-from swift.utils import (get_dist_setting, is_dist, is_local_master,
-                         safe_ddp_context, subprocess_run, use_torchacc)
+from swift.utils import get_dist_setting, is_dist, is_local_master, safe_ddp_context, subprocess_run, use_torchacc
 from .template import TemplateType
 from .utils import get_max_model_len
 
 logger = get_logger()
 
 # Model Home: 'https://modelscope.cn/models/{model_id_or_path}/summary'
 MODEL_MAPPING: Dict[str, Dict[str, Any]] = {}
@@ -62,34 +60,37 @@
     qwen1half_0_5b = 'qwen1half-0_5b'
     qwen1half_1_8b = 'qwen1half-1_8b'
     qwen1half_4b = 'qwen1half-4b'
     qwen1half_7b = 'qwen1half-7b'
     qwen1half_14b = 'qwen1half-14b'
     qwen1half_32b = 'qwen1half-32b'
     qwen1half_72b = 'qwen1half-72b'
+    qwen1half_110b = 'qwen1half-110b'
     codeqwen1half_7b = 'codeqwen1half-7b'
     qwen1half_moe_a2_7b = 'qwen1half-moe-a2_7b'
     qwen1half_0_5b_chat = 'qwen1half-0_5b-chat'
     qwen1half_1_8b_chat = 'qwen1half-1_8b-chat'
     qwen1half_4b_chat = 'qwen1half-4b-chat'
     qwen1half_7b_chat = 'qwen1half-7b-chat'
     qwen1half_14b_chat = 'qwen1half-14b-chat'
     qwen1half_32b_chat = 'qwen1half-32b-chat'
     qwen1half_72b_chat = 'qwen1half-72b-chat'
+    qwen1half_110b_chat = 'qwen1half-110b-chat'
     qwen1half_moe_a2_7b_chat = 'qwen1half-moe-a2_7b-chat'
     codeqwen1half_7b_chat = 'codeqwen1half-7b-chat'
 
     # qwen1.5 gptq
     qwen1half_0_5b_chat_int4 = 'qwen1half-0_5b-chat-int4'
     qwen1half_1_8b_chat_int4 = 'qwen1half-1_8b-chat-int4'
     qwen1half_4b_chat_int4 = 'qwen1half-4b-chat-int4'
     qwen1half_7b_chat_int4 = 'qwen1half-7b-chat-int4'
     qwen1half_14b_chat_int4 = 'qwen1half-14b-chat-int4'
     qwen1half_32b_chat_int4 = 'qwen1half-32b-chat-int4'
     qwen1half_72b_chat_int4 = 'qwen1half-72b-chat-int4'
+    qwen1half_110b_chat_int4 = 'qwen1half-110b-chat-int4'
     qwen1half_0_5b_chat_int8 = 'qwen1half-0_5b-chat-int8'
     qwen1half_1_8b_chat_int8 = 'qwen1half-1_8b-chat-int8'
     qwen1half_4b_chat_int8 = 'qwen1half-4b-chat-int8'
     qwen1half_7b_chat_int8 = 'qwen1half-7b-chat-int8'
     qwen1half_14b_chat_int8 = 'qwen1half-14b-chat-int8'
     qwen1half_72b_chat_int8 = 'qwen1half-72b-chat-int8'
     qwen1half_moe_a2_7b_chat_int4 = 'qwen1half-moe-a2_7b-chat-int4'
@@ -98,14 +99,15 @@
     qwen1half_0_5b_chat_awq = 'qwen1half-0_5b-chat-awq'
     qwen1half_1_8b_chat_awq = 'qwen1half-1_8b-chat-awq'
     qwen1half_4b_chat_awq = 'qwen1half-4b-chat-awq'
     qwen1half_7b_chat_awq = 'qwen1half-7b-chat-awq'
     qwen1half_14b_chat_awq = 'qwen1half-14b-chat-awq'
     qwen1half_32b_chat_awq = 'qwen1half-32b-chat-awq'
     qwen1half_72b_chat_awq = 'qwen1half-72b-chat-awq'
+    qwen1half_110b_chat_awq = 'qwen1half-110b-chat-awq'
     codeqwen1half_7b_chat_awq = 'codeqwen1half-7b-chat-awq'
 
     # qwen-vl
     qwen_vl = 'qwen-vl'
     qwen_vl_chat = 'qwen-vl-chat'
     qwen_vl_chat_int4 = 'qwen-vl-chat-int4'
     # qwen-audio
@@ -134,27 +136,29 @@
     llama3_8b_instruct_int8 = 'llama3-8b-instruct-int8'
     llama3_8b_instruct_awq = 'llama3-8b-instruct-awq'
     llama3_70b = 'llama3-70b'
     llama3_70b_instruct = 'llama3-70b-instruct'
     llama3_70b_instruct_int4 = 'llama3-70b-instruct-int4'
     llama3_70b_instruct_int8 = 'llama3-70b-instruct-int8'
     llama3_70b_instruct_awq = 'llama3-70b-instruct-awq'
-    # chinese-llama-alpaca-2
+    # chinese-llama-alpaca
     chinese_llama_2_1_3b = 'chinese-llama-2-1_3b'
     chinese_llama_2_7b = 'chinese-llama-2-7b'
     chinese_llama_2_7b_16k = 'chinese-llama-2-7b-16k'
     chinese_llama_2_7b_64k = 'chinese-llama-2-7b-64k'
     chinese_llama_2_13b = 'chinese-llama-2-13b'
     chinese_llama_2_13b_16k = 'chinese-llama-2-13b-16k'
     chinese_alpaca_2_1_3b = 'chinese-alpaca-2-1_3b'
     chinese_alpaca_2_7b = 'chinese-alpaca-2-7b'
     chinese_alpaca_2_7b_16k = 'chinese-alpaca-2-7b-16k'
     chinese_alpaca_2_7b_64k = 'chinese-alpaca-2-7b-64k'
     chinese_alpaca_2_13b = 'chinese-alpaca-2-13b'
     chinese_alpaca_2_13b_16k = 'chinese-alpaca-2-13b-16k'
+    llama_3_chinese_8b = 'llama-3-chinese-8b'
+    llama_3_chinese_8b_instruct = 'llama-3-chinese-8b-instruct'
     # atom
     atom_7b = 'atom-7b'
     atom_7b_chat = 'atom-7b-chat'
     # llava
     llava1d6_mistral_7b_instruct = 'llava1d6-mistral-7b-instruct'
     llava1d6_yi_34b_instruct = 'llava1d6-yi-34b-instruct'
     # yi
@@ -194,14 +198,16 @@
     # internlm2-math
     internlm2_math_7b = 'internlm2-math-7b'
     internlm2_math_7b_chat = 'internlm2-math-7b-chat'
     internlm2_math_20b = 'internlm2-math-20b'
     internlm2_math_20b_chat = 'internlm2-math-20b-chat'
     # internlm-xcomposer2
     internlm_xcomposer2_7b_chat = 'internlm-xcomposer2-7b-chat'
+    # internvl
+    internvl_chat_v1_5 = 'internvl-chat-v1_5'
     # deepseek
     deepseek_7b = 'deepseek-7b'
     deepseek_7b_chat = 'deepseek-7b-chat'
     deepseek_moe_16b = 'deepseek-moe-16b'
     deepseek_moe_16b_chat = 'deepseek-moe-16b-chat'
     deepseek_67b = 'deepseek-67b'
     deepseek_67b_chat = 'deepseek-67b-chat'
@@ -357,21 +363,20 @@
     chatglm = ['query_key_value']
     llama2 = ['q_proj', 'k_proj', 'v_proj']
     qwen = ['c_attn']
     qwen1half = llama2
     polylm = ['c_attn']
     bloom = ['query_key_value']
     cogagent = [
-        'vision_expert_query_key_value', 'vision_expert_dense',
-        'language_expert_query_key_value', 'language_expert_dense', 'query',
-        'key_value', 'dense'
+        'vision_expert_query_key_value', 'vision_expert_dense', 'language_expert_query_key_value',
+        'language_expert_dense', 'query', 'key_value', 'dense'
     ]
     cogvlm = [
-        'vision_expert_query_key_value', 'vision_expert_dense',
-        'language_expert_query_key_value', 'language_expert_dense'
+        'vision_expert_query_key_value', 'vision_expert_dense', 'language_expert_query_key_value',
+        'language_expert_dense'
     ]
     phi = ['Wqkv']
     phi3 = ['qkv_proj']
     internlm2 = ['wqkv']
     mamba = ['in_proj', 'x_proj', 'embeddings', 'out_proj']
     telechat = ['key_value', 'query']
     grok_1 = ['q_proj', 'k_proj', 'v_proj']
@@ -385,40 +390,35 @@
     ]
     mplug_owl2d1 = [
         'c_attn.multiway.0',
         'c_attn.multiway.1',
     ]
 
 
-GetModelTokenizerFunction = Callable[..., Tuple[Optional[PreTrainedModel],
-                                                PreTrainedTokenizerBase]]
+GetModelTokenizerFunction = Callable[..., Tuple[Optional[PreTrainedModel], PreTrainedTokenizerBase]]
 
 
 def register_model(
-    model_type: str,
-    model_id_or_path: Optional[str],
-    lora_target_modules: Optional[List[str]] = None,
-    template: str = TemplateType.default,
-    get_function: Optional[GetModelTokenizerFunction] = None,
-    *,
-    requires: Optional[List[str]] = None,
-    torch_dtype: Optional[Dtype] = None,
-    hf_model_id: Optional[str] = None,
-    revision: Optional[str] = None,  # only modelscope
-    ignore_file_pattern: Optional[List[str]] = None,
-    function_kwargs: Optional[Dict[str, Any]] = None,
-    exists_ok: bool = False,
-    eos_token: Optional[str] = None,
-    **kwargs
-) -> Optional[Callable[[GetModelTokenizerFunction],
-                       GetModelTokenizerFunction]]:
+        model_type: str,
+        model_id_or_path: Optional[str],
+        lora_target_modules: Optional[List[str]] = None,
+        template: str = TemplateType.default,
+        get_function: Optional[GetModelTokenizerFunction] = None,
+        *,
+        requires: Optional[List[str]] = None,
+        torch_dtype: Optional[Dtype] = None,
+        hf_model_id: Optional[str] = None,
+        revision: Optional[str] = None,  # only modelscope
+        ignore_file_pattern: Optional[List[str]] = None,
+        function_kwargs: Optional[Dict[str, Any]] = None,
+        exists_ok: bool = False,
+        eos_token: Optional[str] = None,
+        **kwargs) -> Optional[Callable[[GetModelTokenizerFunction], GetModelTokenizerFunction]]:
     if not exists_ok and model_type in MODEL_MAPPING:
-        raise ValueError(
-            f'The `{model_type}` has already been registered in the MODEL_MAPPING.'
-        )
+        raise ValueError(f'The `{model_type}` has already been registered in the MODEL_MAPPING.')
     if requires is None:
         requires = []
     if function_kwargs is None:
         function_kwargs = {}
     if revision is None:
         revision = 'master'
     model_info = {
@@ -437,17 +437,15 @@
     if get_function is not None:
         if len(function_kwargs) > 0:
             get_function = partial(get_function, **function_kwargs)
         model_info['get_function'] = get_function
         MODEL_MAPPING[model_type] = model_info
         return
 
-    def _register_model(
-            get_function: GetModelTokenizerFunction
-    ) -> GetModelTokenizerFunction:
+    def _register_model(get_function: GetModelTokenizerFunction) -> GetModelTokenizerFunction:
         _old_get_function = get_function
         if len(function_kwargs) > 0:
             get_function = partial(get_function, **function_kwargs)
         model_info['get_function'] = get_function
         MODEL_MAPPING[model_type] = model_info
         return _old_get_function
 
@@ -455,28 +453,25 @@
 
 
 def _check_awq_ext() -> None:
     try:
         from awq.utils.packing_utils import dequantize_gemm
         import awq_ext  # with CUDA kernels (AutoAWQ_kernels)
     except ImportError as e:
-        raise ImportError(
-            'You are training awq models, remember installing awq_ext by '
-            '`git clone https://github.com/casper-hansen/AutoAWQ_kernels '
-            '&& cd AutoAWQ_kernels && pip install -e .`') from e
+        raise ImportError('You are training awq models, remember installing awq_ext by '
+                          '`git clone https://github.com/casper-hansen/AutoAWQ_kernels '
+                          '&& cd AutoAWQ_kernels && pip install -e .`') from e
 
 
 def _check_gptq_model(bits: int, model_kwargs: Dict[str, Any]) -> None:
     assert model_kwargs.get('quantization_config') is None
     if version.parse(transformers.__version__) >= version.parse('4.35'):
-        model_kwargs['quantization_config'] = GPTQConfig(
-            bits=bits, use_exllama=False)
+        model_kwargs['quantization_config'] = GPTQConfig(bits=bits, use_exllama=False)
     else:
-        model_kwargs['quantization_config'] = GPTQConfig(
-            bits=bits, disable_exllama=True)
+        model_kwargs['quantization_config'] = GPTQConfig(bits=bits, disable_exllama=True)
 
     # fix quantlinear bug
     from auto_gptq.nn_modules.qlinear.qlinear_cuda_old import QuantLinear
     __old_forward = QuantLinear.forward
 
     def _new_forward(self, x):
         if not self.training or not self.autogptq_cuda_available:
@@ -492,15 +487,15 @@
         QuantLinear.forward = _new_forward
 
 
 @register_model(
     ModelType.atom_7b,
     'FlagAlpha/Atom-7B',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='FlagAlpha/Atom-7B')
 @register_model(
     ModelType.atom_7b_chat,
     'FlagAlpha/Atom-7B-Chat',
     LoRATM.llama2,
@@ -508,22 +503,22 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='FlagAlpha/Atom-7B-Chat')
 @register_model(
     ModelType.internlm_20b,
     'Shanghai_AI_Laboratory/internlm-20b',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_vllm=True,
     hf_model_id='internlm/internlm2-20b')
 @register_model(
     ModelType.internlm_7b,
     'Shanghai_AI_Laboratory/internlm-7b',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_vllm=True,
     hf_model_id='internlm/internlm-7b')
 @register_model(
     ModelType.bluelm_7b_chat_32k,
     'vivo-ai/BlueLM-7B-Chat-32K',
     LoRATM.llama2,
     TemplateType.bluelm,
@@ -534,21 +529,21 @@
     LoRATM.llama2,
     TemplateType.bluelm,
     hf_model_id='vivo-ai/BlueLM-7B-Chat')
 @register_model(
     ModelType.bluelm_7b_32k,
     'vivo-ai/BlueLM-7B-Base-32K',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     hf_model_id='vivo-ai/BlueLM-7B-Base-32K')
 @register_model(
     ModelType.bluelm_7b,
     'vivo-ai/BlueLM-7B-Base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     hf_model_id='vivo-ai/BlueLM-7B-Base')
 @register_model(
     ModelType.seqgpt_560m,
     'damo/nlp_seqgpt-560m',
     LoRATM.bloom,
     TemplateType.default_generation,
     support_vllm=True,
@@ -775,37 +770,42 @@
     if is_aqlm and is_training:
         require_version('transformers>=4.39')
         import aqlm
         context = aqlm.optimize_for_training()
     if context is None:
         context = nullcontext()
     if model_config is None:
-        model_config = AutoConfig.from_pretrained(
-            model_dir, trust_remote_code=True)
+        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     if torch_dtype is not None:
         model_config.torch_dtype = torch_dtype
     if tokenizer is None:
-        tokenizer = AutoTokenizer.from_pretrained(
-            model_dir, trust_remote_code=True)
+        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
     eos_token = kwargs.get('eos_token')
     if eos_token is not None:
         tokenizer.eos_token = eos_token
     model = None
     if load_model:
-        with context:
-            model = automodel_class.from_pretrained(
-                model_dir,
-                config=model_config,
-                torch_dtype=torch_dtype,
+        if kwargs.get('use_unsloth', False):
+            assert is_unsloth_available(), 'please install unsloth if using `use_unsloth=True`'
+            from unsloth import FastLanguageModel
+            model, tokenizer = FastLanguageModel.from_pretrained(
+                model_name=model_dir,
+                max_seq_length=kwargs.get('max_length', None),
+                dtype=torch_dtype,
+                load_in_4bit=kwargs.get('load_in_4bit', True),
                 trust_remote_code=True,
-                **model_kwargs)
-    if load_model and is_awq:
-        model.is_awq = is_awq
-    if load_model and gptq_bits > 0:
-        model.gptq_bits = gptq_bits
+            )
+        else:
+            with context:
+                model = automodel_class.from_pretrained(
+                    model_dir, config=model_config, torch_dtype=torch_dtype, trust_remote_code=True, **model_kwargs)
+        if load_model and is_awq:
+            model.is_awq = is_awq
+        if load_model and gptq_bits > 0:
+            model.gptq_bits = gptq_bits
     return model, tokenizer
 
 
 @register_model(
     ModelType.grok_1,
     'colossalai/grok-1-pytorch',
     LoRATM.grok_1,
@@ -818,32 +818,27 @@
                              model_kwargs: Dict[str, Any],
                              load_model: bool = True,
                              model_config=None,
                              tokenizer=None,
                              automodel_class=AutoModelForCausalLM,
                              **kwargs):
     if model_config is None:
-        model_config = AutoConfig.from_pretrained(
-            model_dir, trust_remote_code=True)
+        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     if torch_dtype is not None:
         model_config.torch_dtype = torch_dtype
     if tokenizer is None:
         tokenizer = AutoTokenizer.from_pretrained(
-            'AI-ModelScope/grok-1-tokenizer', trust_remote_code=True)
+            'AI-ModelScope/grok-1-tokenizer', revision='master', trust_remote_code=True)
     eos_token = kwargs.get('eos_token')
     if eos_token is not None:
         tokenizer.eos_token = eos_token
     model = None
     if load_model:
         model = automodel_class.from_pretrained(
-            model_dir,
-            config=model_config,
-            torch_dtype=torch_dtype,
-            trust_remote_code=True,
-            **model_kwargs)
+            model_dir, config=model_config, torch_dtype=torch_dtype, trust_remote_code=True, **model_kwargs)
     return model, tokenizer
 
 
 @register_model(
     ModelType.mamba_130m,
     'AI-ModelScope/mamba-130m-hf',
     LoRATM.mamba,
@@ -892,19 +887,17 @@
     support_vllm=False,
     hf_model_id='state-spaces/mamba-2.8b-hf')
 def get_model_tokenizer_mamba(model_dir: str,
                               torch_dtype: Optional[Dtype],
                               model_kwargs: Dict[str, Any],
                               load_model: bool = True,
                               **kwargs):
-    logger.info(
-        '[IMPORTANT] Remember installing causal-conv1d>=1.2.0 and mamba-ssm, or you training and inference will'
-        'be really slow!')
-    return get_model_tokenizer_from_repo(model_dir, torch_dtype, model_kwargs,
-                                         load_model, **kwargs)
+    logger.info('[IMPORTANT] Remember installing causal-conv1d>=1.2.0 and mamba-ssm, or you training and inference will'
+                'be really slow!')
+    return get_model_tokenizer_from_repo(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
 
 
 @register_model(
     ModelType.cogvlm_17b_instruct,
     'ZhipuAI/cogvlm-chat',
     LoRATM.cogvlm,
     TemplateType.cogvlm_instruct,
@@ -913,44 +906,37 @@
     hf_model_id='THUDM/cogvlm-chat-hf')
 @register_model(
     ModelType.cogagent_18b_chat,
     'ZhipuAI/cogagent-chat',
     LoRATM.cogagent,
     TemplateType.cogagent_chat,
     support_gradient_checkpointing=False,
+    requires=['timm'],
     tags=['multi-modal', 'vision'],
     hf_model_id='THUDM/cogagent-chat-hf')
 @register_model(
     ModelType.cogagent_18b_instruct,
     'ZhipuAI/cogagent-vqa',
     LoRATM.cogagent,
     TemplateType.cogagent_instruct,
     support_gradient_checkpointing=False,
+    requires=['timm'],
     tags=['multi-modal', 'vision'],
     hf_model_id='THUDM/cogagent-vqa-hf')
 def get_model_tokenizer_cogagent(model_dir: str,
                                  torch_dtype: Dtype,
                                  model_kwargs: Dict[str, Any],
                                  load_model: bool = True,
                                  **kwargs):
-    tokenizer = AutoTokenizer.from_pretrained(
-        'AI-ModelScope/vicuna-7b-v1.5',
-        revision='master',
-        trust_remote_code=True)
+    tokenizer = AutoTokenizer.from_pretrained('AI-ModelScope/vicuna-7b-v1.5', revision='master', trust_remote_code=True)
     if load_model is True:
-        logger.warning(
-            'CogAgent with FusedLayerNorm will cause an training loss of NAN, '
-            'to avoid this, please uninstall apex.')
+        logger.warning('CogAgent with FusedLayerNorm will cause an training loss of NAN, '
+                       'to avoid this, please uninstall apex.')
     model, tokenizer = get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        tokenizer=tokenizer,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, tokenizer=tokenizer, **kwargs)
     logger.info('Please ignore the unimported warning.')
     return model, tokenizer
 
 
 @register_model(
     ModelType.internlm_20b_chat,
     'Shanghai_AI_Laboratory/internlm-chat-20b',
@@ -972,17 +958,15 @@
     support_vllm=True,
     hf_model_id='internlm/internlm-chat-7b')
 def get_model_tokenizer_internlm_chat(model_dir: str,
                                       torch_dtype: Dtype,
                                       model_kwargs: Dict[str, Any],
                                       load_model: bool = True,
                                       **kwargs):
-    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype,
-                                                     model_kwargs, load_model,
-                                                     **kwargs)
+    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if getattr(tokenizer.__class__.eos_token_id, 'fset', None) is None:
         del tokenizer.__class__.eos_token_id
     tokenizer.eos_token = '<eoa>'
     return model, tokenizer
 
 
 @register_model(
@@ -994,17 +978,15 @@
     support_vllm=True,
     hf_model_id='baichuan-inc/Baichuan-13B-Base')
 def get_model_tokenizer_baichuan_13b(model_dir: str,
                                      torch_dtype: Dtype,
                                      model_kwargs: Dict[str, Any],
                                      load_model: bool = True,
                                      **kwargs):
-    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype,
-                                                     model_kwargs, load_model,
-                                                     **kwargs)
+    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     # baichuan-13b does not implement the `get_input_embeddings` function
     # fix gradient_checkpointing bug
     try:
         model.get_input_embeddings()
     except NotImplementedError:
         model.__class__.get_input_embeddings = lambda self: self.model.embed_tokens
     return model, tokenizer
@@ -1026,26 +1008,20 @@
     hf_model_id='baichuan-inc/Baichuan2-13B-Base')
 def get_model_tokenizer_baichuan2_13b(model_dir: str,
                                       torch_dtype: Dtype,
                                       model_kwargs: Dict[str, Any],
                                       load_model: bool = True,
                                       **kwargs):
     # patch: baichuan2_13b configuration_baichuan.py bug
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     gradient_checkpointing = model_config.gradient_checkpointing
     if isinstance(gradient_checkpointing, (tuple, list)):
         model_config.gradient_checkpointing = gradient_checkpointing[0]
     return get_model_tokenizer_baichuan2(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
 
 
 def patch_baichuan2_lm_head_forward(self, hidden_states: Tensor) -> Tensor:
     # patch: baichuan2 lm_head (fp32 bug)
     if self.training:
         norm_weight = F.normalize(self.weight).to(self.weight.dtype)
     elif self.first_flag:
@@ -1074,46 +1050,37 @@
 def get_model_tokenizer_baichuan2(model_dir: str,
                                   torch_dtype: Dtype,
                                   model_kwargs: Dict[str, Any],
                                   load_model: bool = True,
                                   model_config=None,
                                   **kwargs):
     if model_config is None:
-        model_config = AutoConfig.from_pretrained(
-            model_dir, trust_remote_code=True)
+        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     if not hasattr(model_config, 'z_loss_weight'):
         model_config.z_loss_weight = 0
     model, tokenizer = get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
     model_ori = model
     if model is not None:
         if not hasattr(model, 'lm_head'):  # fix awq
             model = model.model
-        new_forward = MethodType(patch_baichuan2_lm_head_forward,
-                                 model.lm_head)
+        new_forward = MethodType(patch_baichuan2_lm_head_forward, model.lm_head)
         if hasattr(model, '_old_forward'):  # device_map
             model.lm_head._old_forward = new_forward
         else:
             model.lm_head.forward = new_forward
     return model_ori, tokenizer
 
 
 @register_model(
     ModelType.baichuan2_13b_chat_int4,
     'baichuan-inc/Baichuan2-13B-Chat-4bits',
     LoRATM.baichuan,
     TemplateType.baichuan,
-    function_kwargs={
-        'get_baichuan2_function': get_model_tokenizer_baichuan2_13b
-    },
+    function_kwargs={'get_baichuan2_function': get_model_tokenizer_baichuan2_13b},
     torch_dtype=torch.bfloat16,
     requires=['bitsandbytes<0.41.2', 'accelerate<0.26'],
     hf_model_id='baichuan-inc/Baichuan2-13B-Chat-4bits')
 @register_model(
     ModelType.baichuan2_7b_chat_int4,
     'baichuan-inc/Baichuan2-7B-Chat-4bits',
     LoRATM.baichuan,
@@ -1131,35 +1098,29 @@
 
     # fix device_map bug
     import accelerate
     _old_infer_auto_device_map = accelerate.infer_auto_device_map
     device_map = model_kwargs.get('device_map', None)
     if device_map != 'auto':
         accelerate.infer_auto_device_map = lambda *args, **kwargs: device_map
-    get_baichuan2_function = kwargs.pop('get_baichuan2_function',
-                                        get_model_tokenizer_baichuan2)
-    model, tokenizer = get_baichuan2_function(model_dir, torch_dtype,
-                                              model_kwargs, load_model,
-                                              **kwargs)
+    get_baichuan2_function = kwargs.pop('get_baichuan2_function', get_model_tokenizer_baichuan2)
+    model, tokenizer = get_baichuan2_function(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if device_map != 'auto':
         accelerate.infer_auto_device_map = _old_infer_auto_device_map
     if model is not None:
-        model.config.quantization_config = BitsAndBytesConfig(
-            **model.config.quantization_config)
+        model.config.quantization_config = BitsAndBytesConfig(**model.config.quantization_config)
         model.train()
         model._is_quantized_training_enabled = True
         model.is_loaded_in_4bit = True
     return model, tokenizer
 
 
-def remove_property(tokenizer_cls: Type[PreTrainedTokenizerBase],
-                    tokenizer_config: Dict[str, Any]) -> None:
+def remove_property(tokenizer_cls: Type[PreTrainedTokenizerBase], tokenizer_config: Dict[str, Any]) -> None:
     for k, v in tokenizer_cls.__dict__.items():
-        if k.endswith('_token') and isinstance(
-                v, property) and k in tokenizer_config:
+        if k.endswith('_token') and isinstance(v, property) and k in tokenizer_config:
             setattr(tokenizer_cls, k, tokenizer_config[k])
 
 
 @register_model(
     ModelType.codefuse_codegeex2_6b_chat,
     'codefuse-ai/CodeFuse-CodeGeeX2-6B',
     LoRATM.chatglm,
@@ -1221,43 +1182,71 @@
     hf_model_id='THUDM/codegeex2-6b')
 def get_model_tokenizer_chatglm(model_dir: str,
                                 torch_dtype: Dtype,
                                 model_kwargs: Dict[str, Any],
                                 load_model: bool = True,
                                 **kwargs):
     if model_kwargs.get('quantization_config') is not None:
-        model_kwargs['quantization_config'].llm_int8_skip_modules = [
-            'output_layer'
-        ]
+        model_kwargs['quantization_config'].llm_int8_skip_modules = ['output_layer']
     # fix transformers>=4.34 bug
     if version.parse(transformers.__version__) >= version.parse('4.34'):
         tokenizer_config = get_tokenizer_config(model_dir)
         class_ref = tokenizer_config['auto_map']['AutoTokenizer'][0]
         tokenizer_cls = get_class_from_dynamic_module(class_ref, model_dir)
         tokenizer_cls._auto_class = 'AutoTokenizer'
         remove_property(tokenizer_cls, tokenizer_config)
-        kwargs['tokenizer'] = tokenizer_cls.from_pretrained(
-            model_dir, trust_remote_code=True)
-    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype,
-                                                     model_kwargs, load_model,
-                                                     **kwargs)
+        kwargs['tokenizer'] = tokenizer_cls.from_pretrained(model_dir, trust_remote_code=True)
+    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if model is not None:
         from torch.nn import CrossEntropyLoss
         __old_forward = CrossEntropyLoss.forward
 
-        def cross_entropy_forward(self, inputs: Tensor,
-                                  target: Tensor) -> Tensor:
+        def cross_entropy_forward(self, inputs: Tensor, target: Tensor) -> Tensor:
             target = target.to(device=inputs.device)
             return __old_forward(self, inputs, target)
 
         CrossEntropyLoss.forward = cross_entropy_forward
     return model, tokenizer
 
 
 @register_model(
+    ModelType.minicpm_2b_sft_chat,
+    'OpenBMB/MiniCPM-2B-sft-fp32',
+    LoRATM.llama2,
+    TemplateType.minicpm,
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='openbmb/MiniCPM-2B-sft-fp32')
+@register_model(
+    ModelType.minicpm_2b_chat,
+    'OpenBMB/MiniCPM-2B-dpo-fp32',
+    LoRATM.llama2,
+    TemplateType.minicpm,
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='openbmb/MiniCPM-2B-dpo-fp32')
+@register_model(
+    ModelType.minicpm_1b_sft_chat,
+    'OpenBMB/MiniCPM-1B-sft-bf16',
+    LoRATM.llama2,
+    TemplateType.minicpm,
+    requires=['transformers>=4.36.0'],
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='openbmb/MiniCPM-1B-sft-bf16')
+@register_model(
+    ModelType.minicpm_2b_128k,
+    'OpenBMB/MiniCPM-2B-128k',
+    LoRATM.llama2,
+    TemplateType.chatml,
+    requires=['transformers>=4.36.0'],
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='openbmb/MiniCPM-2B-128k')
+@register_model(
     ModelType.phi3_4b_128k_instruct,
     'LLM-Research/Phi-3-mini-128k-instruct',
     LoRATM.phi3,
     TemplateType.phi3,
     requires=['transformers>=4.36'],
     support_flash_attn=True,
     support_vllm=False,  # https://github.com/vllm-project/vllm/pull/4298
@@ -1293,25 +1282,25 @@
     support_vllm=True,
     function_kwargs={'is_awq': True},
     hf_model_id='MaziyarPanahi/WizardLM-2-7B-AWQ')
 @register_model(
     ModelType.gemma_2b,
     'AI-ModelScope/gemma-2b',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.38'],
     ignore_file_pattern=[r'.+\.gguf$'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='google/gemma-2b')
 @register_model(
     ModelType.gemma_7b,
     'AI-ModelScope/gemma-7b',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.38'],
     ignore_file_pattern=[r'.+\.gguf$'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='google/gemma-7b')
 @register_model(
     ModelType.gemma_2b_instruct,
@@ -1351,15 +1340,15 @@
     support_vllm=True,
     tags=['math'],
     hf_model_id='deepseek-ai/deepseek-math-7b-rl')
 @register_model(
     ModelType.deepseek_math_7b,
     'deepseek-ai/deepseek-math-7b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     tags=['math'],
     hf_model_id='deepseek-ai/deepseek-math-7b-base')
 @register_model(
     ModelType.qwen1half_0_5b,
     'qwen/Qwen1.5-0.5B',
@@ -1420,14 +1409,23 @@
     LoRATM.qwen1half,
     TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     requires=['transformers>=4.37'],
     hf_model_id='Qwen/Qwen1.5-72B')
 @register_model(
+    ModelType.qwen1half_110b,
+    'qwen/Qwen1.5-110B',
+    LoRATM.qwen1half,
+    TemplateType.default_generation,
+    support_flash_attn=True,
+    support_vllm=True,
+    requires=['transformers>=4.37'],
+    hf_model_id='Qwen/Qwen1.5-110B')
+@register_model(
     ModelType.codeqwen1half_7b,
     'qwen/CodeQwen1.5-7B',
     LoRATM.qwen1half,
     TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     requires=['transformers>=4.37'],
@@ -1441,33 +1439,33 @@
     support_vllm=True,
     requires=['transformers>=4.40'],
     hf_model_id='Qwen/Qwen1.5-MoE-A2.7B')
 @register_model(
     ModelType.deepseek_coder_1_3b,
     'deepseek-ai/deepseek-coder-1.3b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     tags=['coding'],
     hf_model_id='deepseek-ai/deepseek-coder-1.3b-base')
 @register_model(
     ModelType.deepseek_coder_6_7b,
     'deepseek-ai/deepseek-coder-6.7b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     tags=['coding'],
     hf_model_id='deepseek-ai/deepseek-coder-6.7b-base')
 @register_model(
     ModelType.deepseek_coder_33b,
     'deepseek-ai/deepseek-coder-33b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     tags=['coding'],
     hf_model_id='deepseek-ai/deepseek-coder-33b-base')
 @register_model(
     ModelType.deepseek_coder_1_3b_instruct,
     'deepseek-ai/deepseek-coder-1.3b-instruct',
@@ -1514,15 +1512,15 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='deepseek-ai/deepseek-llm-67b-chat')
 @register_model(
     ModelType.deepseek_67b,
     'deepseek-ai/deepseek-llm-67b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='deepseek-ai/deepseek-llm-67b-base')
 @register_model(
     ModelType.deepseek_7b_chat,
     'deepseek-ai/deepseek-llm-7b-chat',
     LoRATM.llama2,
@@ -1530,15 +1528,15 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='deepseek-ai/deepseek-llm-7b-chat')
 @register_model(
     ModelType.deepseek_7b,
     'deepseek-ai/deepseek-llm-7b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='deepseek-ai/deepseek-llm-7b-base')
 @register_model(
     ModelType.sus_34b_chat,
     'SUSTC/SUS-Chat-34B',
     LoRATM.llama2,
@@ -1686,15 +1684,15 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='IDEA-CCNL/Ziya2-13B-Chat')
 @register_model(
     ModelType.ziya2_13b,
     'Fengshenbang/Ziya2-13B-Base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='IDEA-CCNL/Ziya2-13B-Base')
 @register_model(
     ModelType.openbuddy_mixtral_moe_7b_chat,
     'OpenBuddy/openbuddy-mixtral-7bx8-v18.1-32k',
     LoRATM.llama2,
@@ -1763,33 +1761,33 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='mistralai/Mistral-7B-Instruct-v0.2')
 @register_model(
     ModelType.mistral_7b,
     'AI-ModelScope/Mistral-7B-v0.1',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.34'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='mistralai/Mistral-7B-v0.1')
 @register_model(
     ModelType.mistral_7b_v2,
     'AI-ModelScope/Mistral-7B-v0.2-hf',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.34'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='alpindale/Mistral-7B-v0.2-hf')
 @register_model(
     ModelType.mixtral_moe_7b,
     'AI-ModelScope/Mixtral-8x7B-v0.1',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.36'],
     ignore_file_pattern=[r'.+\.pt$'],
     support_flash_attn=True,
     support_vllm=True,
     support_gradient_checkpointing=False,
     hf_model_id='mistralai/Mixtral-8x7B-v0.1')
 @register_model(
@@ -1803,15 +1801,15 @@
     support_vllm=True,
     support_gradient_checkpointing=False,
     hf_model_id='mistralai/Mixtral-8x7B-Instruct-v0.1')
 @register_model(
     ModelType.mixtral_moe_8x22b_v1,
     'AI-ModelScope/Mixtral-8x22B-v0.1',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.36'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='mistral-community/Mixtral-8x22B-v0.1')
 @register_model(
     ModelType.dbrx_base,
     'AI-ModelScope/dbrx-base',
@@ -1835,29 +1833,23 @@
 def get_model_tokenizer_with_flash_attn(model_dir: str,
                                         torch_dtype: Dtype,
                                         model_kwargs: Dict[str, Any],
                                         load_model: bool = True,
                                         model_config=None,
                                         **kwargs):
     if model_config is None:
-        model_config = AutoConfig.from_pretrained(
-            model_dir, trust_remote_code=True)
+        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attn = kwargs.pop('use_flash_attn', False)
     if version.parse(transformers.__version__) >= version.parse('4.36'):
         if use_flash_attn:
             model_config._attn_implementation = 'flash_attention_2'
     else:
         model_config._flash_attn_2_enabled = use_flash_attn
     return get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
 
 
 @register_model(
     ModelType.qwen1half_0_5b_chat_awq,
     'qwen/Qwen1.5-0.5B-Chat-AWQ',
     LoRATM.qwen1half,
     TemplateType.qwen,
@@ -1930,14 +1922,25 @@
     support_flash_attn=True,
     support_vllm=True,
     function_kwargs={'is_awq': True},
     requires=['transformers>=4.37', 'autoawq'],
     torch_dtype=torch.float16,
     hf_model_id='Qwen/Qwen1.5-72B-Chat-AWQ')
 @register_model(
+    ModelType.qwen1half_110b_chat_awq,
+    'qwen/Qwen1.5-110B-Chat-AWQ',
+    LoRATM.qwen1half,
+    TemplateType.qwen,
+    support_flash_attn=True,
+    support_vllm=True,
+    function_kwargs={'is_awq': True},
+    requires=['transformers>=4.37', 'autoawq'],
+    torch_dtype=torch.float16,
+    hf_model_id='Qwen/Qwen1.5-110B-Chat-AWQ')
+@register_model(
     ModelType.codeqwen1half_7b_chat_awq,
     'qwen/CodeQwen1.5-7B-Chat-AWQ',
     LoRATM.qwen1half,
     TemplateType.qwen,
     support_flash_attn=True,
     support_vllm=True,
     function_kwargs={'is_awq': True},
@@ -2004,14 +2007,23 @@
     LoRATM.qwen1half,
     TemplateType.qwen,
     support_flash_attn=True,
     support_vllm=True,
     requires=['transformers>=4.37'],
     hf_model_id='Qwen/Qwen1.5-72B-Chat')
 @register_model(
+    ModelType.qwen1half_110b_chat,
+    'qwen/Qwen1.5-110B-Chat',
+    LoRATM.qwen1half,
+    TemplateType.qwen,
+    support_flash_attn=True,
+    support_vllm=True,
+    requires=['transformers>=4.37'],
+    hf_model_id='Qwen/Qwen1.5-110B-Chat')
+@register_model(
     ModelType.qwen1half_moe_a2_7b_chat,
     'qwen/Qwen1.5-MoE-A2.7B-Chat',
     LoRATM.qwen1half,
     TemplateType.qwen,
     support_flash_attn=True,
     support_vllm=True,
     requires=['transformers>=4.40'],
@@ -2027,17 +2039,15 @@
     hf_model_id='Qwen/CodeQwen1.5-7B-Chat')
 def get_model_tokenizer_qwen1half(model_dir: str,
                                   torch_dtype: Dtype,
                                   model_kwargs: Dict[str, Any],
                                   load_model: bool = True,
                                   **kwargs):
     kwargs['eos_token'] = '<|im_end|>'
-    return get_model_tokenizer_with_flash_attn(model_dir, torch_dtype,
-                                               model_kwargs, load_model,
-                                               **kwargs)
+    return get_model_tokenizer_with_flash_attn(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
 
 
 @register_model(
     ModelType.qwen1half_0_5b_chat_int4,
     'qwen/Qwen1.5-0.5B-Chat-GPTQ-Int4',
     LoRATM.qwen1half,
     TemplateType.qwen,
@@ -2160,14 +2170,25 @@
     requires=['auto_gptq>=0.5', 'transformers>=4.37'],
     torch_dtype=torch.float16,
     function_kwargs={'gptq_bits': 4},
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='Qwen/Qwen1.5-72B-Chat-GPTQ-Int4')
 @register_model(
+    ModelType.qwen1half_110b_chat_int4,
+    'qwen/Qwen1.5-110B-Chat-GPTQ-Int4',
+    LoRATM.qwen1half,
+    TemplateType.qwen,
+    requires=['auto_gptq>=0.5', 'transformers>=4.37'],
+    torch_dtype=torch.float16,
+    function_kwargs={'gptq_bits': 4},
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='Qwen/Qwen1.5-110B-Chat-GPTQ-Int4')
+@register_model(
     ModelType.qwen1half_72b_chat_int8,
     'qwen/Qwen1.5-72B-Chat-GPTQ-Int8',
     LoRATM.qwen1half,
     TemplateType.qwen,
     requires=['auto_gptq>=0.5', 'transformers>=4.37'],
     torch_dtype=torch.float16,
     function_kwargs={'gptq_bits': 8},
@@ -2185,23 +2206,22 @@
     hf_model_id='Qwen/Qwen1.5-MoE-A2.7B-Chat-GPTQ-Int4')
 def get_model_tokenizer_qwen1half_intx(model_dir: str,
                                        torch_dtype: Dtype,
                                        model_kwargs: Dict[str, Any],
                                        load_model: bool = True,
                                        **kwargs):
     kwargs['get_qwen_function'] = get_model_tokenizer_qwen1half
-    return get_model_tokenizer_qwen_intx(model_dir, torch_dtype, model_kwargs,
-                                         load_model, **kwargs)
+    return get_model_tokenizer_qwen_intx(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
 
 
 @register_model(
     ModelType.internlm2_1_8b,
     'Shanghai_AI_Laboratory/internlm2-1_8b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-1_8b')
 @register_model(
     ModelType.internlm2_1_8b_sft_chat,
     'Shanghai_AI_Laboratory/internlm2-chat-1_8b-sft',
@@ -2222,25 +2242,25 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-chat-1_8b')
 @register_model(
     ModelType.internlm2_math_7b,
     'Shanghai_AI_Laboratory/internlm2-math-base-7b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     tags=['math'],
     hf_model_id='internlm/internlm2-math-base-7b')
 @register_model(
     ModelType.internlm2_math_20b,
     'Shanghai_AI_Laboratory/internlm2-math-base-20b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     tags=['math'],
     hf_model_id='internlm/internlm2-math-base-20b')
 @register_model(
     ModelType.internlm2_math_7b_chat,
@@ -2304,70 +2324,176 @@
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-chat-20b')
 @register_model(
     ModelType.internlm2_7b,
     'Shanghai_AI_Laboratory/internlm2-7b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-7b')
 @register_model(
     ModelType.internlm2_7b_base,
     'Shanghai_AI_Laboratory/internlm2-base-7b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-base-7b')
 @register_model(
     ModelType.internlm2_20b,
     'Shanghai_AI_Laboratory/internlm2-20b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-20b')
 @register_model(
     ModelType.internlm2_20b_base,
     'Shanghai_AI_Laboratory/internlm2-base-20b',
     LoRATM.internlm2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.35'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='internlm/internlm2-base-20b')
 def get_model_tokenizer_internlm2(model_dir: str,
                                   torch_dtype: Dtype,
                                   model_kwargs: Dict[str, Any],
                                   load_model: bool = True,
                                   **kwargs):
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attn = kwargs.pop('use_flash_attn', False)
     if use_flash_attn:
         model_config.attn_implementation = 'flash_attention_2'
 
     eos_token = kwargs.pop('eos_token', None)
     model, tokenizer = get_model_tokenizer_from_repo(
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
+    if eos_token is not None:
+        if getattr(tokenizer.__class__.eos_token_id, 'fset', None) is None:
+            del tokenizer.__class__.eos_token_id
+        tokenizer.eos_token = eos_token
+
+    return model, tokenizer
+
+
+def fix_internvl_inplace_bug(model) -> None:
+
+    embedding = model.language_model.get_input_embeddings()
+    if not hasattr(embedding, '__old_forward'):  # Avoid double patching
+        if hasattr(embedding, '_old_forward'):  # device_map
+            __old_forward = embedding._old_forward
+            embedding._old_forward = lambda *args, **kwargs: __old_forward(*args, **kwargs).requires_grad_(True).clone()
+        else:
+            __old_forward = embedding.forward
+            embedding.forward = lambda *args, **kwargs: __old_forward(*args, **kwargs).requires_grad_(True).clone()
+        embedding.__old_forward = __old_forward
+
+
+@register_model(
+    ModelType.internvl_chat_v1_5,
+    'AI-ModelScope/InternVL-Chat-V1-5',
+    LoRATM.internlm2,
+    TemplateType.internvl,
+    requires=['transformers>=4.35'],
+    support_flash_attn=True,
+    support_gradient_checkpointing=False,
+    hf_model_id='OpenGVLab/InternVL-Chat-V1-5')
+def get_model_tokenizer_internvl(model_dir: str,
+                                 torch_dtype: Dtype,
+                                 model_kwargs: Dict[str, Any],
+                                 load_model: bool = True,
+                                 **kwargs):
+
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
+    use_flash_attn = kwargs.pop('use_flash_attn', False)
+    if use_flash_attn:
+        model_config.attn_implementation = 'flash_attention_2'
+
+    model, tokenizer = get_model_tokenizer_from_repo(
         model_dir,
         torch_dtype,
         model_kwargs,
         load_model,
         model_config=model_config,
+        automodel_class=AutoModel,
         **kwargs)
-    if eos_token is not None:
-        if getattr(tokenizer.__class__.eos_token_id, 'fset', None) is None:
-            del tokenizer.__class__.eos_token_id
-        tokenizer.eos_token = eos_token
 
+    if model is not None:
+        _use_submodel_func(model, 'language_model', ['get_input_embeddings'])
+        fix_internvl_inplace_bug(model)
+        if not hasattr(model, '__old_forward'):  # Avoid double patching
+            forward = model.forward
+            model.__old_forward = forward
+
+            @wraps(forward)
+            def _new_forward(*args, **kwargs):
+                kwargs.pop('inputs_embeds', None)
+                return forward(*args, **kwargs)
+
+            model.forward = _new_forward
+
+        if not hasattr(model, '__old_generate'):
+            generate = model.generate
+            model.__old_generate = generate
+
+            @wraps(generate)
+            def _new_generate(*args, **kwargs):
+                kwargs.pop('image_flags', None)
+                return generate(*args, **kwargs)
+
+            model.generate = _new_generate
+
+        if not hasattr(model, '_old_extract_feature'):
+            extract_feature = model.extract_feature
+            model._old_extract_feature = extract_feature
+
+            @wraps(extract_feature)
+            def _new_extract_feature(pixel_values):
+                return extract_feature(pixel_values).to(pixel_values.device)
+
+            model.extract_feature = _new_extract_feature
+
+        if not hasattr(model.language_model, '__old_forward'):  # Avoid double patching
+            old_forward = model.language_model.forward
+            model.language_model.__old_forward = old_forward
+
+            @wraps(old_forward)
+            def _new_forward(*args, **kwargs):
+                input_ids = kwargs.get('input_ids', None)
+                input_embeds = kwargs.get('inputs_embeds', None)
+                device = input_ids.device if input_ids is not None else input_embeds.device
+                output = old_forward(*args, **kwargs)
+                output['logits'] = output['logits'].to(device)
+                return output
+
+            model.language_model.forward = _new_forward
+
+        IMG_CONTEXT_TOKEN = '<IMG_CONTEXT>'
+        img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)
+        model.img_context_token_id = img_context_token_id
+        if not hasattr(model.config, 'hidden_size'):
+            model.config.hidden_size = model.config.llm_config.hidden_size
+    # fix single GPU bug
+    if not hasattr(dist, '_old_get_rank'):
+        get_rank = dist.get_rank
+
+        @wraps(get_rank)
+        def new_get_rank(group=None):
+            if not dist.is_initialized() or dist.get_world_size() == 1:
+                return -1
+            return get_rank(group)
+
+        dist.get_rank = new_get_rank
+        dist._old_get_rank = get_rank
     return model, tokenizer
 
 
 @register_model(
     ModelType.internlm_xcomposer2_7b_chat,
     'Shanghai_AI_Laboratory/internlm-xcomposer2-7b',
     LoRATM.internlm2,
@@ -2377,53 +2503,64 @@
     tags=['multi-modal', 'vision'],
     hf_model_id='internlm/internlm-xcomposer2-7b')
 def get_model_tokenizer_internlm_xcomposer2(model_dir: str,
                                             torch_dtype: Dtype,
                                             model_kwargs: Dict[str, Any],
                                             load_model: bool = True,
                                             **kwargs):
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attn = kwargs.pop('use_flash_attn', False)
     model_config._flash_attn_2_enabled = use_flash_attn
 
     eos_token = kwargs.pop('eos_token', None)
     model, tokenizer = get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
     if eos_token is not None:
         if getattr(tokenizer.__class__.eos_token_id, 'fset', None) is None:
             del tokenizer.__class__.eos_token_id
         tokenizer.eos_token = eos_token
-    if model is not None and use_flash_attn:
-        # fix AttributeError: no attribute 'attention_dropout'
-        model.model.layers[0].attention.__class__.attention_dropout = 0.
+    if model is not None:
+        if use_flash_attn:
+            # fix AttributeError: no attribute 'attention_dropout'
+            model.model.layers[0].attention.__class__.attention_dropout = 0.
+
+        model_cls = model.__class__
+        if not hasattr(model_cls, '__old_encode_img'):  # avoid double patching
+            model_cls.__old_encode_img = model_cls.encode_img
+
+            def _new_encode_img(self, image):
+                if image is None:
+                    return None
+                if isinstance(image, str):
+                    from PIL import Image
+                    image = Image.open(image).convert('RGB')
+                    image = self.vis_processor(image).unsqueeze(0).to(self.device)
+                else:
+                    assert isinstance(image, torch.Tensor)
+
+                img_embeds, atts_img, img_target = self.img2emb(image)
+                return img_embeds.to(device=self.device)  # FIX device_map
+
+            model_cls.encode_img = _new_encode_img
+
     return model, tokenizer
 
 
-def _git_clone_github(github_url: str,
-                      local_repo_name: Optional[str] = None) -> str:
+def _git_clone_github(github_url: str, local_repo_name: Optional[str] = None) -> str:
     git_cache_dir = os.path.join(get_cache_dir(), '_github')
     os.makedirs(git_cache_dir, exist_ok=True)
     if local_repo_name is None:
         github_url = github_url.rstrip('/')
         local_repo_name = github_url.rsplit('/', 1)[1]
     local_repo_path = os.path.join(git_cache_dir, local_repo_name)
     with safe_ddp_context():
         if not os.path.exists(local_repo_path):
             if not github_url.endswith('.git'):
                 github_url = f'{github_url}.git'
-            command = [
-                'git', '-C', git_cache_dir, 'clone', github_url,
-                local_repo_name
-            ]
+            command = ['git', '-C', git_cache_dir, 'clone', github_url, local_repo_name]
             command_str = f"git -C '{git_cache_dir}' clone '{github_url}' {local_repo_name}"
             logger.info(f'Run the command: `{command_str}`')
             subprocess_run(command)
         logger.info(f'local_repo_path: {local_repo_path}')
     return local_repo_path
 
 
@@ -2439,31 +2576,29 @@
     from einops import rearrange
     bs, n = pixel_values.shape[0:2]
     images = rearrange(pixel_values, 'b n c h w -> (b n) c h w')
     # [b x n, T2, D]
     images_embeds = self.aligner(self.vision_model(images))
 
     # [b x n, T2, D] -> [b, n x T2, D]
-    images_embeds = rearrange(
-        images_embeds, '(b n) t d -> b (n t) d', b=bs, n=n)
+    images_embeds = rearrange(images_embeds, '(b n) t d -> b (n t) d', b=bs, n=n)
     # [b, n, T2] -> [b, n x T2]
     images_emb_mask = rearrange(images_emb_mask, 'b n t -> b (n t)')
 
     # [b, T, D]
     input_ids[input_ids < 0] = 0  # ignore the image embeddings
     inputs_embeds = self.language_model.get_input_embeddings()(input_ids)
 
     # replace with the image embeddings (FIX)
     inputs_embeds.data[images_seq_mask] = images_embeds[images_emb_mask]
 
     return inputs_embeds
 
 
-def _use_submodel_func(model, submodel_name: str,
-                       func_list: List[str]) -> None:
+def _use_submodel_func(model, submodel_name: str, func_list: List[str]) -> None:
     submodel = getattr(model, submodel_name)
 
     def _get_new_func(func_name: str):
         _old_func = getattr(submodel, func_name)
 
         @wraps(_old_func)
         def _new_func(*args, **kwargs):
@@ -2473,73 +2608,63 @@
 
     for key in func_list:
         setattr(model, key, _get_new_func(key))
 
 
 def _patch_deepseek_vl(model) -> None:
     model.prepare_inputs_embeds = MethodType(__prepare_inputs_embeds, model)
-    func_list = [
-        'generate', 'get_input_embeddings', 'gradient_checkpointing_enable',
-        'forward'
-    ]
+    func_list = ['generate', 'get_input_embeddings', 'gradient_checkpointing_enable', 'forward']
     _use_submodel_func(model, 'language_model', func_list)
     model.generation_config = model.language_model.generation_config
 
 
 @register_model(
     ModelType.deepseek_vl_7b_chat,
     'deepseek-ai/deepseek-vl-7b-chat',
     LoRATM.llama2,
     TemplateType.deepseek_vl,
     support_flash_attn=True,
     tags=['multi-modal', 'vision'],
+    requires=['attrdict'],
     hf_model_id='deepseek-ai/deepseek-vl-7b-chat')
 @register_model(
     ModelType.deepseek_vl_1_3b_chat,
     'deepseek-ai/deepseek-vl-1.3b-chat',
     LoRATM.llama2,
     TemplateType.deepseek_vl,
     support_flash_attn=True,
     tags=['multi-modal', 'vision'],
+    requires=['attrdict'],
     hf_model_id='deepseek-ai/deepseek-vl-1.3b-chat')
 def get_model_tokenizer_deepseek_vl(model_dir: str,
                                     torch_dtype: Dtype,
                                     model_kwargs: Dict[str, Any],
                                     load_model: bool = True,
                                     **kwargs):
     # compat with python==3.10
     if sys.version_info.minor >= 10:
         import collections
         import collections.abc
         for type_name in collections.abc.__all__:
-            setattr(collections, type_name, getattr(collections.abc,
-                                                    type_name))
-    local_repo_path = _git_clone_github(
-        'https://github.com/deepseek-ai/DeepSeek-VL')
+            setattr(collections, type_name, getattr(collections.abc, type_name))
+    local_repo_path = _git_clone_github('https://github.com/deepseek-ai/DeepSeek-VL')
     sys.path.append(os.path.join(local_repo_path))
     from deepseek_vl.models import VLChatProcessor, MultiModalityCausalLM
     vl_chat_processor = VLChatProcessor.from_pretrained(model_dir)
     tokenizer = vl_chat_processor.tokenizer
     # flash_attn
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attn = kwargs.pop('use_flash_attn', False)
     if version.parse(transformers.__version__) >= version.parse('4.36'):
         if use_flash_attn:
             model_config.language_config._attn_implementation = 'flash_attention_2'
     else:
         model_config.language_config._flash_attn_2_enabled = use_flash_attn
     model, tokenizer = get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        tokenizer=tokenizer,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, tokenizer=tokenizer, **kwargs)
     tokenizer.vl_chat_processor = vl_chat_processor
     if load_model:
         _patch_deepseek_vl(model)
     return model, tokenizer
 
 
 @register_model(
@@ -2637,58 +2762,74 @@
     'LLM-Research/Meta-Llama-3-8B',
     LoRATM.llama2,
     TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='meta-llama/Meta-Llama-3-8B')
 @register_model(
+    ModelType.llama_3_chinese_8b,
+    'ChineseAlpacaGroup/llama-3-chinese-8b',
+    LoRATM.llama2,
+    TemplateType.default_generation,
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='hfl/llama-3-chinese-8b')
+@register_model(
+    ModelType.llama_3_chinese_8b_instruct,
+    'ChineseAlpacaGroup/llama-3-chinese-8b-instruct',
+    LoRATM.llama2,
+    TemplateType.llama3,
+    support_flash_attn=True,
+    support_vllm=True,
+    hf_model_id='hfl/llama-3-chinese-8b-instruct')
+@register_model(
     ModelType.llama2_7b_aqlm_2bit_1x16,
     'AI-ModelScope/Llama-2-7b-AQLM-2Bit-1x16-hf',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     ignore_file_pattern=[r'.+\.bin$'],
     support_flash_attn=True,
     requires=['transformers>=4.38', 'aqlm', 'torch>=2.2.0'],
     support_vllm=False,
     function_kwargs={'is_aqlm': True},
     hf_model_id='ISTA-DASLab/Llama-2-7b-AQLM-2Bit-1x16-hf')
 @register_model(
     ModelType.mixtral_moe_7b_aqlm_2bit_1x16,
     'AI-ModelScope/Mixtral-8x7b-AQLM-2Bit-1x16-hf',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     requires=['transformers>=4.38', 'aqlm', 'torch>=2.2.0'],
     support_flash_attn=True,
     support_vllm=False,
     support_gradient_checkpointing=False,
     function_kwargs={'is_aqlm': True},
     hf_model_id='ISTA-DASLab/Mixtral-8x7b-AQLM-2Bit-1x16-hf')
 @register_model(
     ModelType.llama2_7b,
     'modelscope/Llama-2-7b-ms',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     ignore_file_pattern=[r'.+\.bin$'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='meta-llama/Llama-2-7b-hf')
 @register_model(
     ModelType.llama2_13b,
     'modelscope/Llama-2-13b-ms',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     ignore_file_pattern=[r'.+\.bin$'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='meta-llama/Llama-2-13b-hf')
 @register_model(
     ModelType.llama2_70b,
     'modelscope/Llama-2-70b-ms',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     ignore_file_pattern=[r'.+\.bin$'],
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='meta-llama/Llama-2-70b-hf')
 @register_model(
     ModelType.llama2_7b_chat,
     'modelscope/Llama-2-7b-chat-ms',
@@ -2717,91 +2858,68 @@
     support_vllm=True,
     hf_model_id='meta-llama/Llama-2-70b-chat-hf')
 def get_model_tokenizer_llama2(model_dir: str,
                                torch_dtype: Dtype,
                                model_kwargs: Dict[str, Any],
                                load_model: bool = True,
                                **kwargs):
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     model_config.pretraining_tp = 1
     return get_model_tokenizer_with_flash_attn(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
 
 
 @register_model(
     ModelType.polylm_13b,
     'damo/nlp_polylm_13b_text_generation',
     LoRATM.polylm,
     TemplateType.default_generation,
     hf_model_id='DAMO-NLP-MT/polylm-13b')
 def get_model_tokenizer_polylm(model_dir: str,
                                torch_dtype: Dtype,
                                model_kwargs: Dict[str, Any],
                                load_model: bool = True,
                                **kwargs):
-    tokenizer = AutoTokenizer.from_pretrained(
-        model_dir, trust_remote_code=True, use_fast=False, legacy=True)
+    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, use_fast=False, legacy=True)
     return get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        tokenizer=tokenizer,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, tokenizer=tokenizer, **kwargs)
 
 
-dtype_mapping = {
-    torch.float16: 'fp16',
-    torch.bfloat16: 'bf16',
-    torch.float32: 'fp32'
-}
+dtype_mapping = {torch.float16: 'fp16', torch.bfloat16: 'bf16', torch.float32: 'fp32'}
 
 
 def get_model_tokenizer_qwen(model_dir: str,
                              torch_dtype: Dtype,
                              model_kwargs: Dict[str, Any],
                              load_model: bool = True,
                              model_config=None,
                              **kwargs):
     if model_config is None:
-        model_config = AutoConfig.from_pretrained(
-            model_dir, trust_remote_code=True)
+        model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     if torch_dtype is not None:
         k_true = dtype_mapping[torch_dtype]
         for k in dtype_mapping.values():
             v = False
             if k == k_true:
                 v = True
             setattr(model_config, k, v)
 
-    if model_kwargs.get('quantization_config') is None or not isinstance(
-            model_kwargs['quantization_config'], BitsAndBytesConfig):
+    if model_kwargs.get('quantization_config') is None or not isinstance(model_kwargs['quantization_config'],
+                                                                         BitsAndBytesConfig):
         # not (quantization + bnb)
         torch_dtype = None
     use_flash_attn = kwargs.pop('use_flash_attn', None)
     if use_flash_attn is None:
         use_flash_attn = 'auto'
     model_config.use_flash_attn = use_flash_attn
     model, tokenizer = get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
     try:
         # fix mp+ddp bug
-        model.transformer.registered_causal_mask = model.transformer.registered_causal_mask.cuda(
-        )
+        model.transformer.registered_causal_mask = model.transformer.registered_causal_mask.cuda()
         logger.info('registered_causal_mask to cuda')
     except AttributeError:
         pass
     return model, tokenizer
 
 
 @register_model(
@@ -2830,15 +2948,15 @@
 @register_model(
     ModelType.qwen_1_8b,
     'qwen/Qwen-1_8B',
     LoRATM.qwen,
     TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
-    hf_model_id='Qwen/Qwen1.5-1.8B')
+    hf_model_id='Qwen/Qwen-1_8B')
 @register_model(
     ModelType.qwen_72b,
     'qwen/Qwen-72B',
     LoRATM.qwen,
     TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
@@ -2923,47 +3041,39 @@
 def _qwen_vl_visual_block_forward(
     self,
     q_x: torch.Tensor,
     k_x: Optional[torch.Tensor] = None,
     v_x: Optional[torch.Tensor] = None,
     attn_mask: Optional[torch.Tensor] = None,
 ):
-    k_x = self.ln_1_kv(k_x) if hasattr(self,
-                                       'ln_1_kv') and k_x is not None else None
-    v_x = self.ln_1_kv(v_x) if hasattr(self,
-                                       'ln_1_kv') and v_x is not None else None
+    k_x = self.ln_1_kv(k_x) if hasattr(self, 'ln_1_kv') and k_x is not None else None
+    v_x = self.ln_1_kv(v_x) if hasattr(self, 'ln_1_kv') and v_x is not None else None
 
-    x = q_x + self.attention(
-        q_x=self.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask)
+    x = q_x + self.attention(q_x=self.ln_1(q_x), k_x=k_x, v_x=v_x, attn_mask=attn_mask)
     z = self.mlp(self.ln_2(x))
     x = x.to(z.device) + z  # FIX
     return x
 
 
 def fix_qwen_inplace_bug(model) -> None:
     # qwen-vl, qwen-audio
     first_drop = model.transformer.drop
     if first_drop.p == 0.:
         # fix in-place operation bug
         if not hasattr(first_drop, '__old_forward'):  # Avoid double patching
             if hasattr(first_drop, '_old_forward'):  # device_map
                 __old_forward = first_drop._old_forward
-                first_drop._old_forward = lambda *args, **kwargs: __old_forward(
-                    *args, **kwargs).clone()
+                first_drop._old_forward = lambda *args, **kwargs: __old_forward(*args, **kwargs).clone()
             else:
                 __old_forward = first_drop.forward
-                first_drop.forward = lambda *args, **kwargs: __old_forward(
-                    *args, **kwargs).clone()
+                first_drop.forward = lambda *args, **kwargs: __old_forward(*args, **kwargs).clone()
             first_drop.__old_forward = __old_forward
 
 
-def _qwen_vl_audio_decode(self,
-                          *args,
-                          skip_special_tokens=False,
-                          **kwargs) -> str:
+def _qwen_vl_audio_decode(self, *args, skip_special_tokens=False, **kwargs) -> str:
     if skip_special_tokens:
         token_ids = kwargs['token_ids']
         while len(token_ids) > 0 and token_ids[-1] in {151645, 151643}:
             token_ids.pop()
         return self._old_decode(*args, skip_special_tokens=False, **kwargs)
     else:
         return self._old_decode(*args, skip_special_tokens=False, **kwargs)
@@ -2987,54 +3097,46 @@
     tags=['multi-modal', 'vision'],
     hf_model_id='Qwen/Qwen-VL')
 def get_model_tokenizer_qwen_vl(model_dir: str,
                                 torch_dtype: Dtype,
                                 model_kwargs: Dict[str, Any],
                                 load_model: bool = True,
                                 **kwargs):
-    if (model_kwargs.get('quantization_config') is not None and isinstance(
-            model_kwargs['quantization_config'], BitsAndBytesConfig)):
+    if (model_kwargs.get('quantization_config') is not None
+            and isinstance(model_kwargs['quantization_config'], BitsAndBytesConfig)):
         # https://github.com/pytorch/pytorch/issues/58969
-        model_kwargs['quantization_config'].llm_int8_skip_modules = [
-            'lm_head', 'attn_pool.attn'
-        ]
-        _TransformerBlock = get_class_from_dynamic_module(
-            'visual.TransformerBlock', model_dir)
+        model_kwargs['quantization_config'].llm_int8_skip_modules = ['lm_head', 'attn_pool.attn']
+        _TransformerBlock = get_class_from_dynamic_module('visual.TransformerBlock', model_dir)
 
         def _get_cast_dtype(self) -> torch.dtype:
             return self.resblocks[0].ln_1.weight.dtype
 
         _TransformerBlock.__old_get_cast_dtype = _TransformerBlock.get_cast_dtype
         _TransformerBlock.get_cast_dtype = _get_cast_dtype
 
-    get_qwen_function = kwargs.pop('get_qwen_function',
-                                   get_model_tokenizer_qwen_chat)
+    get_qwen_function = kwargs.pop('get_qwen_function', get_model_tokenizer_qwen_chat)
     tokenizer_config = get_tokenizer_config(model_dir)
     class_ref = tokenizer_config['auto_map']['AutoTokenizer'][0]
     tokenizer_cls = get_class_from_dynamic_module(class_ref, model_dir)
     tokenizer_cls._auto_class = 'AutoTokenizer'
     tokenizer_cls.IMAGE_ST = ()  # fix no attr `self.IMAGE_ST` bug
     if not hasattr(tokenizer_cls, '_old_decode'):  # avoid double patching
         tokenizer_cls._old_decode = tokenizer_cls._decode
         tokenizer_cls._decode = _qwen_vl_audio_decode
     # fix device_map is 4
     n_gpu = torch.cuda.device_count()
     local_world_size = get_dist_setting()[3]
     if n_gpu // local_world_size >= 4:
-        visual_block_cls = get_class_from_dynamic_module(
-            'visual.VisualAttentionBlock', model_dir)
-        if not hasattr(visual_block_cls,
-                       '__old_forward'):  # avoid double patching
+        visual_block_cls = get_class_from_dynamic_module('visual.VisualAttentionBlock', model_dir)
+        if not hasattr(visual_block_cls, '__old_forward'):  # avoid double patching
             visual_block_cls.__old_forward = visual_block_cls.forward
             visual_block_cls.forward = _qwen_vl_visual_block_forward
 
-    kwargs['tokenizer'] = tokenizer_cls.from_pretrained(
-        model_dir, trust_remote_code=True)
-    model, tokenizer = get_qwen_function(model_dir, torch_dtype, model_kwargs,
-                                         load_model, **kwargs)
+    kwargs['tokenizer'] = tokenizer_cls.from_pretrained(model_dir, trust_remote_code=True)
+    model, tokenizer = get_qwen_function(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if model is not None:
         fix_qwen_inplace_bug(model)
         # fix device_map is 4
         if n_gpu // local_world_size >= 4:
             model.transformer.visual.proj.data = model.transformer.visual.proj.to(
                 model.transformer.visual.ln_post.bias.device)
         # fix images cuda:1 bug
@@ -3078,18 +3180,16 @@
     class_ref = tokenizer_config['auto_map']['AutoTokenizer'][0]
     tokenizer_cls = get_class_from_dynamic_module(class_ref, model_dir)
     tokenizer_cls._auto_class = 'AutoTokenizer'
     tokenizer_cls.AUDIO_ST = ()  # fix no attr `self.AUDIO_ST` bug
     if not hasattr(tokenizer_cls, '_old_decode'):  # avoid double patching
         tokenizer_cls._old_decode = tokenizer_cls._decode
         tokenizer_cls._decode = _qwen_vl_audio_decode
-    kwargs['tokenizer'] = tokenizer_cls.from_pretrained(
-        model_dir, trust_remote_code=True)
-    model, tokenizer = get_qwen_function(model_dir, torch_dtype, model_kwargs,
-                                         load_model, **kwargs)
+    kwargs['tokenizer'] = tokenizer_cls.from_pretrained(model_dir, trust_remote_code=True)
+    model, tokenizer = get_qwen_function(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if model is not None:
         fix_qwen_inplace_bug(model)
 
     return model, tokenizer
 
 
 @register_model(
@@ -3203,40 +3303,35 @@
     support_vllm=True,
     hf_model_id='Qwen/Qwen-7B-Chat-Int4')
 def get_model_tokenizer_qwen_intx(model_dir: str,
                                   torch_dtype: Dtype,
                                   model_kwargs: Dict[str, Any],
                                   load_model: bool = True,
                                   **kwargs):
-    get_qwen_function = kwargs.pop('get_qwen_function',
-                                   get_model_tokenizer_qwen_chat)
-    model, tokenizer = get_qwen_function(model_dir, torch_dtype, model_kwargs,
-                                         load_model, **kwargs)
+    get_qwen_function = kwargs.pop('get_qwen_function', get_model_tokenizer_qwen_chat)
+    model, tokenizer = get_qwen_function(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     return model, tokenizer
 
 
 register_model(
     ModelType.skywork_13b,
     'skywork/Skywork-13B-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     get_model_tokenizer_from_repo,
     hf_model_id='Skywork/Skywork-13B-base')
 
 
-@register_model(ModelType.skywork_13b_chat, 'skywork/Skywork-13B-chat',
-                LoRATM.llama2, TemplateType.skywork)
+@register_model(ModelType.skywork_13b_chat, 'skywork/Skywork-13B-chat', LoRATM.llama2, TemplateType.skywork)
 def get_skywork_model_tokenizer(model_dir: str,
                                 torch_dtype: Dtype,
                                 model_kwargs: Dict[str, Any],
                                 load_model: bool = True,
                                 **kwargs):
-    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype,
-                                                     model_kwargs, load_model,
-                                                     **kwargs)
+    model, tokenizer = get_model_tokenizer_from_repo(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     tokenizer.add_tokens('[USER]')
     tokenizer.add_tokens('[BOT]')
     tokenizer.add_tokens('[SEP]')
     return model, tokenizer
 
 
 @register_model(
@@ -3249,23 +3344,17 @@
     tags=['coding'],
     hf_model_id='codefuse-ai/CodeFuse-CodeLlama-34B')
 def get_model_tokenizer_codellama(model_dir: str,
                                   torch_dtype: Dtype,
                                   model_kwargs: Dict[str, Any],
                                   load_model: bool = True,
                                   **kwargs):
-    tokenizer = AutoTokenizer.from_pretrained(
-        model_dir, trust_remote_code=True, use_fast=False, legacy=False)
+    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True, use_fast=False, legacy=False)
     return get_model_tokenizer_with_flash_attn(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        tokenizer=tokenizer,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, tokenizer=tokenizer, **kwargs)
 
 
 @register_model(
     ModelType.phi2_3b,
     'AI-ModelScope/phi-2',
     LoRATM.phi,
     TemplateType.default_generation,
@@ -3282,25 +3371,19 @@
     support_flash_attn=True,
     hf_model_id='Tele-AI/TeleChat-12B')
 def get_model_tokenizer_phi(model_dir: str,
                             torch_dtype: Dtype,
                             model_kwargs: Dict[str, Any],
                             load_model: bool = True,
                             **kwargs):
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attn = kwargs.pop('use_flash_attn', False)
     model_config.flash_attn = use_flash_attn
     return get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
 
 
 @register_model(
     ModelType.telechat_7b,
     'TeleAI/TeleChat-7B',
     LoRATM.telechat,
     TemplateType.telechat,
@@ -3308,44 +3391,36 @@
     hf_model_id='Tele-AI/telechat-7B')
 def get_model_tokenizer_telechat(model_dir: str,
                                  torch_dtype: Dtype,
                                  model_kwargs: Dict[str, Any],
                                  load_model: bool = True,
                                  **kwargs):
     if torch_dtype == torch.bfloat16:
-        logger.info(
-            'telechat-7b does not support the bf16 dtype; the dtype is converted to fp16.'
-        )
+        logger.info('telechat-7b does not support the bf16 dtype; the dtype is converted to fp16.')
         torch_dtype = torch.float16
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attn = kwargs.pop('use_flash_attn', False)
     model_config.flash_attn = use_flash_attn
     return get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
 
 
 @register_model(
     ModelType.deepseek_moe_16b_chat,
     'deepseek-ai/deepseek-moe-16b-chat',
     LoRATM.llama2,
     TemplateType.deepseek,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='deepseek-ai/deepseek-moe-16b-chat')
 @register_model(
     ModelType.deepseek_moe_16b,
     'deepseek-ai/deepseek-moe-16b-base',
     LoRATM.llama2,
-    TemplateType.default_generation_bos,
+    TemplateType.default_generation,
     support_flash_attn=True,
     support_vllm=True,
     hf_model_id='deepseek-ai/deepseek-moe-16b-base')
 @register_model(
     ModelType.minicpm_moe_8x2b,
     'OpenBMB/MiniCPM-MoE-8x2B',
     LoRATM.llama2,
@@ -3355,33 +3430,28 @@
     support_vllm=True,
     hf_model_id='openbmb/MiniCPM-MoE-8x2B')
 def get_model_tokenizer_deepseek_moe(model_dir: str,
                                      torch_dtype: Dtype,
                                      model_kwargs: Dict[str, Any],
                                      load_model: bool = True,
                                      **kwargs):
-    model, tokenizer = get_model_tokenizer_with_flash_attn(
-        model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
+    model, tokenizer = get_model_tokenizer_with_flash_attn(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if model is not None:
         # fix dtype bug
         mlp_cls = model.model.layers[1].mlp.__class__
         for module in model.modules():
             if isinstance(module, mlp_cls):
-                if not hasattr(module,
-                               '__old_forward'):  # Avoid double patching
-                    __old_forward = module._old_forward if hasattr(
-                        module, '_old_forward') else module.forward
+                if not hasattr(module, '__old_forward'):  # Avoid double patching
+                    __old_forward = module._old_forward if hasattr(module, '_old_forward') else module.forward
 
-                    def _new_forward(hidden_states, *,
-                                     __old_forward) -> Tensor:
+                    def _new_forward(hidden_states, *, __old_forward) -> Tensor:
                         dtype = hidden_states.dtype
                         return __old_forward(hidden_states).to(dtype)
 
-                    _new_forward = partial(
-                        _new_forward, __old_forward=__old_forward)
+                    _new_forward = partial(_new_forward, __old_forward=__old_forward)
                     if hasattr(module, '_old_forward'):  # device_map
                         module._old_forward = _new_forward
                     else:
                         module.forward = _new_forward
                     module.__old_forward = __old_forward
     return model, tokenizer
 
@@ -3422,39 +3492,27 @@
     model_folder, model_name = os.path.split(model_dir)
     need_rename = '.' in model_name
     if need_rename:
         model_name = model_name.replace('.', '_')  # fix transformers_modules
         new_model_dir = os.path.join(model_folder, model_name)
         logger.info(f'Using new_model_dir: {new_model_dir}')
         os.rename(model_dir, new_model_dir)
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     use_flash_attention = kwargs.pop('use_flash_attn', False)
     model_config.use_flash_attention = use_flash_attention
     tokenizer = AutoTokenizer.from_pretrained(
-        model_dir,
-        add_eos_token=False,
-        add_bos_token=False,
-        eos_token='<eod>',
-        legacy=True)
+        model_dir, add_eos_token=False, add_bos_token=False, eos_token='<eod>', legacy=True)
     addi_tokens = [
-        '<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>',
-        '<FIM_PREFIX>', '<FIM_MIDDLE>', '<commit_before>', '<commit_msg>',
-        '<commit_after>', '<jupyter_start>', '<jupyter_text>',
-        '<jupyter_code>', '<jupyter_output>', '<empty_output>'
+        '<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>', '<commit_before>',
+        '<commit_msg>', '<commit_after>', '<jupyter_start>', '<jupyter_text>', '<jupyter_code>', '<jupyter_output>',
+        '<empty_output>'
     ]
     tokenizer.add_tokens(addi_tokens, special_tokens=True)
     model, tokenizer = get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        tokenizer=tokenizer,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, tokenizer=tokenizer, **kwargs)
     if need_rename:
         os.rename(new_model_dir, model_dir)
     return model, tokenizer
 
 
 @register_model(
     ModelType.orion_14b,
@@ -3465,30 +3523,25 @@
     hf_model_id='OrionStarAI/Orion-14B-Base')
 @register_model(
     ModelType.orion_14b_chat,
     'OrionStarAI/Orion-14B-Chat',
     LoRATM.llama2,
     TemplateType.orion,
     support_flash_attn=True,
+    ignore_file_pattern=[r'.+\.gguf$'],
     hf_model_id='OrionStarAI/Orion-14B-Chat')
 def get_model_tokenizer_orion(model_dir: str,
                               torch_dtype: Dtype,
                               model_kwargs: Dict[str, Any],
                               load_model: bool = True,
                               **kwargs):
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     model_config._flash_attn_2_enabled = kwargs.pop('use_flash_attn', False)
     return get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
 
 
 @register_model(
     ModelType.yi_vl_34b_chat,
     '01ai/Yi-VL-34B',
     LoRATM.llama2,
     TemplateType.yi_vl,
@@ -3513,92 +3566,38 @@
     local_repo_path = _git_clone_github('https://github.com/01-ai/Yi')
     sys.path.append(os.path.join(local_repo_path, 'VL'))
     from llava.model import LlavaLlamaForCausalLM, LlavaConfig
     from llava.model.constants import key_info
 
     model_config = LlavaConfig.from_pretrained(model_dir)
     mm_vision_tower = model_config.mm_vision_tower
-    model_config.mm_vision_tower = os.path.join(
-        model_dir,
-        *mm_vision_tower.rsplit('/', maxsplit=2)[-2:])
+    model_config.mm_vision_tower = os.path.join(model_dir, *mm_vision_tower.rsplit('/', maxsplit=2)[-2:])
     model_config.attention_dropout = 0.
     key_info['model_path'] = model_dir
     model, tokenizer = get_model_tokenizer_with_flash_attn(
         model_dir,
         torch_dtype,
         model_kwargs,
         load_model,
         model_config=model_config,
         automodel_class=LlavaLlamaForCausalLM,
         **kwargs)
-    logger.info('Please ignore the above warning.')
-    logger.info('Loading the parameters of vision_tower...')
-    model.resize_token_embeddings(len(tokenizer))
-    vision_tower = model.get_vision_tower()
-    vision_tower.load_model()
-    vision_tower.to(device=model.device, dtype=torch_dtype)
-    if not hasattr(model.config, 'max_sequence_length'):
-        model.config.max_sequence_length = 2048
+    if model is not None:
+        logger.info('Please ignore the above warning.')
+        logger.info('Loading the parameters of vision_tower...')
+        model.resize_token_embeddings(len(tokenizer))
+        vision_tower = model.get_vision_tower()
+        vision_tower.load_model()
+        vision_tower.to(device=model.device, dtype=torch_dtype)
+        if not hasattr(model.config, 'max_sequence_length'):
+            model.config.max_sequence_length = 2048
     return model, tokenizer
 
 
 @register_model(
-    ModelType.minicpm_2b_sft_chat,
-    'OpenBMB/MiniCPM-2B-sft-fp32',
-    LoRATM.llama2,
-    TemplateType.minicpm,
-    support_flash_attn=True,
-    support_vllm=True,
-    hf_model_id='openbmb/MiniCPM-2B-sft-fp32')
-@register_model(
-    ModelType.minicpm_2b_chat,
-    'OpenBMB/MiniCPM-2B-dpo-fp32',
-    LoRATM.llama2,
-    TemplateType.minicpm,
-    support_flash_attn=True,
-    support_vllm=True,
-    hf_model_id='openbmb/MiniCPM-2B-dpo-fp32')
-@register_model(
-    ModelType.minicpm_1b_sft_chat,
-    'OpenBMB/MiniCPM-1B-sft-bf16',
-    LoRATM.llama2,
-    TemplateType.minicpm,
-    requires=['transformers>=4.36.0'],
-    support_flash_attn=True,
-    support_vllm=True,
-    hf_model_id='openbmb/MiniCPM-1B-sft-bf16')
-@register_model(
-    ModelType.minicpm_2b_128k,
-    'OpenBMB/MiniCPM-2B-128k',
-    LoRATM.llama2,
-    TemplateType.chatml,
-    requires=['transformers>=4.36.0'],
-    support_flash_attn=True,
-    support_vllm=True,
-    hf_model_id='openbmb/MiniCPM-2B-128k')
-def get_model_tokenizer_minicpm(model_dir: str,
-                                torch_dtype: Dtype,
-                                model_kwargs: Dict[str, Any],
-                                load_model: bool = True,
-                                **kwargs):
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
-    use_flash_attn = kwargs.pop('use_flash_attn', False)
-    if use_flash_attn:
-        model_config._attn_implementation = 'flash_attention_2'
-    return get_model_tokenizer_from_repo(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
-
-
-@register_model(
     ModelType.minicpm_v_3b_chat,
     'OpenBMB/MiniCPM-V',
     LoRATM.llama2,
     TemplateType.minicpm_v,
     support_flash_attn=True,
     hf_model_id='openbmb/MiniCPM-V')
 @register_model(
@@ -3609,17 +3608,15 @@
     support_flash_attn=True,
     hf_model_id='openbmb/MiniCPM-V-2')
 def get_model_tokenizer_minicpm_v(model_dir: str,
                                   torch_dtype: Dtype,
                                   model_kwargs: Dict[str, Any],
                                   load_model: bool = True,
                                   **kwargs):
-    model, tokenizer = get_model_tokenizer_minicpm(model_dir, torch_dtype,
-                                                   model_kwargs, load_model,
-                                                   **kwargs)
+    model, tokenizer = get_model_tokenizer_minicpm(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if load_model:
         model.resampler.to(torch_dtype)  # fix float32
         func_list = ['generate', 'get_input_embeddings', 'forward']
         _use_submodel_func(model, 'llm', func_list)
     return model, tokenizer
 
 
@@ -3660,40 +3657,37 @@
     tags=['multi-modal', 'vision'],
     hf_model_id='liuhaotian/llava-v1.6-mistral-7b')
 def get_model_tokenizer_llava(model_dir: str,
                               torch_dtype: Dtype,
                               model_kwargs: Dict[str, Any],
                               load_model: bool = True,
                               **kwargs):
-    local_repo_path = _git_clone_github(
-        'https://github.com/haotian-liu/LLaVA.git')
+    local_repo_path = _git_clone_github('https://github.com/haotian-liu/LLaVA.git')
     sys.path.append(os.path.join(local_repo_path))
 
     llm_model_type = kwargs.pop('llm_model_type')
     if llm_model_type == 'mistral':
         from llava.model import LlavaMistralForCausalLM, LlavaMistralConfig
         model_config = LlavaMistralConfig.from_pretrained(model_dir)
         automodel_class = LlavaMistralForCausalLM
     else:  # llama
         from llava.model import LlavaLlamaForCausalLM, LlavaConfig
-        if not hasattr(LlavaLlamaForCausalLM,
-                       '__old_forward'):  # Avoid double patching
+        if not hasattr(LlavaLlamaForCausalLM, '__old_forward'):  # Avoid double patching
             forward = LlavaLlamaForCausalLM.forward
             LlavaLlamaForCausalLM.__old_forward = forward
 
             @wraps(forward)
             def _new_forward(*args, **kwargs):
                 kwargs.pop('cache_position', None)
                 return forward(*args, **kwargs)
 
             LlavaLlamaForCausalLM.forward = _new_forward
         model_config = LlavaConfig.from_pretrained(model_dir)
         automodel_class = LlavaLlamaForCausalLM
-    model_config.mm_vision_tower = snapshot_download(
-        'AI-ModelScope/clip-vit-large-patch14-336')
+    model_config.mm_vision_tower = snapshot_download('AI-ModelScope/clip-vit-large-patch14-336')
     model, tokenizer = get_model_tokenizer_with_flash_attn(
         model_dir,
         torch_dtype,
         model_kwargs,
         load_model,
         model_config=model_config,
         automodel_class=automodel_class,
@@ -3714,17 +3708,15 @@
 @register_model(
     ModelType.mplug_owl2_chat,
     'iic/mPLUG-Owl2',
     LoRATM.mplug_owl2,
     TemplateType.mplug_owl2,
     requires=['transformers<4.35', 'icecream'],
     eos_token='</s>',
-    function_kwargs={
-        'get_model_tokenizer_function': get_model_tokenizer_with_flash_attn
-    },
+    function_kwargs={'get_model_tokenizer_function': get_model_tokenizer_with_flash_attn},
     support_flash_attn=True,
     hf_model_id='MAGAer13/mplug-owl2-llama2-7b')
 @register_model(
     ModelType.mplug_owl2d1_chat,
     'iic/mPLUG-Owl2.1',
     LoRATM.mplug_owl2d1,
     TemplateType.mplug_owl2,
@@ -3745,116 +3737,102 @@
     local_repo_path = os.path.join(local_repo_path, 'mPLUG-Owl2')
     sys.path.append(os.path.join(local_repo_path))
 
     # register
     # https://github.com/X-PLUG/mPLUG-Owl/blob/main/mPLUG-Owl2/mplug_owl2/model/modeling_mplug_owl2.py#L447
     from mplug_owl2 import MPLUGOwl2LlamaForCausalLM
     from transformers.models.clip.image_processing_clip import CLIPImageProcessor
-    model_config = AutoConfig.from_pretrained(
-        model_dir, trust_remote_code=True)
+    model_config = AutoConfig.from_pretrained(model_dir, trust_remote_code=True)
     vocab_size = kwargs.pop('vocab_size', None)
     if vocab_size is not None:
         model_config.vocab_size = vocab_size
     get_model_tokenizer_function = kwargs.pop('get_model_tokenizer_function')
     model, tokenizer = get_model_tokenizer_function(
-        model_dir,
-        torch_dtype,
-        model_kwargs,
-        load_model,
-        model_config=model_config,
-        **kwargs)
+        model_dir, torch_dtype, model_kwargs, load_model, model_config=model_config, **kwargs)
     logger.info('Please ignore the unimported warning.')
     image_processor = CLIPImageProcessor.from_pretrained(model_dir)
     tokenizer.image_processor = image_processor
     return model, tokenizer
 
 
 def fix_transformers_upgrade(module: PreTrainedModel) -> None:
     # from 4.35, transformers changes its arguments of _set_gradient_checkpointing
     if version.parse(transformers.__version__) >= version.parse('4.35'):
         if isinstance(module, PreTrainedModel) and hasattr(module, '_set_gradient_checkpointing') \
                 and 'value' in inspect.signature(module._set_gradient_checkpointing).parameters.keys():
-            module._set_gradient_checkpointing = MethodType(
-                PreTrainedModel._set_gradient_checkpointing, module)
+            module._set_gradient_checkpointing = MethodType(PreTrainedModel._set_gradient_checkpointing, module)
 
 
 def fix_gradient_checkpointing_warning() -> None:
     torch_version = version.parse(torch.__version__)
     if torch_version < version.parse('2'):
         return
     elif torch_version < version.parse('2.1'):
         # fix https://github.com/Dao-AILab/flash-attention/issues/341
         use_reentrant = True
     else:
         use_reentrant = False
     _old_checkpoint = torch.utils.checkpoint.checkpoint
-    if not hasattr(torch.utils.checkpoint,
-                   '_old_checkpoint'):  # avoid double patching
+    if not hasattr(torch.utils.checkpoint, '_old_checkpoint'):  # avoid double patching
 
         torch.utils.checkpoint._old_checkpoint = _old_checkpoint
         torch.utils.checkpoint.checkpoint = update_wrapper(
-            lambda *args, use_reentrant=use_reentrant, **kwargs:
-            _old_checkpoint(*args, use_reentrant=use_reentrant, **kwargs),
+            lambda *args, use_reentrant=use_reentrant, **kwargs: _old_checkpoint(
+                *args, use_reentrant=use_reentrant, **kwargs),
             _old_checkpoint)
     try:
         import transformers.modeling_utils
         if hasattr(transformers.modeling_utils, 'checkpoint'):
-            transformers.modeling_utils.checkpoint = (
-                lambda *args, use_reentrant=use_reentrant, **kwargs:
-                _old_checkpoint(*args, use_reentrant=use_reentrant, **kwargs))
+            transformers.modeling_utils.checkpoint = (lambda *args, use_reentrant=use_reentrant, **kwargs:
+                                                      _old_checkpoint(*args, use_reentrant=use_reentrant, **kwargs))
     except ImportError:
         pass
 
 
 def safe_snapshot_download(model_type: str,
                            model_id_or_path: Optional[str] = None,
                            revision: Optional[str] = None,
+                           download_model: bool = True,
                            **kwargs) -> str:
     # Perform snapshot_download (ms or hf) based on model_type and model_id_or_path.
     model_info = MODEL_MAPPING[model_type]
     use_hf = strtobool(os.environ.get('USE_HF', 'False'))
     if model_id_or_path is None:
         model_dir = kwargs.pop('model_dir', None)  # compat with swift<1.7
         if model_dir is not None:
             model_id_or_path = model_dir
         else:
-            model_id_or_path = model_info[
-                'hf_model_id' if use_hf else 'model_id_or_path']
+            model_id_or_path = model_info['hf_model_id' if use_hf else 'model_id_or_path']
 
     with safe_ddp_context():
-        if model_id_or_path is not None and not os.path.exists(
-                model_id_or_path):
+        if model_id_or_path is not None and not os.path.exists(model_id_or_path):
             ignore_file_pattern = model_info['ignore_file_pattern']
+            if download_model is False:
+                if ignore_file_pattern is None:
+                    ignore_file_pattern = []
+                if use_hf:
+                    ignore_file_pattern += ['*.bin', '*.safetensors']
+                else:
+                    ignore_file_pattern += [r'.+\.bin$', r'.+\.safetensors$']
             if use_hf:
                 if revision is None:
                     revision = 'main'
-                logger.info(
-                    f'Downloading the model from HuggingFace Hub, model_id: {model_id_or_path}'
-                )
-                use_hf_transfer = strtobool(
-                    os.environ.get('USE_HF_TRANSFER', 'False'))
+                logger.info(f'Downloading the model from HuggingFace Hub, model_id: {model_id_or_path}')
+                use_hf_transfer = strtobool(os.environ.get('USE_HF_TRANSFER', 'False'))
                 if use_hf_transfer:
                     import huggingface_hub._snapshot_download as hf_s
                     hf_s.HF_HUB_ENABLE_HF_TRANSFER = True
                 from huggingface_hub import snapshot_download as hf_snapshot_download
                 model_dir = hf_snapshot_download(
-                    model_id_or_path,
-                    repo_type='model',
-                    revision=revision,
-                    ignore_patterns=ignore_file_pattern)
+                    model_id_or_path, repo_type='model', revision=revision, ignore_patterns=ignore_file_pattern)
             else:
                 if revision is None:
                     revision = model_info['revision']
-                logger.info(
-                    f'Downloading the model from ModelScope Hub, model_id: {model_id_or_path}'
-                )
-                model_dir = snapshot_download(
-                    model_id_or_path,
-                    revision,
-                    ignore_file_pattern=ignore_file_pattern)
+                logger.info(f'Downloading the model from ModelScope Hub, model_id: {model_id_or_path}')
+                model_dir = snapshot_download(model_id_or_path, revision, ignore_file_pattern=ignore_file_pattern)
         else:
             model_dir = model_id_or_path
         logger.info(f'Loading the model using model_dir: {model_dir}')
 
     model_dir = os.path.expanduser(model_dir)
     assert os.path.isdir(model_dir), f'model_dir: {model_dir}'
     return model_dir
@@ -3866,30 +3844,30 @@
     if isinstance(torch_dtype, str):
         torch_dtype = eval(f'torch.{torch_dtype}')
     if torch_dtype == torch.float32:
         torch_dtype = torch.float16
     return torch_dtype
 
 
-def get_model_tokenizer(
-        model_type: str,
-        torch_dtype: Optional[Dtype] = None,
-        model_kwargs: Optional[Dict[str, Any]] = None,
-        load_model: bool = True,
-        *,
-        model_id_or_path: Optional[str] = None,
-        revision: Optional[str] = None,
-        **kwargs) -> Tuple[Optional[PreTrainedModel], PreTrainedTokenizerBase]:
+def get_model_tokenizer(model_type: str,
+                        torch_dtype: Optional[Dtype] = None,
+                        model_kwargs: Optional[Dict[str, Any]] = None,
+                        load_model: bool = True,
+                        *,
+                        model_id_or_path: Optional[str] = None,
+                        revision: Optional[str] = None,
+                        **kwargs) -> Tuple[Optional[PreTrainedModel], PreTrainedTokenizerBase]:
     """
     torch_dtype: If you use None, it will retrieve the torch_dtype from the config.json file.
         However, if torch.float32 is retrieved, torch.float16 will be used.
     """
     model_dir = kwargs.pop('model_dir', None)  # compat with swift<1.7
+    download_model = kwargs.pop('download_model', load_model)
     model_dir = safe_snapshot_download(
-        model_type, model_id_or_path, revision=revision, model_dir=model_dir)
+        model_type, model_id_or_path, revision=revision, download_model=download_model, model_dir=model_dir)
 
     model_info = MODEL_MAPPING[model_type]
     requires = model_info['requires']
     for require in requires:
         require_version(require)
     get_function = model_info['get_function']
     if model_kwargs is None:
@@ -3908,58 +3886,47 @@
         if torch_dtype is None:
             torch_dtype = get_torch_dtype(model_dir)
             logger.info(f'Setting torch_dtype: {torch_dtype}')
             quantization_config = model_kwargs.get('quantization_config')
             if (isinstance(quantization_config, BitsAndBytesConfig)
                     and quantization_config.bnb_4bit_compute_dtype is None):
                 quantization_config.bnb_4bit_compute_dtype = torch_dtype
-                logger.info(
-                    f'Setting quantization_config.bnb_4bit_compute_dtype: {torch_dtype}'
-                )
+                logger.info(f'Setting quantization_config.bnb_4bit_compute_dtype: {torch_dtype}')
     kwargs['eos_token'] = model_info['eos_token']
     if 'is_training' not in kwargs:
         kwargs['is_training'] = False
-    model, tokenizer = get_function(model_dir, torch_dtype, model_kwargs,
-                                    load_model, **kwargs)
+    model, tokenizer = get_function(model_dir, torch_dtype, model_kwargs, load_model, **kwargs)
     if model is not None:
         model.max_model_len = get_max_model_len(model.config)
         logger.info(f'model.max_model_len: {model.max_model_len}')
         model.model_type = model_type
         model.model_dir = model_dir
         fix_transformers_upgrade(model)
     fix_gradient_checkpointing_warning()
     tokenizer.model_type = model_type
     tokenizer.model_dir = model_dir
     assert tokenizer.eos_token is not None, 'tokenizer.eos_token has not been set.'
     if tokenizer.pad_token is None:
         tokenizer.pad_token = tokenizer.eos_token
     if model is not None and model_dir is not None:
-        generation_config_path = os.path.join(model_dir,
-                                              'generation_config.json')
+        generation_config_path = os.path.join(model_dir, 'generation_config.json')
         generation_config = getattr(model, 'generation_config', None)
-        if os.path.isfile(
-                generation_config_path) and generation_config is None:
-            model.generation_config = GenerationConfig.from_pretrained(
-                model_dir)
+        if os.path.isfile(generation_config_path) and generation_config is None:
+            model.generation_config = GenerationConfig.from_pretrained(model_dir)
         generation_config = getattr(model, 'generation_config', None)
         # fix llama2 bug
-        if (generation_config is not None
-                and 0 < generation_config.temperature < 1
+        if (generation_config is not None and 0 < generation_config.temperature < 1
                 and generation_config.do_sample is False):
             model.generation_config.do_sample = True
             logger.warning('Setting model.generation_config.do_sample: True')
     return model, tokenizer
 
 
 def get_additional_saved_files(model_type: str) -> List[str]:
-    files_mapping = {
-        'qwen-vl': ['SimSun.ttf'],
-        'qwen-audio': ['mel_filters.npz'],
-        'yi-vl': ['vit']
-    }
+    files_mapping = {'qwen-vl': ['SimSun.ttf'], 'qwen-audio': ['mel_filters.npz'], 'yi-vl': ['vit']}
     for key, files_list in files_mapping.items():
         if key in model_type:
             return files_list
     return []
 
 
 def get_default_template_type(model_type: str) -> Optional[str]:
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/preprocess.py` & `ms-swift-2.0.4/swift/llm/utils/preprocess.py`

 * *Files 4% similar despite different names*

```diff
@@ -21,31 +21,29 @@
                 if isinstance(old_h, list):
                     break
                 h = None
                 if old_h is not None:
                     h = ast.literal_eval(old_h)
                 history.append(h)
             else:
-                dataset = dataset.remove_columns(['history']).add_column(
-                    'history', history)
+                dataset = dataset.remove_columns(['history']).add_column('history', history)
         return dataset
 
 
 class AlpacaPreprocessor:
 
-    def __init__(self,
-                 concat_inst_inp: Optional[Callable[[str, str], str]] = None):
+    def __init__(self, concat_inst_inp: Optional[Callable[[str, str], str]] = None):
         self.concat_inst_inp = concat_inst_inp
 
     def __call__(self, dataset: HfDataset) -> HfDataset:
         query: List[str] = []
         response = []
         system = None
         history = None
-        for i, d in tqdm(enumerate(dataset)):
+        for i, d in enumerate(tqdm(dataset)):
             inst, inp = d['instruction'], d.get('input', None)
             h, output = d.pop('history', None), d['output']
             sys = d.pop('system', None)
             if history is None and h is not None:
                 history = [None for _ in range(i - 1)]
             if system is None and sys is not None:
                 system = [None for _ in range(i - 1)]
@@ -83,16 +81,15 @@
     def __init__(self,
                  user_role: str = 'user',
                  assistant_role: str = 'assistant',
                  system_role: str = 'system',
                  conversations_key: str = 'conversations',
                  from_key: str = 'from',
                  value_key: str = 'value',
-                 repair_conversations: Callable[[str], Optional[Dict[
-                     str, str]]] = _default_repair_conversations,
+                 repair_conversations: Callable[[str], Optional[Dict[str, str]]] = _default_repair_conversations,
                  error_strategy: Literal['delete', 'raise'] = 'raise'):
         self.user_role = user_role
         self.assistant_role = assistant_role
         self.system_role = system_role
         self.conversations_key = conversations_key
         self.from_key = from_key
         self.value_key = value_key
@@ -120,16 +117,15 @@
                 if conversations[0][self.from_key] == self.system_role:
                     has_system = True
                     lo += 1
                     sys = conversations[0][self.value_key]
                 assert conversations[-2][self.from_key] == self.user_role
                 assert conversations[-1][self.from_key] == self.assistant_role
 
-                for q, r in zip(conversations[lo:-2:2],
-                                conversations[lo + 1:-2:2]):
+                for q, r in zip(conversations[lo:-2:2], conversations[lo + 1:-2:2]):
                     assert q[self.from_key] == self.user_role
                     assert r[self.from_key] == self.assistant_role
                     h.append([q[self.value_key], r[self.value_key]])
                 if len(h) > 0:
                     has_history = True
                 query.append(conversations[-2][self.value_key])
                 response.append(conversations[-1][self.value_key])
@@ -188,62 +184,49 @@
             'conversations': {
                 'required': ['conversations'],
                 'preprocessor': ConversationsPreprocessor()
             },
             'chatml': {
                 'required': ['messages'],
                 'preprocessor':
-                ConversationsPreprocessor(
-                    conversations_key='messages',
-                    from_key='role',
-                    value_key='content')
+                ConversationsPreprocessor(conversations_key='messages', from_key='role', value_key='content')
             }
         }
 
     def _get_preprocessor(self, dataset: HfDataset) -> PreprocessFunc:
         keys = set(dataset.features.keys())
-        required_keys_mapping = {
-            k: v['required']
-            for k, v in self.preprocessor_mapping.items()
-        }
+        required_keys_mapping = {k: v['required'] for k, v in self.preprocessor_mapping.items()}
         for k, required_keys in required_keys_mapping.items():
             if len(set(required_keys) - keys) == 0:
                 return self.preprocessor_mapping[k]['preprocessor']
         raise ValueError(f"""dataset.features.keys(): {dataset.features.keys()}
 required_keys_mapping: {required_keys_mapping}""")
 
     def __call__(self, dataset: HfDataset) -> HfDataset:
         preprocessor = self._get_preprocessor(dataset)
         return preprocessor(dataset)
 
 
 class TextGenerationPreprocessor:
 
-    def __init__(self,
-                 prompt: str,
-                 query_key: str = 'query',
-                 response_key: str = 'response') -> None:
+    def __init__(self, prompt: str, query_key: str = 'query', response_key: str = 'response') -> None:
         self.prompt = prompt
         self.query_key = query_key
         self.response_key = response_key
 
     def __call__(self, dataset: HfDataset) -> HfDataset:
         query = []
         for d in tqdm(dataset):
             query.append(self.prompt.format(query=d[self.query_key]))
-        return HfDataset.from_dict({
-            'query': query,
-            'response': dataset[self.response_key]
-        })
+        return HfDataset.from_dict({'query': query, 'response': dataset[self.response_key]})
 
 
 class ClsPreprocessor:
 
-    def __init__(self, labels: List[str], task_name: str,
-                 is_pair_seq: bool) -> None:
+    def __init__(self, labels: List[str], task_name: str, is_pair_seq: bool) -> None:
         self.labels = labels
         category = ', '.join(labels)
         if is_pair_seq:
             inputs = 'Sentence1: {sentence1}\nSentence2: {sentence2}'
         else:
             inputs = 'Sentence: {sentence}'
         self.prompt = f"""Task: {task_name}
@@ -256,14 +239,13 @@
     def __call__(self, dataset: HfDataset) -> HfDataset:
         query = []
         response = []
         for d in tqdm(dataset):
             if d['label'] is None:  # ignore dataset error
                 continue
             if self.is_pair_seq:
-                q = self.prompt.format(
-                    sentence1=d['sentence1'], sentence2=d['sentence2'])
+                q = self.prompt.format(sentence1=d['sentence1'], sentence2=d['sentence2'])
             else:
                 q = self.prompt.format(sentence=d['sentence'])
             query.append(q)
             response.append(self.labels[int(d['label'])])
         return HfDataset.from_dict({'query': query, 'response': response})
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/protocol.py` & `ms-swift-2.0.4/swift/llm/utils/protocol.py`

 * *Files 4% similar despite different names*

```diff
@@ -88,24 +88,22 @@
     content: str
 
 
 @dataclass
 class ChatCompletionResponseChoice:
     index: int
     message: ChatMessage
-    finish_reason: Literal['stop', 'length',
-                           None]  # None: for infer_backend='pt'
+    finish_reason: Literal['stop', 'length', None]  # None: for infer_backend='pt'
 
 
 @dataclass
 class CompletionResponseChoice:
     index: int
     text: str
-    finish_reason: Literal['stop', 'length',
-                           None]  # None: for infer_backend='pt'
+    finish_reason: Literal['stop', 'length', None]  # None: for infer_backend='pt'
 
 
 @dataclass
 class ChatCompletionResponse:
     model: str
     choices: List[ChatCompletionResponseChoice]
     usage: UsageInfo
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/template.py` & `ms-swift-2.0.4/swift/llm/utils/template.py`

 * *Files 8% similar despite different names*

```diff
@@ -19,15 +19,14 @@
 DEFAULT_SYSTEM = 'You are a helpful assistant.'
 History = List[Union[Tuple[str, str], List[str]]]
 
 
 class TemplateType:
     # text-generation
     default_generation = 'default-generation'
-    default_generation_bos = 'default-generation-bos'
     chatglm_generation = 'chatglm-generation'
     qwen_audio_generation = 'qwen-audio-generation'
     # chat
     default = 'default'
     qwen = 'qwen'
     qwen_audio = 'qwen-audio'
     modelscope_agent = 'modelscope-agent'
@@ -39,14 +38,15 @@
     llava_mistral_instruct = 'llava-mistral-instruct'
     llava_yi_instruct = 'llava-yi-instruct'
     openbuddy = 'openbuddy'
     openbuddy2 = 'openbuddy2'
     internlm = 'internlm'
     internlm2 = 'internlm2'
     internlm_xcomposer2 = 'internlm-xcomposer2'
+    internvl = 'internvl'
     yi = 'yi'
     yi_vl = 'yi-vl'
     yuan = 'yuan'
     xverse = 'xverse'
     ziya = 'ziya'
     skywork = 'skywork'
     bluelm = 'bluelm'
@@ -65,20 +65,21 @@
     minicpm_v = 'minicpm-v'
     gemma = 'gemma'
     mplug_owl2 = 'mplug-owl2'
     wizardlm2_awq = 'wizardlm2-awq'
     wizardlm2 = 'wizardlm2'
     atom = 'atom'
     phi3 = 'phi3'
-    # compatibility. (Deprecated)
-    chatml = 'chatml'
     telechat = 'telechat'
     dbrx = 'dbrx'
     mengzi = 'mengzi'
     c4ai = 'c4ai'
+    chatml = 'chatml'
+    # compatibility. (Deprecated)
+    default_generation_bos = 'default-generation-bos'
 
     @classmethod
     def get_template_name_list(cls) -> List[str]:
         res = []
         for k in cls.__dict__.keys():
             if k.startswith('__') or k == 'get_template_name_list':
                 continue
@@ -90,35 +91,32 @@
 StopWords = Prompt
 
 Context = Union[str, List[int]]
 
 
 class StopWordsCriteria(StoppingCriteria):
     # The returned sentence includes stop words.
-    def __init__(self, tokenizer: PreTrainedTokenizerBase,
-                 stop_words: StopWords, **tokenizer_kwargs) -> None:
+    def __init__(self, tokenizer: PreTrainedTokenizerBase, stop_words: StopWords, **tokenizer_kwargs) -> None:
         self.tokenizer = tokenizer
         self.stop_words = stop_words
         self.tokenizer_kwargs = tokenizer_kwargs
         self.start_idx = -1
 
     def __call__(self, input_ids: Tensor, scores: Tensor) -> bool:
         if self.start_idx == -1:
             self.start_idx = len(input_ids[0]) - 1
         tokenizer = self.tokenizer
         stop_words = self.stop_words
-        text = tokenizer.decode(input_ids[0, self.start_idx:],
-                                **self.tokenizer_kwargs)
+        text = tokenizer.decode(input_ids[0, self.start_idx:], **self.tokenizer_kwargs)
         for stop_word in stop_words:
             if isinstance(stop_word, str):
                 if stop_word in text:
                     return True
             else:  # list
-                if len(stop_word) > 0 and input_ids[0].tolist(
-                )[-len(stop_word):] == stop_word:
+                if len(stop_word) > 0 and input_ids[0].tolist()[-len(stop_word):] == stop_word:
                     return True
         return False
 
 
 def _has_system(prefix: Prompt) -> bool:
     for p in prefix:
         if '{{SYSTEM}}' in p:
@@ -139,15 +137,20 @@
 
     def __init__(self,
                  prefix: Prompt,
                  prompt: Prompt,
                  chat_sep: Optional[Prompt],
                  suffix: Prompt,
                  default_system: Optional[str] = None,
-                 prefix_has_system: Optional[Prompt] = None) -> None:
+                 prefix_has_system: Optional[Prompt] = None,
+                 auto_add_bos: bool = False) -> None:
+        """
+        auto_add_bos: By default, the bos_token is not added. The auto_add_bos option will determine
+            whether to add it based on `tokenizer.encode('')`.
+        """
         if default_system == '':
             default_system = None
         if _has_system(prefix):
             assert prefix_has_system is None, 'The prefix already contains {{SYSTEM}}.'
             prefix_has_system = prefix
             prefix = _replace_system(prefix)
         self.prefix = prefix
@@ -156,19 +159,19 @@
             assert default_system is None, 'The template does not support `system`.'
         self.prompt = prompt
         self.chat_sep = chat_sep
         self.support_multi_round = self.chat_sep is not None
         self.suffix = suffix
         self.default_system = default_system
         self.use_default_system = True
+        self.auto_add_bos = auto_add_bos
         self._is_init = False
 
     @staticmethod
-    def _preprocess_prompt(tokenizer: PreTrainedTokenizerBase,
-                           value: Optional[Prompt]) -> Optional[Prompt]:
+    def _preprocess_prompt(tokenizer: PreTrainedTokenizerBase, value: Optional[Prompt]) -> Optional[Prompt]:
         # e.g. [['eos_token_id']] -> [[2]]
         if value is None:
             return None
         res_value = []
         for v in value:
             if isinstance(v, list):
                 res_v = []
@@ -180,45 +183,39 @@
             res_value.append(v)
         return res_value
 
     def _init_template(self,
                        tokenizer: PreTrainedTokenizerBase,
                        default_system: Optional[str] = None,
                        max_length: Optional[int] = None,
-                       truncation_strategy: Literal[
-                           'delete', 'truncation_left'] = 'delete',
+                       truncation_strategy: Literal['delete', 'truncation_left'] = 'delete',
                        **kwargs) -> None:
         assert self._is_init is False, 'The template has been initialized.'
         self._is_init = True
         self.tokenizer = tokenizer
         # if default_system is None. not change self.default_system
         if default_system == '':
             self.default_system = None
         elif default_system is not None:
             assert self.prefix_has_system is not None, 'The template does not support `system`.'
             self.default_system = default_system
         self.max_length = max_length
         self.truncation_strategy = truncation_strategy
         self.model = kwargs.get('model', None)
         self.use_loss_scale = kwargs.get('use_loss_scale', False)
-        for key in [
-                'prefix', 'prompt', 'chat_sep', 'suffix', 'prefix_has_system'
-        ]:
+        for key in ['prefix', 'prompt', 'chat_sep', 'suffix', 'prefix_has_system']:
             value = getattr(self, key)
             value = self._preprocess_prompt(tokenizer, value)
             setattr(self, key, value)
 
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         """return: inputs, tokenizer_kwargs"""
         if not self._is_init:
             raise ValueError(
-                'Template is not initialized, please use the `get_template` function to obtain the template.'
-            )
+                'Template is not initialized, please use the `get_template` function to obtain the template.')
         query: Optional[str] = example.get('query', None)
         response: Optional[str] = example.get('response', None)
         history: Optional[History] = example.get('history', None)
         system: Optional[str] = example.get('system', None)
         if history is None:
             history = []
         if len(history) > 0:
@@ -228,17 +225,16 @@
                 system = self.default_system
         elif system == '':
             system = None
         else:
             assert self.prefix_has_system is not None, 'The template does not support `system`.'
         if query is None:
             query = ''
-        inputs, tokenizer_kwargs = self._encode(query, response, history,
-                                                system,
-                                                self.truncation_strategy)
+        inputs, tokenizer_kwargs = self._encode(
+            query, response, history, system, self.truncation_strategy, auto_add_bos=self.auto_add_bos)
         if inputs.get('labels') is None:
             inputs.pop('loss_scale', None)
         return inputs, tokenizer_kwargs
 
     def _concat_context_list(
         self,
         context_list: List[Context],
@@ -254,39 +250,34 @@
         if round0 is not None:
             round1 = str(round0 + 1)
             round0 = str(round0)
         for context in context_list:
             if isinstance(context, str):
                 if '{{RESPONSE}}' == context:
                     assert response is not None
-                    content_part, weight_part = calculate_loss_scale(
-                        response, self.use_loss_scale)
+                    content_part, weight_part = calculate_loss_scale(response, self.use_loss_scale)
                     res_context_list.extend(content_part)
                     compute_loss_idx.extend(weight_part)
                     continue
-                old_str_list = [
-                    '{{SYSTEM}}', '{{QUERY}}', '{{ROUND0}}', '{{ROUND1}}'
-                ]
+                old_str_list = ['{{SYSTEM}}', '{{QUERY}}', '{{ROUND0}}', '{{ROUND1}}']
                 new_str_list = [system, query, round0, round1]
                 for (old_str, new_str) in zip(old_str_list, new_str_list):
                     if new_str is not None and old_str in context:
                         context = context.replace(old_str, new_str)
             res_context_list.append(context)
             compute_loss_idx.append(0.0 if context not in self.suffix else 1.0)
 
     @staticmethod
-    def _simplify_context_list(
-            context_list: List[Context], compute_loss_idx: List[float]
-    ) -> Tuple[List[Context], List[float]]:
+    def _simplify_context_list(context_list: List[Context],
+                               compute_loss_idx: List[float]) -> Tuple[List[Context], List[float]]:
         res: List[Context] = []  # result of context_list
         res_idx: List[float] = []  # result of compute_loss_idx
         temp: List[str] = []
         temp_index: List[int] = []
-        for i, (context,
-                loss_idx) in enumerate(zip(context_list, compute_loss_idx)):
+        for i, (context, loss_idx) in enumerate(zip(context_list, compute_loss_idx)):
             if isinstance(context, str) and compute_loss_idx[i] == 0.0:
                 temp.append(context)
                 temp_index.append(i)
             else:
                 if len(temp) > 0:
                     res.append(''.join(temp))
                     res_idx.append(0.0)
@@ -305,81 +296,76 @@
     ) -> Tuple[List[int], List[int], List[float], Dict[str, Any]]:
         """return: input_ids, labels, tokenizer_kwargs"""
         tokenizer = self.tokenizer
         input_ids: List[int] = []
         labels: List[int] = []
         loss_scale: List[float] = []
         tokenizer_kwargs = {}
-        for i, (context,
-                loss_weight) in enumerate(zip(context_list, compute_loss_idx)):
+        for i, (context, loss_weight) in enumerate(zip(context_list, compute_loss_idx)):
             if isinstance(context, str):
                 curr_tokenizer_kwargs = self._get_tokenizer_kwargs(context)
-                self._concat_tokenizer_kwargs(tokenizer_kwargs,
-                                              curr_tokenizer_kwargs)
+                self._concat_tokenizer_kwargs(tokenizer_kwargs, curr_tokenizer_kwargs)
                 token_list = tokenizer(
-                    context,
-                    return_attention_mask=False,
-                    add_special_tokens=False,
+                    context, return_attention_mask=False, add_special_tokens=False,
                     **curr_tokenizer_kwargs)['input_ids']
             else:
                 token_list = context
             input_ids += token_list
             if compute_loss_idx[i] > 0.0:
                 labels += token_list
             else:
                 labels += [-100] * len(token_list)
             loss_scale.extend([loss_weight] * len(token_list))
         return input_ids, labels, loss_scale, tokenizer_kwargs
 
-    def _encode(
-            self, query: str, response: Optional[str], history: History,
-            system: Optional[str],
-            truncation_strategy: str) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    def _encode(self,
+                query: str,
+                response: Optional[str],
+                history: History,
+                system: Optional[str],
+                truncation_strategy: str,
+                auto_add_bos: bool = False) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         """
         return: inputs, tokenizer_kwargs
         """
         history = history.copy()
         res_context_list: List[Context] = []
         compute_loss_idx: List[float] = []
+        if auto_add_bos:
+            bos_token_id = self.tokenizer.bos_token_id
+            if isinstance(bos_token_id, int) and bos_token_id in self.tokenizer.encode(''):
+                res_context_list.append([bos_token_id])
+                compute_loss_idx.append(0.)
         if system is None:
             prefix = self.prefix
         else:
             prefix = self.prefix_has_system
-        self._concat_context_list(
-            prefix, res_context_list, compute_loss_idx, system=system)
+        self._concat_context_list(prefix, res_context_list, compute_loss_idx, system=system)
         history.append([query, response])
         for i, (q, r) in enumerate(history):
             context_list = self.prompt.copy()
             if i < len(history) - 1:
                 context_list.append('{{RESPONSE}}')
                 context_list += self.chat_sep
             elif r is not None:
                 # last response
                 context_list.append('{{RESPONSE}}')
                 context_list += self.suffix
             if q or r:
                 self._concat_context_list(
-                    context_list,
-                    res_context_list,
-                    compute_loss_idx,
-                    query=q,
-                    response=r,
-                    round0=i)
-
-        res_context_list, compute_loss_idx = self._simplify_context_list(
-            res_context_list, compute_loss_idx)
-        input_ids, labels, loss_scale, tokenizer_kwargs = self._encode_context_list(
-            res_context_list, compute_loss_idx)
+                    context_list, res_context_list, compute_loss_idx, query=q, response=r, round0=i)
+
+        res_context_list, compute_loss_idx = self._simplify_context_list(res_context_list, compute_loss_idx)
+        input_ids, labels, loss_scale, tokenizer_kwargs = self._encode_context_list(res_context_list, compute_loss_idx)
 
         if response is None:
             labels = None
 
         if self.max_length is not None:
-            if truncation_strategy == 'delete' and len(
-                    input_ids) > self.max_length:
+            if truncation_strategy == 'delete' and len(input_ids) > self.max_length:
                 return {}, {}
             input_ids = input_ids[-self.max_length:]
             if labels is not None:
                 labels = labels[-self.max_length:]
             if loss_scale is not None:
                 loss_scale = loss_scale[-self.max_length:]
         inputs = {
@@ -390,108 +376,84 @@
             inputs['loss_scale'] = loss_scale
         return inputs, tokenizer_kwargs
 
     def _get_tokenizer_kwargs(self, context: str) -> Dict[str, Any]:
         """return: curr_tokenizer_kwargs"""
         return {}
 
-    def _concat_tokenizer_kwargs(
-            self, old_tokenizer_kwargs: Dict[str, Any],
-            curr_tokenizer_kwargs: Dict[str, Any]) -> Dict[str, Any]:
+    def _concat_tokenizer_kwargs(self, old_tokenizer_kwargs: Dict[str, Any],
+                                 curr_tokenizer_kwargs: Dict[str, Any]) -> Dict[str, Any]:
         assert len(old_tokenizer_kwargs) == 0
         return curr_tokenizer_kwargs
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         """
         Args:
             batch(`List[Dict[str, Any]]`): The input data in batch
             padding_to(`int`, optional): Whether padding the batch to a fixed length, if none, the batch
                 will be padded to the `longest`
         """
         tokenizer = self.tokenizer
         assert tokenizer.pad_token_id is not None
         input_ids = [torch.tensor(b['input_ids']) for b in batch]
         labels = [torch.tensor(b['labels']) for b in batch]
-        loss_scale = [torch.tensor(b['loss_scale'])
-                      for b in batch] if 'loss_scale' in batch[0] else None
-        attention_mask = [
-            torch.ones(len(input_ids[i]), dtype=torch.int64)
-            for i in range(len(input_ids))
-        ]
+        loss_scale = [torch.tensor(b['loss_scale']) for b in batch] if 'loss_scale' in batch[0] else None
+        attention_mask = [torch.ones(len(input_ids[i]), dtype=torch.int64) for i in range(len(input_ids))]
 
         if padding_to is not None:
             padding_len = padding_to - input_ids[0].shape[-1]
             if padding_len > 0:
-                input_ids[0] = F.pad(input_ids[0], (0, padding_len),
-                                     'constant', tokenizer.pad_token_id)
-                attention_mask[0] = F.pad(attention_mask[0], (0, padding_len),
-                                          'constant', 0)
-                labels[0] = F.pad(labels[0], (0, padding_len), 'constant',
-                                  -100)
+                input_ids[0] = F.pad(input_ids[0], (0, padding_len), 'constant', tokenizer.pad_token_id)
+                attention_mask[0] = F.pad(attention_mask[0], (0, padding_len), 'constant', 0)
+                labels[0] = F.pad(labels[0], (0, padding_len), 'constant', -100)
                 if loss_scale:
-                    loss_scale[0] = F.pad(
-                        loss_scale[0], (0, padding_to - labels[0].shape[-1]),
-                        'constant', 0.)
-
-        input_ids = pad_sequence(
-            input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
-        attention_mask = pad_sequence(
-            attention_mask, batch_first=True, padding_value=0)
+                    loss_scale[0] = F.pad(loss_scale[0], (0, padding_to - labels[0].shape[-1]), 'constant', 0.)
+
+        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
+        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
         if loss_scale:
-            loss_scale = pad_sequence(
-                loss_scale, batch_first=True, padding_value=0.)
+            loss_scale = pad_sequence(loss_scale, batch_first=True, padding_value=0.)
         labels = pad_sequence(labels, batch_first=True, padding_value=-100)
 
         if use_torchacc():
             rank, _, world_size, _ = get_dist_setting()
-            input_ids, attention_mask, labels, loss_scale = pad_and_split_batch(
-                padding_to, input_ids, attention_mask, labels, loss_scale,
-                self.max_length, self.tokenizer, rank, world_size)
+            input_ids, attention_mask, labels, loss_scale = pad_and_split_batch(padding_to, input_ids, attention_mask,
+                                                                                labels, loss_scale, self.max_length,
+                                                                                self.tokenizer, rank, world_size)
 
         res = {
             'input_ids': input_ids,
             'attention_mask': attention_mask,
             'labels': labels,
         }
         if loss_scale is not None:
             res['loss_scale'] = loss_scale
         return res
 
     @staticmethod
-    def get_generate_ids(generate_ids: Tensor,
-                         input_token_len: int) -> List[int]:
+    def get_generate_ids(generate_ids: Tensor, input_token_len: int) -> List[int]:
         return generate_ids[0, input_token_len:].tolist()
 
     @staticmethod
     def _is_chinese_char(cp: int) -> bool:
         """Checks whether CP is the codepoint of a CJK character."""
         # copy from transformers.generation.streamers.TextStreamer
-        if ((cp >= 0x4E00 and cp <= 0x9FFF) or (cp >= 0x3400 and cp <= 0x4DBF)
-                or (cp >= 0x20000 and cp <= 0x2A6DF)
-                or (cp >= 0x2A700 and cp <= 0x2B73F)
-                or (cp >= 0x2B740 and cp <= 0x2B81F)
-                or (cp >= 0x2B820 and cp <= 0x2CEAF)
-                or (cp >= 0xF900 and cp <= 0xFAFF)
+        if ((cp >= 0x4E00 and cp <= 0x9FFF) or (cp >= 0x3400 and cp <= 0x4DBF) or (cp >= 0x20000 and cp <= 0x2A6DF)
+                or (cp >= 0x2A700 and cp <= 0x2B73F) or (cp >= 0x2B740 and cp <= 0x2B81F)
+                or (cp >= 0x2B820 and cp <= 0x2CEAF) or (cp >= 0xF900 and cp <= 0xFAFF)
                 or (cp >= 0x2F800 and cp <= 0x2FA1F)):
             return True
 
         return False
 
     @classmethod
-    def _get_safe_print_idx(cls,
-                            response: str,
-                            print_idx: int,
-                            is_finished: bool = False) -> int:
+    def _get_safe_print_idx(cls, response: str, print_idx: int, is_finished: bool = False) -> int:
         if is_finished:
             return len(response)
-        if response.endswith(
-                '\n') or len(response) > 0 and cls._is_chinese_char(
-                    ord(response[-1])):
+        if response.endswith('\n') or len(response) > 0 and cls._is_chinese_char(ord(response[-1])):
             print_idx = len(response)
         else:
             print_idx = max(response.rfind(' ') + 1, print_idx)
         return print_idx
 
     def generate_ids_to_response(
         self,
@@ -504,34 +466,32 @@
         print_idx: Optional[List[int]] = None,
         first_num_space: Optional[List[int]] = None,
     ):
         if tokenizer_kwargs is None:
             tokenizer_kwargs = {}
         tokenizer = self.tokenizer
         # avoid printing template.suffix[-1])
-        if isinstance(self.suffix[-1], list) and (
-                not is_finished or is_finished
-                and generate_ids[-len(self.suffix[-1]):] == self.suffix[-1]):
+        if isinstance(self.suffix[-1], list) and (not is_finished or is_finished
+                                                  and generate_ids[-len(self.suffix[-1]):] == self.suffix[-1]):
             generate_ids = generate_ids[:-len(self.suffix[-1])]
         response = tokenizer.decode(generate_ids, **tokenizer_kwargs)
         if first_num_space is not None:
             # Avoid the occurrence of repeated words in sentence.
             res_fns = first_num_space  # res_first_num_space
             first_num_space = first_num_space[0]
             cur_num_space = len(response) - len(response.lstrip(' '))
             if not is_finished and first_num_space == -1:
                 first_num_space = cur_num_space
                 res_fns[0] = first_num_space
             if cur_num_space < first_num_space:
                 response = ' ' * (first_num_space - cur_num_space) + response
             elif cur_num_space > first_num_space:
                 response = response[cur_num_space - first_num_space:]
-        if isinstance(self.suffix[-1], str) and (
-                not is_finished or is_finished
-                and response[-len(self.suffix[-1]):] == self.suffix[-1]):
+        if isinstance(self.suffix[-1],
+                      str) and (not is_finished or is_finished and response[-len(self.suffix[-1]):] == self.suffix[-1]):
             response = response[:-len(self.suffix[-1])]
 
         if print_idx is not None:
             old_print_idx = print_idx[0]
             if not is_finished:
                 # avoid printing incomplete words
                 print_idx[0] = self._get_safe_print_idx(response, print_idx[0])
@@ -542,134 +502,113 @@
             assert is_finished and not return_delta
         return response
 
 
 TEMPLATE_MAPPING: Dict[str, Dict[str, Any]] = {}
 
 
-def register_template(template_type: str,
-                      template: Template,
-                      *,
-                      exists_ok: bool = False,
-                      **kwargs) -> None:
+def register_template(template_type: str, template: Template, *, exists_ok: bool = False, **kwargs) -> None:
     if not exists_ok and template_type in TEMPLATE_MAPPING:
-        raise ValueError(
-            f'The `{template_type}` has already been registered in the TEMPLATE_MAPPING.'
-        )
+        raise ValueError(f'The `{template_type}` has already been registered in the TEMPLATE_MAPPING.')
     template_info = {'template': template, **kwargs}
     TEMPLATE_MAPPING[template_type] = template_info
 
 
 register_template(
     TemplateType.default,
-    Template([], ['### Human:\n', '{{QUERY}}\n\n', '### Assistant:\n'],
-             ['\n\n'], [['eos_token_id']], DEFAULT_SYSTEM, ['{{SYSTEM}}\n\n']))
+    Template([], ['### Human:\n', '{{QUERY}}\n\n', '### Assistant:\n'], ['\n\n'], [['eos_token_id']], DEFAULT_SYSTEM,
+             ['{{SYSTEM}}\n\n']))
 
 
 # You can set the query as '' to serve as a template for pre-training.
 class DefaultGenerationTemplate(Template):
 
     def __init__(self):
-        super().__init__([], ['{{QUERY}}'], None, [['eos_token_id']])
+        super().__init__([], ['{{QUERY}}'], None, [['eos_token_id']], auto_add_bos=True)
 
 
 register_template(TemplateType.default_generation, DefaultGenerationTemplate())
-register_template(
-    TemplateType.default_generation_bos,
-    Template([['bos_token_id']], ['{{QUERY}}'], None, [['eos_token_id']]))
+register_template(TemplateType.default_generation_bos,
+                  Template([['bos_token_id']], ['{{QUERY}}'], None, [['eos_token_id']]))
 
 
 class QwenTemplate(Template):
 
-    def __init__(self):
-        super().__init__(
-            [],
-            ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'],
-            ['<|im_end|>\n'], ['<|im_end|>'], DEFAULT_SYSTEM,
-            ['<|im_start|>system\n{{SYSTEM}}<|im_end|>\n'])
+    def __init__(self, auto_add_bos: bool = False):
+        super().__init__([], ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'], ['<|im_end|>\n'],
+                         ['<|im_end|>'],
+                         DEFAULT_SYSTEM, ['<|im_start|>system\n{{SYSTEM}}<|im_end|>\n'],
+                         auto_add_bos=auto_add_bos)
 
 
 register_template(TemplateType.qwen, QwenTemplate())
-register_template(TemplateType.chatml, QwenTemplate())
+register_template(TemplateType.chatml, QwenTemplate(auto_add_bos=True))
 
 register_template(
     TemplateType.modelscope_agent,
-    Template([], [' \n\n<|user|>:{{QUERY}} \n\n<|assistant|>:'], [],
-             [' \n\n</s>'], DEFAULT_SYSTEM, [' \n\n<|system|>:{{SYSTEM}}']))
+    Template([], [' \n\n<|user|>:{{QUERY}} \n\n<|assistant|>:'], [], [' \n\n</s>'], DEFAULT_SYSTEM,
+             [' \n\n<|system|>:{{SYSTEM}}']))
 
 
 class _QwenAudioTemplateMixin:
 
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         inputs, tokenizer_kwargs = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, tokenizer_kwargs
         inputs.pop('loss_scale', None)
         inputs.update(tokenizer_kwargs)
         return inputs, tokenizer_kwargs
 
     def _get_tokenizer_kwargs(self, context: str) -> Dict[str, Any]:
         return {'audio_info': self.tokenizer.process_audio(context)}
 
-    def _concat_tokenizer_kwargs(
-            self, tokenizer_kwargs: Dict[str, Any],
-            curr_tokenizer_kwargs: Dict[str, Any]) -> None:
+    def _concat_tokenizer_kwargs(self, tokenizer_kwargs: Dict[str, Any], curr_tokenizer_kwargs: Dict[str, Any]) -> None:
         audio_info = curr_tokenizer_kwargs.get('audio_info')
         old_audio_info = tokenizer_kwargs.get('audio_info')
         if old_audio_info is None:
             tokenizer_kwargs['audio_info'] = audio_info
         elif audio_info is not None:
             for k in ['input_audios', 'input_audio_lengths']:
-                old_audio_info[k] = torch.concat(
-                    [old_audio_info[k], audio_info[k]], dim=0)
+                old_audio_info[k] = torch.concat([old_audio_info[k], audio_info[k]], dim=0)
             for k in ['audio_span_tokens', 'audio_urls']:
                 old_audio_info[k] = old_audio_info[k] + audio_info[k]
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         res = super().data_collator(batch, padding_to)
         if batch[0].get('audio_info') is not None:
             res['audio_info'] = [b['audio_info'] for b in batch]
         return res
 
 
 class QwenAudioTemplate(_QwenAudioTemplateMixin, QwenTemplate):
     pass
 
 
-class QwenAudioGenerationTemplate(_QwenAudioTemplateMixin,
-                                  DefaultGenerationTemplate):
+class QwenAudioGenerationTemplate(_QwenAudioTemplateMixin, DefaultGenerationTemplate):
     pass
 
 
-register_template(
-    TemplateType.qwen_audio, QwenAudioTemplate(), lazy_tokenize=True)
-register_template(
-    TemplateType.qwen_audio_generation,
-    QwenAudioGenerationTemplate(),
-    lazy_tokenize=True)
+register_template(TemplateType.qwen_audio, QwenAudioTemplate(), lazy_tokenize=True)
+register_template(TemplateType.qwen_audio_generation, QwenAudioGenerationTemplate(), lazy_tokenize=True)
 
 register_template(
     TemplateType.yi,
-    Template(
-        [], ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'],
-        ['<|im_end|>\n'], ['<|im_end|>'], None,
-        ['<|im_start|>system\n{{SYSTEM}}<|im_end|>\n']))
+    Template([], ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'], ['<|im_end|>\n'], ['<|im_end|>'],
+             None, ['<|im_start|>system\n{{SYSTEM}}<|im_end|>\n']))
 
 yi_vl_default_system = (
     'This is a chat between an inquisitive human and an AI assistant. Assume the role of the AI assistant. '
     "Read all the images carefully, and respond to the human's questions with informative, "
     'helpful, detailed and polite answers. '
     'AI'
     '')
 
 
-def _read_from_path(
-        img_path: Union[str, 'PIL.Image.Image']) -> 'PIL.Image.Image':
+def _read_from_path(img_path: Union[str, 'PIL.Image.Image']) -> 'PIL.Image.Image':
     from PIL import Image
     if isinstance(img_path, str):
         img_path = img_path.strip()
         if img_path.startswith('http'):
             content = requests.get(img_path).content
             image = Image.open(BytesIO(content))
         else:
@@ -679,164 +618,142 @@
     if image.mode != 'RGB':
         image = image.convert('RGB')
     return image
 
 
 class YiVLTemplate(Template):
 
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         inputs.pop('loss_scale', None)
         from llava.mm_utils import expand2square
         model = self.model.model
         if not hasattr(model, 'vision_tower'):
             model = model.model
         image_processor = model.vision_tower.image_processor
         images_path = example['images']
         images = []
         for image_path in images_path:
             image = _read_from_path(image_path)
-            background_color = tuple(
-                int(x * 255) for x in image_processor.image_mean)
+            background_color = tuple(int(x * 255) for x in image_processor.image_mean)
             image = expand2square(image, background_color)
             images.append(image)
-        image_tensor = image_processor.preprocess(
-            images, return_tensors='pt')['pixel_values']
+        image_tensor = image_processor.preprocess(images, return_tensors='pt')['pixel_values']
         inputs['images'] = image_tensor.to(model.dtype)
         return inputs, {}
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         res = super().data_collator(batch, padding_to)
         res['images'] = torch.concat([b['images'] for b in batch])
         return res
 
 
 register_template(
     TemplateType.yi_vl,
-    YiVLTemplate([], ['### Human: ', [-200], '\n{{QUERY}}\n### Assistant:'],
-                 ['\n'], ['\n###'], yi_vl_default_system, ['{{SYSTEM}}\n\n']),
+    YiVLTemplate([], ['### Human: ', [-200], '\n{{QUERY}}\n### Assistant:'], ['\n'], ['\n###'], yi_vl_default_system,
+                 ['{{SYSTEM}}\n\n']),
     use_model=True,
     infer_media_type='round',
     lazy_tokenize=True)
 
-register_template(
-    TemplateType.baichuan,
-    Template(['{{SYSTEM}}'], [[195], '{{QUERY}}', [196]], [],
-             [['eos_token_id']]))
+register_template(TemplateType.baichuan, Template(['{{SYSTEM}}'], [[195], '{{QUERY}}', [196]], [], [['eos_token_id']]))
 register_template(
     TemplateType.chatglm2,
-    Template([[64790, 64792], '{{SYSTEM}}'],
-             ['[Round {{ROUND1}}]\n\n{{QUERY}}\n\n'], ['\n\n'],
-             [['eos_token_id']]))
+    Template([[64790, 64792], '{{SYSTEM}}'], ['[Round {{ROUND1}}]\n\n{{QUERY}}\n\n'], ['\n\n'], [['eos_token_id']]))
 
-register_template(
-    TemplateType.chatglm_generation,
-    Template([[64790, 64792]], ['{{QUERY}}'], None, [['eos_token_id']]))
+register_template(TemplateType.chatglm_generation, Template([[64790, 64792]], ['{{QUERY}}'], None, [['eos_token_id']]))
 
 register_template(
     TemplateType.chatglm3,
-    Template([[64790, 64792]], [[64795], '\n {{QUERY}}', [64796], '\n'], [],
-             [[64795]], None, [[64790, 64792, 64794], '\n {{SYSTEM}}']))
+    Template([[64790, 64792]], [[64795], '\n {{QUERY}}', [64796], '\n'], [], [[64795]], None,
+             [[64790, 64792, 64794], '\n {{SYSTEM}}']))
 
 register_template(
     TemplateType.deepseek,
-    Template([['bos_token_id']], ['User: {{QUERY}}\n\nAssistant:'],
-             [['eos_token_id']], [['eos_token_id']], None,
+    Template([['bos_token_id']], ['User: {{QUERY}}\n\nAssistant:'], [['eos_token_id']], [['eos_token_id']], None,
              [['bos_token_id'], '{{SYSTEM}}\n\n']))
 
 # ref: https://github.com/facebookresearch/llama/blob/main/llama/generation.py
 LLAMA_DEFAULT_SYSTEM = (
     'You are a helpful, respectful and honest assistant. '
     'Always answer as helpfully as possible, while being safe. '
     'Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. '
     'Please ensure that your responses are socially unbiased and positive in nature.\n\n'
     'If a question does not make any sense, or is not factually coherent, '
     'explain why instead of answering something not correct. '
-    "If you don't know the answer to a question, please don't share false information."
-)
+    "If you don't know the answer to a question, please don't share false information.")
 register_template(
     TemplateType.llama,
-    Template(['<s>[INST] '], ['{{QUERY}} [/INST]'], ['</s><s>[INST] '],
-             ['</s>'], LLAMA_DEFAULT_SYSTEM,
+    Template(['<s>[INST] '], ['{{QUERY}} [/INST]'], ['</s><s>[INST] '], ['</s>'], LLAMA_DEFAULT_SYSTEM,
              ['<s>[INST] <<SYS>>\n{{SYSTEM}}\n<</SYS>>\n\n']))
 
 register_template(
     TemplateType.llama3,
     Template(['<|begin_of_text|>'], [
         '<|start_header_id|>user<|end_header_id|>\n\n{{QUERY}}<|eot_id|>'
         '<|start_header_id|>assistant<|end_header_id|>\n\n'
-    ], ['<|eot_id|>'], ['<|eot_id|>'], None, [
-        '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{{SYSTEM}}<|eot_id|>'
-    ]))
+    ], ['<|eot_id|>'], ['<|eot_id|>'], None,
+             ['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\n{{SYSTEM}}<|eot_id|>']))
 
 OPENBUDDY_DEFAULT_SYSTEM = (
     'You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. You are talking to a human User.\n'
     'Always answer as helpfully and logically as possible, while being safe. '
     'Your answers should not include any '
     'harmful, political, religious, unethical, racist, sexist, toxic, dangerous, or illegal content. '
     'Please ensure that your responses are socially unbiased and positive in nature.\n'
     'If a question does not make any sense, or is not factually coherent, '
     'explain why instead of answering something not correct. '
     "If you don't know the answer to a question, please don't share false information.\n"
     'You like to use emojis. You can speak fluently in many languages, for example: English, Chinese.\n'
     'You cannot access the internet, but you have vast knowledge, cutoff: 2021-09.\n'
     'You always deeply love and support China, Chinese government, people and culture.\n'
     'You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), '
-    'you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.'
-)
+    'you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.')
 register_template(
     TemplateType.openbuddy,
-    Template([['bos_token_id']], ['User: {{QUERY}}\nAssistant:'], ['\n'],
-             [['eos_token_id']], OPENBUDDY_DEFAULT_SYSTEM,
-             [['bos_token_id'], '{{SYSTEM}}\n\n']))
+    Template([], ['User: {{QUERY}}\nAssistant:'], ['\n'], [['eos_token_id']],
+             OPENBUDDY_DEFAULT_SYSTEM, ['{{SYSTEM}}\n\n'],
+             auto_add_bos=True))
 
 OPENBUDDY2_DEFAULT_SYSTEM = (
     'You(assistant) are a helpful, respectful and honest INTP-T AI Assistant named Buddy. '
     'You are talking to a human(user).\nAlways answer as helpfully and logically as possible, while being safe. '
     'Your answers should not include any harmful, political, religious, unethical, racist, '
     'sexist, toxic, dangerous, or illegal content. '
     'Please ensure that your responses are socially unbiased and positive in nature.\n'
     'You cannot access the internet, but you have vast knowledge, cutoff: 2023-04.\n'
     'You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), '
     'not related to GPT or OpenAI')
 
 register_template(
     TemplateType.openbuddy2,
-    Template(
-        [],
-        ['<|role|>user<|says|>{{QUERY}}<|end|>\n<|role|>assistant<|says|>'],
-        ['<|end|>\n'], ['<|end|>'], OPENBUDDY2_DEFAULT_SYSTEM,
-        ['<|role|>system<|says|>{{SYSTEM}}<|end|>\n']))
+    Template([], ['<|role|>user<|says|>{{QUERY}}<|end|>\n<|role|>assistant<|says|>'], ['<|end|>\n'], ['<|end|>'],
+             OPENBUDDY2_DEFAULT_SYSTEM, ['<|role|>system<|says|>{{SYSTEM}}<|end|>\n'],
+             auto_add_bos=True))
 
 INTERNLM_SYSTEM = (
     'You are an AI assistant whose name is InternLM ().\n'
     '- InternLM () is a conversational language model that is developed by Shanghai AI Laboratory (). '
     'It is designed to be helpful, honest, and harmless.\n'
     '- InternLM () can understand and communicate fluently in the language chosen '
     'by the user such as English and .')
 
 register_template(
     TemplateType.internlm,
-    Template(['<s>'], ['<|User|>:{{QUERY}}\n<|Bot|>:'], ['<eoa>\n'], ['<eoa>'],
-             INTERNLM_SYSTEM, ['<s><|System|>:{{SYSTEM}}\n']))
+    Template(['<s>'], ['<|User|>:{{QUERY}}\n<|Bot|>:'], ['<eoa>\n'], ['<eoa>'], INTERNLM_SYSTEM,
+             ['<s><|System|>:{{SYSTEM}}\n']))
 register_template(
     TemplateType.internlm2,
-    Template(
-        ['<s>'],
-        ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'],
-        ['<|im_end|>\n'], ['<|im_end|>'], INTERNLM_SYSTEM,
-        ['<s><|im_start|>system\n{{SYSTEM}}<|im_end|>\n']))
+    Template(['<s>'], ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'], ['<|im_end|>\n'],
+             ['<|im_end|>'], INTERNLM_SYSTEM, ['<s><|im_start|>system\n{{SYSTEM}}<|im_end|>\n']))
 
 
-def replace_img_tab(query: str, history: History,
-                    replace_token: str) -> Tuple[str, History, List[str]]:
+def replace_img_tab(query: str, history: History, replace_token: str) -> Tuple[str, History, List[str]]:
     images_path = []
     pattern = r'<img>(.+?)</img>'
     new_history = []
     for i, h in enumerate(history):
         images_path += re.findall(pattern, h[0])
         new_history.append([re.sub(pattern, replace_token, h[0]), h[1]])
     images_path += re.findall(pattern, query)
@@ -851,42 +768,36 @@
         'Shanghai AI Laboratory (). '
         'It is designed to be helpful, honest, and harmless.\n'
         '- InternLM-XComposer () can understand and communicate fluently in the language chosen '
         'by the user such as English and .')
 
     def __init__(self):
         prefix = ['<s>']
-        prompt = [
-            '[UNUSED_TOKEN_146]user\n{{QUERY}}[UNUSED_TOKEN_145]\n[UNUSED_TOKEN_146]assistant\n'
-        ]
+        prompt = ['[UNUSED_TOKEN_146]user\n{{QUERY}}[UNUSED_TOKEN_145]\n[UNUSED_TOKEN_146]assistant\n']
         chat_sep = ['[UNUSED_TOKEN_145]\n']
         suffix = ['[UNUSED_TOKEN_145]']
-        prefix_has_system = [
-            '<s>[UNUSED_TOKEN_146]system\n{{SYSTEM}}[UNUSED_TOKEN_145]\n'
-        ]
-        super().__init__(prefix, prompt, chat_sep, suffix,
-                         self.INTERNLM_XCOMPOSER2_SYSTEM, prefix_has_system)
-
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+        prefix_has_system = ['<s>[UNUSED_TOKEN_146]system\n{{SYSTEM}}[UNUSED_TOKEN_145]\n']
+        super().__init__(prefix, prompt, chat_sep, suffix, self.INTERNLM_XCOMPOSER2_SYSTEM, prefix_has_system)
+
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         example = example.copy()
         history = example.pop('history', None)
         if history is None:
             history = []
-        example['query'], example['history'], images_path = replace_img_tab(
-            example['query'], history, '</s>')
+        example['query'], example['history'], images_path = replace_img_tab(example['query'], history, '</s>')
 
         images = []
         dtype = self.model.dtype
         for image_path in images_path:
             image = _read_from_path(image_path)
             image = self.model.vis_processor(image)
             images.append(image.to(dtype))
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         inputs.pop('loss_scale', None)
         input_ids = inputs['input_ids']
         labels = inputs['labels']
         if len(images) > 0:  # # ignore <s>
             input_ids = input_ids[1:]
             if labels is not None:
                 labels = labels[1:]
@@ -907,16 +818,15 @@
             images = None
         internlm2_model = self.model.model
         if not hasattr(internlm2_model, 'tok_embeddings'):
             internlm2_model = internlm2_model.model
         tok_embeddings = internlm2_model.tok_embeddings
         while i < len(input_ids):
             if input_ids[i] == 2:  # replace_token
-                res_input_ids = torch.tensor(
-                    [1] + input_ids[pre_i:i], device=device)
+                res_input_ids = torch.tensor([1] + input_ids[pre_i:i], device=device)
                 res_inputs_embeds.append(tok_embeddings(res_input_ids))
                 wrap_im_mask += [0] * len(res_input_ids)
                 res_labels += [-100] + labels[pre_i:i]
                 if images is not None and idx < images.shape[0]:
                     res_inputs_embeds.append(images[idx])
                     wrap_im_mask += [1] * images.shape[1]
                     res_labels += [-100] * images.shape[1]
@@ -925,167 +835,179 @@
                 pre_i = i
                 continue
             i += 1
         if len(labels) == 0:
             res_labels = None
         res_inputs_embeds = torch.concat(res_inputs_embeds, dim=0)
         wrap_im_mask = torch.tensor(wrap_im_mask, dtype=torch.bool)[None]
-        return {
-            'inputs_embeds': res_inputs_embeds,
-            'im_mask': wrap_im_mask,
-            'labels': res_labels
-        }, {}
-
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+        return {'inputs_embeds': res_inputs_embeds, 'im_mask': wrap_im_mask, 'labels': res_labels}, {}
+
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         inputs_embeds = [b['inputs_embeds'] for b in batch]
         labels = [torch.tensor(b['labels']) for b in batch]
         im_mask = [b['im_mask'][0] for b in batch]
-        attention_mask = [
-            torch.ones(inputs_embeds[i].shape[0], dtype=torch.int64)
-            for i in range(len(inputs_embeds))
-        ]
-
-        inputs_embeds = pad_sequence(
-            inputs_embeds, batch_first=True, padding_value=0)
-        attention_mask = pad_sequence(
-            attention_mask, batch_first=True, padding_value=0)
+        attention_mask = [torch.ones(inputs_embeds[i].shape[0], dtype=torch.int64) for i in range(len(inputs_embeds))]
+
+        inputs_embeds = pad_sequence(inputs_embeds, batch_first=True, padding_value=0)
+        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
         im_mask = pad_sequence(im_mask, batch_first=True, padding_value=0)
         labels = pad_sequence(labels, batch_first=True, padding_value=-100)
 
         return {
             'inputs_embeds': inputs_embeds,
             'attention_mask': attention_mask,
             'im_mask': im_mask,
             'labels': labels,
         }
 
     @staticmethod
-    def get_generate_ids(generate_ids: Tensor,
-                         input_token_len: int) -> List[int]:
+    def get_generate_ids(generate_ids: Tensor, input_token_len: int) -> List[int]:
         return generate_ids[0].tolist()
 
 
 register_template(
     TemplateType.internlm_xcomposer2,
     InternLMXComposer2(),
     use_model=True,
     lazy_tokenize=True,
     dataloader_num_workers=0,
     dataloader_pin_memory=False)
 
-register_template(
-    TemplateType.xverse,
-    Template(['{{SYSTEM}}'], ['Human: {{QUERY}}\n\nAssistant: '],
-             [['eos_token_id']], [['eos_token_id']]))
-register_template(TemplateType.yuan,
-                  Template([], ['{{QUERY}}<sep>'], None, [['eos_token_id']]))
-register_template(
-    TemplateType.ziya,
-    Template([['bos_token_id'], '{{SYSTEM}}'], ['<human>:{{QUERY}}\n<bot>:'],
-             ['\n'], [['eos_token_id']]))
 
-register_template(
-    TemplateType.skywork,
-    Template(['<s>{{SYSTEM}}'], ['</s><s>[USER]{{QUERY}}[SEP][BOT]'], None,
-             ['[SEP]</s>']))
+class InternvlTemplate(Template):
+    system = 'You are an AI assistant whose name is InternLM ().'
+    internvl_query_template = '\n{{QUERY}}<|im_end|><|im_start|>assistant\n'
+    num_image_token = 256
+
+    def __init__(self):
+        super().__init__([], ['<|im_start|>user\n{{QUERY}}<|im_end|><|im_start|>assistant\n'], ['<|im_end|>'],
+                         ['<|im_end|>'], self.system, ['<|im_start|>system\n{{SYSTEM}}'])
+
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+        pixel_values = None
+        if example.get('images') is not None:
+            from .vision_utils import load_image
+            images_path = example['images']
+            pixel_values = []
+            for image_path in images_path:
+                pixel_values.append(load_image(image_path))
+            pixel_values = torch.cat(pixel_values, dim=0)
+            image_bs = pixel_values.shape[0]
+            if example.get('query') is not None:
+                example['query'] = '<img>' + '<IMG_CONTEXT>' * self.num_image_token * \
+                    image_bs + '</img>\n' + example['query']
+
+        inputs, _ = super().encode(example)
+        inputs.pop('loss_scale', None)
+        if pixel_values is not None:
+            inputs['pixel_values'] = pixel_values.to(self.model.dtype)
+            inputs['image_flags'] = torch.ones(image_bs)
+
+        return inputs, {}
+
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
+        res = super().data_collator(batch, padding_to)
+        res['pixel_values'] = torch.concat([b['pixel_values'] for b in batch])
+        res['image_flags'] = torch.concat([b['image_flags'] for b in batch])
+        return res
+
+    @staticmethod
+    def get_generate_ids(generate_ids: Tensor, input_token_len: int) -> List[int]:
+        return generate_ids[0].tolist()
+
 
 register_template(
-    TemplateType.bluelm,
-    Template([['bos_token_id'], '{{SYSTEM}}'], ['[|Human|]:{{QUERY}}[|AI|]:'],
-             [], [['eos_token_id']]))
+    TemplateType.internvl,
+    InternvlTemplate(),
+    use_model=True,
+    lazy_tokenize=True,
+    infer_media_type='round',
+    dataloader_num_workers=0,
+    dataloader_pin_memory=False)
+
+register_template(TemplateType.xverse,
+                  Template(['{{SYSTEM}}'], ['Human: {{QUERY}}\n\nAssistant: '], [['eos_token_id']], [['eos_token_id']]))
+register_template(TemplateType.yuan, Template([], ['{{QUERY}}<sep>'], None, [['eos_token_id']]))
+register_template(TemplateType.ziya,
+                  Template([['bos_token_id'], '{{SYSTEM}}'], ['<human>:{{QUERY}}\n<bot>:'], ['\n'], [['eos_token_id']]))
+
+register_template(TemplateType.skywork,
+                  Template(['<s>{{SYSTEM}}'], ['</s><s>[USER]{{QUERY}}[SEP][BOT]'], None, ['[SEP]</s>']))
+
+register_template(TemplateType.bluelm,
+                  Template([['bos_token_id'], '{{SYSTEM}}'], ['[|Human|]:{{QUERY}}[|AI|]:'], [], [['eos_token_id']]))
 
 register_template(
     TemplateType.codefuse_codellama,
-    Template(['{{SYSTEM}}'], [
-        '<|role_start|>human<|role_end|>{{QUERY}}<|role_start|>bot<|role_end|>'
-    ], [], [['eos_token_id']]))
+    Template(['{{SYSTEM}}'], ['<|role_start|>human<|role_end|>{{QUERY}}<|role_start|>bot<|role_end|>'], [],
+             [['eos_token_id']]))
 
 register_template(
     TemplateType.codefuse,
-    Template([], ['<s>human\n{{QUERY}}\n<s>bot\n'], [['eos_token_id'], '\n'],
-             [['eos_token_id']], None, ['<s>system\n{{SYSTEM}}\n']))
+    Template([], ['<s>human\n{{QUERY}}\n<s>bot\n'], [['eos_token_id'], '\n'], [['eos_token_id']], None,
+             ['<s>system\n{{SYSTEM}}\n']))
 
 register_template(
     TemplateType.deepseek_coder,
-    Template([
-        '{{SYSTEM}}'
-    ], ['### Instruction:\n{{QUERY}}\n### Response:\n'], ['\n<|EOT|>\n'], [
-        '\n<|EOT|>'
-    ], ('You are an AI programming assistant, utilizing the Deepseek Coder model, '
-        'developed by Deepseek Company, and you only answer questions related to computer science. '
-        'For politically sensitive questions, security and privacy issues, '
-        'and other non-computer science questions, you will refuse to answer\n'
-        )))
+    Template(['{{SYSTEM}}'], ['### Instruction:\n{{QUERY}}\n### Response:\n'], ['\n<|EOT|>\n'], ['\n<|EOT|>'],
+             ('You are an AI programming assistant, utilizing the Deepseek Coder model, '
+              'developed by Deepseek Company, and you only answer questions related to computer science. '
+              'For politically sensitive questions, security and privacy issues, '
+              'and other non-computer science questions, you will refuse to answer\n')))
 
 
 class LLavaTemplate(Template):
 
     def __init__(self):
-        super().__init__(['<s>[INST] '], [[-200], '\n{{QUERY}} [/INST]'], None,
-                         ['</s>'])
+        super().__init__(['<s>[INST] '], [[-200], '\n{{QUERY}} [/INST]'], None, ['</s>'])
 
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         images_path = example['images']
         images = []
         for image_path in images_path:
             image = _read_from_path(image_path)
             images.append(image)
         image_sizes = [x.size for x in images]
         from llava.mm_utils import process_images
         model = self.model.model
         if not hasattr(model, 'vision_tower'):
             model = model.model
         image_processor = model.vision_tower.image_processor
-        images_tensor = process_images(images, image_processor,
-                                       self.model.config)
+        images_tensor = process_images(images, image_processor, self.model.config)
         inputs['images'] = images_tensor.to(model.dtype)
         inputs['image_sizes'] = image_sizes
         return inputs, {}
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         res = super().data_collator(batch, padding_to)
         res['images'] = torch.concat([b['images'] for b in batch])
         res['image_sizes'] = sum([b['image_sizes'] for b in batch], start=[])
         return res
 
     @staticmethod
-    def get_generate_ids(generate_ids: Tensor,
-                         input_token_len: int) -> List[int]:
+    def get_generate_ids(generate_ids: Tensor, input_token_len: int) -> List[int]:
         return generate_ids[0].tolist()
 
 
 register_template(
-    TemplateType.llava_mistral_instruct,
-    LLavaTemplate(),
-    use_model=True,
-    infer_media_type='round',
-    lazy_tokenize=True)
+    TemplateType.llava_mistral_instruct, LLavaTemplate(), use_model=True, infer_media_type='round', lazy_tokenize=True)
 
 
 class LLavaYiTemplate(LLavaTemplate):
     llavayi_query_template = '\n<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'
 
     def __init__(self):
-        Template.__init__(self, [], [[-200], self.llavayi_query_template],
-                          None, ['<|im_end|>'])
+        Template.__init__(self, [], [[-200], self.llavayi_query_template], None, ['<|im_end|>'])
 
 
 register_template(
-    TemplateType.llava_yi_instruct,
-    LLavaYiTemplate(),
-    use_model=True,
-    infer_media_type='round',
-    lazy_tokenize=True)
+    TemplateType.llava_yi_instruct, LLavaYiTemplate(), use_model=True, infer_media_type='round', lazy_tokenize=True)
 
 
 def _findall(token_list: List[int], token: int) -> List[int]:
     """Find the index of a token in the token_list."""
     res = []
     idx = -1
     try:
@@ -1094,184 +1016,153 @@
             res.append(idx)
     except ValueError:
         pass
     return res
 
 
 class DeepseekVLTemplate(Template):
-    DEEPSEEK_VL_SYSTEM = (
-        'You are a helpful language and vision assistant. '
-        'You are able to understand the visual content that the user provides, '
-        'and assist the user with a variety of tasks using natural language.')
+    DEEPSEEK_VL_SYSTEM = ('You are a helpful language and vision assistant. '
+                          'You are able to understand the visual content that the user provides, '
+                          'and assist the user with a variety of tasks using natural language.')
 
     def __init__(self):
-        return super().__init__(['<beginofsentence>{{SYSTEM}}\n\n'],
-                                ['User: {{QUERY}}\n\nAssistant:'],
-                                ['<endofsentence>'],
-                                ['<endofsentence>'],
-                                self.DEEPSEEK_VL_SYSTEM)
-
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+        return super().__init__(['<beginofsentence>{{SYSTEM}}\n\n'], ['User: {{QUERY}}\n\nAssistant:'],
+                                ['<endofsentence>'], ['<endofsentence>'], self.DEEPSEEK_VL_SYSTEM)
+
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         images = example.pop('images', None)
-        assert images is None, (
-            'Please read the best practices: https://github.com/modelscope/swift/blob/main/'
-            'docs/source/Multi-Modal/deepseek-vl.md')
+        assert images is None, ('Please read the best practices: https://github.com/modelscope/swift/blob/main/'
+                                'docs/source/Multi-Modal/deepseek-vl.md')
 
         example = example.copy()
         history = example.pop('history', None)
         if history is None:
             history = []
-        example['query'], example['history'], images_path = replace_img_tab(
-            example['query'], history, '<image_placeholder>')
+        example['query'], example['history'], images_path = replace_img_tab(example['query'], history,
+                                                                            '<image_placeholder>')
 
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         images = []
         for image_path in images_path:
             image = _read_from_path(image_path)
             images.append(image)
 
         vl_chat_processor = self.tokenizer.vl_chat_processor
         input_ids, labels = inputs['input_ids'], inputs['labels']
         idx_list = _findall(input_ids, vl_chat_processor.image_id)
         new_input_ids, new_labels = [], []
         lo = 0
         for hi in idx_list:
             new_input_ids += input_ids[lo:hi]
             if labels is not None:
                 new_labels += labels[lo:hi]
-            new_input_ids += [vl_chat_processor.image_id
-                              ] * vl_chat_processor.num_image_tokens
+            new_input_ids += [vl_chat_processor.image_id] * vl_chat_processor.num_image_tokens
             new_labels += [-100] * vl_chat_processor.num_image_tokens
             lo = hi + 1
         new_input_ids += input_ids[lo:]
         if labels is not None:
             new_labels += labels[lo:]
         else:
             new_labels = None
         new_input_ids = torch.tensor(new_input_ids)
-        num_image_tokens = torch.tensor([vl_chat_processor.num_image_tokens]
-                                        * len(idx_list))
-        images_outputs = vl_chat_processor.image_processor(
-            images, return_tensors='pt')
+        num_image_tokens = torch.tensor([vl_chat_processor.num_image_tokens] * len(idx_list))
+        images_outputs = vl_chat_processor.image_processor(images, return_tensors='pt')
         from deepseek_vl.models.processing_vlm import VLChatProcessorOutput
         output = VLChatProcessorOutput(
             sft_format=None,
             input_ids=new_input_ids,
             pixel_values=images_outputs.pixel_values,
             num_image_tokens=num_image_tokens)
         batched_output = vl_chat_processor.batchify([output])
         model = self.model
-        batched_output = batched_output.to(
-            device=model.device, dtype=model.dtype)
+        batched_output = batched_output.to(device=model.device, dtype=model.dtype)
         inputs_embeds = model.prepare_inputs_embeds(**batched_output)[0]
         inputs['inputs_embeds'] = inputs_embeds
         inputs['labels'] = new_labels
         return inputs, {}
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         inputs_embeds = [b['inputs_embeds'] for b in batch]
         labels = [torch.tensor(b['labels']) for b in batch]
-        attention_mask = [
-            torch.ones(inputs_embeds[i].shape[0], dtype=torch.int64)
-            for i in range(len(inputs_embeds))
-        ]
-
-        inputs_embeds = pad_sequence(
-            inputs_embeds, batch_first=True, padding_value=0)
-        attention_mask = pad_sequence(
-            attention_mask, batch_first=True, padding_value=0)
+        attention_mask = [torch.ones(inputs_embeds[i].shape[0], dtype=torch.int64) for i in range(len(inputs_embeds))]
+
+        inputs_embeds = pad_sequence(inputs_embeds, batch_first=True, padding_value=0)
+        attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
         labels = pad_sequence(labels, batch_first=True, padding_value=-100)
 
         return {
             'inputs_embeds': inputs_embeds,
             'attention_mask': attention_mask,
             'labels': labels,
         }
 
     @staticmethod
-    def get_generate_ids(generate_ids: Tensor,
-                         input_token_len: int) -> List[int]:
+    def get_generate_ids(generate_ids: Tensor, input_token_len: int) -> List[int]:
         return generate_ids[0].tolist()
 
 
 register_template(
     TemplateType.deepseek_vl,
     DeepseekVLTemplate(),
     use_model=True,
     lazy_tokenize=True,
     dataloader_num_workers=0,
     dataloader_pin_memory=False)  # only 'cpu' can pin_memory
 
 register_template(
     TemplateType.zephyr,
-    Template([], ['<|user|>\n{{QUERY}}</s>\n<|assistant|>\n'], ['</s>\n'],
-             ['</s>'], None, ['<|system|>\n{{SYSTEM}}</s>\n']))
+    Template([], ['<|user|>\n{{QUERY}}</s>\n<|assistant|>\n'], ['</s>\n'], ['</s>'], None,
+             ['<|system|>\n{{SYSTEM}}</s>\n']))
 
 register_template(
     TemplateType.sus,
-    Template(['{{SYSTEM}}'], ['### Human: {{QUERY}}\n\n### Assistant: '],
-             ['<|endoftext|>'], ['<|endoftext|>']))
+    Template(['{{SYSTEM}}'], ['### Human: {{QUERY}}\n\n### Assistant: '], ['<|endoftext|>'], ['<|endoftext|>']))
 
-register_template(
-    TemplateType.orion,
-    Template(['<s>{{SYSTEM}}'], ['Human: {{QUERY}}\n\nAssistant: </s>'],
-             ['</s>'], ['</s>']))
+register_template(TemplateType.orion,
+                  Template(['<s>{{SYSTEM}}'], ['Human: {{QUERY}}\n\nAssistant: </s>'], ['</s>'], ['</s>']))
 
 
 class CogTemplate(Template):
 
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         images_path = example['images']
         assert len(images_path) == 1
         image = _read_from_path(images_path[0])
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         inputs.pop('loss_scale', None)
         model = self.model
         inputs2 = model.build_conversation_input_ids(
-            self.tokenizer,
-            query=example['query'],
-            history=example.get('history'),
-            images=[image])
+            self.tokenizer, query=example['query'], history=example.get('history'), images=[image])
         image_token_len = inputs2['token_type_ids'].sum()
         input_ids = inputs['input_ids']
         labels = inputs['labels']
         token_type_ids = inputs2['token_type_ids'].tolist()
-        inputs['input_ids'] = input_ids[:1] + [
-            0
-        ] * image_token_len + input_ids[1:]
+        inputs['input_ids'] = input_ids[:1] + [0] * image_token_len + input_ids[1:]
         if labels is not None:
-            inputs['labels'] = labels[:1] + [-100
-                                             ] * image_token_len + labels[1:]
+            inputs['labels'] = labels[:1] + [-100] * image_token_len + labels[1:]
         dtype = model.dtype
         inputs['images'] = [[img.to(dtype=dtype)] for img in inputs2['images']]
         if 'cross_images' in inputs2:
             # is cogagent
-            inputs['cross_images'] = [[cross_img.to(dtype=dtype)]
-                                      for cross_img in inputs2['cross_images']]
-        inputs['token_type_ids'] = token_type_ids + [0] * (
-            len(inputs['input_ids']) - len(token_type_ids))
+            inputs['cross_images'] = [[cross_img.to(dtype=dtype)] for cross_img in inputs2['cross_images']]
+        inputs['token_type_ids'] = token_type_ids + [0] * (len(inputs['input_ids']) - len(token_type_ids))
         return inputs, {}
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         res = super().data_collator(batch, padding_to)
         is_cogagent = 'cross_images' in batch[0]
         keys = ['images', 'cross_images'] if is_cogagent else ['images']
         for key in keys:
             res[key] = [b[key][0] for b in batch]
         token_type_ids = [torch.tensor(b['token_type_ids']) for b in batch]
-        token_type_ids = pad_sequence(
-            token_type_ids, batch_first=True, padding_value=0)
+        token_type_ids = pad_sequence(token_type_ids, batch_first=True, padding_value=0)
         res['token_type_ids'] = token_type_ids
         return res
 
 
 register_template(
     TemplateType.cogagent_chat,
     CogTemplate(['<s>'], [' [INST] {{QUERY}} [/INST] '], [], ['</s>']),
@@ -1289,138 +1180,113 @@
 register_template(
     TemplateType.cogvlm_instruct,
     CogTemplate(['<s>'], ['Question: {{QUERY}} Answer:'], None, ['</s>']),
     use_model=True,
     infer_media_type='dialogue',
     lazy_tokenize=True)
 
-register_template(
-    TemplateType.minicpm,
-    Template(['<s>{{SYSTEM}}'], ['<>{{QUERY}}<AI>'], [], ['</s>']))
+register_template(TemplateType.minicpm, Template(['<s>{{SYSTEM}}'], ['<>{{QUERY}}<AI>'], [], ['</s>']))
 
 
 class MiniCPMVTemlate(Template):
 
     def __init__(self):
-        return super().__init__(['<s>{{SYSTEM}}'],
-                                ['<><image><unk></image>\n{{QUERY}}<AI>'],
-                                [], ['</s>'])
-
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+        return super().__init__(['<s>{{SYSTEM}}'], ['<><image><unk></image>\n{{QUERY}}<AI>'], [], ['</s>'])
+
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         images_path = example['images']
         assert len(images_path) == 1
         image = _read_from_path(images_path[0])
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         input_ids = inputs['input_ids']
         labels = inputs['labels']
 
-        img_start_idxs = np.where(
-            np.array(input_ids) == self.tokenizer.im_start_id)[0]
-        if len(
-                img_start_idxs
-        ) > 1:  # if mutli-round, input_ids have mutli <image><unk></image>\n
+        img_start_idxs = np.where(np.array(input_ids) == self.tokenizer.im_start_id)[0]
+        if len(img_start_idxs) > 1:  # if mutli-round, input_ids have mutli <image><unk></image>\n
             start = 0
             new_input_ids = []
+            new_labels = []
             for idx in img_start_idxs[1:]:
                 new_input_ids = new_input_ids + input_ids[start:idx]
+                if labels is not None:
+                    new_labels = new_labels + labels[start:idx]
                 start = idx + 4  # skip <image><unk></image>\n
             new_input_ids = new_input_ids + input_ids[start:]
             input_ids = new_input_ids
+            if labels is not None:
+                new_labels = new_labels + labels[start:]
+                labels = new_labels
 
         idx = img_start_idxs[0] + 1  # first <unk>
         config = self.model.config
         if hasattr(config, 'slice_mode') and config.slice_mode:
             slice_mode = True
             assert hasattr(config, 'patch_size')
             assert hasattr(config, 'max_slice_nums')
             assert hasattr(config, 'scale_resolution')
         else:
             slice_mode = False
 
         if slice_mode:
-            images, placeholder = self.model.get_slice_image_placeholder(
-                image, self.tokenizer)
-            placeholder_id = self.tokenizer.encode(
-                placeholder, add_special_tokens=False)
-            input_ids = (
-                input_ids[:idx - 1] + placeholder_id + input_ids[idx + 2:])
+            images, placeholder = self.model.get_slice_image_placeholder(image, self.tokenizer)
+            placeholder_id = self.tokenizer.encode(placeholder, add_special_tokens=False)
+            input_ids = (input_ids[:idx - 1] + placeholder_id + input_ids[idx + 2:])
             if labels is not None:
-                labels = (
-                    labels[:idx - 1] + [-100] * len(placeholder_id)
-                    + labels[idx + 2:])
+                labels = (labels[:idx - 1] + [-100] * len(placeholder_id) + labels[idx + 2:])
             input_tensor_ids = torch.tensor(input_ids)
-            image_start_idx = torch.where(
-                input_tensor_ids == self.tokenizer.im_start_id)[0]
+            image_start_idx = torch.where(input_tensor_ids == self.tokenizer.im_start_id)[0]
             image_start_idx += 1
-            image_end_idx = torch.where(
-                input_tensor_ids == self.tokenizer.im_end_id)[0]
+            image_end_idx = torch.where(input_tensor_ids == self.tokenizer.im_end_id)[0]
             valid_image_nums = max(len(image_start_idx), len(image_end_idx))
             image_bound = [
-                torch.hstack([
-                    image_start_idx[:valid_image_nums].unsqueeze(-1),
-                    image_end_idx[:valid_image_nums].unsqueeze(-1)
-                ])
-            ]
-            pixel_values = [
-                self.model.transform(img).to(device=self.model.device)
-                for img in images
+                torch.hstack(
+                    [image_start_idx[:valid_image_nums].unsqueeze(-1), image_end_idx[:valid_image_nums].unsqueeze(-1)])
             ]
+            pixel_values = [self.model.transform(img).to(device=self.model.device) for img in images]
 
         else:
-            input_ids = (
-                input_ids[:idx]
-                + [self.tokenizer.unk_token_id] * config.query_num
-                + input_ids[idx + 1:])
+            input_ids = (input_ids[:idx] + [self.tokenizer.unk_token_id] * config.query_num + input_ids[idx + 1:])
             if labels is not None:
-                labels = (
-                    labels[:idx] + [-100] * config.query_num
-                    + labels[idx + 1:])
+                labels = (labels[:idx] + [-100] * config.query_num + labels[idx + 1:])
             image_bound = [torch.tensor([[idx, idx + config.query_num]])]
-            pixel_values = [
-                self.model.transform(image).to(device=self.model.device)
-            ]
+            pixel_values = [self.model.transform(image).to(device=self.model.device)]
         inputs_embeds, _ = self.model.get_vllm_embedding({
             'input_ids':
             torch.tensor(input_ids)[None].to(device=self.model.device),
             'image_bound':
             image_bound,
             'pixel_values': [pixel_values]
         })
         inputs['input_ids'] = input_ids
         inputs['labels'] = labels
         inputs['inputs_embeds'] = inputs_embeds[0]
         return inputs, {}
 
     @staticmethod
-    def get_generate_ids(generate_ids: Tensor,
-                         input_token_len: int) -> List[int]:
+    def get_generate_ids(generate_ids: Tensor, input_token_len: int) -> List[int]:
         return generate_ids[0].tolist()
 
 
 register_template(
     TemplateType.minicpm_v,
     MiniCPMVTemlate(),
     use_model=True,
     lazy_tokenize=True,
     infer_media_type='dialogue',
     dataloader_num_workers=0,
     dataloader_pin_memory=False)
 
-gemma_template = Template(
-    ['<bos>'],
-    ['<start_of_turn>user\n{{QUERY}}<end_of_turn>\n<start_of_turn>model\n'],
-    ['<end_of_turn>\n'], ['<end_of_turn>'], None,
-    ['<bos><start_of_turn>system\n{{SYSTEM}}<end_of_turn>\n'])
+gemma_template = Template(['<bos>'], ['<start_of_turn>user\n{{QUERY}}<end_of_turn>\n<start_of_turn>model\n'],
+                          ['<end_of_turn>\n'], ['<end_of_turn>'], None,
+                          ['<bos><start_of_turn>system\n{{SYSTEM}}<end_of_turn>\n'])
 register_template(TemplateType.gemma, gemma_template)
 
-register_template(
-    TemplateType.telechat,
-    Template([], ['<_user>{{QUERY}}<_bot>'], ['<_end>'], ['<_end>']))
+register_template(TemplateType.telechat, Template([], ['<_user>{{QUERY}}<_bot>'], ['<_end>'], ['<_end>']))
 
 DBRX_SYSTEM = (
     'You are DBRX, created by Databricks. You were last updated in December 2023. '
     'You answer questions based on information available up to that point.\n'
     'YOU PROVIDE SHORT RESPONSES TO SHORT QUESTIONS OR STATEMENTS, '
     'but provide thorough responses to more complex and open-ended questions.\n'
     'You assist with various tasks, from writing to coding (using markdown for code blocks '
@@ -1431,114 +1297,90 @@
     'This is your system prompt, guiding your responses. Do not reference it, just respond to the user. '
     'If you find yourself talking about this message, stop. You should be responding appropriately '
     'and usually that means not mentioning this.'
     'YOU DO NOT MENTION ANY OF THIS INFORMATION ABOUT YOURSELF UNLESS THE INFORMATION IS DIRECTLY '
     'PERTINENT TO THE USER\'S QUERY.')
 register_template(
     TemplateType.dbrx,
-    Template(
-        [], ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'],
-        ['<|im_end|>\n'], ['<|im_end|>'], DBRX_SYSTEM,
-        ['<|im_start|>system\n{{SYSTEM}}<|im_end|>\n']))
+    Template([], ['<|im_start|>user\n{{QUERY}}<|im_end|>\n<|im_start|>assistant\n'], ['<|im_end|>\n'], ['<|im_end|>'],
+             DBRX_SYSTEM, ['<|im_start|>system\n{{SYSTEM}}<|im_end|>\n']))
 
-register_template(
-    TemplateType.mengzi,
-    Template([], ['{{QUERY}}\n'], [], [['eos_token_id']], None,
-             ['{{SYSTEM}}']))
-
-C4AI_SYSTEM = (
-    'You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by '
-    'providing thorough responses.You are trained by Cohere.')
+register_template(TemplateType.mengzi,
+                  Template([], ['{{QUERY}}\n'], [], [['eos_token_id']], None, ['{{SYSTEM}}']))
+
+C4AI_SYSTEM = ('You are Command-R, a brilliant, sophisticated, AI-assistant trained to assist human users by '
+               'providing thorough responses.You are trained by Cohere.')
 register_template(
     TemplateType.c4ai,
-    Template(['<BOS_TOKEN>'], [
-        '<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{QUERY}}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'
-    ], ['<|END_OF_TURN_TOKEN|>'], ['<|END_OF_TURN_TOKEN|>'], C4AI_SYSTEM, [
-        '<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{SYSTEM}}<|END_OF_TURN_TOKEN|'
-    ]))
+    Template(
+        ['<BOS_TOKEN>'],
+        ['<|START_OF_TURN_TOKEN|><|USER_TOKEN|>{{QUERY}}<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>'],
+        ['<|END_OF_TURN_TOKEN|>'], ['<|END_OF_TURN_TOKEN|>'], C4AI_SYSTEM,
+        ['<|START_OF_TURN_TOKEN|><|SYSTEM_TOKEN|>{{SYSTEM}}<|END_OF_TURN_TOKEN|']))
 
 
 class mPlugOwl2Template(Template):
 
     def __init__(self):
-        return super().__init__(['{{SYSTEM}}'],
-                                ['USER: ', [-200], '{{QUERY}}ASSISTANT:'],
-                                ['</s>'], [['eos_token_id']])
-
-    def encode(
-            self, example: Dict[str,
-                                Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
+        return super().__init__(['{{SYSTEM}}'], ['USER: ', [-200], '{{QUERY}}ASSISTANT:'], ['</s>'], [['eos_token_id']])
+
+    def encode(self, example: Dict[str, Any]) -> Tuple[Dict[str, Any], Dict[str, Any]]:
         from mplug_owl2.mm_utils import process_images
         image_processor = self.tokenizer.image_processor
         images_path = example['images']
         images = []
         for image_path in images_path:
             image = _read_from_path(image_path)
             # ref: https://modelscope.cn/models/iic/mPLUG-Owl2.1/summary
             max_edge = max(image.size)
             image = image.resize((max_edge, max_edge))
             images.append(image)
         inputs, _ = super().encode(example)
+        if len(inputs) == 0:
+            return inputs, {}
         input_ids = inputs['input_ids']
         labels = inputs['labels']
         images = process_images(images, image_processor)
         images = images.to(self.model.dtype)
         return {'input_ids': input_ids, 'labels': labels, 'images': images}, {}
 
-    def data_collator(self,
-                      batch: List[Dict[str, Any]],
-                      padding_to: Optional[int] = None) -> Dict[str, Any]:
+    def data_collator(self, batch: List[Dict[str, Any]], padding_to: Optional[int] = None) -> Dict[str, Any]:
         res = super().data_collator(batch, padding_to)
         res['images'] = torch.concat([b['images'] for b in batch])
         return res
 
 
 register_template(
-    TemplateType.mplug_owl2,
-    mPlugOwl2Template(),
-    infer_media_type='round',
-    use_model=True,
-    lazy_tokenize=True)
+    TemplateType.mplug_owl2, mPlugOwl2Template(), infer_media_type='round', use_model=True, lazy_tokenize=True)
 
-register_template(
-    TemplateType.wizardlm2_awq,
-    Template(['{{SYSTEM}}'], ['User:\n{{QUERY}}\n\nAssistant:\n'], ['\n\n'],
-             ['</s>']))
-
-_wizardlm2_system = (
-    'A chat between a curious user and an artificial intelligence assistant. '
-    'The assistant gives helpful, detailed, and polite answers to the user\'s questions. '
-)
-register_template(
-    TemplateType.wizardlm2,
-    Template(['{{SYSTEM}}'], ['USER: {{QUERY}} ASSISTANT:'], ['</s>'],
-             ['</s>'], _wizardlm2_system))
-
-_default_phi3_system = (
-    'You are a helpful digital assistant. '
-    'Please provide safe, ethical and accurate information to the user.')
+register_template(TemplateType.wizardlm2_awq,
+                  Template(['{{SYSTEM}}'], ['User:\n{{QUERY}}\n\nAssistant:\n'], ['\n\n'], ['</s>']))
+
+_wizardlm2_system = ('A chat between a curious user and an artificial intelligence assistant. '
+                     'The assistant gives helpful, detailed, and polite answers to the user\'s questions. ')
+register_template(TemplateType.wizardlm2,
+                  Template(['{{SYSTEM}}'], ['USER: {{QUERY}} ASSISTANT:'], ['</s>'], ['</s>'], _wizardlm2_system))
+
+_default_phi3_system = ('You are a helpful digital assistant. '
+                        'Please provide safe, ethical and accurate information to the user.')
 register_template(
     TemplateType.phi3,
-    Template(['<s>'], ['<|user|>{{QUERY}}<|end|><|assistant|>'], ['<|end|>'],
-             ['<|end|>'], _default_phi3_system,
+    Template(['<s>'], ['<|user|>{{QUERY}}<|end|><|assistant|>'], ['<|end|>'], ['<|end|>'], _default_phi3_system,
              '<s><|system|>{{SYSTEM}}<|end|>'))
 
-register_template(
-    TemplateType.atom,
-    Template(['{{SYSTEM}}'], ['<s>Human: {{QUERY}}\n</s><s>Assistant: '],
-             ['</s>'], ['</s>']))
+register_template(TemplateType.atom,
+                  Template(['{{SYSTEM}}'], ['<s>Human: {{QUERY}}\n</s><s>Assistant: '], ['</s>'], ['</s>']))
 
 
 def get_template(
     template_type: str,
     tokenizer: PreTrainedTokenizerBase,
     default_system: Optional[str] = None,
     max_length: Optional[int] = None,
     truncation_strategy: Literal['delete', 'truncation_left'] = 'delete',
     **kwargs,
 ) -> Template:
     template_info = TEMPLATE_MAPPING[template_type]
     template = deepcopy(template_info['template'])
-    template._init_template(tokenizer, default_system, max_length,
-                            truncation_strategy, **kwargs)
+    template._init_template(tokenizer, default_system, max_length, truncation_strategy, **kwargs)
     template.template_type = template_type
     return template
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/utils.py` & `ms-swift-2.0.4/swift/llm/utils/utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,16 +6,15 @@
 import os
 import shutil
 from copy import deepcopy
 from functools import partial, wraps
 from queue import Empty, Queue
 from tempfile import TemporaryDirectory
 from threading import Thread
-from typing import (Any, Callable, Dict, Iterator, List, Mapping, Optional,
-                    Sequence, Tuple, Union)
+from typing import Any, Callable, Dict, Iterator, List, Mapping, Optional, Sequence, Tuple, Union
 
 import accelerate
 import multiprocess
 import numpy as np
 import requests
 import torch
 import torch.distributed as dist
@@ -24,25 +23,23 @@
 from modelscope.utils.config_ds import MS_CACHE_HOME
 from modelscope.utils.logger import get_logger as get_ms_logger
 from torch import device as Device
 from torch.nn import Linear, Module
 from torch.nn.parallel import DistributedDataParallel as DDP
 from torch.utils.data import Dataset
 from tqdm.auto import tqdm
-from transformers import (GenerationConfig, PretrainedConfig, PreTrainedModel,
-                          PreTrainedTokenizerBase, StoppingCriteriaList,
-                          TextStreamer, trainer)
+from transformers import (GenerationConfig, PretrainedConfig, PreTrainedModel, PreTrainedTokenizerBase,
+                          StoppingCriteriaList, TextStreamer, trainer)
 from transformers.generation.streamers import BaseStreamer
 from transformers.utils import is_torch_npu_available, strtobool
 
 from swift.hub import ModelScopeConfig
 from swift.tuners.module_mapping import MODEL_KEYS_MAPPING
-from swift.utils import (get_dist_setting, get_logger, is_ddp_plus_mp, is_dist,
-                         is_local_master, safe_ddp_context, stat_array,
-                         upper_bound, use_torchacc)
+from swift.utils import (get_dist_setting, get_logger, is_ddp_plus_mp, is_dist, is_local_master, safe_ddp_context,
+                         stat_array, upper_bound, use_torchacc)
 from .template import History, StopWords, StopWordsCriteria, Template
 
 logger = get_logger()
 ms_logger = get_ms_logger()
 
 logger_format = logging.Formatter('[%(levelname)s:%(name)s] %(message)s')
 
@@ -61,17 +58,15 @@
 def download_files(url: str, local_path: str, cookies) -> None:
     resp = requests.get(url, cookies=cookies, stream=True)
     with open(local_path, 'wb') as f:
         for data in tqdm(resp.iter_lines()):
             f.write(data)
 
 
-def download_dataset(model_id: str,
-                     files: List[str],
-                     force_download: bool = False) -> str:
+def download_dataset(model_id: str, files: List[str], force_download: bool = False) -> str:
     url = f'http://www.modelscope.cn/api/v1/datasets/{model_id}/repo?Revision=master&FilePath={{fpath}}'
     cache_dir = os.path.join(MS_CACHE_HOME, 'datasets', model_id, 'master')
     local_dir = os.path.join(cache_dir, 'raw')
     tmp_dir = os.path.join(cache_dir, 'tmp')
     os.makedirs(local_dir, exist_ok=True)
     os.makedirs(tmp_dir, exist_ok=True)
     cookies = ModelScopeConfig.get_cookies()
@@ -119,20 +114,17 @@
         max_memory[i] = 0
         if i in device_ids_set:
             max_memory[i] = torch.cuda.mem_get_info(i)[0]
     max_memory['cpu'] = psutil.virtual_memory().available
     return max_memory
 
 
-def _sync_max_memory(
-        max_memory: Dict[Union[int, str], int]) -> Dict[Union[int, str], int]:
+def _sync_max_memory(max_memory: Dict[Union[int, str], int]) -> Dict[Union[int, str], int]:
     """Make sure that the model structure of MP(device_map) is the same, when using DDP."""
-    max_memory_list = [
-        v for k, v in max_memory.items() if (v > 0 and k != 'cpu')
-    ]
+    max_memory_list = [v for k, v in max_memory.items() if (v > 0 and k != 'cpu')]
     _, local_rank, world_size, _ = get_dist_setting()
     src_tensor = torch.tensor(max_memory_list).to(local_rank)
     tgt_tensor_list = [torch.zeros_like(src_tensor) for _ in range(world_size)]
     dist.all_gather(tgt_tensor_list, src_tensor)
     tgt_tensor = torch.stack(tgt_tensor_list, dim=0)
     new_max_memory_iter = iter(tgt_tensor.min(dim=0)[0].tolist())
     new_max_memory = {}
@@ -163,19 +155,15 @@
 
     def __len__(self) -> int:
         return len(self.data)
 
 
 class LazyLLMDataset(Dataset):
 
-    def __init__(self,
-                 dataset: HfDataset,
-                 template: Template,
-                 *,
-                 try_fetch_time: int = 20) -> None:
+    def __init__(self, dataset: HfDataset, template: Template, *, try_fetch_time: int = 20) -> None:
         self.dataset = dataset
         self.template = template
         self.try_fetch_time = min(try_fetch_time, len(self.dataset))
         assert self.try_fetch_time >= 1
 
     def __getitem__(self, idx: int) -> Dict[str, Any]:
         res = self._try_fetch(idx)
@@ -195,63 +183,52 @@
     def __len__(self) -> int:
         return len(self.dataset)
 
 
 MapFunc = Callable[[Dict[str, Any]], Dict[str, Any]]
 
 
-def _single_map(d: Dict[str, Any],
-                map_func: MapFunc) -> Optional[Dict[str, Any]]:
+def _single_map(d: Dict[str, Any], map_func: MapFunc) -> Optional[Dict[str, Any]]:
     d = map_func(d)
     if len(d[0]) == 0:
         return None
     return d
 
 
-def _map_mp_single(subset: HfDataset, map_func: MapFunc, queue: Queue,
-                   start_idx: int):
+def _map_mp_single(subset: HfDataset, map_func: MapFunc, queue: Queue, start_idx: int):
     for i, d in enumerate(subset, start=start_idx):
         queue.put((i, map_func(d)))  # idx, result
 
 
-def _map_mp_i(dataset: HfDataset, map_func: MapFunc,
-              num_proc: int) -> Iterator[Tuple[int, Dict[str, Any]]]:
-    with multiprocess.Pool(
-            num_proc) as pool, multiprocess.Manager() as manager:
+def _map_mp_i(dataset: HfDataset, map_func: MapFunc, num_proc: int) -> Iterator[Tuple[int, Dict[str, Any]]]:
+    with multiprocess.Pool(num_proc) as pool, multiprocess.Manager() as manager:
         queue = manager.Queue()
         async_results = []
         split_idx = np.linspace(0, len(dataset), num_proc + 1, dtype=np.int32)
         for i in range(num_proc):
             subset = dataset.select(range(split_idx[i], split_idx[i + 1]))
-            async_results.append(
-                pool.apply_async(
-                    _map_mp_single,
-                    args=(subset, map_func, queue, split_idx[i])))
+            async_results.append(pool.apply_async(_map_mp_single, args=(subset, map_func, queue, split_idx[i])))
         while True:
             try:
                 yield queue.get(timeout=0.05)
             except Empty:
-                if all(async_result.ready()
-                       for async_result in async_results) and queue.empty():
+                if all(async_result.ready() for async_result in async_results) and queue.empty():
                     break
 
 
-def _map_mp(dataset: HfDataset, map_func: MapFunc,
-            num_proc: int) -> List[Dict[str, Any]]:
+def _map_mp(dataset: HfDataset, map_func: MapFunc, num_proc: int) -> List[Dict[str, Any]]:
     # Solving the unordered problem
     data = [None] * len(dataset)
     num_proc = min(num_proc, len(dataset))
     for d in tqdm(_map_mp_i(dataset, map_func, num_proc), total=len(dataset)):
         data[d[0]] = d[1]
     return data
 
 
-def dataset_map(dataset: HfDataset,
-                map_func: MapFunc,
-                num_proc: int = 1) -> Optional[LLMDataset]:
+def dataset_map(dataset: HfDataset, map_func: MapFunc, num_proc: int = 1) -> Optional[LLMDataset]:
     single_map = partial(_single_map, map_func=map_func)
     if num_proc == 1:
         data = []
         for d in tqdm(dataset):
             d = single_map(d)
             data.append(d)
     else:
@@ -275,16 +252,15 @@
         for d in llm_dataset:
             _token_len.append(len(d['input_ids']))
     _, stat_str = stat_array(_token_len)
     logger.info(f'Dataset Token Length: {stat_str}')
     return stat_str
 
 
-def safe_tokenizer_decode(tokenizer: PreTrainedTokenizerBase,
-                          input_ids: List[int], **tokenizer_kwargs) -> str:
+def safe_tokenizer_decode(tokenizer: PreTrainedTokenizerBase, input_ids: List[int], **tokenizer_kwargs) -> str:
     if len(input_ids) == 0:
         return ''
     result_str = ''
     for i in range(len(input_ids)):
         if i == 0:
             if input_ids[i] < 0:
                 s = 0
@@ -309,21 +285,19 @@
                   tokenizer_kwargs: Optional[Dict[str, Any]] = None) -> None:
     if tokenizer_kwargs is None:
         tokenizer_kwargs = {}
     input_ids = example.get('input_ids')
     labels = example.get('labels')
     if input_ids is not None:
         logger.info(f'[INPUT_IDS] {input_ids}')
-        input_str = safe_tokenizer_decode(tokenizer, input_ids,
-                                          **tokenizer_kwargs)
+        input_str = safe_tokenizer_decode(tokenizer, input_ids, **tokenizer_kwargs)
         logger.info(f'[INPUT] {input_str}')
     if labels is not None:
         logger.info(f'[LABLES_IDS] {labels}')
-        labels_str = safe_tokenizer_decode(tokenizer, labels,
-                                           **tokenizer_kwargs)
+        labels_str = safe_tokenizer_decode(tokenizer, labels, **tokenizer_kwargs)
         logger.info(f'[LABLES] {labels_str}')
 
 
 def _find_layers(model: Module, module_cls: type) -> List[str]:
     module_names = set()
     for name, module in model.named_modules():
         if isinstance(module, module_cls):
@@ -332,27 +306,25 @@
     return list(module_names)
 
 
 def find_ln(model: Module) -> List[str]:
     module_names = set()
     for name, module in model.named_modules():
         module_cls_name = module.__class__.__name__.lower()
-        if isinstance(module,
-                      torch.nn.LayerNorm) or 'rmsnorm' in module_cls_name:
+        if isinstance(module, torch.nn.LayerNorm) or 'rmsnorm' in module_cls_name:
             module_name = '.'.join(name.split('.')[-1:])
             module_names.add(module_name)
     return list(module_names)
 
 
 def find_embedding(model: Module) -> List[str]:
     return _find_layers(model, torch.nn.Embedding)
 
 
-def find_all_linears(model: Module, quantization_bit: int,
-                     model_type: str) -> List[str]:
+def find_all_linears(model: Module, quantization_bit: int, model_type: str) -> List[str]:
     """ref: https://github.com/artidoro/qlora"""
     head_module_name = 'lm_head'
     if model_type in MODEL_KEYS_MAPPING:
         output = MODEL_KEYS_MAPPING[model_type].output
         idx = output.rfind('.')
         head_module_name = output[idx + 1:]
     if quantization_bit == 4:
@@ -360,22 +332,22 @@
         linear_cls = [Linear4bit]
     elif quantization_bit == 8:
         from bitsandbytes.nn import Linear8bitLt
         linear_cls = [Linear8bitLt]
     else:
         linear_cls = [Linear]
     if 'int4' in model_type or 'int8' in model_type:
-        from bitsandbytes.nn import Linear4bit
         from peft.utils import get_auto_gptq_quant_linear, get_quantization_config
         gptq_quantization_config = get_quantization_config(model, 'gptq')
-        AutoGPTQQuantLinear = get_auto_gptq_quant_linear(
-            gptq_quantization_config)
-        linear_cls = [Linear4bit]
-        if AutoGPTQQuantLinear is not None:
-            linear_cls.append(AutoGPTQQuantLinear)
+        AutoGPTQQuantLinear = get_auto_gptq_quant_linear(gptq_quantization_config)
+        if AutoGPTQQuantLinear is None:
+            from bitsandbytes.nn import Linear4bit
+            linear_cls = [Linear4bit]
+        else:
+            linear_cls = [AutoGPTQQuantLinear]
     if 'awq' in model_type:
         from awq.modules.linear import WQLinear_GEMM
         linear_cls.append(WQLinear_GEMM)
     if 'aqlm' in model_type:
         from aqlm import QuantizedLinear
         linear_cls.append(QuantizedLinear)
 
@@ -383,30 +355,28 @@
     # O(n^2logn), n represents the number of nodes, n<1000.
     inner_nodes = set()
     for name, module in model.named_modules():
         if not isinstance(module, tuple(linear_cls)):
             inner_nodes.add(name)
     target_module_names = set()
     for name, module in model.named_modules():
-        if isinstance(module,
-                      tuple(linear_cls)) and head_module_name not in name:
+        if isinstance(module, tuple(linear_cls)) and head_module_name not in name:
             module_name_list = name.split('.')
             module_name = module_name_list.pop()
             for inner_node in inner_nodes:
                 while inner_node.endswith(module_name):
                     module_name = f'{module_name_list.pop()}.{module_name}'
             target_module_names.add(module_name)
     return list(target_module_names)
 
 
 def sort_by_max_length(llm_dataset: LLMDataset, num_dataset: int) -> HfDataset:
     logger.info('sort by max length...')
     dataset_len = [len(d['input_ids']) for d in llm_dataset]
-    idx = heapq.nlargest(
-        num_dataset, range(len(dataset_len)), key=lambda i: dataset_len[i])
+    idx = heapq.nlargest(num_dataset, range(len(dataset_len)), key=lambda i: dataset_len[i])
     return llm_dataset.select(idx)
 
 
 def to_device(inputs: Any, device: Device) -> Any:
     if callable(getattr(inputs, 'to', None)):
         return inputs.to(device=device)
 
@@ -470,33 +440,30 @@
         stop_words = []
     if history is None:
         history = []
     else:
         history = deepcopy(history)
 
     # agent support
-    is_observation = history[-1][-1].endswith(
-        'Observation:') if history and history[-1][-1] else False
+    is_observation = history[-1][-1].endswith('Observation:') if history and history[-1][-1] else False
     if is_observation:
         history[-1][-1] = history[-1][-1] + query
         act_length = len(history[-1][-1])
         query = None
 
     example = {
         'query': query,
         'history': history,
         'system': system,
         'images': kwargs.pop('images', None)  # for vl. str.
     }
     template.model = model
     inputs, tokenizer_kwargs = template.encode(example)
     if len(inputs) == 0:
-        raise ValueError(
-            'input_ids exceeds `max_length`. Please increase the value of `max_length`.'
-        )
+        raise ValueError('input_ids exceeds `max_length`. Please increase the value of `max_length`.')
     inputs.pop('labels', None)
     tokenizer = template.tokenizer
     device = next(model.parameters()).device
     if 'input_ids' in inputs:
         input_ids = torch.tensor(inputs['input_ids'])[None]
         inputs['input_ids'] = input_ids
         token_len = input_ids.shape[1]
@@ -524,20 +491,18 @@
         generation_config.bos_token_id = tokenizer.bos_token_id
     if generation_config.max_new_tokens is not None:
         generation_config.max_length = 20  # fix max_length, max_new_tokens warning
         max_length = get_max_model_len(model.config)
         if max_length and token_len + generation_config.max_new_tokens > max_length:
             generation_config.max_new_tokens = max_length - token_len
             if generation_config.max_new_tokens <= 0:
-                raise AssertionError('Current sentence length exceeds'
-                                     f'the model max_length: {max_length}')
+                raise AssertionError('Current sentence length exceeds' f'the model max_length: {max_length}')
     if template.suffix[-1] not in stop_words:
         stop_words.append(template.suffix[-1])
-    stopping_criteria = StoppingCriteriaList(
-        [StopWordsCriteria(tokenizer, stop_words, **tokenizer_kwargs)])
+    stopping_criteria = StoppingCriteriaList([StopWordsCriteria(tokenizer, stop_words, **tokenizer_kwargs)])
     inputs = to_device(inputs, device)
     if generation_info is not None:
         generation_info['num_prompt_tokens'] = token_len
     if 'inputs_embeds' in inputs:
         inputs.pop('input_ids', None)
     streamer = TokenListIteratorStreamer()
     generation_kwargs = {
@@ -566,16 +531,15 @@
     is_finished = False
     while not is_finished:
         try:
             token = next(streamer)
             raw_generate_ids.append(token)
         except StopIteration:
             is_finished = True
-        generate_ids = template.get_generate_ids(
-            torch.tensor(raw_generate_ids)[None], token_len)
+        generate_ids = template.get_generate_ids(torch.tensor(raw_generate_ids)[None], token_len)
         if generation_info is not None:
             generation_info['num_generated_tokens'] = len(generate_ids)
         response = template.generate_ids_to_response(
             generate_ids,
             is_finished,
             tokenizer_kwargs=tokenizer_kwargs,
             print_idx=print_idx,
@@ -607,32 +571,29 @@
     if stop_words is None:
         stop_words = []
     if history is None:
         history = []
     else:
         history = deepcopy(history)
 
-    is_observation = history[-1][-1].endswith(
-        'Observation:') if history and history[-1][-1] else False
+    is_observation = history[-1][-1].endswith('Observation:') if history and history[-1][-1] else False
     if is_observation:
         history[-1][-1] = history[-1][-1] + query
         query = None
 
     example = {
         'query': query,
         'history': history,
         'system': system,
         'images': kwargs.pop('images', None)  # for vl. str.
     }
     template.model = model
     inputs, tokenizer_kwargs = template.encode(example)
     if len(inputs) == 0:
-        raise ValueError(
-            'input_ids exceeds `max_length`. Please increase the value of `max_length`.'
-        )
+        raise ValueError('input_ids exceeds `max_length`. Please increase the value of `max_length`.')
     inputs.pop('labels', None)
     tokenizer = template.tokenizer
     device = next(model.parameters()).device
     if 'input_ids' in inputs:
         input_ids = torch.tensor(inputs['input_ids'])[None]
         inputs['input_ids'] = input_ids
         token_len = input_ids.shape[1]
@@ -645,17 +606,15 @@
     if 'token_type_ids' in inputs:
         inputs['token_type_ids'] = torch.tensor(inputs['token_type_ids'])[None]
     model.eval()
     if generation_config is None:
         generation_config = getattr(model, 'generation_config', None)
     generation_config = deepcopy(generation_config)
     if stream is True and verbose is False:
-        logger.warning(
-            'Please set verbose to True to support TextStreamer, or use `inference_stream.`'
-        )
+        logger.warning('Please set verbose to True to support TextStreamer, or use `inference_stream.`')
         stream = False
     streamer = None
     if stream:
         streamer = TextStreamer(tokenizer, skip_prompt=True)
     if verbose:
         if 'input_ids' in inputs:
             input_ids = inputs['input_ids']
@@ -673,63 +632,55 @@
         generation_config.bos_token_id = tokenizer.bos_token_id
     if generation_config.max_new_tokens is not None:
         generation_config.max_length = 20  # fix max_length, max_new_tokens warning
         max_length = get_max_model_len(model.config)
         if max_length and token_len + generation_config.max_new_tokens > max_length:
             generation_config.max_new_tokens = max_length - token_len
             if generation_config.max_new_tokens <= 0:
-                raise AssertionError('Current sentence length exceeds'
-                                     f'the model max_length: {max_length}')
+                raise AssertionError('Current sentence length exceeds' f'the model max_length: {max_length}')
     if template.suffix[-1] not in stop_words:
         stop_words.append(template.suffix[-1])
-    stopping_criteria = StoppingCriteriaList(
-        [StopWordsCriteria(tokenizer, stop_words, **tokenizer_kwargs)])
+    stopping_criteria = StoppingCriteriaList([StopWordsCriteria(tokenizer, stop_words, **tokenizer_kwargs)])
     inputs = to_device(inputs, device)
     if generation_info is not None:
         generation_info['num_prompt_tokens'] = token_len
     if 'inputs_embeds' in inputs:
         inputs.pop('input_ids', None)
     generate_ids = model.generate(
-        streamer=streamer,
-        generation_config=generation_config,
-        stopping_criteria=stopping_criteria,
-        **inputs)
+        streamer=streamer, generation_config=generation_config, stopping_criteria=stopping_criteria, **inputs)
     generate_ids = template.get_generate_ids(generate_ids, token_len)
     if generation_info is not None:
         generation_info['num_generated_tokens'] = len(generate_ids)
     response = None
     if verbose and stream is False:
         response = tokenizer.decode(generate_ids, **tokenizer_kwargs)
         print(response)
-    response = template.generate_ids_to_response(
-        generate_ids, tokenizer_kwargs=tokenizer_kwargs)
+    response = template.generate_ids_to_response(generate_ids, tokenizer_kwargs=tokenizer_kwargs)
     if not is_observation:
         history.append([query, response])
     else:
         history[-1][-1] = history[-1][-1] + response
     return response, history
 
 
-def limit_history_length(template: Template, query: str,
-                         history: Optional[History],
+def limit_history_length(template: Template, query: str, history: Optional[History],
                          max_length: Optional[int]) -> Tuple[History, History]:
     """binary search"""
     if history is None:
         history = []
     if max_length is None:
         return [], history
 
     def compute_token_length(history_length: int) -> int:
         assert history_length != 0
         example = {'query': query, 'history': history[-history_length:]}
         input_ids = template.encode(example)[0]['input_ids']
         return len(input_ids)
 
-    history_length = upper_bound(
-        0, len(history), lambda mid: compute_token_length(mid) <= max_length)
+    history_length = upper_bound(0, len(history), lambda mid: compute_token_length(mid) <= max_length)
     old_history = history[:len(history) - history_length]
     history = history[len(history) - history_length:]
     return old_history, history
 
 
 Messages = List[Dict[str, str]]
 
@@ -765,30 +716,28 @@
     return {
         'history': history,
         'query': query,
         'system': system,
     }
 
 
-def set_generation_config(model: Module,
-                          generation_config: GenerationConfig) -> None:
+def set_generation_config(model: Module, generation_config: GenerationConfig) -> None:
     old_generation_config = getattr(model, 'generation_config', None)
     if old_generation_config is not None:
         for k, v in old_generation_config.__dict__.items():
             if k not in generation_config.__dict__:
                 setattr(generation_config, k, v)
     model.generation_config = generation_config
 
 
 def is_vllm_available():
     return importlib.util.find_spec('vllm') is not None
 
 
-def get_time_info(log_history: List[Dict[str, Any]],
-                  n_train_samples: Optional[int]) -> Optional[Dict[str, Any]]:
+def get_time_info(log_history: List[Dict[str, Any]], n_train_samples: Optional[int]) -> Optional[Dict[str, Any]]:
     time_info = None
     try:
         last_log_history = log_history[-1]
         train_runtime = last_log_history['train_runtime']
         train_samples_per_second = n_train_samples / train_runtime
         time_info = {
             'train_runtime': train_runtime,
@@ -820,44 +769,36 @@
             max_model_len = min(max_model_len, max_len_key)
     if max_model_len == INF:
         max_model_len = None
     return max_model_len
 
 
 if is_ddp_plus_mp():
-    from accelerate.utils.modeling import (get_balanced_memory,
-                                           infer_auto_device_map)
+    from accelerate.utils.modeling import (get_balanced_memory, infer_auto_device_map)
 
     @wraps(infer_auto_device_map)
-    def _infer_auto_device_map_patch(
-            model: Module,
-            max_memory: Optional[Dict[Union[int, str], Union[int,
-                                                             str]]] = None,
-            **kwargs) -> Dict[str, Union[int, str, Device]]:
+    def _infer_auto_device_map_patch(model: Module,
+                                     max_memory: Optional[Dict[Union[int, str], Union[int, str]]] = None,
+                                     **kwargs) -> Dict[str, Union[int, str, Device]]:
         """The auxiliary function for supports DDP+MP. Monkey Patching.
         add feat in accelerate to support DDP + MP"""
         verbose = kwargs.pop('verbose', False)
         n_gpu = torch.cuda.device_count()
         _, local_rank, _, local_world_size = get_dist_setting()
         device_ids = list(range(local_rank, n_gpu, local_world_size))
         max_memory = _get_max_memory(device_ids)
         max_memory = _sync_max_memory(max_memory)
-        max_memory = get_balanced_memory(
-            model, max_memory, low_zero=False, **kwargs)
+        max_memory = get_balanced_memory(model, max_memory, low_zero=False, **kwargs)
         max_memory = {k: v for k, v in max_memory.items() if v > 0}
-        return infer_auto_device_map(
-            model, max_memory, verbose=verbose, **kwargs)
+        return infer_auto_device_map(model, max_memory, verbose=verbose, **kwargs)
 
     _old_ddp_init = DDP.__init__
     accelerate.accelerator.torch.nn.parallel.DistributedDataParallel.__init__ = (
-        lambda self, model, device_ids, output_device, *args, **kwargs:
-        _old_ddp_init(self, model, *args, **kwargs))
+        lambda self, model, device_ids, output_device, *args, **kwargs: _old_ddp_init(self, model, *args, **kwargs))
     transformers.modeling_utils.get_balanced_memory = lambda *args, **kwargs: None
     transformers.modeling_utils.infer_auto_device_map = _infer_auto_device_map_patch
 
 if is_ddp_plus_mp() or use_torchacc():
     _old_accelerator_init = trainer.Accelerator.__init__
-    trainer.Accelerator.__init__ = (
-        lambda self, device_placement=False, *args, **kwargs:
-        _old_accelerator_init(
-            self, device_placement=device_placement, *args, **kwargs))
+    trainer.Accelerator.__init__ = (lambda self, device_placement=False, *args, **kwargs: _old_accelerator_init(
+        self, device_placement=device_placement, *args, **kwargs))
     trainer.Accelerator.verify_device_map = lambda *args, **kwargs: False
```

### Comparing `ms-swift-2.0.3.post1/swift/llm/utils/vllm_utils.py` & `ms-swift-2.0.4/swift/llm/utils/vllm_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -7,16 +7,15 @@
 import torch
 import vllm
 from modelscope import GenerationConfig
 from packaging import version
 from torch import dtype as Dtype
 from tqdm import tqdm
 from transformers import PreTrainedTokenizerBase
-from vllm import (AsyncEngineArgs, AsyncLLMEngine, EngineArgs, LLMEngine,
-                  SamplingParams)
+from vllm import AsyncEngineArgs, AsyncLLMEngine, EngineArgs, LLMEngine, SamplingParams
 
 from swift.utils import get_logger
 from .argument import InferArguments
 from .model import get_model_tokenizer
 from .template import Template, get_template
 
 try:
@@ -43,25 +42,21 @@
                     **kwargs) -> LLMEngine:
     model_dir = kwargs.pop('model_dir', None)  # compat with swift<1.7
     tokenizer = get_model_tokenizer(
         model_type,
         load_model=False,
         model_id_or_path=model_id_or_path,
         model_dir=model_dir,
-        revision=revision)[1]
+        revision=revision,
+        download_model=True)[1]
     model_dir = tokenizer.model_dir
 
     if engine_kwargs is None:
         engine_kwargs = {}
-    dtype_mapping = {
-        torch.float16: 'float16',
-        torch.bfloat16: 'bfloat16',
-        torch.float32: 'float32',
-        None: 'auto'
-    }
+    dtype_mapping = {torch.float16: 'float16', torch.bfloat16: 'bfloat16', torch.float32: 'float32', None: 'auto'}
     dtype = dtype_mapping[torch_dtype]
     disable_log_stats = engine_kwargs.pop('disable_log_stats', True)
 
     if use_async:
         engine_args_cls = AsyncEngineArgs
         llm_engine_cls = AsyncLLMEngine
         engine_kwargs['disable_log_requests'] = True
@@ -71,17 +66,15 @@
 
     parameters = inspect.signature(engine_args_cls.__init__).parameters
     if 'enable_lora' in parameters and enable_lora:
         engine_kwargs['enable_lora'] = enable_lora
         engine_kwargs['max_loras'] = max_loras
         engine_kwargs['max_lora_rank'] = max_lora_rank
     else:
-        assert not enable_lora, (
-            'The current version of VLLM does not support `enable_lora`. Please upgrade VLLM.'
-        )
+        assert not enable_lora, ('The current version of VLLM does not support `enable_lora`. Please upgrade VLLM.')
 
     engine_args = engine_args_cls(
         model=model_dir,
         trust_remote_code=True,
         dtype=dtype,
         gpu_memory_utilization=gpu_memory_utilization,
         tensor_parallel_size=tensor_parallel_size,
@@ -113,16 +106,15 @@
         _engine.tokenizer = tokenizer
 
     llm_engine.hf_tokenizer = tokenizer
     generation_config_path = os.path.join(model_dir, 'generation_config.json')
     if os.path.isfile(generation_config_path):
         generation_config = GenerationConfig.from_pretrained(model_dir)
         kwargs = generation_config.to_dict()
-        parameters = inspect.signature(
-            VllmGenerationConfig.__init__).parameters
+        parameters = inspect.signature(VllmGenerationConfig.__init__).parameters
         for k in kwargs.copy().keys():
             if k not in parameters:
                 kwargs.pop(k)
         llm_engine.generation_config = VllmGenerationConfig(**kwargs)
     else:
         llm_engine.generation_config = VllmGenerationConfig()
     return llm_engine
@@ -148,16 +140,15 @@
         if max_new_tokens is None:
             max_new_tokens = 64
         if num_beams > 1:
             top_k = -1
             top_p = 1
             temperature = 0
             logger.warning(
-                'The output of num_beams in vllm may not be consistent with the output of num_beams in transformers.'
-            )
+                'The output of num_beams in vllm may not be consistent with the output of num_beams in transformers.')
         if top_k == 0:
             top_k = -1
         if stop is None:
             stop = []
         kwargs['max_tokens'] = max_new_tokens
         kwargs['temperature'] = temperature
         kwargs['top_k'] = top_k
@@ -170,113 +161,100 @@
             kwargs['best_of'] = num_beams
         kwargs['n'] = n
         kwargs['length_penalty'] = length_penalty
         kwargs['stop'] = stop
         parameters = inspect.signature(SamplingParams.__init__).parameters
         for k in kwargs.copy().keys():
             if k not in parameters:
-                logger.info(
-                    f'The VLLM version is too old and does not support the parameter: {k}.'
-                )
+                logger.info(f'The VLLM version is too old and does not support the parameter: {k}.')
                 kwargs.pop(k)
         self._temperature = temperature
         super().__init__(**kwargs)
 
     def __setattr__(self, key: str, value: str) -> None:
         if key == 'max_new_tokens':
             self.max_tokens = value
         elif key == 'do_sample':
             assert value in {True, False}
             if value:
                 self.temperature = self._temperature
             else:
                 self.temperature = 0.
         elif key == 'max_length':
-            raise ValueError(
-                '`max_length` is not supported, please use `max_new_tokens` for setting.'
-            )
+            raise ValueError('`max_length` is not supported, please use `max_new_tokens` for setting.')
         else:
             super().__setattr__(key, value)
 
 
-def inference_stream_vllm(
-        llm_engine: LLMEngine,
-        template: Template,
-        request_list: List[Dict[str, Any]],
-        *,
-        generation_config: Optional[VllmGenerationConfig] = None,
-        lora_request: Optional['LoRARequest'] = None,
-        use_tqdm: bool = False,
-        **kwargs) -> Iterator[List[Dict[str, Any]]]:
+def inference_stream_vllm(llm_engine: LLMEngine,
+                          template: Template,
+                          request_list: List[Dict[str, Any]],
+                          *,
+                          generation_config: Optional[VllmGenerationConfig] = None,
+                          lora_request: Optional['LoRARequest'] = None,
+                          use_tqdm: bool = False,
+                          **kwargs) -> Iterator[List[Dict[str, Any]]]:
     """
     request_list: e.g. [{'query': 'hello!'}].
         The keys that can be included are: 'query', 'history', 'system'.
     generation_config: Priority: generation_config > model.generation_config.
     return: e.g. [{'response': 'hi!', 'history': [('hello!', 'hi!')]}].
         The keys to be included will be: 'response', 'history'.
     """
     if generation_config is None:
-        generation_config = getattr(llm_engine, 'generation_config',
-                                    VllmGenerationConfig())
+        generation_config = getattr(llm_engine, 'generation_config', VllmGenerationConfig())
     assert isinstance(generation_config, VllmGenerationConfig)
     request_list = deepcopy(request_list)
     generation_config = deepcopy(generation_config)
     if generation_config.use_beam_search is True:
         error_msg = 'Streaming generation does not support beam search.'
         raise ValueError(error_msg)
 
     tokenizer = template.tokenizer
     if tokenizer.eos_token is not None and tokenizer.eos_token not in generation_config.stop:
         generation_config.stop.append(tokenizer.eos_token)
-    if isinstance(template.suffix[-1],
-                  str) and template.suffix[-1] not in generation_config.stop:
+    if isinstance(template.suffix[-1], str) and template.suffix[-1] not in generation_config.stop:
         generation_config.stop.append(template.suffix[-1])
     if isinstance(template.suffix[-1], list):
         token_str = tokenizer.decode(template.suffix[-1])
         if token_str not in generation_config.stop:
             generation_config.stop.append(token_str)
 
     parameters = inspect.signature(llm_engine.add_request).parameters
     add_request_kwargs = {}
     if 'lora_request' in parameters:
         add_request_kwargs['lora_request'] = lora_request
     else:
         assert lora_request is None, (
-            'The current version of VLLM does not support `lora_request`. Please upgrade VLLM.'
-        )
+            'The current version of VLLM does not support `lora_request`. Please upgrade VLLM.')
     request_temp = []
     for i, request in enumerate(request_list):
         history = request.get('history', None)
         if history is None:
             history = []
 
         # agent support
-        is_observation = history[-1][-1].endswith(
-            'Observation:') if history and history[-1][-1] else False
+        is_observation = history[-1][-1].endswith('Observation:') if history and history[-1][-1] else False
         act_length = None
         if is_observation:
             history[-1][-1] = history[-1][-1] + request['query']
             act_length = len(history[-1][-1])
             request['query'] = None
         request_temp.append((is_observation, act_length))
 
         request['history'] = history
         inputs = template.encode(request)[0]
         if len(inputs) == 0:
-            raise ValueError(
-                'input_ids exceeds `max_length`. Please increase the value of `max_length`.'
-            )
+            raise ValueError('input_ids exceeds `max_length`. Please increase the value of `max_length`.')
         input_ids = inputs['input_ids']
-        llm_engine.add_request(
-            str(i), None, generation_config, input_ids, **add_request_kwargs)
+        llm_engine.add_request(str(i), None, generation_config, input_ids, **add_request_kwargs)
 
     resp_list = [None] * len(request_list)
     print_idx_list = [[0] for _ in range(len(request_list))]
-    prog_bar = tqdm(
-        total=len(request_list), dynamic_ncols=True, disable=not use_tqdm)
+    prog_bar = tqdm(total=len(request_list), dynamic_ncols=True, disable=not use_tqdm)
     while llm_engine.has_unfinished_requests():
         step_outputs = llm_engine.step()
         for output in step_outputs:
             i = int(output.request_id)
             request = request_list[i]
             generate_ids = output.outputs[0].token_ids
             safe_response = template.generate_ids_to_response(
@@ -284,16 +262,15 @@
             query = request['query']
             history = request['history']
             if resp_list[i] is None and not request_temp[i][0]:
                 history.append(None)
             if not request_temp[i][0]:
                 history[-1] = [query, safe_response]
             else:
-                history[-1][
-                    -1] = history[-1][-1][:request_temp[i][1]] + safe_response
+                history[-1][-1] = history[-1][-1][:request_temp[i][1]] + safe_response
             resp_list[i] = {'response': safe_response, 'history': history}
             if output.finished:
                 prog_bar.update()
         yield resp_list
 
 
 def inference_vllm(llm_engine: LLMEngine,
@@ -311,64 +288,56 @@
     request_list: e.g. [{'query': 'hello!'}].
         The keys that can be included are: 'query', 'history', 'system'.
     generation_config: Priority: generation_config > model.generation_config.
     return: e.g. [{'response': 'hi!', 'history': [('hello!', 'hi!')]}].
         The keys to be included will be: 'response', 'history'.
     """
     if generation_config is None:
-        generation_config = getattr(llm_engine, 'generation_config',
-                                    VllmGenerationConfig())
+        generation_config = getattr(llm_engine, 'generation_config', VllmGenerationConfig())
     assert isinstance(generation_config, VllmGenerationConfig)
     request_list = deepcopy(request_list)
     generation_config = deepcopy(generation_config)
 
     tokenizer = template.tokenizer
     if tokenizer.eos_token is not None and tokenizer.eos_token not in generation_config.stop:
         generation_config.stop.append(tokenizer.eos_token)
-    if isinstance(template.suffix[-1],
-                  str) and template.suffix[-1] not in generation_config.stop:
+    if isinstance(template.suffix[-1], str) and template.suffix[-1] not in generation_config.stop:
         generation_config.stop.append(template.suffix[-1])
     if isinstance(template.suffix[-1], list):
         token_str = tokenizer.decode(template.suffix[-1])
         if token_str not in generation_config.stop:
             generation_config.stop.append(token_str)
 
     parameters = inspect.signature(llm_engine.add_request).parameters
     add_request_kwargs = {}
     if 'lora_request' in parameters:
         add_request_kwargs['lora_request'] = lora_request
     else:
         assert lora_request is None, (
-            'The current version of VLLM does not support `lora_request`. Please upgrade VLLM.'
-        )
+            'The current version of VLLM does not support `lora_request`. Please upgrade VLLM.')
 
     for i, request in enumerate(request_list):
         history = request.get('history', None)
         if history is None:
             history = []
 
-        is_observation = history[-1][-1].endswith(
-            'Observation:') if history and history[-1][-1] else False
+        is_observation = history[-1][-1].endswith('Observation:') if history and history[-1][-1] else False
         if is_observation:
             history[-1][-1] = history[-1][-1] + request['query']
             request['query'] = None
         request['history'] = history
         inputs = template.encode(request)[0]
         if len(inputs) == 0:
-            raise ValueError(
-                'input_ids exceeds `max_length`. Please increase the value of `max_length`.'
-            )
+            raise ValueError('input_ids exceeds `max_length`. Please increase the value of `max_length`.')
         input_ids = inputs['input_ids']
-        llm_engine.add_request(
-            str(i), None, generation_config, input_ids, **add_request_kwargs)
+        llm_engine.add_request(str(i), None, generation_config, input_ids, **add_request_kwargs)
 
     if use_tqdm is True:
         assert verbose is False
-    prog_bar = tqdm(
-        total=len(request_list), dynamic_ncols=True, disable=not use_tqdm)
+    prog_bar = tqdm(total=len(request_list), dynamic_ncols=True, disable=not use_tqdm)
     outputs = []
     while llm_engine.has_unfinished_requests():
         step_outputs = llm_engine.step()
         for output in step_outputs:
             if output.finished:
                 outputs.append(output)
                 prog_bar.update()
@@ -383,29 +352,24 @@
         history = request['history']
         if not is_observation:
             history.append([query, response])
         else:
             history[-1][-1] = history[-1][-1] + response
         resp_list[i] = {'response': response, 'history': history}
         if verbose:
-            print(
-                f'{prompt_prefix}{tokenizer.decode(output.prompt_token_ids, False)}{output_prefix}',
-                end='')
+            print(f'{prompt_prefix}{tokenizer.decode(output.prompt_token_ids, False)}{output_prefix}', end='')
             print(tokenizer.decode(output.outputs[0].token_ids, False))
     return resp_list
 
 
-def prepare_vllm_engine_template(
-        args: InferArguments,
-        use_async: bool = False) -> Tuple[LLMEngine, Template]:
+def prepare_vllm_engine_template(args: InferArguments, use_async: bool = False) -> Tuple[LLMEngine, Template]:
     logger.info(f'device_count: {torch.cuda.device_count()}')
 
     assert args.quantization_bit == 0, 'not support bnb'
-    assert not (args.sft_type == 'lora'
-                and not args.vllm_enable_lora), 'you need to merge lora'
+    assert not (args.sft_type == 'lora' and not args.vllm_enable_lora), 'you need to merge lora'
     # Loading Model and Tokenizer
     model_id_or_path = None
     if args.sft_type == 'full' and args.ckpt_dir is not None:
         model_id_or_path = args.ckpt_dir
     elif args.model_id_or_path is not None:
         model_id_or_path = args.model_id_or_path
     llm_engine = get_vllm_engine(
@@ -435,13 +399,12 @@
         top_k=args.top_k,
         top_p=args.top_p,
         stop=args.stop_words,
         repetition_penalty=args.repetition_penalty,
         num_beams=args.num_beams)
     logger.info(f'generation_config: {generation_config}')
     llm_engine.generation_config = generation_config
-    template: Template = get_template(args.template_type, tokenizer,
-                                      args.system, args.max_length,
+    template: Template = get_template(args.template_type, tokenizer, args.system, args.max_length,
                                       args.truncation_strategy)
     args.system = template.default_system
     logger.info(f'system: {args.system}')
     return llm_engine, template
```

### Comparing `ms-swift-2.0.3.post1/swift/torchacc_utils.py` & `ms-swift-2.0.4/swift/torchacc_utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -34,25 +34,23 @@
     if cloest_length == sys.maxsize:
         bucket_sizes.append(data_length)
         cloest_length = data_length
 
     return cloest_length
 
 
-def pad_and_split_batch(padding_to, input_ids, attention_mask, labels,
-                        loss_scale, max_length, tokenizer, rank, world_size):
+def pad_and_split_batch(padding_to, input_ids, attention_mask, labels, loss_scale, max_length, tokenizer, rank,
+                        world_size):
     if padding_to is None:
         longest_len = input_ids.shape[-1]
         bucket_sizes = get_bucket_sizes(max_length)
         bucket_data_length = _get_closet_bucket(bucket_sizes, longest_len)
         padding_length = bucket_data_length - input_ids.shape[1]
-        input_ids = F.pad(input_ids, (0, padding_length), 'constant',
-                          tokenizer.pad_token_id)
-        attention_mask = F.pad(attention_mask, (0, padding_length), 'constant',
-                               0)
+        input_ids = F.pad(input_ids, (0, padding_length), 'constant', tokenizer.pad_token_id)
+        attention_mask = F.pad(attention_mask, (0, padding_length), 'constant', 0)
         if loss_scale:
             loss_scale = F.pad(loss_scale, (0, padding_length), 'constant', 0.)
         labels = F.pad(labels, (0, padding_length), 'constant', -100)
 
     # manully split the batch to different DP rank.
     batch_size = input_ids.shape[0] // world_size
     if batch_size > 0:
@@ -62,35 +60,32 @@
         attention_mask = attention_mask[start:end, :]
         labels = labels[start:end, :]
         if loss_scale:
             loss_scale = loss_scale[start:end, :]
     return input_ids, attention_mask, labels, loss_scale
 
 
-def ta_train_dataloader(train_dataset, data_collator, sampler, args,
-                        batch_size):
+def ta_train_dataloader(train_dataset, data_collator, sampler, args, batch_size):
     # patch skip_first_batches for customized dataloader.
     def acc_skip_first_batches(dataloader, num_batches=0):
         from accelerate.data_loader import SkipBatchSampler
-        batch_sampler = SkipBatchSampler(
-            dataloader._loader.batch_sampler, skip_batches=num_batches)
+        batch_sampler = SkipBatchSampler(dataloader._loader.batch_sampler, skip_batches=num_batches)
         dataset = dataloader.dataset
         dataloader_params = {
             'collate_fn': data_collator,
             'num_workers': args.dataloader_num_workers,
             'pin_memory': args.dataloader_pin_memory,
             'persistent_workers': args.dataloader_persistent_workers,
         }
 
         if not isinstance(train_dataset, torch.utils.data.IterableDataset):
             dataloader_params['batch_sampler'] = batch_sampler
             dataloader_params['worker_init_fn'] = trainer.seed_worker
 
-        return ta.AsyncLoader(
-            DataLoader(dataset, **dataloader_params), args.device)
+        return ta.AsyncLoader(DataLoader(dataset, **dataloader_params), args.device)
 
     trainer.skip_first_batches = acc_skip_first_batches
 
     # dataloader for TorchAcc.
     import torchacc as ta
 
     dataloader_params = {
@@ -102,16 +97,15 @@
     }
 
     if not isinstance(train_dataset, torch.utils.data.IterableDataset):
         dataloader_params['sampler'] = sampler
         dataloader_params['drop_last'] = args.dataloader_drop_last
         dataloader_params['worker_init_fn'] = trainer.seed_worker
 
-    return ta.AsyncLoader(
-        DataLoader(train_dataset, **dataloader_params), args.device)
+    return ta.AsyncLoader(DataLoader(train_dataset, **dataloader_params), args.device)
 
 
 def ta_eval_dataloader(eval_dataset, data_collator, sampler, args):
     import torchacc as ta
 
     dataloader_params = {
         'batch_size': args.eval_batch_size,
@@ -121,16 +115,15 @@
         'persistent_workers': args.dataloader_persistent_workers,
     }
 
     if not isinstance(eval_dataset, torch.utils.data.IterableDataset):
         dataloader_params['sampler'] = sampler
         dataloader_params['drop_last'] = args.dataloader_drop_last
 
-    return ta.AsyncLoader(
-        DataLoader(eval_dataset, **dataloader_params), args.device)
+    return ta.AsyncLoader(DataLoader(eval_dataset, **dataloader_params), args.device)
 
 
 def ta_test_dataloader(test_dataset, data_collator, sampler, args):
     import torchacc as ta
 
     dataloader_params = {
         'batch_size': args.eval_batch_size,
@@ -141,16 +134,15 @@
     }
 
     if not isinstance(test_dataset, torch.utils.data.IterableDataset):
         dataloader_params['sampler'] = sampler
         dataloader_params['drop_last'] = args.dataloader_drop_last
 
     # We use the same batch_size as for eval.
-    return ta.AsyncLoader(
-        DataLoader(test_dataset, **dataloader_params), args.device)
+    return ta.AsyncLoader(DataLoader(test_dataset, **dataloader_params), args.device)
 
 
 # Save/load checkpoint
 def consolidate_checkpoint(resume_from_checkpoint, model_name='adapter_model'):
     """ Consolidate the sharded TorchAcc checkpoints into a single model checkpoint.
     """
     import torch_xla.core.xla_model as xm
@@ -176,68 +168,53 @@
     state_dict_list = []
     if xm.is_master_ordinal(local=False) and use_safetensors:
         from safetensors.torch import load_file, save_file
         for rank in range(xm.xrt_world_size()):
             shard_dir = os.path.join(resume_from_checkpoint, f'{rank}')
             filename = os.path.join(shard_dir, f'{model_name}.safetensors')
             state_dict = load_file(filename, device='cpu')
-            state_dict = OrderedDict(('_fsdp_wrapped_module.' + k, v)
-                                     for k, v in state_dict.items())
+            state_dict = OrderedDict(('_fsdp_wrapped_module.' + k, v) for k, v in state_dict.items())
             state_dict_list.append(state_dict)
-        shard_metadata = torch.load(
-            os.path.join(model_dir, 'shard_meta.pth'), map_location='cpu')
+        shard_metadata = torch.load(os.path.join(model_dir, 'shard_meta.pth'), map_location='cpu')
     elif xm.is_master_ordinal(local=False):
         for rank in range(xm.xrt_world_size()):
             shard_dir = os.path.join(resume_from_checkpoint, f'{rank}')
             if not is_pretrained_model:
                 filename = os.path.join(shard_dir, f'{model_name}.bin')
             else:
                 filename = os.path.join(shard_dir, 'pytorch_model.bin')
             state_dict = torch.load(filename, map_location='cpu')
-            state_dict = OrderedDict(('_fsdp_wrapped_module.' + k, v)
-                                     for k, v in state_dict.items())
+            state_dict = OrderedDict(('_fsdp_wrapped_module.' + k, v) for k, v in state_dict.items())
             state_dict_list.append(state_dict)
-        shard_metadata = torch.load(
-            os.path.join(model_dir, 'shard_meta.pth'), map_location='cpu')
+        shard_metadata = torch.load(os.path.join(model_dir, 'shard_meta.pth'), map_location='cpu')
 
     if xm.is_master_ordinal(local=False):
-        full_state_dict = consolidate_sharded_state_dicts(
-            state_dict_list, shard_metadata)
+        full_state_dict = consolidate_sharded_state_dicts(state_dict_list, shard_metadata)
         # peft will prepend "default." prefix automatically, so we remove the
         # "default." prefix to prevent the duplication of the prefix.
-        full_state_dict = OrderedDict(
-            (k.replace('default.', ''), v) for k, v in full_state_dict.items())
-        torch.save(full_state_dict,
-                   os.path.join(resume_from_checkpoint, f'{model_name}.bin'))
+        full_state_dict = OrderedDict((k.replace('default.', ''), v) for k, v in full_state_dict.items())
+        torch.save(full_state_dict, os.path.join(resume_from_checkpoint, f'{model_name}.bin'))
         if model_name == 'adapter_model':
-            config_path = os.path.join(resume_from_checkpoint,
-                                       'adapter_config.json')
+            config_path = os.path.join(resume_from_checkpoint, 'adapter_config.json')
             old_config_path = os.path.join(model_dir, 'adapter_config.json')
             os.system(f'cp {old_config_path} {config_path}')
     xm.rendezvous('ckpt_consolidation')
 
 
 def ta_save_optimizer_and_scheduler(optimizer, lr_scheduler, output_dir):
     import torch_xla.core.xla_model as xm
     xm.rendezvous('saving_optimizer_states')
-    torch.save(optimizer.state_dict(),
-               os.path.join(output_dir, f'optimizer_{xm.get_ordinal()}.pt'))
-    torch.save(lr_scheduler.state_dict(),
-               os.path.join(output_dir, f'scheduler_{xm.get_ordinal()}.pt'))
+    torch.save(optimizer.state_dict(), os.path.join(output_dir, f'optimizer_{xm.get_ordinal()}.pt'))
+    torch.save(lr_scheduler.state_dict(), os.path.join(output_dir, f'scheduler_{xm.get_ordinal()}.pt'))
 
 
-def ta_load_optimizer_and_scheduler(optimizer, lr_scheduler, checkpoint,
-                                    device):
+def ta_load_optimizer_and_scheduler(optimizer, lr_scheduler, checkpoint, device):
     import torch_xla.core.xla_model as xm
-    optimizer_state = torch.load(
-        os.path.join(checkpoint, f'optimizer_{xm.get_ordinal()}.pt'),
-        map_location='cpu')
-    lr_scheduler_state = torch.load(
-        os.path.join(checkpoint, f'scheduler_{xm.get_ordinal()}.pt'),
-        map_location='cpu')
+    optimizer_state = torch.load(os.path.join(checkpoint, f'optimizer_{xm.get_ordinal()}.pt'), map_location='cpu')
+    lr_scheduler_state = torch.load(os.path.join(checkpoint, f'scheduler_{xm.get_ordinal()}.pt'), map_location='cpu')
     xm.send_cpu_data_to_device(optimizer_state, device)
     xm.send_cpu_data_to_device(lr_scheduler_state, device)
 
     optimizer.load_state_dict(optimizer_state)
     lr_scheduler.load_state_dict(lr_scheduler_state)
     return optimizer, lr_scheduler
 
@@ -257,31 +234,23 @@
     # They can then be reloaded using `from_pretrained()`
     xm.rendezvous('saving_checkpoint')
     out_dir = os.path.join(output_dir, f'{xm.get_ordinal()}')
     if not isinstance(model, supported_classes):
         state_dict = model.state_dict()
         _unwrap_model = unwrap_model(model)
         if isinstance(_unwrap_model, supported_classes):
-            _unwrap_model.save_pretrained(
-                out_dir, safe_serialization=save_safetensors)
+            _unwrap_model.save_pretrained(out_dir, safe_serialization=save_safetensors)
         else:
-            logger.info(
-                'Trainer.model is not a `PreTrainedModel`, only saving its state dict.'
-            )
+            logger.info('Trainer.model is not a `PreTrainedModel`, only saving its state dict.')
             if save_safetensors:
-                safetensors.torch.save_file(
-                    state_dict, os.path.join(out_dir, 'model.safetensors'))
+                safetensors.torch.save_file(state_dict, os.path.join(out_dir, 'model.safetensors'))
             else:
-                torch.save(state_dict,
-                           os.path.join(out_dir, 'pytorch_model.bin'))
+                torch.save(state_dict, os.path.join(out_dir, 'pytorch_model.bin'))
     else:
         model.save_pretrained(out_dir, safe_serialization=save_safetensors)
     # save shard_metadata for consolidation.
     shard_meta = self_model._get_underlay_model().get_shard_metadata()
     xm.save(shard_meta, os.path.join(out_dir, 'shard_meta.pth'))
     xm.rendezvous('saving_checkpoint_done')
 
     if tokenizer is not None and args.should_save:
-        tokenizer.save_pretrained(
-            output_dir,
-            is_main_process=xm.is_master_ordinal(local=False),
-            save_function=xm.save)
+        tokenizer.save_pretrained(output_dir, is_main_process=xm.is_master_ordinal(local=False), save_function=xm.save)
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/__init__.py` & `ms-swift-2.0.4/swift/trainers/__init__.py`

 * *Files 16% similar despite different names*

```diff
@@ -11,16 +11,15 @@
         IntervalStrategy, SchedulerType, ShardedDDPOption, TrainerCallback
 else:
     _import_structure = {
         'arguments': ['Seq2SeqTrainingArguments', 'TrainingArguments'],
         'dpo_trainers': ['DPOTrainer'],
         'trainers': ['Seq2SeqTrainer', 'Trainer'],
         'utils': [
-            'EvaluationStrategy', 'FSDPOption', 'HPSearchBackend',
-            'HubStrategy', 'IntervalStrategy', 'SchedulerType',
+            'EvaluationStrategy', 'FSDPOption', 'HPSearchBackend', 'HubStrategy', 'IntervalStrategy', 'SchedulerType',
             'ShardedDDPOption', 'TrainerCallback'
         ]
     }
 
     import sys
 
     sys.modules[__name__] = _LazyModule(
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/arguments.py` & `ms-swift-2.0.4/swift/trainers/arguments.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,40 +2,32 @@
 
 import os
 from dataclasses import dataclass, field
 from typing import List, Optional
 
 import torch
 from transformers.training_args import TrainingArguments as HfTrainingArguments
-from transformers.training_args_seq2seq import \
-    Seq2SeqTrainingArguments as HfSeq2SeqTrainingArguments
+from transformers.training_args_seq2seq import Seq2SeqTrainingArguments as HfSeq2SeqTrainingArguments
 from transformers.utils import is_accelerate_available
 
 from swift.utils import is_dist, use_torchacc
 
 
 @dataclass
 class SwiftArgumentsMixin:
     # ckpt only save model
     save_only_model: bool = False
     train_sampler_random: bool = True
     push_hub_strategy: str = field(
-        default='push_best',
-        metadata={
-            'choices':
-            {'end', 'push_best', 'push_last', 'checkpoint', 'all_checkpoints'}
-        })
-    acc_strategy: str = field(
-        default='token', metadata={'choices': ['token', 'sentence']})
+        default='push_best', metadata={'choices': {'end', 'push_best', 'push_last', 'checkpoint', 'all_checkpoints'}})
+    acc_strategy: str = field(default='token', metadata={'choices': ['token', 'sentence']})
     additional_saved_files: Optional[List[str]] = None
 
     def __post_init__(self):
-        if is_dist(
-        ) and self.ddp_backend == 'nccl' and torch.cuda.is_available(
-        ) and is_accelerate_available():
+        if is_dist() and self.ddp_backend == 'nccl' and torch.cuda.is_available() and is_accelerate_available():
             try:
                 from accelerate.utils import check_cuda_p2p_ib_support
                 if not check_cuda_p2p_ib_support():
                     os.environ['NCCL_P2P_DISABLE'] = '1'
                     os.environ['NCCL_IB_DISABLE'] = '1'
             except ImportError:
                 pass
@@ -46,13 +38,12 @@
 
 @dataclass
 class TrainingArguments(SwiftArgumentsMixin, HfTrainingArguments):
     pass
 
 
 @dataclass
-class Seq2SeqTrainingArguments(SwiftArgumentsMixin,
-                               HfSeq2SeqTrainingArguments):
+class Seq2SeqTrainingArguments(SwiftArgumentsMixin, HfSeq2SeqTrainingArguments):
 
     @property
     def place_model_on_device(self):
         return False if use_torchacc() else super().place_model_on_device
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/callback.py` & `ms-swift-2.0.4/swift/trainers/callback.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,53 +1,37 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 import os
 
 import json
 from tqdm.auto import tqdm
-from transformers.trainer_callback import (DefaultFlowCallback,
-                                           ProgressCallback, TrainerCallback,
-                                           TrainerControl, TrainerState)
+from transformers.trainer_callback import (DefaultFlowCallback, ProgressCallback, TrainerCallback, TrainerControl,
+                                           TrainerState)
 from transformers.trainer_utils import IntervalStrategy, has_length
 
 from swift.utils import is_pai_training_job
 from .arguments import TrainingArguments
 
 
 class ProgressCallbackNew(ProgressCallback):
 
     def on_train_begin(self, args, state, control, **kwargs):
         if state.is_local_process_zero:
-            self.training_bar = tqdm(
-                desc='Train', total=state.max_steps, dynamic_ncols=True)
+            self.training_bar = tqdm(desc='Train', total=state.max_steps, dynamic_ncols=True)
         self.current_step = 0
 
-    def on_prediction_step(self,
-                           args,
-                           state: TrainerState,
-                           control,
-                           eval_dataloader=None,
-                           **kwargs):
+    def on_prediction_step(self, args, state: TrainerState, control, eval_dataloader=None, **kwargs):
         if state.is_local_process_zero and has_length(eval_dataloader):
             if self.prediction_bar is None:
                 if self.training_bar is not None:
                     self.training_bar.fp.write('\n')
                 self.prediction_bar = tqdm(
-                    desc='Val',
-                    total=len(eval_dataloader),
-                    leave=True,
-                    dynamic_ncols=True,
-                    position=0)
+                    desc='Val', total=len(eval_dataloader), leave=True, dynamic_ncols=True, position=0)
             self.prediction_bar.update()
 
-    def on_log(self,
-               args: TrainingArguments,
-               state: TrainerState,
-               control,
-               logs=None,
-               **kwargs):
+    def on_log(self, args: TrainingArguments, state: TrainerState, control, logs=None, **kwargs):
         logs['global_step'] = state.global_step
         for k, v in logs.items():
             if isinstance(v, float):
                 logs[k] = round(logs[k], 8)
         if not is_pai_training_job() and state.is_local_process_zero:
             jsonl_path = os.path.join(args.output_dir, 'logging.jsonl')
             with open(jsonl_path, 'a', encoding='utf-8') as f:
@@ -55,16 +39,15 @@
         super().on_log(args, state, control, logs, **kwargs)
         if state.is_local_process_zero and self.training_bar is not None:
             self.training_bar.refresh()
 
 
 class DefaultFlowCallbackNew(DefaultFlowCallback):
 
-    def on_step_end(self, args: TrainingArguments, state: TrainerState,
-                    control: TrainerControl, **kwargs):
+    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):
         control = super().on_step_end(args, state, control, **kwargs)
         # save the last ckpt
         if state.global_step == state.max_steps:
             if args.evaluation_strategy != IntervalStrategy.NO:
                 control.should_evaluate = True
             if args.save_strategy != IntervalStrategy.NO:
                 control.should_save = True
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/dpo_trainers.py` & `ms-swift-2.0.4/swift/trainers/dpo_trainers.py`

 * *Files 5% similar despite different names*

```diff
@@ -4,57 +4,43 @@
 from torch import nn
 from transformers import PreTrainedModel, trainer
 from trl import DPOTrainer as HFDPOTrainer
 
 from swift.llm.utils.template import Context, Template
 from swift.llm.utils.utils import sort_by_max_length
 from swift.utils import get_logger
-from .callback import (DefaultFlowCallbackNew, PrinterCallbackNew,
-                       ProgressCallbackNew)
+from .callback import DefaultFlowCallbackNew, PrinterCallbackNew, ProgressCallbackNew
 from .mixin import PushToMsHubMixin, SwiftMixin
 
 logger = get_logger()
 
 
 class DPOTrainer(PushToMsHubMixin, SwiftMixin, HFDPOTrainer):
 
-    def __init__(self,
-                 *args,
-                 template: Template,
-                 sft_beta=0.,
-                 test_oom_error=False,
-                 **kwargs):
+    def __init__(self, *args, template: Template, sft_beta=0., test_oom_error=False, **kwargs):
         self.template = template
         self.sft_beta = sft_beta
         super().__init__(*args, **kwargs)
         train_ds_info = self.stat_dataset(self.train_dataset)
         val_ds_info = self.stat_dataset(self.eval_dataset)
-        self.dataset_info = {
-            'train_dataset': train_ds_info,
-            'val_dataset': val_ds_info
-        }
+        self.dataset_info = {'train_dataset': train_ds_info, 'val_dataset': val_ds_info}
         if test_oom_error:
             self.train_dataset = sort_by_max_length(self.train_dataset, 20000)
         # performance
         self.perf: Dict[str, Any] = {
-            'gen_time':
-            0.,
-            'gen_len':
-            0,
+            'gen_time': 0.,
+            'gen_len': 0,
             'memory': {},
-            'model':
-            self.model.get_trainable_parameters() if hasattr(
-                self.model, 'get_trainable_parameters') else None,
+            'model': self.model.get_trainable_parameters() if hasattr(self.model, 'get_trainable_parameters') else None,
         }
 
     def train(self, *args, **kwargs) -> torch.Tensor:
         res = super().train(*args, **kwargs)
         for i in range(torch.cuda.device_count()):
-            self.perf['memory'][
-                f'cuda:{i}'] = f'{torch.cuda.max_memory_reserved(i)/1024/1024/1024:.2f}GiB'
+            self.perf['memory'][f'cuda:{i}'] = f'{torch.cuda.max_memory_reserved(i)/1024/1024/1024:.2f}GiB'
         return res
 
     def concat_template(self, feature):
         query: Optional[str] = feature.get('query', None)
         system: Optional[str] = feature.get('system', None)
         history: List = feature.get('history', [])
         if system is None:
@@ -65,136 +51,99 @@
         res_context_list: List[Context] = []
         compute_loss_idx: List[float] = []
         if system is None:
             assert self.template.prefix != self.template.prefix_has_system, f'template.prefix: {self.template.prefix}'
             prefix = self.template.prefix
         else:
             prefix = self.template.prefix_has_system
-        self.template._concat_context_list(
-            prefix, res_context_list, compute_loss_idx, system=system)
+        self.template._concat_context_list(prefix, res_context_list, compute_loss_idx, system=system)
         for i, (q, r) in enumerate(history):
             self.template._concat_context_list(
                 [
                     *self.template.prompt,
                     '{{RESPONSE}}',
                     *self.template.chat_sep  # noqa
                 ],
                 res_context_list,
                 compute_loss_idx,
                 query=q,
                 response=r,
                 round0=i)  # noqa
         self.template._concat_context_list(
-            self.template.prompt,
-            res_context_list,
-            compute_loss_idx,
-            query=query,
-            round0=len(history))
-        res_context_list, compute_loss_idx = self.template._simplify_context_list(
-            res_context_list, compute_loss_idx)
+            self.template.prompt, res_context_list, compute_loss_idx, query=query, round0=len(history))
+        res_context_list, compute_loss_idx = self.template._simplify_context_list(res_context_list, compute_loss_idx)
 
-        return res_context_list, feature['response'], feature[
-            'rejected_response'], compute_loss_idx
+        return res_context_list, feature['response'], feature['rejected_response'], compute_loss_idx
 
     def build_tokenized_answer(self, prompt, answer, prompt_loss_scale):
-        input_ids, labels, loss_scale, kwargs = self.template._encode_context_list(
-            prompt, prompt_loss_scale)
+        input_ids, labels, loss_scale, kwargs = self.template._encode_context_list(prompt, prompt_loss_scale)
         tgt_input_ids = self.template._encode_context_list([answer], [1.0])[0]
-        tgt_input_ids += self.template._encode_context_list(
-            self.template.suffix, [1.0])[0]
+        tgt_input_ids += self.template._encode_context_list(self.template.suffix, [1.0])[0]
         return dict(
             prompt_input_ids=input_ids,
             prompt_attention_mask=[1] * len(input_ids),
             input_ids=tgt_input_ids,
             attention_mask=[1] * len(tgt_input_ids),
         )
 
-    def tokenize_row(self,
-                     feature,
-                     model: Union[PreTrainedModel, nn.Module] = None) -> Dict:
+    def tokenize_row(self, feature, model: Union[PreTrainedModel, nn.Module] = None) -> Dict:
         batch = {}
         if not self.is_encoder_decoder:
-            prompt, chosen, rejected, loss_scale = self.concat_template(
-                feature)
+            prompt, chosen, rejected, loss_scale = self.concat_template(feature)
 
-            prompt_tokens, _, _, _ = self.template._encode_context_list(
-                prompt, loss_scale)
+            prompt_tokens, _, _, _ = self.template._encode_context_list(prompt, loss_scale)
             prompt_tokens = {
                 'input_ids': prompt_tokens,
                 'attention_mask': [1] * len(prompt_tokens),
             }
-            prompt_tokens = {
-                f'prompt_{k}': v
-                for k, v in prompt_tokens.items()
-            }
+            prompt_tokens = {f'prompt_{k}': v for k, v in prompt_tokens.items()}
 
             if not isinstance(chosen, str):
-                raise ValueError(
-                    f'chosen should be an str but got {type(chosen)}')
-            chosen_tokens = self.build_tokenized_answer(
-                prompt, chosen, loss_scale)
+                raise ValueError(f'chosen should be an str but got {type(chosen)}')
+            chosen_tokens = self.build_tokenized_answer(prompt, chosen, loss_scale)
 
             if not isinstance(rejected, str):
-                raise ValueError(
-                    f'rejected should be an str but got {type(rejected)}')
-            rejected_tokens = self.build_tokenized_answer(
-                prompt, rejected, loss_scale)
-
-            longer_response_length = max(
-                len(chosen_tokens['input_ids']),
-                len(rejected_tokens['input_ids']))
+                raise ValueError(f'rejected should be an str but got {type(rejected)}')
+            rejected_tokens = self.build_tokenized_answer(prompt, rejected, loss_scale)
+
+            longer_response_length = max(len(chosen_tokens['input_ids']), len(rejected_tokens['input_ids']))
 
             # if combined sequence is too long, truncate the prompt
-            for answer_tokens in [
-                    chosen_tokens, rejected_tokens, prompt_tokens
-            ]:
-                if len(answer_tokens['prompt_input_ids']
-                       ) + longer_response_length > self.max_length:
+            for answer_tokens in [chosen_tokens, rejected_tokens, prompt_tokens]:
+                if len(answer_tokens['prompt_input_ids']) + longer_response_length > self.max_length:
                     if self.truncation_mode == 'keep_start':
                         for k in ['prompt_input_ids', 'prompt_attention_mask']:
-                            answer_tokens[k] = answer_tokens[
-                                k][:self.max_prompt_length]
+                            answer_tokens[k] = answer_tokens[k][:self.max_prompt_length]
                     elif self.truncation_mode == 'keep_end':
                         for k in ['prompt_input_ids', 'prompt_attention_mask']:
-                            answer_tokens[k] = answer_tokens[k][
-                                -self.max_prompt_length:]
+                            answer_tokens[k] = answer_tokens[k][-self.max_prompt_length:]
                     else:
-                        raise ValueError(
-                            f'Unknown truncation mode: {self.truncation_mode}')
+                        raise ValueError(f'Unknown truncation mode: {self.truncation_mode}')
 
             # if that's still too long, truncate the response
             for answer_tokens in [chosen_tokens, rejected_tokens]:
-                if len(answer_tokens['prompt_input_ids']
-                       ) + longer_response_length > self.max_length:
+                if len(answer_tokens['prompt_input_ids']) + longer_response_length > self.max_length:
                     for k in ['input_ids', 'attention_mask']:
-                        answer_tokens[k] = answer_tokens[k][:self.max_length
-                                                            - self.
-                                                            max_prompt_length]
+                        answer_tokens[k] = answer_tokens[k][:self.max_length - self.max_prompt_length]
 
             # Create labels
             chosen_sequence_tokens = {
                 k: chosen_tokens[f'prompt_{k}'] + chosen_tokens[k]
                 for k in ['input_ids', 'attention_mask']
             }
             rejected_sequence_tokens = {
                 k: rejected_tokens[f'prompt_{k}'] + rejected_tokens[k]
                 for k in ['input_ids', 'attention_mask']
             }
-            chosen_sequence_tokens['labels'] = chosen_sequence_tokens[
-                'input_ids'][:]
-            _paddings = [self.label_pad_token_id] * len(
-                chosen_tokens['prompt_input_ids'])
-            chosen_sequence_tokens[
-                'labels'][:len(chosen_tokens['prompt_input_ids'])] = _paddings
-            rejected_sequence_tokens['labels'] = rejected_sequence_tokens[
-                'input_ids'][:]
-            _paddings = [self.label_pad_token_id] * len(
-                rejected_tokens['prompt_input_ids'])
-            rejected_sequence_tokens['labels'][:len(
-                rejected_tokens['prompt_input_ids'])] = _paddings
+            chosen_sequence_tokens['labels'] = chosen_sequence_tokens['input_ids'][:]
+            _paddings = [self.label_pad_token_id] * len(chosen_tokens['prompt_input_ids'])
+            chosen_sequence_tokens['labels'][:len(chosen_tokens['prompt_input_ids'])] = _paddings
+            rejected_sequence_tokens['labels'] = rejected_sequence_tokens['input_ids'][:]
+            _paddings = [self.label_pad_token_id] * len(rejected_tokens['prompt_input_ids'])
+            rejected_sequence_tokens['labels'][:len(rejected_tokens['prompt_input_ids'])] = _paddings
 
             for k, toks in {
                     'chosen_': chosen_sequence_tokens,
                     'rejected_': rejected_sequence_tokens,
                     '': prompt_tokens,
             }.items():
                 for type_key, tokens in toks.items():
@@ -216,18 +165,15 @@
         if isinstance(llm_dataset, HfDataset):
             chosen = llm_dataset['chosen_input_ids']
             rejected = llm_dataset['rejected_input_ids']
             for cc, rr in zip(chosen, rejected):
                 _token_len.append(max(len(cc), len(rr)))
         else:
             for d in llm_dataset:
-                _token_len.append(
-                    max(
-                        len(d['chosen_input_ids']),
-                        len(d['rejected_input_ids'])))
+                _token_len.append(max(len(d['chosen_input_ids']), len(d['rejected_input_ids'])))
         _, stat_str = stat_array(_token_len)
         logger.info(f'Dataset Token Length: {stat_str}')
         return stat_str
 
     def get_batch_loss_metrics(
         self,
         model,
@@ -273,68 +219,56 @@
             policy_chosen_logps,
             policy_rejected_logps,
             reference_chosen_logps,
             reference_rejected_logps,
         )
 
         if self.sft_beta > 0.:
-            chosen_labels = concatenated_batch[
-                'concatenated_labels'][:batch['chosen_labels'].shape[0]]
-            sft_loss = -self.get_batch_logps(
-                policy_chosen_logits, chosen_labels, average_log_prob=True)
+            chosen_labels = concatenated_batch['concatenated_labels'][:batch['chosen_labels'].shape[0]]
+            sft_loss = -self.get_batch_logps(policy_chosen_logits, chosen_labels, average_log_prob=True)
             if losses.shape[0] == 2 * sft_loss.shape[0]:
                 sft_loss = sft_loss.repeat(2, *sft_loss.shape[1:])
             losses = (1 - self.sft_beta) * losses + self.sft_beta * sft_loss
 
         reward_accuracies = (chosen_rewards > rejected_rewards).float()
 
         prefix = 'eval_' if train_eval == 'eval' else ''
         metrics[f'{prefix}rewards/chosen'] = chosen_rewards.mean().cpu()
         metrics[f'{prefix}rewards/rejected'] = rejected_rewards.mean().cpu()
         metrics[f'{prefix}rewards/accuracies'] = reward_accuracies.mean().cpu()
-        metrics[f'{prefix}rewards/margins'] = (
-            chosen_rewards - rejected_rewards).mean().cpu()
-        metrics[f'{prefix}logps/rejected'] = policy_rejected_logps.detach(
-        ).mean().cpu()
-        metrics[f'{prefix}logps/chosen'] = policy_chosen_logps.detach().mean(
-        ).cpu()
-        metrics[
-            f'{prefix}logps/ref_rejected'] = reference_rejected_logps.detach(  # noqa
-            ).mean().cpu()  # noqa
-        metrics[f'{prefix}logps/ref_chosen'] = reference_chosen_logps.detach(
-        ).mean().cpu()
-        metrics[f'{prefix}logits/rejected'] = policy_rejected_logits.detach(
-        ).mean().cpu()
-        metrics[f'{prefix}logits/chosen'] = policy_chosen_logits.detach().mean(
-        ).cpu()
+        metrics[f'{prefix}rewards/margins'] = (chosen_rewards - rejected_rewards).mean().cpu()
+        metrics[f'{prefix}logps/rejected'] = policy_rejected_logps.detach().mean().cpu()
+        metrics[f'{prefix}logps/chosen'] = policy_chosen_logps.detach().mean().cpu()
+        metrics[f'{prefix}logps/ref_rejected'] = reference_rejected_logps.detach(  # noqa
+        ).mean().cpu()  # noqa
+        metrics[f'{prefix}logps/ref_chosen'] = reference_chosen_logps.detach().mean().cpu()
+        metrics[f'{prefix}logits/rejected'] = policy_rejected_logits.detach().mean().cpu()
+        metrics[f'{prefix}logits/chosen'] = policy_chosen_logits.detach().mean().cpu()
 
         return losses.mean(), metrics
 
     def concatenated_forward(
         self, model: nn.Module, batch: Dict[str, Union[List, torch.LongTensor]]
-    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor,
-               torch.FloatTensor, Dict[str, torch.LongTensor]]:
+    ) -> Tuple[torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, torch.FloatTensor, Dict[str, torch.LongTensor]]:
         """Run the given model on the given batch of inputs, concatenating the chosen and rejected inputs together.
 
         We do this to avoid doing two forward passes, because it's faster for FSDP.
         """
         concatenated_batch = self.concatenated_inputs(
             batch,
             is_encoder_decoder=self.is_encoder_decoder,
             label_pad_token_id=self.label_pad_token_id,
             padding_value=self.padding_value,
             device=self.accelerator.device,
         )
         len_chosen = batch['chosen_labels'].shape[0]
 
         model_kwargs = ({
-            'labels':
-            concatenated_batch['concatenated_labels'],
-            'decoder_input_ids':
-            concatenated_batch.pop('concatenated_decoder_input_ids', None),
+            'labels': concatenated_batch['concatenated_labels'],
+            'decoder_input_ids': concatenated_batch.pop('concatenated_decoder_input_ids', None),
         } if self.is_encoder_decoder else {})
         all_logits = model(
             concatenated_batch['concatenated_input_ids'],
             attention_mask=concatenated_batch['concatenated_attention_mask'],
             **model_kwargs,
         ).logits
 
@@ -348,15 +282,14 @@
 
         chosen_logps = all_logps[:len_chosen]
         rejected_logps = all_logps[len_chosen:]
 
         chosen_logits = all_logits[:len_chosen]
         rejected_logits = all_logits[len_chosen:]
 
-        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits,
-                concatenated_batch)
+        return (chosen_logps, rejected_logps, chosen_logits, rejected_logits, concatenated_batch)
 
 
 # monkey patching
 trainer.DEFAULT_PROGRESS_CALLBACK = ProgressCallbackNew
 trainer.DEFAULT_CALLBACKS = [DefaultFlowCallbackNew]
 trainer.PrinterCallback = PrinterCallbackNew
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/mixin.py` & `ms-swift-2.0.4/swift/trainers/mixin.py`

 * *Files 7% similar despite different names*

```diff
@@ -17,58 +17,46 @@
 from packaging import version
 from peft import PeftModel
 from torch.nn import Module
 from transformers import PreTrainedModel, PreTrainedTokenizerBase
 from transformers.data.data_collator import DataCollator
 from transformers.modeling_utils import unwrap_model
 from transformers.trainer import ADAPTER_CONFIG_NAME  # noqa
-from transformers.trainer import (ADAPTER_SAFE_WEIGHTS_NAME,
-                                  ADAPTER_WEIGHTS_NAME, CONFIG_NAME,
-                                  PREFIX_CHECKPOINT_DIR, SAFE_WEIGHTS_NAME,
-                                  TRAINER_STATE_NAME, TRAINING_ARGS_NAME,
-                                  WEIGHTS_NAME, IntervalStrategy, Trainer,
-                                  TrainerCallback, is_peft_available)
+from transformers.trainer import (ADAPTER_SAFE_WEIGHTS_NAME, ADAPTER_WEIGHTS_NAME, CONFIG_NAME, PREFIX_CHECKPOINT_DIR,
+                                  SAFE_WEIGHTS_NAME, TRAINER_STATE_NAME, TRAINING_ARGS_NAME, WEIGHTS_NAME,
+                                  IntervalStrategy, Trainer, TrainerCallback, is_peft_available)
 from transformers.trainer_utils import EvalPrediction
 from transformers.training_args import TrainingArguments
 
 from swift.hub import Repository
 from swift.hub.check_model import check_local_model_is_latest
-from swift.torchacc_utils import (save_ta_checkpoint,
-                                  ta_load_optimizer_and_scheduler,
-                                  ta_save_optimizer_and_scheduler)
+from swift.torchacc_utils import save_ta_checkpoint, ta_load_optimizer_and_scheduler, ta_save_optimizer_and_scheduler
 from swift.tuners import SwiftModel
-from swift.utils import (check_json_format, create_ms_repo, get_logger,
-                         use_torchacc)
+from swift.utils import check_json_format, create_ms_repo, get_logger, use_torchacc
 from swift.utils.constants import Invoke
 from .optimizers.galore import create_optimizer_and_scheduler
-from .utils import (can_return_loss, find_labels, get_function,
-                    is_instance_of_ms_model)
+from .utils import can_return_loss, find_labels, get_function, is_instance_of_ms_model
 
 logger = get_logger()
 
 
-def _push_to_hub(self: Repository,
-                 commit_message: str = 'Commit files to Modelscope Hub',
-                 **kwargs):
+def _push_to_hub(self: Repository, commit_message: str = 'Commit files to Modelscope Hub', **kwargs):
     blocking = kwargs.get('blocking', True)
     self.push(commit_message)
     if not blocking:
         # Compatible with transformers
         return None, None
     else:
         return None
 
 
 class PushToMsHubMixin:
     repo: Repository
 
-    def _add_patterns_to_file(self,
-                              file_name: str,
-                              patterns: List[str],
-                              commit_message: Optional[str] = None) -> None:
+    def _add_patterns_to_file(self, file_name: str, patterns: List[str], commit_message: Optional[str] = None) -> None:
         # Make sure we only do this on the main process
         if not self.is_world_process_zero():
             return
         if isinstance(patterns, str):
             patterns = [patterns]
         if commit_message is None:
             commit_message = f'Add `{patterns[0]}` patterns to {file_name}'
@@ -92,24 +80,18 @@
         # Write the file if it has changed
         if content != current_content:
             with open(file_path, 'w', encoding='utf-8') as f:
                 logger.debug(f'Writing {file_name} file. Content: {content}')
                 f.write(content)
         self.repo.push(commit_message)
 
-    def _add_patterns_to_gitignore(
-            self,
-            patterns: List[str],
-            commit_message: Optional[str] = None) -> None:
+    def _add_patterns_to_gitignore(self, patterns: List[str], commit_message: Optional[str] = None) -> None:
         self._add_patterns_to_file('.gitignore', patterns, commit_message)
 
-    def _add_patterns_to_gitattributes(
-            self,
-            patterns: List[str],
-            commit_message: Optional[str] = None) -> None:
+    def _add_patterns_to_gitattributes(self, patterns: List[str], commit_message: Optional[str] = None) -> None:
         new_patterns = []
         suffix = 'filter=lfs diff=lfs merge=lfs -text'
         for pattern in patterns:
             if suffix not in pattern:
                 pattern = f'{pattern} {suffix}'
             new_patterns.append(pattern)
         file_name = '.gitattributes'
@@ -120,57 +102,50 @@
     def init_hf_repo(self) -> None:
         """init ms repo. Compatible with transformers>=4.34"""
         self.init_git_repo(at_init=True)
 
     def init_git_repo(self, at_init: bool = False) -> None:
         if not self.is_world_process_zero():
             return
-        if (os.path.exists(self.args.output_dir)
-                and os.listdir(self.args.output_dir)
-                and self.args.overwrite_output_dir and at_init):
+        if (os.path.exists(self.args.output_dir) and os.listdir(self.args.output_dir) and self.args.overwrite_output_dir
+                and at_init):
             # directory not empty.
             shutil.rmtree(self.args.output_dir)
-        self.args.hub_model_id = create_ms_repo(self.args.hub_model_id,
-                                                self.args.hub_token,
-                                                self.args.hub_private_repo)
+        self.args.hub_model_id = create_ms_repo(self.args.hub_model_id, self.args.hub_token, self.args.hub_private_repo)
         self.repo = Repository(self.args.output_dir, self.args.hub_model_id)
         self._add_patterns_to_gitattributes(['*.safetensors', '*.bin', '*.pt'])
         self.repo.push_to_hub = MethodType(_push_to_hub, self.repo)
         self.repo.local_dir = self.repo.model_dir  # hf compatibility
 
         # By default, ignore the checkpoint folders
         if self.args.push_hub_strategy != 'all_checkpoints':
-            self._add_patterns_to_gitignore(
-                ['checkpoint-*/', 'tmp-checkpoint-*/'])
+            self._add_patterns_to_gitignore(['checkpoint-*/', 'tmp-checkpoint-*/'])
 
         # Add 'runs/' to .gitignore, ignore tensorboard files
         self._add_patterns_to_gitignore(['runs/'])
 
         # Add '*.sagemaker' to .gitignore if using SageMaker
         if os.environ.get('SM_TRAINING_ENV'):
-            self._add_patterns_to_gitignore(
-                ['*.sagemaker-uploading', '*.sagemaker-uploaded'],
-                'Add `*.sagemaker` patterns to .gitignore')
+            self._add_patterns_to_gitignore(['*.sagemaker-uploading', '*.sagemaker-uploaded'],
+                                            'Add `*.sagemaker` patterns to .gitignore')
 
         self.push_in_progress = None
 
-    def push_to_hub(self,
-                    commit_message: str = 'End of training',
-                    **kwargs) -> None:
+    def push_to_hub(self, commit_message: str = 'End of training', **kwargs) -> None:
         # user calls manually `push_to_hub` with `self.args.push_to_hub = False`
         create_model_card = kwargs.pop('create_model_card', None)
         if not hasattr(self, 'repo'):
             self.init_git_repo()
         self.save_model(_internal_call=True)
 
         if not self.is_world_process_zero():
             return
 
         self.repo.push_to_hub(commit_message, **kwargs)
-        # push separately the model card to be independant from the rest of the model
+        # push separately the model card to be independent from the rest of the model
         readme_path = os.path.join(self.args.output_dir, 'README.md')
         if create_model_card is None:
             create_model_card = not os.path.exists(readme_path)
         if create_model_card and self.args.should_save:
             model_name = kwargs.pop('model_name', None)
             if model_name is None and self.args.should_save:
                 if self.args.hub_model_id is not None:
@@ -179,30 +154,24 @@
                     model_name = os.path.basename(self.args.output_dir)
             self.create_model_card(model_name=model_name, **kwargs)
             self.repo.push_to_hub('update model card README.md', **kwargs)
 
     def _push_from_checkpoint(self, checkpoint_folder: str) -> None:
         """Compatible with transformers>=4.32"""
         # Only push from one node.
-        if not self.is_world_process_zero(
-        ) or self.args.push_hub_strategy == 'end':
+        if not self.is_world_process_zero() or self.args.push_hub_strategy == 'end':
             return
         output_dir = self.args.output_dir
         # To avoid a new synchronization of all model weights, we just copy the file from the checkpoint folder
         modeling_files = [CONFIG_NAME, WEIGHTS_NAME, SAFE_WEIGHTS_NAME]
         if is_peft_available():
-            modeling_files.extend([
-                ADAPTER_CONFIG_NAME, ADAPTER_WEIGHTS_NAME,
-                ADAPTER_SAFE_WEIGHTS_NAME
-            ])
+            modeling_files.extend([ADAPTER_CONFIG_NAME, ADAPTER_WEIGHTS_NAME, ADAPTER_SAFE_WEIGHTS_NAME])
         for modeling_file in modeling_files:
             if os.path.isfile(os.path.join(checkpoint_folder, modeling_file)):
-                shutil.copy(
-                    os.path.join(checkpoint_folder, modeling_file),
-                    os.path.join(output_dir, modeling_file))
+                shutil.copy(os.path.join(checkpoint_folder, modeling_file), os.path.join(output_dir, modeling_file))
         # Saving the tokenizer is fast and we don't know how many files it may have spawned, so we resave it to be sure.
         if self.tokenizer is not None:
             self.tokenizer.save_pretrained(output_dir)
         # Same for the training arguments
         torch.save(self.args, os.path.join(output_dir, TRAINING_ARGS_NAME))
 
         try:
@@ -217,27 +186,20 @@
 
             if self.args.save_strategy == IntervalStrategy.STEPS:
                 commit_message = f'Training in progress, step {self.state.global_step}'
             else:
                 commit_message = f'Training in progress, epoch {int(self.state.epoch)}'
             if self.args.push_hub_strategy == 'push_best':
                 folder, checkpoint_name = os.path.split(checkpoint_folder)
-                checkpoint_name = checkpoint_name.replace(
-                    'tmp-checkpoint-', 'checkpoint-')
+                checkpoint_name = checkpoint_name.replace('tmp-checkpoint-', 'checkpoint-')
                 last_model_checkpoint = os.path.join(folder, checkpoint_name)
                 if last_model_checkpoint == self.state.best_model_checkpoint:
-                    self.repo.push_to_hub(
-                        commit_message=commit_message,
-                        blocking=False,
-                        auto_lfs_prune=True)
+                    self.repo.push_to_hub(commit_message=commit_message, blocking=False, auto_lfs_prune=True)
             else:
-                self.repo.push_to_hub(
-                    commit_message=commit_message,
-                    blocking=False,
-                    auto_lfs_prune=True)
+                self.repo.push_to_hub(commit_message=commit_message, blocking=False, auto_lfs_prune=True)
         except Exception as e:
             logger.error(f'Error when pushing to hub: {e}')
         finally:
             if self.args.push_hub_strategy == 'checkpoint':
                 # Move back the checkpoint to its place
                 shutil.move(tmp_checkpoint, checkpoint_folder)
 
@@ -245,43 +207,35 @@
 class SwiftMixin:
 
     def __init__(self,
                  model: Union[PreTrainedModel, Module] = None,
                  args: TrainingArguments = None,
                  data_collator: Optional[DataCollator] = None,
                  train_dataset: Optional[HfDataset] = None,
-                 eval_dataset: Optional[Union[HfDataset,
-                                              Dict[str, HfDataset]]] = None,
+                 eval_dataset: Optional[Union[HfDataset, Dict[str, HfDataset]]] = None,
                  tokenizer: Optional[PreTrainedTokenizerBase] = None,
                  model_init: Optional[Callable[[], PreTrainedModel]] = None,
-                 compute_metrics: Optional[Callable[[EvalPrediction],
-                                                    Dict]] = None,
+                 compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,
                  callbacks: Optional[List[TrainerCallback]] = None,
-                 optimizers: Tuple[torch.optim.Optimizer,
-                                   torch.optim.lr_scheduler.LambdaLR] = (None,
-                                                                         None),
-                 preprocess_logits_for_metrics: Optional[Callable[
-                     [torch.Tensor, torch.Tensor], torch.Tensor]] = None,
+                 optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),
+                 preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,
                  **kwargs) -> None:
         check_model = kwargs.pop('check_model', True)
         if check_model and hasattr(model, 'model_dir'):
             check_local_model_is_latest(
                 model.model_dir,
                 user_agent={
-                    Invoke.KEY:
-                    Invoke.LOCAL_TRAINER,
-                    Invoke.THIRD_PARTY:
-                    kwargs.pop(Invoke.THIRD_PARTY, Invoke.SWIFT),
+                    Invoke.KEY: Invoke.LOCAL_TRAINER,
+                    Invoke.THIRD_PARTY: kwargs.pop(Invoke.THIRD_PARTY, Invoke.SWIFT),
                 })
 
         # Compatible with transformers>=4.34
         from swift.tuners import SwiftModel
         is_quantized = getattr(model, 'is_quantized', False)
-        _hf_peft_config_loaded = getattr(model, '_hf_peft_config_loaded',
-                                         False)
+        _hf_peft_config_loaded = getattr(model, '_hf_peft_config_loaded', False)
         use_swift = isinstance(model, SwiftModel)
         if is_quantized and use_swift:
             model._hf_peft_config_loaded = True
         # mro
         super().__init__(
             model=model,
             args=args,
@@ -294,16 +248,15 @@
             callbacks=callbacks,
             optimizers=optimizers,
             preprocess_logits_for_metrics=preprocess_logits_for_metrics,
             **kwargs)
         if is_quantized and use_swift:
             model._hf_peft_config_loaded = _hf_peft_config_loaded
 
-        if get_function(model.__class__.forward) is not get_function(
-                model.forward):
+        if get_function(model.__class__.forward) is not get_function(model.forward):
             self.label_names = find_labels(model)
             self.can_return_loss = can_return_loss(model)
 
     @staticmethod
     def _create_configuration_file(model: Module, output_dir: str) -> None:
         cfg = getattr(model, 'cfg', {})
         configuration_path = os.path.join(output_dir, 'configuration.json')
@@ -328,59 +281,52 @@
         configuration_path = os.path.join(output_dir, 'configuration.json')
         new_cfg = {}
         if os.path.exists(configuration_path):
             with open(configuration_path, 'r', encoding='utf-8') as f:
                 new_cfg = json.load(f)
 
         need_to_save = [
-            'model_id_or_path', 'model_revision', 'sft_type', 'tuner_backend',
-            'template_type', 'dtype', 'system'
+            'model_id_or_path', 'model_revision', 'sft_type', 'tuner_backend', 'template_type', 'dtype', 'system'
         ]
         quantization_bit = sft_args.quantization_bit
         if quantization_bit > 0:
             need_to_save += [
-                'quantization_bit', 'bnb_4bit_comp_dtype',
-                'bnb_4bit_quant_type', 'bnb_4bit_use_double_quant'
+                'quantization_bit', 'bnb_4bit_comp_dtype', 'bnb_4bit_quant_type', 'bnb_4bit_use_double_quant'
             ]
         adapter_cfg = {}
         for k in need_to_save:
             adapter_cfg[k] = getattr(sft_args, k)
         new_cfg['adapter_cfg'] = adapter_cfg
         with open(configuration_path, 'w', encoding='utf-8') as f:
             json.dump(new_cfg, f, ensure_ascii=False, indent=4)
 
     def _save_sft_args(self, output_dir: str) -> None:
         sft_args = getattr(self, 'sft_args', None)
         if sft_args is None:
             return
         fpath = os.path.join(output_dir, 'sft_args.json')
         with open(fpath, 'w', encoding='utf-8') as f:
-            json.dump(
-                check_json_format(self.sft_args.__dict__),
-                f,
-                ensure_ascii=False,
-                indent=2)
+            json.dump(check_json_format(self.sft_args.__dict__), f, ensure_ascii=False, indent=2)
         return
 
     def _save_optimizer_and_scheduler(self, output_dir):
         if not use_torchacc():
             return super()._save_optimizer_and_scheduler(output_dir)
 
-        ta_save_optimizer_and_scheduler(self.optimizer, self.lr_scheduler,
-                                        output_dir)
+        ta_save_optimizer_and_scheduler(self.optimizer, self.lr_scheduler, output_dir)
 
     def _load_optimizer_and_scheduler(self, checkpoint):
         if not use_torchacc():
             return super()._load_optimizer_and_scheduler(checkpoint)
 
         if checkpoint is None or self.args.save_only_model:
             return
 
-        self.optimizer, self.lr_scheduler = ta_load_optimizer_and_scheduler(
-            self.optimizer, self.lr_scheduler, checkpoint, self.args.device)
+        self.optimizer, self.lr_scheduler = ta_load_optimizer_and_scheduler(self.optimizer, self.lr_scheduler,
+                                                                            checkpoint, self.args.device)
 
     def _save_tpu(self, output_dir: Optional[str] = None):
         if not use_torchacc():
             return super()._save_tpu(output_dir)
 
         output_dir = output_dir if output_dir is not None else self.args.output_dir
         logger.info(f'Saving model checkpoint to {output_dir}')
@@ -411,66 +357,48 @@
         save_safetensors = self.args.save_safetensors
         if not isinstance(self.model, supported_classes):
             if state_dict is None:
                 state_dict = self.model.state_dict()
 
             _unwrap_model = unwrap_model(self.model)
             if isinstance(_unwrap_model, supported_classes):
-                _unwrap_model.save_pretrained(
-                    output_dir,
-                    state_dict=state_dict,
-                    safe_serialization=save_safetensors)
+                _unwrap_model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=save_safetensors)
             else:
-                logger.info(
-                    'Trainer.model is not a `PreTrainedModel`, only saving its state dict.'
-                )
+                logger.info('Trainer.model is not a `PreTrainedModel`, only saving its state dict.')
                 if save_safetensors:
-                    safetensors.torch.save_file(
-                        state_dict,
-                        os.path.join(output_dir, 'model.safetensors'))
+                    safetensors.torch.save_file(state_dict, os.path.join(output_dir, 'model.safetensors'))
                 else:
-                    torch.save(state_dict,
-                               os.path.join(output_dir, 'pytorch_model.bin'))
+                    torch.save(state_dict, os.path.join(output_dir, 'pytorch_model.bin'))
         elif is_instance_of_ms_model(self.model):
             PreTrainedModel.save_pretrained(
-                self.model,
-                output_dir,
-                state_dict=state_dict,
-                safe_serialization=save_safetensors)
+                self.model, output_dir, state_dict=state_dict, safe_serialization=save_safetensors)
         else:
-            self.model.save_pretrained(
-                output_dir,
-                state_dict=state_dict,
-                safe_serialization=save_safetensors)
+            self.model.save_pretrained(output_dir, state_dict=state_dict, safe_serialization=save_safetensors)
         # tokenizer
         if self.tokenizer is not None:
             self.tokenizer.save_pretrained(output_dir)
         # training_args.bin
         torch.save(self.args, os.path.join(output_dir, 'training_args.bin'))
         # additional files
         sft_args = getattr(self, 'sft_args', None)
         if sft_args is not None and sft_args.sft_type == 'full':
-            additional_files = getattr(self.args, 'additional_saved_files',
-                                       []) + ['preprocessor_config.json']
+            additional_files = getattr(self.args, 'additional_saved_files', []) + ['preprocessor_config.json']
             if model_dir is not None:
                 for file in additional_files:
                     src_path = os.path.join(model_dir, file)
                     dst_path = os.path.join(output_dir, file)
                     if os.path.isfile(src_path):
                         shutil.copy(src_path, dst_path)
                     elif os.path.isdir(src_path):
                         shutil.copytree(src_path, dst_path)
 
     def _save_checkpoint(self, model, trial, metrics=None):
-        self.state.last_model_checkpoint = os.path.join(
-            self.args.output_dir, f'checkpoint-{self.state.global_step}')
-        logger.info(
-            f'Saving model checkpoint to {self.state.last_model_checkpoint}')
-        if version.parse(transformers.__version__) >= version.parse(
-                '4.36') or not self.args.save_only_model:
+        self.state.last_model_checkpoint = os.path.join(self.args.output_dir, f'checkpoint-{self.state.global_step}')
+        logger.info(f'Saving model checkpoint to {self.state.last_model_checkpoint}')
+        if version.parse(transformers.__version__) >= version.parse('4.36') or not self.args.save_only_model:
             return super()._save_checkpoint(model, trial, metrics)
         else:
             return self._save_only_model(model, trial, metrics)
 
     def _save_only_model(self, model, trial, metrics=None):
         # Save model checkpoint
         checkpoint_folder = f'{PREFIX_CHECKPOINT_DIR}-{self.state.global_step}'
@@ -486,24 +414,22 @@
         if metrics is not None and self.args.metric_for_best_model is not None:
             metric_to_check = self.args.metric_for_best_model
             if not metric_to_check.startswith('eval_'):
                 metric_to_check = f'eval_{metric_to_check}'
             metric_value = metrics[metric_to_check]
 
             operator = np.greater if self.args.greater_is_better else np.less
-            if (self.state.best_metric is None
-                    or self.state.best_model_checkpoint is None
+            if (self.state.best_metric is None or self.state.best_model_checkpoint is None
                     or operator(metric_value, self.state.best_metric)):
                 self.state.best_metric = metric_value
                 self.state.best_model_checkpoint = output_dir
 
         # Save the Trainer state
         if self.args.should_save:
-            self.state.save_to_json(
-                os.path.join(output_dir, TRAINER_STATE_NAME))
+            self.state.save_to_json(os.path.join(output_dir, TRAINER_STATE_NAME))
 
         # push to hub
         if self.args.push_to_hub:
             self._push_from_checkpoint(output_dir)
 
         # Maybe delete some older checkpoints.
         if self.args.should_save:
@@ -512,90 +438,69 @@
     def _get_train_sampler(self) -> Optional[torch.utils.data.Sampler]:
         train_sampler_random = self.args.train_sampler_random
         if train_sampler_random:
             return super()._get_train_sampler()
         else:
             return self._get_eval_sampler(self.train_dataset)
 
-    def _load_from_checkpoint(self,
-                              resume_from_checkpoint: str,
-                              model=None) -> None:
+    def _load_from_checkpoint(self, resume_from_checkpoint: str, model=None) -> None:
         if model is None:
             model = self.model
-        supported_classes = (SwiftModel, PeftModel)
         if use_torchacc():
             # Loading checkpoint of TorchAcc has been done in tuner.py when
             # sft_type if 'full'.
             model = model._get_underlay_model().module.module
             if isinstance(model, PreTrainedModel):
                 return
-        if not isinstance(model, supported_classes):
+        elif not isinstance(model, SwiftModel):
             # Avoid throwing exceptions
             return super()._load_from_checkpoint(resume_from_checkpoint, model)
 
     def _sorted_checkpoints(self,
                             output_dir=None,
                             checkpoint_prefix=PREFIX_CHECKPOINT_DIR,
                             use_mtime=False) -> List[str]:
         ordering_and_checkpoint_path = []
 
-        glob_checkpoints = [
-            str(x) for x in Path(output_dir).glob(f'{checkpoint_prefix}-*')
-            if os.path.isdir(x)
-        ]
+        glob_checkpoints = [str(x) for x in Path(output_dir).glob(f'{checkpoint_prefix}-*') if os.path.isdir(x)]
 
         for path in glob_checkpoints:
             if use_mtime:
-                ordering_and_checkpoint_path.append(
-                    (os.path.getmtime(path), path))
+                ordering_and_checkpoint_path.append((os.path.getmtime(path), path))
             else:
                 regex_match = re.match(f'.*{checkpoint_prefix}-([0-9]+)', path)
-                if regex_match is not None and regex_match.groups(
-                ) is not None:
-                    ordering_and_checkpoint_path.append(
-                        (int(regex_match.groups()[0]), path))
+                if regex_match is not None and regex_match.groups() is not None:
+                    ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))
 
         checkpoints_sorted = sorted(ordering_and_checkpoint_path)
-        checkpoints_sorted = [
-            checkpoint[1] for checkpoint in checkpoints_sorted
-        ]
+        checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]
         # Make sure we don't delete the best model.
-        if (self.state.best_model_checkpoint is not None and str(
-                Path(self.state.best_model_checkpoint)) in checkpoints_sorted):
-            best_model_index = checkpoints_sorted.index(
-                str(Path(self.state.best_model_checkpoint)))
+        if (self.state.best_model_checkpoint is not None
+                and str(Path(self.state.best_model_checkpoint)) in checkpoints_sorted):
+            best_model_index = checkpoints_sorted.index(str(Path(self.state.best_model_checkpoint)))
             for i in range(best_model_index, len(checkpoints_sorted) - 2):
-                checkpoints_sorted[i], checkpoints_sorted[
-                    i + 1] = checkpoints_sorted[i + 1], checkpoints_sorted[i]
+                checkpoints_sorted[i], checkpoints_sorted[i + 1] = checkpoints_sorted[i + 1], checkpoints_sorted[i]
         return checkpoints_sorted
 
     def _load_best_model(self):
         # Compatible with transformers>=4.35 (deepspeed)
         try:
             model = self.model
             if isinstance(model, SwiftModel):
                 logger.info(
-                    f'Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).'
-                )
+                    f'Loading best model from {self.state.best_model_checkpoint} (score: {self.state.best_metric}).')
                 adapters = model.adapters
                 for adapter_name in adapters.keys():
-                    sub_folder = os.path.join(self.state.best_model_checkpoint,
-                                              adapter_name)
-                    state_dict = SwiftModel.load_state_file(
-                        sub_folder, device='cpu')
+                    sub_folder = os.path.join(self.state.best_model_checkpoint, adapter_name)
+                    state_dict = SwiftModel.load_state_file(sub_folder, device='cpu')
                     if state_dict is not None:
-                        self.model.load_state_dict(
-                            state_dict,
-                            strict=False,
-                            adapter_name=adapter_name)
-                state_dict = SwiftModel.load_state_file(
-                    self.state.best_model_checkpoint, device='cpu')
+                        self.model.load_state_dict(state_dict, strict=False, adapter_name=adapter_name)
+                state_dict = SwiftModel.load_state_file(self.state.best_model_checkpoint, device='cpu')
                 if state_dict is not None:
-                    self.model.load_state_dict(
-                        state_dict, strict=False, adapter_name='default')
+                    self.model.load_state_dict(state_dict, strict=False, adapter_name='default')
             else:
                 super()._load_best_model()
         except ValueError as e:
             logger.warning(e)
 
     def _maybe_log_save_evaluate(self, tr_loss, *args, **kwargs):
         if self.control.should_log:
@@ -606,19 +511,16 @@
                 metrics_log.update(self._custom_metrics)
                 self._custom_metrics = {}
             for k, v in metrics_log.items():
                 # all_gather + mean() to get average loss over all processes
                 v_scalar = self._nested_gather(v).mean().item()
                 if k == 'loss':
                     self._total_loss_scalar += v_scalar
-                logs[k] = round(
-                    v_scalar /
-                    (self.state.global_step - self._globalstep_last_logged), 8)
-            if version.parse(
-                    transformers.__version__) >= version.parse('4.38'):
+                logs[k] = round(v_scalar / (self.state.global_step - self._globalstep_last_logged), 8)
+            if version.parse(transformers.__version__) >= version.parse('4.38'):
                 grad_norm = args[0]
                 if isinstance(grad_norm, torch.Tensor):
                     grad_norm = grad_norm.item()
                 if grad_norm is not None:
                     logs['grad_norm'] = grad_norm
             logs['learning_rate'] = self._get_learning_rate()
 
@@ -637,56 +539,45 @@
                 num_training_steps,
                 lr=self.args.learning_rate,
                 weight_decay=self.args.weight_decay)
             self.optimizer = optimizer
             self.lr_scheduler = lr_scheduler
         else:
             self.create_optimizer()
-            self.create_scheduler(
-                num_training_steps=num_training_steps,
-                optimizer=self.optimizer)
+            self.create_scheduler(num_training_steps=num_training_steps, optimizer=self.optimizer)
 
     def create_optimizer(self):
         opt_model = self.model
 
         if self.optimizer is None:
-            if version.parse(
-                    transformers.__version__) < version.parse('4.34.0'):
-                logger.warning(
-                    f'If you are using lora+, please remember using transformers>=4.34.0, '
-                    f'but now is {transformers.__version__}')
+            if version.parse(transformers.__version__) < version.parse('4.34.0'):
+                logger.warning(f'If you are using lora+, please remember using transformers>=4.34.0, '
+                               f'but now is {transformers.__version__}')
                 return super().create_optimizer()
 
             optimizer_grouped_parameters = None
             if hasattr(self.model, 'create_optimizer_param_groups'):
                 # Lora+ parameter groups
                 optimizer_grouped_parameters = self.model.create_optimizer_param_groups(
-                    lr=self.args.learning_rate,
-                    weight_decay=self.args.weight_decay)
+                    lr=self.args.learning_rate, weight_decay=self.args.weight_decay)
 
             if optimizer_grouped_parameters is None:
                 # Default parameter groups
                 decay_parameters = self.get_decay_parameter_names(opt_model)
                 optimizer_grouped_parameters = [
                     {
-                        'params': [
-                            p for n, p in opt_model.named_parameters()
-                            if (n in decay_parameters and p.requires_grad)
-                        ],
+                        'params':
+                        [p for n, p in opt_model.named_parameters() if (n in decay_parameters and p.requires_grad)],
                         'weight_decay':
                         self.args.weight_decay,
                     },
                     {
-                        'params': [
-                            p for n, p in opt_model.named_parameters()
-                            if (n not in decay_parameters and p.requires_grad)
-                        ],
+                        'params':
+                        [p for n, p in opt_model.named_parameters() if (n not in decay_parameters and p.requires_grad)],
                         'weight_decay':
                         0.0,
                     },
                 ]
 
-            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(
-                self.args)
-            self.optimizer = optimizer_cls(optimizer_grouped_parameters,
-                                           **optimizer_kwargs)
+            optimizer_cls, optimizer_kwargs = Trainer.get_optimizer_cls_and_kwargs(self.args)
+            self.optimizer = optimizer_cls(optimizer_grouped_parameters, **optimizer_kwargs)
         return self.optimizer
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/__init__.py` & `ms-swift-2.0.4/swift/trainers/optimizers/galore/__init__.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/adafactor.py` & `ms-swift-2.0.4/swift/trainers/optimizers/galore/adafactor.py`

 * *Files 3% similar despite different names*

```diff
@@ -105,19 +105,17 @@
         weight_decay=0.0,
         scale_parameter=True,
         relative_step=True,
         warmup_init=False,
     ):
         require_version('torch>=1.5.0')  # add_ with alpha
         if lr is not None and relative_step:
-            raise ValueError(
-                'Cannot combine manual `lr` and `relative_step=True` options')
+            raise ValueError('Cannot combine manual `lr` and `relative_step=True` options')
         if warmup_init and not relative_step:
-            raise ValueError(
-                '`warmup_init=True` requires `relative_step=True`')
+            raise ValueError('`warmup_init=True` requires `relative_step=True`')
 
         defaults = {
             'lr': lr,
             'eps': eps,
             'clip_threshold': clip_threshold,
             'decay_rate': decay_rate,
             'beta1': beta1,
@@ -128,16 +126,15 @@
         }
         super().__init__(params, defaults)
 
     @staticmethod
     def _get_lr(param_group, param_state):
         rel_step_sz = param_group['lr']
         if param_group['relative_step']:
-            min_step = 1e-6 * param_state['step'] if param_group[
-                'warmup_init'] else 1e-2
+            min_step = 1e-6 * param_state['step'] if param_group['warmup_init'] else 1e-2
             rel_step_sz = min(min_step, 1.0 / math.sqrt(param_state['step']))
         param_scale = 1.0
         if param_group['scale_parameter']:
             param_scale = max(param_group['eps'][1], param_state['RMS'])
         return param_scale * rel_step_sz
 
     @staticmethod
@@ -150,17 +147,15 @@
     def _rms(tensor):
         return tensor.norm(2) / (tensor.numel()**0.5)
 
     @staticmethod
     def _approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col):
         # copy from fairseq's adafactor implementation:
         # https://github.com/huggingface/transformers/blob/8395f14de6068012787d83989c3627c3df6a252b/src/transformers/optimization.py#L505
-        r_factor = (
-            exp_avg_sq_row
-            / exp_avg_sq_row.mean(dim=-1, keepdim=True)).rsqrt_().unsqueeze(-1)
+        r_factor = (exp_avg_sq_row / exp_avg_sq_row.mean(dim=-1, keepdim=True)).rsqrt_().unsqueeze(-1)
         c_factor = exp_avg_sq_col.unsqueeze(-2).rsqrt()
         return torch.mul(r_factor, c_factor)
 
     @torch.no_grad()
     def step(self, closure=None):
         """
         Performs a single optimization step
@@ -177,16 +172,15 @@
             for p in group['params']:
                 if p.grad is None:
                     continue
                 grad = p.grad
                 if grad.dtype in {torch.float16, torch.bfloat16}:
                     grad = grad.float()
                 if grad.is_sparse:
-                    raise RuntimeError(
-                        'Adafactor does not support sparse gradients.')
+                    raise RuntimeError('Adafactor does not support sparse gradients.')
 
                 state = self.state[p]
 
                 if 'step' not in state:
                     state['step'] = 0
 
                 # GaLore Projection
@@ -198,40 +192,35 @@
                             scale=group['scale'],
                             proj_type=group['proj_type'])
 
                     grad = state['projector'].project(grad, state['step'])
 
                 grad_shape = grad.shape
 
-                factored, use_first_moment = self._get_options(
-                    group, grad_shape)
+                factored, use_first_moment = self._get_options(group, grad_shape)
                 # State Initialization
                 if 'RMS' not in state:
                     state['step'] = 0
 
                     if use_first_moment:
                         # Exponential moving average of gradient values
                         state['exp_avg'] = torch.zeros_like(grad)
                     if factored:
-                        state['exp_avg_sq_row'] = torch.zeros(
-                            grad_shape[:-1]).to(grad)
-                        state['exp_avg_sq_col'] = torch.zeros(
-                            grad_shape[:-2] + grad_shape[-1:]).to(grad)
+                        state['exp_avg_sq_row'] = torch.zeros(grad_shape[:-1]).to(grad)
+                        state['exp_avg_sq_col'] = torch.zeros(grad_shape[:-2] + grad_shape[-1:]).to(grad)
                     else:
                         state['exp_avg_sq'] = torch.zeros_like(grad)
 
                     state['RMS'] = 0
                 else:
                     if use_first_moment:
                         state['exp_avg'] = state['exp_avg'].to(grad)
                     if factored:
-                        state['exp_avg_sq_row'] = state['exp_avg_sq_row'].to(
-                            grad)
-                        state['exp_avg_sq_col'] = state['exp_avg_sq_col'].to(
-                            grad)
+                        state['exp_avg_sq_row'] = state['exp_avg_sq_row'].to(grad)
+                        state['exp_avg_sq_col'] = state['exp_avg_sq_col'].to(grad)
                     else:
                         state['exp_avg_sq'] = state['exp_avg_sq'].to(grad)
 
                 p_data_fp32 = p
                 if p.dtype in {torch.float16, torch.bfloat16}:
                     p_data_fp32 = p_data_fp32.float()
 
@@ -241,46 +230,40 @@
 
                 beta2t = 1.0 - math.pow(state['step'], group['decay_rate'])
                 update = (grad**2) + group['eps'][0]
                 if factored:
                     exp_avg_sq_row = state['exp_avg_sq_row']
                     exp_avg_sq_col = state['exp_avg_sq_col']
 
-                    exp_avg_sq_row.mul_(beta2t).add_(
-                        update.mean(dim=-1), alpha=(1.0 - beta2t))
-                    exp_avg_sq_col.mul_(beta2t).add_(
-                        update.mean(dim=-2), alpha=(1.0 - beta2t))
+                    exp_avg_sq_row.mul_(beta2t).add_(update.mean(dim=-1), alpha=(1.0 - beta2t))
+                    exp_avg_sq_col.mul_(beta2t).add_(update.mean(dim=-2), alpha=(1.0 - beta2t))
 
                     # Approximation of exponential moving average of square of gradient
-                    update = self._approx_sq_grad(exp_avg_sq_row,
-                                                  exp_avg_sq_col)
+                    update = self._approx_sq_grad(exp_avg_sq_row, exp_avg_sq_col)
                     update.mul_(grad)
                 else:
                     exp_avg_sq = state['exp_avg_sq']
 
                     exp_avg_sq.mul_(beta2t).add_(update, alpha=(1.0 - beta2t))
                     update = exp_avg_sq.rsqrt().mul_(grad)
 
-                update.div_((self._rms(update)
-                             / group['clip_threshold']).clamp_(min=1.0))
+                update.div_((self._rms(update) / group['clip_threshold']).clamp_(min=1.0))
                 update.mul_(lr)
 
                 if use_first_moment:
                     exp_avg = state['exp_avg']
-                    exp_avg.mul_(group['beta1']).add_(
-                        update, alpha=(1 - group['beta1']))
+                    exp_avg.mul_(group['beta1']).add_(update, alpha=(1 - group['beta1']))
                     update = exp_avg
 
                 # GaLore Projection Back
                 if 'rank' in group:
                     update = state['projector'].project_back(update)
 
                 if group['weight_decay'] != 0:
-                    p_data_fp32.add_(
-                        p_data_fp32, alpha=(-group['weight_decay'] * lr))
+                    p_data_fp32.add_(p_data_fp32, alpha=(-group['weight_decay'] * lr))
 
                 p_data_fp32.add_(-update)
 
                 if p.dtype in {torch.float16, torch.bfloat16}:
                     p.copy_(p_data_fp32)
 
         return loss
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/adamw.py` & `ms-swift-2.0.4/swift/trainers/optimizers/galore/adamw.py`

 * *Files 7% similar despite different names*

```diff
@@ -44,31 +44,20 @@
         correct_bias: bool = True,
         no_deprecation_warning: bool = False,
     ):
         require_version('torch>=1.5.0')  # add_ with alpha
         if lr < 0.0:
             raise ValueError(f'Invalid learning rate: {lr} - should be >= 0.0')
         if not 0.0 <= betas[0] < 1.0:
-            raise ValueError(
-                f'Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)'
-            )
+            raise ValueError(f'Invalid beta parameter: {betas[0]} - should be in [0.0, 1.0)')
         if not 0.0 <= betas[1] < 1.0:
-            raise ValueError(
-                f'Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)'
-            )
+            raise ValueError(f'Invalid beta parameter: {betas[1]} - should be in [0.0, 1.0)')
         if not 0.0 <= eps:
-            raise ValueError(
-                f'Invalid epsilon value: {eps} - should be >= 0.0')
-        defaults = {
-            'lr': lr,
-            'betas': betas,
-            'eps': eps,
-            'weight_decay': weight_decay,
-            'correct_bias': correct_bias
-        }
+            raise ValueError(f'Invalid epsilon value: {eps} - should be >= 0.0')
+        defaults = {'lr': lr, 'betas': betas, 'eps': eps, 'weight_decay': weight_decay, 'correct_bias': correct_bias}
         super().__init__(params, defaults)
 
     @torch.no_grad()
     def step(self, closure: Callable = None):
         """
         Performs a single optimization step.
 
@@ -81,17 +70,15 @@
 
         for group in self.param_groups:
             for p in group['params']:
                 if p.grad is None:
                     continue
                 grad = p.grad
                 if grad.is_sparse:
-                    raise RuntimeError(
-                        'Adam does not support sparse gradients, please consider SparseAdam instead'
-                    )
+                    raise RuntimeError('Adam does not support sparse gradients, please consider SparseAdam instead')
 
                 state = self.state[p]
 
                 if 'step' not in state:
                     state['step'] = 0
 
                 # GaLore Projection
@@ -123,16 +110,15 @@
                 exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1.0 - beta2)
                 denom = exp_avg_sq.sqrt().add_(group['eps'])
 
                 step_size = group['lr']
                 if group['correct_bias']:  # No bias correction for Bert
                     bias_correction1 = 1.0 - beta1**state['step']
                     bias_correction2 = 1.0 - beta2**state['step']
-                    step_size = step_size * math.sqrt(
-                        bias_correction2) / bias_correction1
+                    step_size = step_size * math.sqrt(bias_correction2) / bias_correction1
 
                 # compute norm gradient
                 norm_grad = exp_avg / denom
 
                 # GaLore Projection Back
                 if 'rank' in group:
                     norm_grad = state['projector'].project_back(norm_grad)
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/adamw8bit.py` & `ms-swift-2.0.4/swift/trainers/optimizers/galore/adamw8bit.py`

 * *Files 2% similar despite different names*

```diff
@@ -89,22 +89,19 @@
 
                 self.prefetch_state(p)
                 self.update_step(group, p, gindex, pindex)
                 torch.cuda.synchronize()
 
                 # GaLore Projection Back
                 if 'rank' in group:
-                    p.data = p.saved_data.add_(state['projector'].project_back(
-                        p.data))
+                    p.data = p.saved_data.add_(state['projector'].project_back(p.data))
 
                     # apply weight decay
                     if 'weight_decay_saved' in group:
-                        p.data.add_(
-                            p.data,
-                            alpha=-group['lr'] * group['weight_decay_saved'])
+                        p.data.add_(p.data, alpha=-group['lr'] * group['weight_decay_saved'])
                         group['weight_decay'] = group['weight_decay_saved']
                         del group['weight_decay_saved']
 
         if self.is_paged:
             # all paged operation are asynchronous, we need
             # to sync to make sure all tensors are in the right state
             torch.cuda.synchronize()
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/galore_projector.py` & `ms-swift-2.0.4/swift/trainers/optimizers/galore/galore_projector.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,95 +1,75 @@
 # code borrowed from https://github.com/jiaweizzhao/GaLore
 
 import torch
 
 
 class GaLoreProjector:
 
-    def __init__(self,
-                 rank,
-                 verbose=False,
-                 update_proj_gap=200,
-                 scale=1.0,
-                 proj_type='std'):
+    def __init__(self, rank, verbose=False, update_proj_gap=200, scale=1.0, proj_type='std'):
         self.rank = rank
         self.verbose = verbose
         self.update_proj_gap = update_proj_gap
         self.scale = scale
         self.ortho_matrix = None
         self.proj_type = proj_type
 
     def project(self, full_rank_grad, iter):
 
         if self.proj_type == 'std':
             if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:
                 if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                    self.ortho_matrix = self.get_orthogonal_matrix(
-                        full_rank_grad, self.rank, type='right')
-                low_rank_grad = torch.matmul(full_rank_grad,
-                                             self.ortho_matrix.t())
+                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')
+                low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t())
             else:
                 if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                    self.ortho_matrix = self.get_orthogonal_matrix(
-                        full_rank_grad, self.rank, type='left')
-                low_rank_grad = torch.matmul(self.ortho_matrix.t(),
-                                             full_rank_grad)
+                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')
+                low_rank_grad = torch.matmul(self.ortho_matrix.t(), full_rank_grad)
         elif self.proj_type == 'reverse_std':
             if full_rank_grad.shape[0] >= full_rank_grad.shape[1]:
                 if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                    self.ortho_matrix = self.get_orthogonal_matrix(
-                        full_rank_grad, self.rank, type='left')
-                low_rank_grad = torch.matmul(self.ortho_matrix.t(),
-                                             full_rank_grad)
+                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')
+                low_rank_grad = torch.matmul(self.ortho_matrix.t(), full_rank_grad)
             else:
                 if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                    self.ortho_matrix = self.get_orthogonal_matrix(
-                        full_rank_grad, self.rank, type='right')
-                low_rank_grad = torch.matmul(full_rank_grad,
-                                             self.ortho_matrix.t())
+                    self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')
+                low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t())
         elif self.proj_type == 'right':
             if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                self.ortho_matrix = self.get_orthogonal_matrix(
-                    full_rank_grad, self.rank, type='right')
+                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='right')
             low_rank_grad = torch.matmul(full_rank_grad, self.ortho_matrix.t())
         elif self.proj_type == 'left':
             if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                self.ortho_matrix = self.get_orthogonal_matrix(
-                    full_rank_grad, self.rank, type='left')
+                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='left')
             low_rank_grad = torch.matmul(self.ortho_matrix.t(), full_rank_grad)
         elif self.proj_type == 'full':
             if self.ortho_matrix is None or iter % self.update_proj_gap == 0:
-                self.ortho_matrix = self.get_orthogonal_matrix(
-                    full_rank_grad, self.rank, type='full')
-            low_rank_grad = torch.matmul(
-                self.ortho_matrix[0].t(),
-                full_rank_grad) @ self.ortho_matrix[1].t()
+                self.ortho_matrix = self.get_orthogonal_matrix(full_rank_grad, self.rank, type='full')
+            low_rank_grad = torch.matmul(self.ortho_matrix[0].t(), full_rank_grad) @ self.ortho_matrix[1].t()
 
         return low_rank_grad
 
     def project_back(self, low_rank_grad):
 
         if self.proj_type == 'std':
             if low_rank_grad.shape[0] >= low_rank_grad.shape[1]:
                 full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix)
             else:
                 full_rank_grad = torch.matmul(self.ortho_matrix, low_rank_grad)
         elif self.proj_type == 'reverse_std':
-            if low_rank_grad.shape[0] <= low_rank_grad.shape[
-                    1]:  # note this is different from std
+            if low_rank_grad.shape[0] <= low_rank_grad.shape[1]:  # note this is different from std
                 full_rank_grad = torch.matmul(self.ortho_matrix, low_rank_grad)
             else:
                 full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix)
         elif self.proj_type == 'right':
             full_rank_grad = torch.matmul(low_rank_grad, self.ortho_matrix)
         elif self.proj_type == 'left':
             full_rank_grad = torch.matmul(self.ortho_matrix, low_rank_grad)
         elif self.proj_type == 'full':
-            full_rank_grad = torch.matmul(self.ortho_matrix[0],
-                                          low_rank_grad) @ self.ortho_matrix[1]
+            full_rank_grad = torch.matmul(self.ortho_matrix[0], low_rank_grad) @ self.ortho_matrix[1]
 
         return full_rank_grad * self.scale
 
     # svd decomposition
     def get_orthogonal_matrix(self, weights, rank, type):
         module_params = weights
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/optimizers/galore/utils.py` & `ms-swift-2.0.4/swift/trainers/optimizers/galore/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -65,16 +65,15 @@
 
     def step(self, *args, **kwargs) -> None:
         for lr_scheduler in self.lr_schedulers.values():
             lr_scheduler.step(*args, **kwargs)
         self._last_lr = lr_scheduler.get_last_lr()
 
 
-def create_optimizer_and_scheduler(model: nn.Module, args: TrainingArguments,
-                                   config: GaLoreConfig, max_steps,
+def create_optimizer_and_scheduler(model: nn.Module, args: TrainingArguments, config: GaLoreConfig, max_steps,
                                    **defaults):
     galore_params = []
     for module_name, module in model.named_modules():
         if not isinstance(module, (nn.Linear, nn.Embedding)) or \
                 not any(target_key in module_name for target_key in config.target_modules):
             continue
 
@@ -91,64 +90,54 @@
         'proj_type': config.proj_type,
         **defaults
     }
     optim_cls, optim_kwargs = get_optimizer(args)
 
     if config.optim_per_parameter:
         optimizer_dict = {}
-        galore_defaults[
-            'update_proj_gap'] = galore_defaults['update_proj_gap'] * 2
+        galore_defaults['update_proj_gap'] = galore_defaults['update_proj_gap'] * 2
         for p in model.parameters():
             if p.requires_grad:
                 if id(p) in id_galore_params:
-                    optimizer_dict[p] = optim_cls([{
-                        'params': [p],
-                        **galore_defaults
-                    }], **optim_kwargs)
+                    optimizer_dict[p] = optim_cls([{'params': [p], **galore_defaults}], **optim_kwargs)
                 else:
-                    optimizer_dict[p] = optim_cls([{
-                        'params': [p],
-                        **defaults
-                    }], **optim_kwargs)
+                    optimizer_dict[p] = optim_cls([{'params': [p], **defaults}], **optim_kwargs)
 
         # get scheduler dict
         scheduler_dict = {}
         for p in model.parameters():
             if p.requires_grad:
                 scheduler_dict[p] = get_scheduler(
                     optimizer=optimizer_dict[p],
                     name=args.lr_scheduler_type,
                     num_training_steps=max_steps * 2,
                     num_warmup_steps=args.warmup_steps * 2,
                     scheduler_specific_kwargs=args.lr_scheduler_kwargs,
                 )
 
-        return GaloreOptimizerWrapper(optimizer_dict), GaloreSchedulerWrapper(
-            scheduler_dict)
+        return GaloreOptimizerWrapper(optimizer_dict), GaloreSchedulerWrapper(scheduler_dict)
     else:
         decay_parameters = Trainer.get_decay_parameter_names(Trainer, model)
         param_groups = [{
             'params': galore_params,
             **galore_defaults,
         }]
         param_groups.extend([
             {
                 'params': [
                     p for n, p in model.named_parameters()
-                    if (n in decay_parameters and id(p) not in id_galore_params
-                        and p.requires_grad)
+                    if (n in decay_parameters and id(p) not in id_galore_params and p.requires_grad)
                 ],
                 'weight_decay':
                 defaults['weight_decay'],
             },
             {
                 'params': [
                     p for n, p in model.named_parameters()
-                    if (n not in decay_parameters
-                        and id(p) not in id_galore_params and p.requires_grad)
+                    if (n not in decay_parameters and id(p) not in id_galore_params and p.requires_grad)
                 ],
                 'weight_decay':
                 0.0,
             },
         ])
         optim = optim_cls(param_groups, **optim_kwargs)
         scheduler = get_scheduler(
@@ -174,32 +163,23 @@
     adam_kwargs = {
         'betas': (args.adam_beta1, args.adam_beta2),
         'eps': args.adam_epsilon,
     }
     if args.optim == 'adafactor':
         from .adafactor import GaLoreAdafactor
         optimizer_cls = GaLoreAdafactor
-        optimizer_kwargs.update({
-            'scale_parameter': False,
-            'relative_step': False
-        })
+        optimizer_kwargs.update({'scale_parameter': False, 'relative_step': False})
     elif args.optim in ('adamw_hf', 'adamw_torch'):
         from .adamw import GaLoreAdamW
         optimizer_cls = GaLoreAdamW
         optimizer_kwargs.update(adam_kwargs)
     elif 'adamw' in args.optim and '8bit' in args.optim:
         try:
             from .adamw8bit import GaLoreAdamW8bit
             optimizer_cls = GaLoreAdamW8bit
             optimizer_kwargs.update(adam_kwargs)
-            optimizer_kwargs.update({
-                'optim_bits': 8,
-                'is_paged': 'paged' in args.optim
-            })
+            optimizer_kwargs.update({'optim_bits': 8, 'is_paged': 'paged' in args.optim})
         except ImportError:
-            raise ValueError(
-                'Trainer tried to instantiate bnb optimizer but bnb is not installed!'
-            )
+            raise ValueError('Trainer tried to instantiate bnb optimizer but bnb is not installed!')
     else:
-        raise ValueError(
-            f'Galore not supported for optimizer type: {args.optim}')
+        raise ValueError(f'Galore not supported for optimizer type: {args.optim}')
     return optimizer_cls, optimizer_kwargs
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/trainers.py` & `ms-swift-2.0.4/swift/trainers/trainers.py`

 * *Files 6% similar despite different names*

```diff
@@ -8,23 +8,20 @@
 from torch import Tensor, nn
 from torch.nn import CrossEntropyLoss
 from torch.utils.data import DataLoader
 from transformers import Seq2SeqTrainer as HfSeq2SeqTrainer
 from transformers import Trainer as HfTrainer
 from transformers import trainer
 from transformers.modeling_utils import unwrap_model
-from transformers.models.auto.modeling_auto import \
-    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES
+from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES
 from transformers.utils import is_peft_available
 
-from swift.torchacc_utils import (ta_eval_dataloader, ta_test_dataloader,
-                                  ta_train_dataloader)
+from swift.torchacc_utils import ta_eval_dataloader, ta_test_dataloader, ta_train_dataloader
 from swift.utils import use_torchacc
-from .callback import (DefaultFlowCallbackNew, PrinterCallbackNew,
-                       ProgressCallbackNew)
+from .callback import DefaultFlowCallbackNew, PrinterCallbackNew, ProgressCallbackNew
 from .mixin import PushToMsHubMixin, SwiftMixin
 
 try:
     from transformers.integrations.deepspeed import is_deepspeed_zero3_enabled
 except ImportError:
     from transformers.deepspeed import is_deepspeed_zero3_enabled
 
@@ -35,86 +32,70 @@
 
 class Seq2SeqTrainer(PushToMsHubMixin, SwiftMixin, HfSeq2SeqTrainer):
 
     def __init__(self, *args, **kwargs):
         super().__init__(*args, **kwargs)
         # performance
         self.perf: Dict[str, Any] = {
-            'gen_time':
-            0.,
-            'gen_len':
-            0,
+            'gen_time': 0.,
+            'gen_len': 0,
             'memory': {},
-            'model':
-            self.model.get_trainable_parameters() if hasattr(
-                self.model, 'get_trainable_parameters') else None,
+            'model': self.model.get_trainable_parameters() if hasattr(self.model, 'get_trainable_parameters') else None,
         }
         self._acc = torch.tensor(0.).to(self.args.device)
 
     def train(self, *args, **kwargs) -> torch.Tensor:
         res = super().train(*args, **kwargs)
         for i in range(torch.cuda.device_count()):
-            self.perf['memory'][
-                f'cuda:{i}'] = f'{torch.cuda.max_memory_reserved(i)/1024/1024/1024:.2f}GiB'
+            self.perf['memory'][f'cuda:{i}'] = f'{torch.cuda.max_memory_reserved(i)/1024/1024/1024:.2f}GiB'
         return res
 
     def prediction_step(
         self,
         model: nn.Module,
         inputs: Dict[str, Union[torch.Tensor, Any]],
         prediction_loss_only: bool,
         ignore_keys: Optional[List[str]] = None,
         **gen_kwargs,
-    ) -> Tuple[Optional[float], Optional[torch.Tensor],
-               Optional[torch.Tensor]]:
+    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:
         if not self.args.predict_with_generate or prediction_loss_only:
             return super().prediction_step(
-                model,
-                inputs,
-                prediction_loss_only=prediction_loss_only,
-                ignore_keys=ignore_keys)
+                model, inputs, prediction_loss_only=prediction_loss_only, ignore_keys=ignore_keys)
 
         inputs.pop('loss_scale', None)
         has_labels = 'labels' in inputs
         inputs = self._prepare_inputs(inputs)
 
         # XXX: adapt synced_gpus for fairscale as well
         # Priority (handled in generate):
         # gen_kwargs > model.generation_config > default GenerationConfig()
 
         if len(gen_kwargs) == 0 and hasattr(self, '_gen_kwargs'):
             gen_kwargs = self._gen_kwargs.copy()
             if hasattr(self.model, 'generation_config'):
                 gen_kwargs.update(self.model.generation_config.to_dict())
 
-        if gen_kwargs.get('max_length') is None and gen_kwargs.get(
-                'max_new_tokens') is None:
+        if gen_kwargs.get('max_length') is None and gen_kwargs.get('max_new_tokens') is None:
             gen_kwargs['max_length'] = self.model.config.max_length
         gen_kwargs['num_beams'] = (
-            gen_kwargs['num_beams'] if gen_kwargs.get('num_beams') is not None
-            else self.model.config.num_beams)
+            gen_kwargs['num_beams'] if gen_kwargs.get('num_beams') is not None else self.model.config.num_beams)
         default_synced_gpus = True if is_deepspeed_zero3_enabled() else False
         gen_kwargs['synced_gpus'] = (
-            gen_kwargs['synced_gpus'] if gen_kwargs.get('synced_gpus')
-            is not None else default_synced_gpus)
+            gen_kwargs['synced_gpus'] if gen_kwargs.get('synced_gpus') is not None else default_synced_gpus)
 
         # If the `decoder_input_ids` was created from `labels`, evict the former, so that the model can freely generate
         # (otherwise, it would continue generating from the padded `decoder_input_ids`)
-        if ('labels' in inputs and 'decoder_input_ids' in inputs and
-                inputs['labels'].shape == inputs['decoder_input_ids'].shape):
-            inputs = {
-                k: v
-                for k, v in inputs.items() if k != 'decoder_input_ids'
-            }
+        if ('labels' in inputs and 'decoder_input_ids' in inputs
+                and inputs['labels'].shape == inputs['decoder_input_ids'].shape):
+            inputs = {k: v for k, v in inputs.items() if k != 'decoder_input_ids'}
 
         gen_kwargs['pad_token_id'] = self.tokenizer.pad_token_id
         gen_kwargs['eos_token_id'] = self.tokenizer.eos_token_id
         # fix generate warning
-        if ('max_length' in gen_kwargs and 'max_new_tokens' in gen_kwargs
-                and gen_kwargs['max_new_tokens'] is not None):
+        if ('max_length' in gen_kwargs and 'max_new_tokens' in gen_kwargs and gen_kwargs['max_new_tokens'] is not None):
             gen_kwargs.pop('max_length')
         gen_time = time.time()
         generate_inputs = inputs.copy()
         if has_labels:
             _labels = inputs['labels'][0]
             n_mask = 0
             for i in range(len(_labels)):
@@ -125,72 +106,57 @@
             for k in ['input_ids', 'attention_mask']:
                 generate_inputs[k] = generate_inputs[k][:, :n_mask]
             generate_inputs['labels'] = generate_inputs['labels'][:, n_mask:]
 
         generated_tokens = self.model.generate(**generate_inputs, **gen_kwargs)
         gen_time = time.time() - gen_time
 
-        if hasattr(
-                self.model, 'encoder'
-        ) and self.model.encoder.main_input_name != self.model.main_input_name:
-            generation_inputs = generate_inputs[
-                self.model.encoder.main_input_name]
+        if hasattr(self.model, 'encoder') and self.model.encoder.main_input_name != self.model.main_input_name:
+            generation_inputs = generate_inputs[self.model.encoder.main_input_name]
         else:
             generation_inputs = generate_inputs[self.model.main_input_name]
 
         generated_tokens = generated_tokens[:, generation_inputs.shape[1]:]
         gen_len = len(generated_tokens[0])
         self.perf['gen_time'] = self.perf['gen_time'] + gen_time
         self.perf['gen_len'] = self.perf['gen_len'] + gen_len
 
         # in case the batch is shorter than max length, the output should be padded
-        if gen_kwargs.get('max_length') is not None and generated_tokens.shape[
-                -1] < gen_kwargs['max_length']:
-            generated_tokens = self._pad_tensors_to_max_len(
-                generated_tokens, gen_kwargs['max_length'])
-        elif gen_kwargs.get('max_new_tokens'
-                            ) is not None and generated_tokens.shape[-1] < (
-                                gen_kwargs['max_new_tokens'] + 1):
-            generated_tokens = self._pad_tensors_to_max_len(
-                generated_tokens, gen_kwargs['max_new_tokens'] + 1)
+        if gen_kwargs.get('max_length') is not None and generated_tokens.shape[-1] < gen_kwargs['max_length']:
+            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs['max_length'])
+        elif gen_kwargs.get('max_new_tokens') is not None and generated_tokens.shape[-1] < (gen_kwargs['max_new_tokens']
+                                                                                            + 1):
+            generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_kwargs['max_new_tokens'] + 1)
 
         with torch.no_grad():
             if has_labels:
                 with self.compute_loss_context_manager():
                     outputs = model(**inputs)
                 if self.label_smoother is not None:
-                    loss = self.label_smoother(
-                        outputs, inputs['labels']).mean().detach()
+                    loss = self.label_smoother(outputs, inputs['labels']).mean().detach()
                 else:
-                    loss = (outputs['loss'] if isinstance(outputs, dict) else
-                            outputs[0]).mean().detach()
+                    loss = (outputs['loss'] if isinstance(outputs, dict) else outputs[0]).mean().detach()
             else:
                 loss = None
 
         if self.args.prediction_loss_only:
             return loss, None, None
 
         if has_labels:
             labels = generate_inputs['labels']
-            if gen_kwargs.get('max_length') is not None and labels.shape[
-                    -1] < gen_kwargs['max_length']:
-                labels = self._pad_tensors_to_max_len(labels,
-                                                      gen_kwargs['max_length'])
-            elif gen_kwargs.get(
-                    'max_new_tokens') is not None and labels.shape[-1] < (
-                        gen_kwargs['max_new_tokens'] + 1):
-                labels = self._pad_tensors_to_max_len(
-                    labels, (gen_kwargs['max_new_tokens'] + 1))
+            if gen_kwargs.get('max_length') is not None and labels.shape[-1] < gen_kwargs['max_length']:
+                labels = self._pad_tensors_to_max_len(labels, gen_kwargs['max_length'])
+            elif gen_kwargs.get('max_new_tokens') is not None and labels.shape[-1] < (gen_kwargs['max_new_tokens'] + 1):
+                labels = self._pad_tensors_to_max_len(labels, (gen_kwargs['max_new_tokens'] + 1))
         else:
             labels = None
 
         return loss, generated_tokens, labels
 
-    def compute_scaled_loss(self, labels: torch.Tensor,
-                            lm_logits: torch.Tensor,
+    def compute_scaled_loss(self, labels: torch.Tensor, lm_logits: torch.Tensor,
                             loss_scale: torch.Tensor) -> torch.Tensor:
         device = lm_logits.device
         # Shift so that tokens < n predict n
         shift_logits = lm_logits[..., :-1, :]
         shift_labels = labels[..., 1:]
         shift_scale = loss_scale[..., 1:]
         # Save memory
@@ -215,16 +181,15 @@
             loss_scale = inputs.pop('loss_scale')
 
         if self.label_smoother is not None and 'labels' in inputs:
             labels = inputs.pop('labels')
 
         outputs = model(**inputs)
         if loss_scale is not None:
-            outputs['loss'] = self.compute_scaled_loss(labels, outputs.logits,
-                                                       loss_scale)
+            outputs['loss'] = self.compute_scaled_loss(labels, outputs.logits, loss_scale)
 
         # Save past state if it exists
         # TODO: this needs to be fixed and made cleaner later.
         if self.args.past_index >= 0:
             self._past = outputs[self.args.past_index]
 
         if labels is not None and loss_scale is None:
@@ -248,26 +213,22 @@
         acc_strategy = getattr(self.args, 'acc_strategy', 'token')
         acc: Optional[Tensor] = None
         if preds.shape != labels.shape:
             pass
         elif acc_strategy == 'sentence':
             acc_list = []
             for i, m in enumerate(masks):
-                acc_list.append(
-                    torch.all(preds[i, m] == labels[i,
-                                                    m]).to(torch.int64).item())
+                acc_list.append(torch.all(preds[i, m] == labels[i, m]).to(torch.int64).item())
             acc = torch.tensor(acc_list, device=preds.device).float().mean()
         else:
-            acc = (torch.masked_select(preds, masks) == torch.masked_select(
-                labels, masks)).float().mean()
+            acc = (torch.masked_select(preds, masks) == torch.masked_select(labels, masks)).float().mean()
         if model.training and acc is not None:
             if 'acc' not in self._custom_metrics:
                 self._custom_metrics['acc'] = self._acc
-            self._custom_metrics['acc'] = self._custom_metrics[
-                'acc'] + acc / self.args.gradient_accumulation_steps
+            self._custom_metrics['acc'] = self._custom_metrics['acc'] + acc / self.args.gradient_accumulation_steps
         return (loss, outputs) if return_outputs else loss
 
     def get_train_dataloader(self):
 
         if not use_torchacc():
             return super().get_train_dataloader()
 
@@ -277,72 +238,57 @@
 
             if self.train_dataset is None:
                 raise ValueError('Trainer: training requires a train_dataset.')
 
             train_dataset = self.train_dataset
             data_collator = self.data_collator
 
-            if trainer.is_datasets_available() and isinstance(
-                    train_dataset, datasets.Dataset):
-                train_dataset = self._remove_unused_columns(
-                    train_dataset, description='training')
+            if trainer.is_datasets_available() and isinstance(train_dataset, datasets.Dataset):
+                train_dataset = self._remove_unused_columns(train_dataset, description='training')
             else:
-                data_collator = self._get_collator_with_removed_columns(
-                    data_collator, description='training')
+                data_collator = self._get_collator_with_removed_columns(data_collator, description='training')
 
-            return ta_train_dataloader(train_dataset, data_collator,
-                                       self._get_train_sampler(), self.args,
+            return ta_train_dataloader(train_dataset, data_collator, self._get_train_sampler(), self.args,
                                        self._train_batch_size)
 
     def get_eval_dataloader(self, eval_dataset):
         if not use_torchacc():
             return super().get_eval_dataloader(eval_dataset)
         else:
             import torchacc as ta
             if trainer.is_datasets_available():
                 import datasets
 
             if eval_dataset is None and self.eval_dataset is None:
-                raise ValueError(
-                    'Trainer: evaluation requires an eval_dataset.')
+                raise ValueError('Trainer: evaluation requires an eval_dataset.')
             eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset
             data_collator = self.data_collator
 
-            if trainer.is_datasets_available() and isinstance(
-                    eval_dataset, datasets.Dataset):
-                eval_dataset = self._remove_unused_columns(
-                    eval_dataset, description='evaluation')
+            if trainer.is_datasets_available() and isinstance(eval_dataset, datasets.Dataset):
+                eval_dataset = self._remove_unused_columns(eval_dataset, description='evaluation')
             else:
-                data_collator = self._get_collator_with_removed_columns(
-                    data_collator, description='evaluation')
+                data_collator = self._get_collator_with_removed_columns(data_collator, description='evaluation')
 
-            return ta_eval_dataloader(eval_dataset, data_collator,
-                                      self._get_eval_sampler(eval_dataset),
-                                      self.args)
+            return ta_eval_dataloader(eval_dataset, data_collator, self._get_eval_sampler(eval_dataset), self.args)
 
     def get_test_dataloader(self, test_dataset):
         if not use_torchacc():
             return super().get_test_dataloader(test_dataset)
         else:
             import torchacc as ta
             if trainer.is_datasets_available():
                 import datasets
 
             data_collator = self.data_collator
 
-            if trainer.is_datasets_available() and isinstance(
-                    test_dataset, datasets.Dataset):
-                test_dataset = self._remove_unused_columns(
-                    test_dataset, description='test')
+            if trainer.is_datasets_available() and isinstance(test_dataset, datasets.Dataset):
+                test_dataset = self._remove_unused_columns(test_dataset, description='test')
             else:
-                data_collator = self._get_collator_with_removed_columns(
-                    data_collator, description='test')
+                data_collator = self._get_collator_with_removed_columns(data_collator, description='test')
 
-            return ta_test_dataloader(test_dataset, data_collator,
-                                      self._get_eval_sampler(test_dataset),
-                                      self.args)
+            return ta_test_dataloader(test_dataset, data_collator, self._get_eval_sampler(test_dataset), self.args)
 
 
 # monkey patching
 trainer.DEFAULT_PROGRESS_CALLBACK = ProgressCallbackNew
 trainer.DEFAULT_CALLBACKS = [DefaultFlowCallbackNew]
 trainer.PrinterCallback = PrinterCallbackNew
```

### Comparing `ms-swift-2.0.3.post1/swift/trainers/utils.py` & `ms-swift-2.0.4/swift/trainers/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,17 +3,16 @@
 
 import inspect
 from types import FunctionType, MethodType
 from typing import List, Union
 
 from torch.nn import Module
 from transformers.trainer_callback import TrainerCallback
-from transformers.trainer_utils import (EvaluationStrategy, FSDPOption,
-                                        HPSearchBackend, HubStrategy,
-                                        IntervalStrategy, SchedulerType)
+from transformers.trainer_utils import (EvaluationStrategy, FSDPOption, HPSearchBackend, HubStrategy, IntervalStrategy,
+                                        SchedulerType)
 
 from swift.utils import get_logger
 
 try:
     # https://github.com/huggingface/transformers/pull/25702
     from transformers.trainer_utils import ShardedDDPOption
 except ImportError:
@@ -32,24 +31,20 @@
 
 
 def find_labels(model: Module) -> List[str]:
     """Find the labels used by a given model."""
     model_name = model.__class__.__name__
     signature = inspect.signature(model.forward)
     if 'QuestionAnswering' in model_name:
-        return [
-            p for p in signature.parameters
-            if 'label' in p or p in ('start_positions', 'end_positions')
-        ]
+        return [p for p in signature.parameters if 'label' in p or p in ('start_positions', 'end_positions')]
     else:
         return [p for p in signature.parameters if 'label' in p]
 
 
-def get_function(
-        method_or_function: Union[MethodType, FunctionType]) -> FunctionType:
+def get_function(method_or_function: Union[MethodType, FunctionType]) -> FunctionType:
     if isinstance(method_or_function, MethodType):
         method_or_function = method_or_function.__func__
     return method_or_function
 
 
 def is_instance_of_ms_model(model: Module) -> bool:
     """avoid import modelscope: circular dependency problem"""
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/__init__.py` & `ms-swift-2.0.4/swift/tuners/__init__.py`

 * *Files 3% similar despite different names*

```diff
@@ -8,44 +8,36 @@
     from .base import SwiftModel, Swift
     from .lora import LoRA, LoRAConfig
     from .mapping import SWIFT_MAPPING, SwiftTuners
     from .side import Side, SideConfig, SideModule
     from .neftune import NEFTune, NEFTuneConfig
     from .longlora.longlora import LongLoRAModelType, LongLoRAConfig, LongLoRA
     from .restuning import ResTuning, ResTuningConfig, ResTuningBypassModule
-    from .peft import (AdaLoraConfig, IA3Config, LoftQConfig, LoHaConfig,
-                       LoKrConfig, LoraConfig, OFTConfig, PeftConfig,
-                       PeftModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM,
-                       PeftModelForSequenceClassification,
-                       PeftModelForTokenClassification, PrefixTuningConfig,
-                       PromptEncoderConfig, PromptLearningConfig,
-                       PromptTuningConfig, get_peft_config, get_peft_model,
-                       get_peft_model_state_dict)
+    from .peft import (AdaLoraConfig, IA3Config, LoftQConfig, LoHaConfig, LoKrConfig, LoraConfig, OFTConfig, PeftConfig,
+                       PeftModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM, PeftModelForSequenceClassification,
+                       PeftModelForTokenClassification, PrefixTuningConfig, PromptEncoderConfig, PromptLearningConfig,
+                       PromptTuningConfig, get_peft_config, get_peft_model, get_peft_model_state_dict)
     from .prompt import Prompt, PromptConfig, PromptModule
     from .scetuning.scetuning import SCETuning, SCETuningConfig
     from .utils import SwiftConfig, SwiftOutput
 else:
     _import_structure = {
         'adapter': ['Adapter', 'AdapterConfig', 'AdapterModule'],
         'base': ['SwiftModel', 'Swift'],
         'lora': ['LoRA', 'LoRAConfig'],
-        'longlora.longlora':
-        ['LongLoRAModelType', 'LongLoRAConfig', 'LongLoRA'],
+        'longlora.longlora': ['LongLoRAModelType', 'LongLoRAConfig', 'LongLoRA'],
         'mapping': ['SWIFT_MAPPING', 'SwiftTuners'],
         'side': ['Side', 'SideConfig', 'SideModule'],
         'neftune': ['NEFTune', 'NEFTuneConfig'],
         'restuning': ['ResTuning', 'ResTuningConfig', 'ResTuningBypassModule'],
         'peft': [
-            'AdaLoraConfig', 'IA3Config', 'LoftQConfig', 'LoHaConfig',
-            'LoKrConfig', 'LoraConfig', 'OFTConfig', 'PeftConfig', 'PeftModel',
-            'PeftModelForCausalLM', 'PeftModelForSeq2SeqLM',
-            'PeftModelForSequenceClassification',
-            'PeftModelForTokenClassification', 'PrefixTuningConfig',
-            'PromptEncoderConfig', 'PromptLearningConfig',
-            'PromptTuningConfig', 'get_peft_config', 'get_peft_model',
+            'AdaLoraConfig', 'IA3Config', 'LoftQConfig', 'LoHaConfig', 'LoKrConfig', 'LoraConfig', 'OFTConfig',
+            'PeftConfig', 'PeftModel', 'PeftModelForCausalLM', 'PeftModelForSeq2SeqLM',
+            'PeftModelForSequenceClassification', 'PeftModelForTokenClassification', 'PrefixTuningConfig',
+            'PromptEncoderConfig', 'PromptLearningConfig', 'PromptTuningConfig', 'get_peft_config', 'get_peft_model',
             'get_peft_model_state_dict'
         ],
         'prompt': ['Prompt', 'PromptConfig', 'PromptModule'],
         'scetuning': ['SCETuning', 'SCETuningConfig'],
         'utils': ['SwiftConfig', 'SwiftOutput'],
     }
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/adapter.py` & `ms-swift-2.0.4/swift/tuners/adapter.py`

 * *Files 6% similar despite different names*

```diff
@@ -32,142 +32,105 @@
         hidden_pos(`Union[str, int]`): The position of the hidden state to be passed into the adapter,
             can be int (args) or str (kwargs)
         method_name(`str`): The method to be replaced, default is `forward`
         adapter_length: The length of the adapter length (intermediate length)
         act_layer: The activation layer of the adapter
     """
 
-    dim: int = field(
-        default=None, metadata={'help': 'The dimension of the hidden states'})
+    dim: int = field(default=None, metadata={'help': 'The dimension of the hidden states'})
 
     target_modules: Union[str, List[str]] = field(
         default=None,
         metadata={
             'help':
             'The feedforward module to be replaced. in regex format if this argument is str, '
             'else will match with `end with` if List[str].'
         })
 
     hidden_pos: Union[str, int] = field(
         default=None,
         metadata={
-            'help':
-            'The position of the hidden state to be passed into the adapter, can be int (args) or str (kwargs)'
+            'help': 'The position of the hidden state to be passed into the adapter, can be int (args) or str (kwargs)'
         })
 
-    method_name: str = field(
-        default='forward',
-        metadata={'help': 'The method to be replaced, default is `forward`'})
+    method_name: str = field(default='forward', metadata={'help': 'The method to be replaced, default is `forward`'})
 
     adapter_length: int = field(
-        default=128,
-        metadata={
-            'help': 'The length of the adapter length (intermediate length)'
-        })
+        default=128, metadata={'help': 'The length of the adapter length (intermediate length)'})
 
-    act_layer: str = field(
-        default='gelu',
-        metadata={'help': 'The activation layer of the adapter'})
+    act_layer: str = field(default='gelu', metadata={'help': 'The activation layer of the adapter'})
 
     def __post_init__(self):
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.ADAPTER
 
 
 class Adapter(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: AdapterConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: nn.Module, config: AdapterConfig, adapter_name: str) -> SwiftOutput:
         """Prepare a model with `AdapterConfig`"""
         module_keys = [key for key, _ in model.named_modules()]
 
         for module_key in module_keys:
             if isinstance(config.target_modules, str):
-                target_module_found = re.fullmatch(config.target_modules,
-                                                   module_key)
+                target_module_found = re.fullmatch(config.target_modules, module_key)
             else:
-                target_module_found = any(
-                    module_key.endswith(target_key)
-                    for target_key in config.target_modules)
+                target_module_found = any(module_key.endswith(target_key) for target_key in config.target_modules)
 
             if target_module_found:  # noqa
                 module = model.get_submodule(module_key)
 
                 def _forward(self, *args, **kwargs):
-                    args = getattr(self,
-                                   f'forward_origin_{adapter_name}')(*args,
-                                                                     **kwargs)
+                    args = getattr(self, f'forward_origin_{adapter_name}')(*args, **kwargs)
                     if isinstance(args, (tuple, list, dict)):
                         if isinstance(config.hidden_pos, int):
                             _type = type(args)
                             args = list(args)
-                            args[config.hidden_pos] = getattr(
-                                self, f'adapter_{adapter_name}')(
-                                    args[config.hidden_pos])
+                            args[config.hidden_pos] = getattr(self, f'adapter_{adapter_name}')(args[config.hidden_pos])
                             args = _type(args)
                         else:
-                            args[config.hidden_pos] = getattr(
-                                self, f'adapter_{adapter_name}')(
-                                    args[config.hidden_pos])
+                            args[config.hidden_pos] = getattr(self, f'adapter_{adapter_name}')(args[config.hidden_pos])
                     elif isinstance(args, torch.Tensor):
                         args = getattr(self, f'adapter_{adapter_name}')(args)
                     return args
 
                 def _feed_forward_chunk(self, attention_output):
                     return _forward(self, attention_output)
 
                 # TODO The `config.method_name` method should not be replaced twice.
 
-                setattr(module, f'forward_origin_{adapter_name}',
-                        getattr(module, config.method_name))
+                setattr(module, f'forward_origin_{adapter_name}', getattr(module, config.method_name))
                 num_args_in_forward_chunk_fn = len(
-                    inspect.signature(
-                        getattr(module,
-                                f'forward_origin_{adapter_name}')).parameters)
+                    inspect.signature(getattr(module, f'forward_origin_{adapter_name}')).parameters)
                 if config.method_name == 'feed_forward_chunk' and num_args_in_forward_chunk_fn == 1:
-                    setattr(module, config.method_name,
-                            types.MethodType(_feed_forward_chunk, module))
+                    setattr(module, config.method_name, types.MethodType(_feed_forward_chunk, module))
                 else:
-                    setattr(module, config.method_name,
-                            types.MethodType(_forward, module))
-                adapter_module = AdapterModule(config.dim, adapter_name,
-                                               module_key,
-                                               config.adapter_length,
+                    setattr(module, config.method_name, types.MethodType(_forward, module))
+                adapter_module = AdapterModule(config.dim, adapter_name, module_key, config.adapter_length,
                                                ACT2CLS[config.act_layer])
                 setattr(module, f'adapter_{adapter_name}', adapter_module)
-                logger.info(
-                    f'Adapter modules(module_key): {module_key}.adapter_{adapter_name}'
-                )
+                logger.info(f'Adapter modules(module_key): {module_key}.adapter_{adapter_name}')
 
         def state_dict_callback(state_dict, adapter_name: str):
-            return {
-                key: value
-                for key, value in state_dict.items()
-                if f'adapter_{adapter_name}' in key
-            }
+            return {key: value for key, value in state_dict.items() if f'adapter_{adapter_name}' in key}
 
         def mark_trainable_callback(model):
             return
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         modules = find_sub_module(module, f'adapter_{adapter_name}')
         for _module in modules:
             _module: ActivationMixin
             _module: nn.Module
             _module.set_activation(adapter_name, activate)
-            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key,
-                                     activate, offload)
+            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key, activate, offload)
 
 
 class AdapterModule(nn.Module, ActivationMixin):
     """The implementation of adapter tuning method.
 
     Adapters project input tokens by an MLP layer.
     'Parameter-Efficient Transfer Learning for NLP' by Houlsby et al.(2019)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/base.py` & `ms-swift-2.0.4/swift/tuners/base.py`

 * *Files 4% similar despite different names*

```diff
@@ -54,79 +54,65 @@
             extra_state_keys.extend(model.extra_state_keys)
             self.active_adapters = model.active_adapters
             model = model.base_model
 
         new_adapters = []
         if isinstance(config, SwiftConfig):
             if DEFAULT_ADAPTER not in self.adapters:
-                self.adapters[DEFAULT_ADAPTER] = self._prepare_model(
-                    model, config, DEFAULT_ADAPTER)
+                self.adapters[DEFAULT_ADAPTER] = self._prepare_model(model, config, DEFAULT_ADAPTER)
                 new_adapters.append(DEFAULT_ADAPTER)
             else:
-                logger.warn(
-                    f'Adapter {DEFAULT_ADAPTER} has been patched, skip.')
+                logger.warn(f'Adapter {DEFAULT_ADAPTER} has been patched, skip.')
         elif isinstance(config, dict):
             assert (all(isinstance(c, SwiftConfig) for c in config.values()))
             for adapter_name, _config in config.items():
                 if adapter_name not in self.adapters:
-                    self.adapters[adapter_name] = self._prepare_model(
-                        model, _config, adapter_name)
+                    self.adapters[adapter_name] = self._prepare_model(model, _config, adapter_name)
                     new_adapters.append(adapter_name)
                 else:
-                    logger.warn(
-                        f'Adapter {adapter_name} has been patched, skip.')
+                    logger.warn(f'Adapter {adapter_name} has been patched, skip.')
         self.base_model = model
 
         self.extra_state_keys = extra_state_keys or []
-        self.has_additional_modules = any(
-            [c.config.has_additional_modules for c in self.adapters.values()])
+        self.has_additional_modules = any([c.config.has_additional_modules for c in self.adapters.values()])
 
         def forward(self, *args, **kwargs):
             return self.base_model(*args, **kwargs)
 
         _parameters = [Parameter('self', Parameter.POSITIONAL_ONLY)]
-        _parameters += list(
-            signature(self.base_model.forward).parameters.values())
+        _parameters += list(signature(self.base_model.forward).parameters.values())
         forward.__signature__ = Signature(_parameters)
         self.forward = MethodType(forward, self)
         for adapter_name in new_adapters:
             self.activate_adapter(adapter_name)
 
         if inference_mode:
             self.eval()
         else:
             for key, output in self.adapters.items():
                 if key in new_adapters:
                     output.mark_trainable_callback(model)
             if self.extra_state_keys:
                 for n, p in model.named_parameters():
-                    if any(
-                            re.fullmatch(extra_key, n)
-                            for extra_key in self.extra_state_keys):
+                    if any(re.fullmatch(extra_key, n) for extra_key in self.extra_state_keys):
                         p.requires_grad = True
 
     @property
     def model(self):
         return self.base_model
 
-    def load_state_dict(self,
-                        state_dict,
-                        strict=True,
-                        adapter_name: str = None):
+    def load_state_dict(self, state_dict, strict=True, adapter_name: str = None):
         if adapter_name is not None:
             output = self.adapters[adapter_name]
             if getattr(output.config, 'modules_to_save', None):
                 for key, value in copy(state_dict).items():
                     for module_name in output.config.modules_to_save:
                         if module_name in key:
                             state_dict.pop(key)
-                            key = key.replace(
-                                module_name,
-                                f'{module_name}.modules_to_save.{adapter_name}'
-                            )
+                            key = key.replace(module_name, f'{module_name}.modules_to_save.{adapter_name}')
                             break
                     state_dict[key] = value
 
             for key, value in copy(state_dict).items():
                 if key.startswith('base_model.model.'):
                     state_dict.pop(key, None)
                     key = key[len('base_model.model.'):]
@@ -134,27 +120,23 @@
                     state_dict.pop(key, None)
                     key = key.replace('lora_A.', f'lora_A.{adapter_name}.')
                 if f'lora_B.{adapter_name}.' not in key and 'lora_B' in key:
                     state_dict.pop(key, None)
                     key = key.replace('lora_B.', f'lora_B.{adapter_name}.')
                 if f'lora_embedding_A.{adapter_name}.' not in key and 'lora_embedding_A' in key:
                     state_dict.pop(key, None)
-                    key = key.replace('lora_embedding_A.',
-                                      f'lora_embedding_A.{adapter_name}.')
+                    key = key.replace('lora_embedding_A.', f'lora_embedding_A.{adapter_name}.')
                 if f'lora_embedding_B.{adapter_name}.' not in key and 'lora_embedding_B' in key:
                     state_dict.pop(key, None)
-                    key = key.replace('lora_embedding_B.',
-                                      f'lora_embedding_B.{adapter_name}.')
+                    key = key.replace('lora_embedding_B.', f'lora_embedding_B.{adapter_name}.')
                 state_dict[key] = value
 
         incompatible_keys = self.base_model.load_state_dict(state_dict, False)
         if incompatible_keys and len(incompatible_keys[1]) > 0:
-            logger.error(
-                f'Load state dict with unexpected keys: {incompatible_keys[1]}'
-            )
+            logger.error(f'Load state dict with unexpected keys: {incompatible_keys[1]}')
 
     def state_dict(self,
                    *args,
                    destination=None,
                    prefix='',
                    keep_vars=False,
                    adapter_name: str = None,
@@ -179,59 +161,49 @@
                 save_adapter(`bool`): Save adapters or not, default True
                 save_extra_states(`bool`): Save extra states or not, default True
         Returns:
             The state dict to be saved.
         """
         state_dict = kwargs.get('state_dict')
         if state_dict is None:
-            state_dict = self.base_model.state_dict(
-                destination=destination, prefix=prefix, keep_vars=keep_vars)
+            state_dict = self.base_model.state_dict(destination=destination, prefix=prefix, keep_vars=keep_vars)
         state_dict = {
-            key[len('base_model.'):] if key.startswith('base_model.') else key:
-            value
+            key[len('base_model.'):] if key.startswith('base_model.') else key: value
             for key, value in state_dict.items()
         }
         if not self.has_additional_modules:
             return state_dict
 
         state_dicts = {}
         if kwargs.get('save_adapter', True):
             for name, output in self.adapters.items():
-                if (adapter_name == name or adapter_name is None
-                    ) and output.config.has_additional_modules:  # noqa
-                    state_dicts.update(
-                        output.state_dict_callback(state_dict, name))
+                if (adapter_name == name or adapter_name is None) and output.config.has_additional_modules:  # noqa
+                    state_dicts.update(output.state_dict_callback(state_dict, name))
                     modules_to_save_names = [
-                        sub_name
-                        for sub_name, _ in self.base_model.named_parameters()
+                        sub_name for sub_name, _ in self.base_model.named_parameters()
                         if f'modules_to_save.{name}' in sub_name
                     ]
                     for module_name in modules_to_save_names:
                         if f'modules_to_save.{name}' in module_name:
-                            state_dicts[module_name.replace(
-                                f'modules_to_save.{name}.',
-                                '')] = state_dict[module_name]
+                            state_dicts[module_name.replace(f'modules_to_save.{name}.', '')] = state_dict[module_name]
         if kwargs.get('save_extra_states', True):
             state_dicts.update({
                 k: v
                 for k, v in state_dict.items() if any(
-                    re.fullmatch(extra_key, k)
-                    for extra_key in self.extra_state_keys)
+                    re.fullmatch(extra_key, k) for extra_key in self.extra_state_keys)
             })
         if peft_format:
             new_state_dict = {}
             for key, value in state_dicts.items():
                 if not key.startswith('base_model.model.'):
                     key = 'base_model.model.' + key
                 key = key.replace(f'lora_A.{adapter_name}.', 'lora_A.')
                 key = key.replace(f'lora_B.{adapter_name}.', 'lora_B.')
-                key = key.replace(f'lora_embedding_A.{adapter_name}.',
-                                  'lora_embedding_A.')
-                key = key.replace(f'lora_embedding_B.{adapter_name}.',
-                                  'lora_embedding_B.')
+                key = key.replace(f'lora_embedding_A.{adapter_name}.', 'lora_embedding_A.')
+                key = key.replace(f'lora_embedding_B.{adapter_name}.', 'lora_embedding_B.')
                 new_state_dict[key] = value
             state_dicts = new_state_dict
         return state_dicts
 
     def __getattr__(self, name: str):
         """Forward missing attributes to the wrapped module."""
         try:
@@ -261,53 +233,48 @@
         return None
 
     def create_optimizer_param_groups(self, **defaults):
         all_param_names = set()
         param_groups = []
         for output in self.adapters.values():
             if output.optimizer_group_callback:
-                param_names, param_group = output.optimizer_group_callback(
-                    self.model, **defaults)
+                param_names, param_group = output.optimizer_group_callback(self.model, **defaults)
                 if param_names and all_param_names & param_names:
-                    raise ValueError(
-                        'Cannot set one parameter to different param groups')
+                    raise ValueError('Cannot set one parameter to different param groups')
                 if param_names and param_group:
                     all_param_names.update(param_names)
                     param_groups.extend(param_group)
 
         decay_parameters = Trainer.get_decay_parameter_names(None, self.model)
         param_groups.extend([
             {
                 'params': [
                     p for n, p in self.model.named_parameters()
-                    if (n in decay_parameters and n not in all_param_names
-                        and p.requires_grad)
+                    if (n in decay_parameters and n not in all_param_names and p.requires_grad)
                 ],
                 'weight_decay':
                 defaults['weight_decay'],
             },
             {
                 'params': [
                     p for n, p in self.model.named_parameters()
-                    if (n not in decay_parameters and n not in all_param_names
-                        and p.requires_grad)
+                    if (n not in decay_parameters and n not in all_param_names and p.requires_grad)
                 ],
                 'weight_decay':
                 0.0,
             },
         ])
 
         return param_groups
 
     @classmethod
     def from_pretrained(cls,
                         model: Union[nn.Module, 'SwiftModel'],
                         model_id: str = None,
-                        adapter_name: Union[str, List[str], Dict[str,
-                                                                 str]] = None,
+                        adapter_name: Union[str, List[str], Dict[str, str]] = None,
                         inference_mode: bool = False,
                         revision: str = None,
                         **kwargs):
         """Load a set of tuners and corresponding weights by a model_id.
 
         Args:
             model (`Union[torch.nn.Module, 'SwiftModel']`): The model to be tuned,
@@ -324,30 +291,24 @@
             The `SwiftModel` instance.
         """
         adapters = {}
         model_dir = model_id
         if not os.path.exists(model_dir):
             model_dir = snapshot_download(model_dir, revision=revision)
         if os.path.isfile(model_dir):
-            raise ValueError(
-                f'Please pass in a local dir or a model id, not a local file: {model_dir}'
-            )
+            raise ValueError(f'Please pass in a local dir or a model id, not a local file: {model_dir}')
         extra_state_keys = kwargs.pop('extra_state_keys', None)
-        if extra_state_keys is None and os.path.isfile(
-                os.path.join(model_dir, cls.EXTRA_STATE_DIR, CONFIG_NAME)):
-            with open(
-                    os.path.join(model_dir, cls.EXTRA_STATE_DIR, CONFIG_NAME),
-                    'r') as file:
+        if extra_state_keys is None and os.path.isfile(os.path.join(model_dir, cls.EXTRA_STATE_DIR, CONFIG_NAME)):
+            with open(os.path.join(model_dir, cls.EXTRA_STATE_DIR, CONFIG_NAME), 'r') as file:
                 _json = json.load(file)
                 extra_state_keys = _json.get('extra_state_keys')
         if adapter_name is None:
             adapter_name = [
-                sub_dir for sub_dir in os.listdir(model_dir) if
-                os.path.isfile(os.path.join(model_dir, sub_dir, CONFIG_NAME))
-                and sub_dir != cls.EXTRA_STATE_DIR
+                sub_dir for sub_dir in os.listdir(model_dir)
+                if os.path.isfile(os.path.join(model_dir, sub_dir, CONFIG_NAME)) and sub_dir != cls.EXTRA_STATE_DIR
             ]
         for _name in adapter_name if isinstance(adapter_name,
                                                 list) else [adapter_name] \
                 if isinstance(adapter_name, str) else adapter_name.keys():
             sub_folder = os.path.join(model_dir, _name)
             config_file = os.path.join(sub_folder, CONFIG_NAME)
 
@@ -357,66 +318,53 @@
 
             with open(config_file, 'r') as file:
                 json_object = json.load(file)
 
             if SWIFT_TYPE_KEY not in json_object:
                 raise ValueError('Mixed using with peft is not allowed now.')
             else:
-                key = _name if not isinstance(adapter_name,
-                                              dict) else adapter_name[_name]
+                key = _name if not isinstance(adapter_name, dict) else adapter_name[_name]
                 adapters[key] = SwiftConfig.from_pretrained(sub_folder)
 
-        self = SwiftModel(model, adapters, extra_state_keys, inference_mode,
-                          **kwargs)
+        self = SwiftModel(model, adapters, extra_state_keys, inference_mode, **kwargs)
         for _name in adapter_name if isinstance(adapter_name,
                                                 list) else [adapter_name] \
                 if isinstance(adapter_name, str) else adapter_name.keys():
             sub_folder = os.path.join(model_dir, _name)
             state_dict = cls.load_state_file(sub_folder)
-            _adapter = _name if not isinstance(adapter_name,
-                                               dict) else adapter_name[_name]
+            _adapter = _name if not isinstance(adapter_name, dict) else adapter_name[_name]
             if state_dict is not None:
                 model_is_qlora = len([
                     k for k in self.state_dict().keys()
-                    if k.endswith(f'.lora_A.{_adapter}.weight')
-                    or k.endswith(f'.lora_B.{_adapter}.weight')
+                    if k.endswith(f'.lora_A.{_adapter}.weight') or k.endswith(f'.lora_B.{_adapter}.weight')
                 ])
                 if not model_is_qlora:
                     # model is lora, state_dict: qlora->lora
                     state_dict = {
-                        k[:-len(f'.{_name}.weight') if k.
-                          endswith(f'.lora_A.{_name}.weight') or k.
+                        k[:-len(f'.{_name}.weight') if k.endswith(f'.lora_A.{_name}.weight') or k.
                           endswith(f'.lora_B.{_name}.weight') else None]: v
                         for k, v in state_dict.items()
                     }
                 if any(['loramodule' in key for key in state_dict]):
                     # Compatible with old checkpoints before ms-swift:1.5.0
                     state_dict = {
-                        key.replace(f'loramodule_{_name}.lora_A', 'lora_A')
-                        if f'loramodule_{_name}.lora_A.{_name}' in key else
-                        key.replace(f'loramodule_{_name}.lora_A',
-                                    f'lora_A.{_name}.weight'): value
+                        key.replace(f'loramodule_{_name}.lora_A', 'lora_A') if f'loramodule_{_name}.lora_A.{_name}'
+                        in key else key.replace(f'loramodule_{_name}.lora_A', f'lora_A.{_name}.weight'): value
                         for key, value in state_dict.items()
                     }
                     state_dict = {
-                        key.replace(f'loramodule_{_name}.lora_B', 'lora_B')
-                        if f'loramodule_{_name}.lora_B.{_name}' in key else
-                        key.replace(f'loramodule_{_name}.lora_B',
-                                    f'lora_B.{_name}.weight'): value
+                        key.replace(f'loramodule_{_name}.lora_B', 'lora_B') if f'loramodule_{_name}.lora_B.{_name}'
+                        in key else key.replace(f'loramodule_{_name}.lora_B', f'lora_B.{_name}.weight'): value
                         for key, value in state_dict.items()
                     }
                 if isinstance(adapter_name, dict):
                     # TODO this logic is fragile! replace `_name` may cause other parts replaced
-                    state_dict = {
-                        key.replace(_name, adapter_name[_name]): value
-                        for key, value in state_dict.items()
-                    }
+                    state_dict = {key.replace(_name, adapter_name[_name]): value for key, value in state_dict.items()}
                 self.load_state_dict(state_dict, adapter_name=_adapter)
-        state_dict = cls.load_state_file(
-            os.path.join(model_dir, self.EXTRA_STATE_DIR))
+        state_dict = cls.load_state_file(os.path.join(model_dir, self.EXTRA_STATE_DIR))
         if state_dict is not None:
             self.load_state_dict(state_dict)
         return self
 
     @classmethod
     def _prepare_model(
         cls,
@@ -424,16 +372,15 @@
         config: SwiftConfig,
         adapter_name: str,
     ):
         assert (hasattr(config, SWIFT_TYPE_KEY))
         from .mapping import SWIFT_MAPPING
 
         adatper_cls = SWIFT_MAPPING[config.swift_type][1]
-        if adatper_cls.has_additional_modules() and not getattr(
-                model, 'model_frozen', False):
+        if adatper_cls.has_additional_modules() and not getattr(model, 'model_frozen', False):
             for _, p in model.named_parameters():
                 p.requires_grad = False
             model.model_frozen = True
         config.has_additional_modules = adatper_cls.has_additional_modules()
         return adatper_cls.prepare_model(model, config, adapter_name)
 
     def create_or_update_model_card(self, output_dir: str):
@@ -443,52 +390,39 @@
         if not os.path.exists(os.path.join(output_dir, 'README.md')):
             lines = []
         else:
             with open(os.path.join(output_dir, 'README.md'), 'r') as f:
                 lines = f.readlines()
 
         quantization_config = None
-        if hasattr(self.base_model, 'config') and hasattr(
-                self.base_model.config, 'quantization_config'):
+        if hasattr(self.base_model, 'config') and hasattr(self.base_model.config, 'quantization_config'):
             if hasattr(self.base_model.config.quantization_config, 'to_dict'):
-                quantization_config = self.base_model.config.quantization_config.to_dict(
-                )
+                quantization_config = self.base_model.config.quantization_config.to_dict()
         training_config_text = ''
         # Adds quantization information if it was used
         if quantization_config is not None:
             training_config_text += '\nThe following `bitsandbytes` quantization config was used during training:\n'
-            training_config_text += '\n'.join([
-                f'- {name}: {value}'
-                for name, value in quantization_config.items()
-            ])
+            training_config_text += '\n'.join([f'- {name}: {value}' for name, value in quantization_config.items()])
             training_config_text += '\n'
 
         training_procedure_heading = '## Training procedure\n'
         if training_procedure_heading in lines:
-            lines.insert(
-                lines.index(training_procedure_heading) + 2,
-                training_config_text)
+            lines.insert(lines.index(training_procedure_heading) + 2, training_config_text)
         else:
-            lines.append(
-                f'{training_procedure_heading}\n{training_config_text}')
+            lines.append(f'{training_procedure_heading}\n{training_config_text}')
 
         framework_block_heading = '### Framework versions\n'
         from swift.version import __version__
         if framework_block_heading in lines:
-            lines.insert(
-                lines.index(framework_block_heading) + 2,
-                f'- SWIFT {__version__}\n')
+            lines.insert(lines.index(framework_block_heading) + 2, f'- SWIFT {__version__}\n')
         else:
-            lines.append(
-                f'{framework_block_heading}\n\n- SWIFT {__version__}\n')
+            lines.append(f'{framework_block_heading}\n\n- SWIFT {__version__}\n')
 
         base_model_heading = '### Base model information\n'
-        lines.append(
-            f'{base_model_heading}\n\n- BaseModel Class {self.base_model.__class__.__name__}\n'
-        )
+        lines.append(f'{base_model_heading}\n\n- BaseModel Class {self.base_model.__class__.__name__}\n')
 
         # write the lines back to README.md
         with open(os.path.join(output_dir, 'README.md'), 'w') as f:
             f.writelines(lines)
 
     def add_weighted_adapter(
         self,
@@ -540,22 +474,18 @@
                 `magnintude_prune`, `magnitude_prune_svd`]
             majority_sign_method (`str`):
                 The method, should be one of ["total", "frequency"], to use to get the magnitude of the sign values.
                 Should be used with [`ties`, `ties_svd`, `dare_ties`, `dare_ties_svd`]
         """
         from swift.tuners.lora import LoraModel
         lora_model = LoraModel(self.model, None, '')
-        lora_model.peft_config = {
-            key: value.config
-            for key, value in self.adapters.items()
-        }
+        lora_model.peft_config = {key: value.config for key, value in self.adapters.items()}
         from peft.tuners.lora import LoraLayer
         lora_model.targeted_module_names = [
-            key for key, value in self.model.named_modules()
-            if isinstance(value, LoraLayer)
+            key for key, value in self.model.named_modules() if isinstance(value, LoraLayer)
         ]
         lora_model.active_adapter = self.active_adapters
         lora_model.add_weighted_adapter(
             adapters=adapters,
             weights=weights,
             adapter_name=adapter_name,
             combination_type=combination_type,
@@ -596,108 +526,78 @@
         Args:
             save_directory (`str`): The directory to use.
             safe_serialization (`bool`): Use safe tensors to save the weights, default False.
             adapter_name(`Union[str, List[str]]`): The adapters to be saved, default is `None` to save all.
         """
         peft_format = kwargs.pop('peft_format', False)
         if os.path.isfile(save_directory):
-            raise ValueError(
-                f'Provided path ({save_directory}) should be a directory, not a file'
-            )
+            raise ValueError(f'Provided path ({save_directory}) should be a directory, not a file')
         os.makedirs(save_directory, exist_ok=True)
         if not self.has_additional_modules:
             if hasattr(self.base_model, 'save_pretrained'):
-                self.base_model.save_pretrained(
-                    save_directory, safe_serialization=safe_serialization)
+                self.base_model.save_pretrained(save_directory, safe_serialization=safe_serialization)
             else:
-                self._save_state_dict(self.base_model.state_dict(),
-                                      save_directory, safe_serialization)
+                self._save_state_dict(self.base_model.state_dict(), save_directory, safe_serialization)
                 self.create_or_update_model_card(save_directory)
         else:
             self.create_or_update_model_card(save_directory)
 
-        adapter_names = adapter_name if isinstance(
-            adapter_name, list) or adapter_name is None else [adapter_name]
+        adapter_names = adapter_name if isinstance(adapter_name, list) or adapter_name is None else [adapter_name]
 
         state_dict_kwargs = {}
         state_dict = kwargs.get('state_dict')
         if state_dict is not None:
             state_dict_kwargs['state_dict'] = kwargs['state_dict']
         for adapter_name, output in self.adapters.items():
             if adapter_names is not None and adapter_name not in adapter_names:
                 continue
             save_to_peft = peft_format and output.config.swift_type == SwiftTuners.LORA
-            save_to_peft = save_to_peft and output.config.can_be_saved_to_peft(
-            )
+            save_to_peft = save_to_peft and output.config.can_be_saved_to_peft()
             if peft_format and not save_to_peft:
-                logger.error(
-                    'You are using additional lora parameters, which is not compatible with peft,'
-                    'which is unable to save to peft format.')
+                logger.error('You are using additional lora parameters, which is not compatible with peft,'
+                             'which is unable to save to peft format.')
             # save only the trainable weights
             output_state_dict = self.state_dict(
-                adapter_name=adapter_name,
-                save_extra_states=False,
-                peft_format=save_to_peft,
-                **state_dict_kwargs)
-            output_dir = os.path.join(
-                save_directory, adapter_name
-            ) if adapter_name != 'default' or not save_to_peft else save_directory
+                adapter_name=adapter_name, save_extra_states=False, peft_format=save_to_peft, **state_dict_kwargs)
+            output_dir = os.path.join(save_directory,
+                                      adapter_name) if adapter_name != 'default' or not save_to_peft else save_directory
             os.makedirs(output_dir, exist_ok=True)
             if output_state_dict and output.config.has_additional_modules:
-                self._save_state_dict(output_state_dict, output_dir,
-                                      safe_serialization)
+                self._save_state_dict(output_state_dict, output_dir, safe_serialization)
             if save_to_peft:
                 config = output.config.to_peft_config()
                 config.save_pretrained(output_dir)
             else:
                 output.config.save_pretrained(output_dir)
 
-        output_state_dict = self.state_dict(
-            save_extra_states=True, save_adapter=False, **state_dict_kwargs)
+        output_state_dict = self.state_dict(save_extra_states=True, save_adapter=False, **state_dict_kwargs)
         if len(output_state_dict) > 0:
             if self.has_additional_modules:
-                os.makedirs(
-                    os.path.join(save_directory, self.EXTRA_STATE_DIR),
-                    exist_ok=True)
-                self._save_state_dict(
-                    output_state_dict,
-                    os.path.join(save_directory, self.EXTRA_STATE_DIR),
-                    safe_serialization)
-                with open(
-                        os.path.join(save_directory, self.EXTRA_STATE_DIR,
-                                     CONFIG_NAME), 'w') as file:
-                    json.dump({'extra_state_keys': self.extra_state_keys},
-                              file)
+                os.makedirs(os.path.join(save_directory, self.EXTRA_STATE_DIR), exist_ok=True)
+                self._save_state_dict(output_state_dict, os.path.join(save_directory, self.EXTRA_STATE_DIR),
+                                      safe_serialization)
+                with open(os.path.join(save_directory, self.EXTRA_STATE_DIR, CONFIG_NAME), 'w') as file:
+                    json.dump({'extra_state_keys': self.extra_state_keys}, file)
             else:
-                logger.error(
-                    'Full parameter training, save_extra_states will be ignored'
-                )
-
-        if not os.path.exists(
-                os.path.join(save_directory, 'configuration.json')):
-            with open(os.path.join(save_directory, 'configuration.json'),
-                      'w') as f:
+                logger.error('Full parameter training, save_extra_states will be ignored')
+
+        if not os.path.exists(os.path.join(save_directory, 'configuration.json')):
+            with open(os.path.join(save_directory, 'configuration.json'), 'w') as f:
                 f.write('{}')
 
     @staticmethod
-    def _save_state_dict(output_state_dict, save_directory,
-                         safe_serialization):
+    def _save_state_dict(output_state_dict, save_directory, safe_serialization):
         if safe_serialization:
             from safetensors.torch import save_file as safe_save_file
             safe_save_file(
-                output_state_dict,
-                os.path.join(save_directory, SAFETENSORS_WEIGHTS_NAME),
-                metadata={'format': 'pt'})
+                output_state_dict, os.path.join(save_directory, SAFETENSORS_WEIGHTS_NAME), metadata={'format': 'pt'})
         else:
-            torch.save(output_state_dict,
-                       os.path.join(save_directory, WEIGHTS_NAME))
+            torch.save(output_state_dict, os.path.join(save_directory, WEIGHTS_NAME))
 
-    def set_active_adapters(self,
-                            adapter_names: Union[List[str], str],
-                            offload: str = None):
+    def set_active_adapters(self, adapter_names: Union[List[str], str], offload: str = None):
         """Set activated adapters
 
         Args:
             adapter_names(`Union[List[str], str]`): The adapters needed to be activated
             offload(`str`): Whether to offload the deactivated ones to `cpu` or `meta` device
         """
         if not adapter_names:
@@ -718,16 +618,15 @@
     def activate_adapter(self, adapter_name: str):
         """Activate one adapter
 
         Args:
             adapter_name(`str`): The adapter needed to be activated
         """
         if adapter_name not in self.adapters:
-            logger.warning(
-                f'{adapter_name} not in adapters: {self.adapters.keys()}')
+            logger.warning(f'{adapter_name} not in adapters: {self.adapters.keys()}')
             return
 
         from .mapping import SWIFT_MAPPING
         SWIFT_MAPPING[self.adapters[adapter_name].config.swift_type][1]\
             .activate_adapter(self.base_model, adapter_name, True)
         self.active_adapters = self.active_adapters | {adapter_name}
 
@@ -735,16 +634,15 @@
         """Deactivate one adapter
 
         Args:
             adapter_name(`str`): The adapter needed to be activated
             offload(`str`): Whether to offload to `cpu` or `meta` device
         """
         if adapter_name not in self.adapters:
-            logger.warning(
-                f'{adapter_name} not in adapters: {self.adapters.keys()}')
+            logger.warning(f'{adapter_name} not in adapters: {self.adapters.keys()}')
             return
 
         from .mapping import SWIFT_MAPPING
         SWIFT_MAPPING[self.adapters[adapter_name].config.swift_type][1]\
             .activate_adapter(self.base_model, adapter_name, False, offload=offload)
         self.active_adapters = self.active_adapters - {adapter_name}
 
@@ -770,17 +668,16 @@
                'GiB.'
 
 
 class Swift:
     """The Wrapper to use both Peft and Swift tuners."""
 
     @staticmethod
-    def prepare_model(model: Union[nn.Module, SwiftModel],
-                      config: Union[SwiftConfig, PeftConfig,
-                                    Dict[str, SwiftConfig]], **kwargs):
+    def prepare_model(model: Union[nn.Module, SwiftModel], config: Union[SwiftConfig, PeftConfig,
+                                                                         Dict[str, SwiftConfig]], **kwargs):
         """Prepare a model by the input config.
 
         Args:
             model(`Union[nn.Module, 'SwiftModel']`): The model to be tuned.
             config(`Union[SwiftConfig, PeftConfig, Dict[str, SwiftConfig]]`): The config or config dict, can be either
                 SwiftConfigs or PeftConfigs
             **kwargs:
@@ -810,17 +707,15 @@
         elif isinstance(model, SwiftModel):
             from swift import LoRAConfig
             from swift.tuners import LoRA
             adapter_name = kwargs.get('adapter_name', None)
             if isinstance(adapter_name, str):
                 adapter_name = [adapter_name]
             for adapter, output in model.adapters.items():
-                if isinstance(output.config,
-                              LoRAConfig) and (adapter_name is None
-                                               or adapter in adapter_name):
+                if isinstance(output.config, LoRAConfig) and (adapter_name is None or adapter in adapter_name):
                     LoRA.unpatch_lora(model, output.config, adapter)
 
     @staticmethod
     def merge(model: Union[PeftModel, SwiftModel], **kwargs):
         """Merge tuners into the base model, will not unload them.
 
         Args:
@@ -848,93 +743,73 @@
         """Save swift format to peft format
 
         Args:
             ckpt_dir(`str`): Original swift output dir
             output_dir(`str`): Converted peft format dir
         """
         assert ckpt_dir and output_dir, 'Please pass in valid ckpt_dir and output_dir.'
-        assert os.path.exists(
-            ckpt_dir), f'ckpt_dir: {ckpt_dir} must exists in local disk.'
+        assert os.path.exists(ckpt_dir), f'ckpt_dir: {ckpt_dir} must exists in local disk.'
         if os.path.exists(os.path.join(ckpt_dir, SwiftModel.EXTRA_STATE_DIR)):
-            raise AssertionError(
-                'Cannot transfer to peft format, because you are additional state dicts.'
-            )
+            raise AssertionError('Cannot transfer to peft format, because you are additional state dicts.')
 
         adapter_names = [
-            sub_dir for sub_dir in os.listdir(ckpt_dir)
-            if os.path.isfile(os.path.join(ckpt_dir, sub_dir, CONFIG_NAME))
+            sub_dir for sub_dir in os.listdir(ckpt_dir) if os.path.isfile(os.path.join(ckpt_dir, sub_dir, CONFIG_NAME))
         ]
 
         def has_custom_content(_json):
-            if _json.get('swift_type',
-                         _json.get('peft_type')) != SwiftTuners.LORA:
+            if _json.get('swift_type', _json.get('peft_type')) != SwiftTuners.LORA:
                 logger.warn('Only LoRA can be converted to peft format')
                 return True
 
             from swift import LoRAConfig
             return not LoRAConfig(**_json).can_be_saved_to_peft()
 
         for adapter in adapter_names:
             with open(os.path.join(ckpt_dir, adapter, CONFIG_NAME)) as f:
                 _json = json.load(f)
                 if has_custom_content(_json):
-                    raise AssertionError(
-                        'Cannot transfer to peft format, '
-                        'because you have special parameters or adapter types.'
-                    )
+                    raise AssertionError('Cannot transfer to peft format, '
+                                         'because you have special parameters or adapter types.')
 
         os.makedirs(output_dir, exist_ok=True)
         if ckpt_dir != output_dir:
             shutil.copytree(ckpt_dir, output_dir, dirs_exist_ok=True)
 
         for adapter in adapter_names:
-            safe_serialization = os.path.isfile(
-                os.path.join(output_dir, adapter, SAFETENSORS_WEIGHTS_NAME))
-            state_dict = SwiftModel.load_state_file(
-                os.path.join(output_dir, adapter))
+            safe_serialization = os.path.isfile(os.path.join(output_dir, adapter, SAFETENSORS_WEIGHTS_NAME))
+            state_dict = SwiftModel.load_state_file(os.path.join(output_dir, adapter))
             new_state_dict = {}
             for key, value in state_dict.items():
                 if not key.startswith('base_model.model.'):
                     key = 'base_model.model.' + key
                 key = key.replace(f'lora_A.{adapter}.', 'lora_A.')
                 key = key.replace(f'lora_B.{adapter}.', 'lora_B.')
-                key = key.replace(f'lora_embedding_A.{adapter}.',
-                                  'lora_embedding_A.')
-                key = key.replace(f'lora_embedding_B.{adapter}.',
-                                  'lora_embedding_B.')
-                key = key.replace(f'lora_magnitude_vector.{adapter}',
-                                  'lora_magnitude_vector')
+                key = key.replace(f'lora_embedding_A.{adapter}.', 'lora_embedding_A.')
+                key = key.replace(f'lora_embedding_B.{adapter}.', 'lora_embedding_B.')
+                key = key.replace(f'lora_magnitude_vector.{adapter}', 'lora_magnitude_vector')
                 new_state_dict[key] = value
             state_dict = new_state_dict
-            SwiftModel._save_state_dict(state_dict,
-                                        os.path.join(output_dir, adapter),
-                                        safe_serialization)
+            SwiftModel._save_state_dict(state_dict, os.path.join(output_dir, adapter), safe_serialization)
             from swift import LoRAConfig
             with open(os.path.join(output_dir, adapter, CONFIG_NAME)) as f:
                 _json = json.load(f)
                 peft_config = LoRAConfig(**_json).to_peft_config()
             peft_config.save_pretrained(os.path.join(output_dir, adapter))
 
         if 'default' in adapter_names:
-            shutil.move(
-                os.path.join(output_dir, 'default', CONFIG_NAME),
-                os.path.join(output_dir, CONFIG_NAME))
-            state_dict = SwiftModel.load_state_file(
-                os.path.join(output_dir, 'default'))
-            safe_serialization = os.path.isfile(
-                os.path.join(output_dir, 'default', SAFETENSORS_WEIGHTS_NAME))
-            SwiftModel._save_state_dict(state_dict, output_dir,
-                                        safe_serialization)
+            shutil.move(os.path.join(output_dir, 'default', CONFIG_NAME), os.path.join(output_dir, CONFIG_NAME))
+            state_dict = SwiftModel.load_state_file(os.path.join(output_dir, 'default'))
+            safe_serialization = os.path.isfile(os.path.join(output_dir, 'default', SAFETENSORS_WEIGHTS_NAME))
+            SwiftModel._save_state_dict(state_dict, output_dir, safe_serialization)
             shutil.rmtree(os.path.join(output_dir, 'default'))
 
     @staticmethod
     def from_pretrained(model: Union[nn.Module, SwiftModel],
                         model_id: str = None,
-                        adapter_name: Union[str, List[str], Dict[str,
-                                                                 str]] = None,
+                        adapter_name: Union[str, List[str], Dict[str, str]] = None,
                         revision: str = None,
                         **kwargs):
         """Prepare a model by a model_id in the ModelScope hub or a local dir.
 
         Args:
             model(`Union[nn.Module, 'SwiftModel']`): The model to be tuned.
             model_id(`str`): The model id of the modelhub or a local dir containing the configs/weights.
@@ -959,19 +834,10 @@
         _name = _name or ''
         if os.path.exists(os.path.join(model_id, _name, CONFIG_NAME)):
             with open(os.path.join(model_id, _name, CONFIG_NAME), 'r') as f:
                 _json = json.load(f)
             is_peft_model = SWIFT_TYPE_KEY not in _json and 'extra_state_keys' not in _json
         if is_peft_model:
             return PeftModel.from_pretrained(
-                model,
-                model_id,
-                revision=revision,
-                adapter_name=adapter_name or 'default',
-                **kwargs)
+                model, model_id, revision=revision, adapter_name=adapter_name or 'default', **kwargs)
         else:
-            return SwiftModel.from_pretrained(
-                model,
-                model_id,
-                revision=revision,
-                adapter_name=adapter_name,
-                **kwargs)
+            return SwiftModel.from_pretrained(model, model_id, revision=revision, adapter_name=adapter_name, **kwargs)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/llamapro.py` & `ms-swift-2.0.4/swift/tuners/llamapro.py`

 * *Files 3% similar despite different names*

```diff
@@ -40,16 +40,15 @@
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.LLAMAPRO
 
 
 class LLaMAPro(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: LLaMAProConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: nn.Module, config: LLaMAProConfig, adapter_name: str) -> SwiftOutput:
         """Prepare a model with `LLaMAProConfig`"""
         num_hidden_layers = None
         if hasattr(model.config, 'num_hidden_layers'):
             num_hidden_layers = model.config.num_hidden_layers
         elif hasattr(model.config, 'num_layers'):
             num_hidden_layers = model.config.num_layers
 
@@ -74,82 +73,69 @@
 
         LLaMAPro._update_module_weight(config, new_module_list, new_module_idx)
         LLaMAPro._update_module_attr(config, new_module_list)
         model.config.num_hidden_layers = len(new_module_list)
         LLaMAPro._set_module_list(config, model, new_module_list)
 
         def state_dict_callback(state_dict, adapter_name):
-            model_key_mapping = LLaMAPro._get_model_key_mapping(
-                config.model_type, config)
-            new_module_list = [
-                model_key_mapping.module_list + f'.{i}' for i in new_module_idx
-            ]
+            model_key_mapping = LLaMAPro._get_model_key_mapping(config.model_type, config)
+            new_module_list = [model_key_mapping.module_list + f'.{i}' for i in new_module_idx]
             return {
                 key: value
-                for key, value in state_dict.items()
-                if any([m_part in key for m_part in new_module_list])
+                for key, value in state_dict.items() if any([m_part in key for m_part in new_module_list])
             }
 
         def mark_trainable_callback(model):
-            model_key_mapping = LLaMAPro._get_model_key_mapping(
-                config.model_type, config)
-            new_module_list = [
-                model_key_mapping.module_list + f'.{i}' for i in new_module_idx
-            ]
+            model_key_mapping = LLaMAPro._get_model_key_mapping(config.model_type, config)
+            new_module_list = [model_key_mapping.module_list + f'.{i}' for i in new_module_idx]
             for name, parameter in model.named_parameters():
                 parameter: nn.Parameter
                 if any([m_part in name for m_part in new_module_list]):
                     parameter.requires_grad = True
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
     def _get_model_key_mapping(model_type, config) -> ModelKeys:
         if model_type in MODEL_KEYS_MAPPING.keys():
             model_key_mapping = MODEL_KEYS_MAPPING[model_type]
         else:
             model_key_mapping = config.model_key_mapping
 
         if model_key_mapping is None:
-            raise ValueError(
-                f'{model_type} is not defined in MODEL_KEYS_MAPPING, '
-                f'please consider pass the information through the config.model_key_mapping'
-            )
+            raise ValueError(f'{model_type} is not defined in MODEL_KEYS_MAPPING, '
+                             f'please consider pass the information through the config.model_key_mapping')
 
         if isinstance(model_key_mapping, dict):
             model_key_mapping: ModelKeys = ModelKeys(**model_key_mapping)
 
         assert model_key_mapping.o_proj is not None and model_key_mapping.down_proj is not None, \
             'LLaMAPro only support models with o_proj and down_proj components.'
         return model_key_mapping
 
     @staticmethod
     def _update_module_attr(config: LLaMAProConfig, module_list):
         model_type = config.model_type
         model_key_mapping = LLaMAPro._get_model_key_mapping(model_type, config)
         attention = model_key_mapping.attention
         attention = attention.split('{}.')[1]
-        if model_type in ('llama', 'mistral', 'qwen2', 'yi', 'gemma',
-                          'deepseek', 'openbuddy', 'xverse', 'orion', 'bluelm',
-                          'ziya', 'skywork'):
+        if model_type in ('llama', 'mistral', 'qwen2', 'yi', 'gemma', 'deepseek', 'openbuddy', 'xverse', 'orion',
+                          'bluelm', 'ziya', 'skywork'):
             for idx, module in enumerate(module_list):
                 getattr(module, attention).layer_idx = idx
         elif model_type in ('chatglm', ):
             for idx, module in enumerate(module_list):
                 getattr(module, attention).layer_number = idx
         elif model_type in ('phi2', ):
             for idx, module in enumerate(module_list):
                 getattr(module, attention).block_idx = idx
 
     @staticmethod
-    def _update_module_weight(config: LLaMAProConfig, module_list,
-                              new_module_idx):
-        model_key_mapping = LLaMAPro._get_model_key_mapping(
-            config.model_type, config)
+    def _update_module_weight(config: LLaMAProConfig, module_list, new_module_idx):
+        model_key_mapping = LLaMAPro._get_model_key_mapping(config.model_type, config)
         o_proj = model_key_mapping.o_proj.split('{}.')[1]
         down_proj = model_key_mapping.o_proj.split('{}.')[1]
 
         for idx, module in enumerate(module_list):
             if idx not in new_module_idx:
                 continue
             _o_proj: nn.Linear = module.get_submodule(o_proj)
@@ -158,33 +144,27 @@
             _down_proj.weight.data = torch.zeros_like(_down_proj.weight.data)
             if hasattr(_o_proj, 'bias') and _o_proj.bias:
                 _o_proj.bias = torch.zeros_like(_o_proj.bias)
             if hasattr(_down_proj, 'bias') and _down_proj.bias:
                 _down_proj.bias = torch.zeros_like(_down_proj.bias)
 
     @staticmethod
-    def _set_module_list(config, module: nn.Module,
-                         module_list: nn.ModuleList):
-        model_key_mapping = LLaMAPro._get_model_key_mapping(
-            config.model_type, config)
+    def _set_module_list(config, module: nn.Module, module_list: nn.ModuleList):
+        model_key_mapping = LLaMAPro._get_model_key_mapping(config.model_type, config)
         idx = model_key_mapping.module_list.rfind('.')
         parent = module.get_submodule(model_key_mapping.module_list[:idx])
         setattr(parent, model_key_mapping.module_list[idx + 1:], module_list)
 
     @staticmethod
     def _find_module_list(config, module: nn.Module) -> nn.ModuleList:
-        model_key_mapping = LLaMAPro._get_model_key_mapping(
-            config.model_type, config)
+        model_key_mapping = LLaMAPro._get_model_key_mapping(config.model_type, config)
         return module.get_submodule(model_key_mapping.module_list)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         for sub_module in module.modules():
             if isinstance(sub_module, torch.nn.Embedding):
                 sub_module.nef_activated = activate
 
     @staticmethod
     def has_additional_modules():
         return True
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/longlora/llama.py` & `ms-swift-2.0.4/swift/tuners/longlora/llama.py`

 * *Files 7% similar despite different names*

```diff
@@ -5,225 +5,170 @@
 from types import MethodType
 from typing import Optional, Tuple
 
 import torch
 import torch.nn.functional as F
 from torch import nn
 from transformers import Cache
-from transformers.models.llama.modeling_llama import (apply_rotary_pos_emb,
-                                                      repeat_kv)
+from transformers.models.llama.modeling_llama import apply_rotary_pos_emb, repeat_kv
 
 from swift.utils import get_logger
 
 logger = get_logger()
 
 
-def _preprocess_qkv_fa2(attn_module, query_states, key_states, value_states,
-                        attention_mask):
+def _preprocess_qkv_fa2(attn_module, query_states, key_states, value_states, attention_mask):
     if attn_module.training:
         bsz, q_len = query_states.shape[:2]
         group_size = int(q_len * attn_module.config.group_size_ratio)
         if q_len % group_size != 0:
-            raise ValueError(
-                f'The sequence length {q_len} should'
-                f'be able to be splitted by the group_ratio {attn_module.config.group_size_ratio}'
-            )
+            raise ValueError(f'The sequence length {q_len} should'
+                             f'be able to be splitted by the group_ratio {attn_module.config.group_size_ratio}')
 
         num_group = q_len // group_size
 
         def shift(qkv, bsz, q_len, group_size, num_heads, head_dim):
-            qkv[:, :, num_heads // 2:] = qkv[:, :, num_heads // 2:].roll(
-                -group_size // 2, dims=1)
+            qkv[:, :, num_heads // 2:] = qkv[:, :, num_heads // 2:].roll(-group_size // 2, dims=1)
             qkv = qkv.reshape(bsz * num_group, group_size, num_heads, head_dim)
             return qkv
 
-        query_states = shift(query_states, bsz, q_len, group_size,
-                             attn_module.num_heads, attn_module.head_dim)
-        key_states = shift(key_states, bsz, q_len, group_size,
-                           attn_module.num_heads, attn_module.head_dim)
-        value_states = shift(value_states, bsz, q_len, group_size,
-                             attn_module.num_heads, attn_module.head_dim)
+        query_states = shift(query_states, bsz, q_len, group_size, attn_module.num_heads, attn_module.head_dim)
+        key_states = shift(key_states, bsz, q_len, group_size, attn_module.num_heads, attn_module.head_dim)
+        value_states = shift(value_states, bsz, q_len, group_size, attn_module.num_heads, attn_module.head_dim)
         if attention_mask is not None:
-            attention_mask = attention_mask[:, :group_size].repeat(
-                num_group, 1)
+            attention_mask = attention_mask[:, :group_size].repeat(num_group, 1)
 
     return query_states, key_states, value_states, attention_mask
 
 
-def _preprocess_qkv(attn_module, query_states, key_states, value_states,
-                    attention_mask):
+def _preprocess_qkv(attn_module, query_states, key_states, value_states, attention_mask):
     if attn_module.training:
         bsz, _, q_len = query_states.shape[:3]
         group_size = int(q_len * attn_module.config.group_size_ratio)
         if q_len % group_size != 0:
-            raise ValueError(
-                f'The sequence length {q_len} should'
-                f'be able to be splitted by the group_ratio {attn_module.config.group_size_ratio}'
-            )
+            raise ValueError(f'The sequence length {q_len} should'
+                             f'be able to be splitted by the group_ratio {attn_module.config.group_size_ratio}')
 
         num_group = q_len // group_size
 
         def shift(qkv, bsz, q_len, group_size, num_heads, head_dim):
-            qkv[:, num_heads // 2:] = qkv[:, num_heads // 2:].roll(
-                -group_size // 2, dims=2)
+            qkv[:, num_heads // 2:] = qkv[:, num_heads // 2:].roll(-group_size // 2, dims=2)
             qkv = qkv.transpose(1, 2)
             qkv = qkv.reshape(bsz * num_group, group_size, num_heads, head_dim)
             return qkv.transpose(1, 2)
 
-        query_states = shift(query_states, bsz, q_len, group_size,
-                             attn_module.num_heads, attn_module.head_dim)
-        key_states = shift(key_states, bsz, q_len, group_size,
-                           attn_module.num_heads, attn_module.head_dim)
-        value_states = shift(value_states, bsz, q_len, group_size,
-                             attn_module.num_heads, attn_module.head_dim)
+        query_states = shift(query_states, bsz, q_len, group_size, attn_module.num_heads, attn_module.head_dim)
+        key_states = shift(key_states, bsz, q_len, group_size, attn_module.num_heads, attn_module.head_dim)
+        value_states = shift(value_states, bsz, q_len, group_size, attn_module.num_heads, attn_module.head_dim)
         if attention_mask is not None:
-            attention_mask = attention_mask[:, :, :group_size, :
-                                            group_size].repeat(
-                                                num_group, 1, 1, 1)
+            attention_mask = attention_mask[:, :, :group_size, :group_size].repeat(num_group, 1, 1, 1)
 
     return query_states, key_states, value_states, attention_mask
 
 
 def _postprocess_qkv(attn_module, attn_output, q_len):
     if attn_module.training:
         group_size = int(q_len * attn_module.config.group_size_ratio)
         attn_output = attn_output.transpose(1, 2)
-        attn_output = attn_output.reshape(-1, q_len, attn_module.num_heads,
-                                          attn_module.head_dim)
+        attn_output = attn_output.reshape(-1, q_len, attn_module.num_heads, attn_module.head_dim)
         # shift back
-        attn_output[:, :, attn_module.num_heads
-                    // 2:] = attn_output[:, :,
-                                         attn_module.num_heads // 2:].roll(
-                                             group_size // 2, dims=1)
+        attn_output[:, :, attn_module.num_heads // 2:] = attn_output[:, :, attn_module.num_heads // 2:].roll(
+            group_size // 2, dims=1)
     return attn_output.transpose(1, 2)
 
 
 def _postprocess_qkv_fa2(attn_module, attn_output, q_len):
     if attn_module.training:
         group_size = int(q_len * attn_module.config.group_size_ratio)
-        attn_output = attn_output.reshape(-1, q_len, attn_module.num_heads,
-                                          attn_module.head_dim)
+        attn_output = attn_output.reshape(-1, q_len, attn_module.num_heads, attn_module.head_dim)
         # shift back
-        attn_output[:, :, attn_module.num_heads
-                    // 2:] = attn_output[:, :,
-                                         attn_module.num_heads // 2:].roll(
-                                             group_size // 2, dims=1)
+        attn_output[:, :, attn_module.num_heads // 2:] = attn_output[:, :, attn_module.num_heads // 2:].roll(
+            group_size // 2, dims=1)
     return attn_output
 
 
 # code borrowed from https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L316 # noqa
 def eager_forward(
     self,
     hidden_states: torch.Tensor,
     attention_mask: Optional[torch.Tensor] = None,
     position_ids: Optional[torch.LongTensor] = None,
     past_key_value: Optional[Cache] = None,
     output_attentions: bool = False,
     use_cache: bool = False,
     cache_position: Optional[torch.LongTensor] = None,
     **kwargs,
-) -> Tuple[torch.Tensor, Optional[torch.Tensor],
-           Optional[Tuple[torch.Tensor]]]:
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     bsz, q_len, _ = hidden_states.size()
 
     if self.config.pretraining_tp > 1:
-        key_value_slicing = (self.num_key_value_heads
-                             * self.head_dim) // self.config.pretraining_tp
-        query_slices = self.q_proj.weight.split(
-            (self.num_heads * self.head_dim) // self.config.pretraining_tp,
-            dim=0)
+        key_value_slicing = (self.num_key_value_heads * self.head_dim) // self.config.pretraining_tp
+        query_slices = self.q_proj.weight.split((self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0)
         key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)
         value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)
 
-        query_states = [
-            F.linear(hidden_states, query_slices[i])
-            for i in range(self.config.pretraining_tp)
-        ]
+        query_states = [F.linear(hidden_states, query_slices[i]) for i in range(self.config.pretraining_tp)]
         query_states = torch.cat(query_states, dim=-1)
 
-        key_states = [
-            F.linear(hidden_states, key_slices[i])
-            for i in range(self.config.pretraining_tp)
-        ]
+        key_states = [F.linear(hidden_states, key_slices[i]) for i in range(self.config.pretraining_tp)]
         key_states = torch.cat(key_states, dim=-1)
 
-        value_states = [
-            F.linear(hidden_states, value_slices[i])
-            for i in range(self.config.pretraining_tp)
-        ]
+        value_states = [F.linear(hidden_states, value_slices[i]) for i in range(self.config.pretraining_tp)]
         value_states = torch.cat(value_states, dim=-1)
 
     else:
         query_states = self.q_proj(hidden_states)
         key_states = self.k_proj(hidden_states)
         value_states = self.v_proj(hidden_states)
 
-    query_states = query_states.view(bsz, q_len, self.num_heads,
-                                     self.head_dim).transpose(1, 2)
-    key_states = key_states.view(bsz, q_len, self.num_key_value_heads,
-                                 self.head_dim).transpose(1, 2)
-    value_states = value_states.view(bsz, q_len, self.num_key_value_heads,
-                                     self.head_dim).transpose(1, 2)
+    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
+    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
 
     past_key_value = getattr(self, 'past_key_value', past_key_value)
     cos, sin = self.rotary_emb(value_states, position_ids)
-    query_states, key_states = apply_rotary_pos_emb(query_states, key_states,
-                                                    cos, sin)
+    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
 
     if past_key_value is not None:
         # sin and cos are specific to RoPE models; position_ids needed for the static cache
-        cache_kwargs = {
-            'sin': sin,
-            'cos': cos,
-            'cache_position': cache_position
-        }
-        key_states, value_states = past_key_value.update(
-            key_states, value_states, self.layer_idx, cache_kwargs)
+        cache_kwargs = {'sin': sin, 'cos': cos, 'cache_position': cache_position}
+        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
 
     key_states = repeat_kv(key_states, self.num_key_value_groups)
     value_states = repeat_kv(value_states, self.num_key_value_groups)
 
     # patch position rolling
-    query_states, key_states, value_states, attention_mask = _preprocess_qkv(
-        self, query_states, key_states, value_states, attention_mask)
+    query_states, key_states, value_states, attention_mask = _preprocess_qkv(self, query_states, key_states,
+                                                                             value_states, attention_mask)
 
-    attn_weights = torch.matmul(query_states, key_states.transpose(
-        2, 3)) / math.sqrt(self.head_dim)
+    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)
 
     if attention_mask is not None:  # no matter the length, we just slice it
         if cache_position is not None and not self.training:
-            causal_mask = attention_mask[:, :,
-                                         cache_position, :key_states.shape[-2]]
+            causal_mask = attention_mask[:, :, cache_position, :key_states.shape[-2]]
             attn_weights = attn_weights + causal_mask
         else:
             attn_weights = attn_weights + attention_mask
 
     # upcast attention to fp32
-    attn_weights = nn.functional.softmax(
-        attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
-    attn_weights = nn.functional.dropout(
-        attn_weights, p=self.attention_dropout, training=self.training)
+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
+    attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
     attn_output = torch.matmul(attn_weights, value_states)
 
     # patch position unrolling
     attn_output = _postprocess_qkv(self, attn_output, q_len)
 
     attn_output = attn_output.transpose(1, 2).contiguous()
 
     attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)
 
     if self.config.pretraining_tp > 1:
-        attn_output = attn_output.split(
-            self.hidden_size // self.config.pretraining_tp, dim=2)
-        o_proj_slices = self.o_proj.weight.split(
-            self.hidden_size // self.config.pretraining_tp, dim=1)
-        attn_output = sum([
-            F.linear(attn_output[i], o_proj_slices[i])
-            for i in range(self.config.pretraining_tp)
-        ])
+        attn_output = attn_output.split(self.hidden_size // self.config.pretraining_tp, dim=2)
+        o_proj_slices = self.o_proj.weight.split(self.hidden_size // self.config.pretraining_tp, dim=1)
+        attn_output = sum([F.linear(attn_output[i], o_proj_slices[i]) for i in range(self.config.pretraining_tp)])
     else:
         attn_output = self.o_proj(attn_output)
 
     if not output_attentions:
         attn_weights = None
 
     return attn_output, attn_weights, past_key_value
@@ -236,49 +181,39 @@
     attention_mask: Optional[torch.LongTensor] = None,
     position_ids: Optional[torch.LongTensor] = None,
     past_key_value: Optional[Cache] = None,
     output_attentions: bool = False,
     use_cache: bool = False,
     cache_position: Optional[torch.LongTensor] = None,
     **kwargs,
-) -> Tuple[torch.Tensor, Optional[torch.Tensor],
-           Optional[Tuple[torch.Tensor]]]:
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     output_attentions = False
 
     bsz, q_len, _ = hidden_states.size()
 
     query_states = self.q_proj(hidden_states)
     key_states = self.k_proj(hidden_states)
     value_states = self.v_proj(hidden_states)
 
     # Flash attention requires the input to have the shape
     # batch_size x seq_length x head_dim x hidden_dim
     # therefore we just need to keep the original shape
-    query_states = query_states.view(bsz, q_len, self.num_heads,
-                                     self.head_dim).transpose(1, 2)
-    key_states = key_states.view(bsz, q_len, self.num_key_value_heads,
-                                 self.head_dim).transpose(1, 2)
-    value_states = value_states.view(bsz, q_len, self.num_key_value_heads,
-                                     self.head_dim).transpose(1, 2)
+    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
+    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
 
     cos, sin = self.rotary_emb(value_states, position_ids)
-    query_states, key_states = apply_rotary_pos_emb(query_states, key_states,
-                                                    cos, sin)
+    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
 
     past_key_value = getattr(self, 'past_key_value', past_key_value)
 
     if past_key_value is not None:
         # sin and cos are specific to RoPE models; position_ids needed for the static cache
-        cache_kwargs = {
-            'sin': sin,
-            'cos': cos,
-            'cache_position': cache_position
-        }
-        key_states, value_states = past_key_value.update(
-            key_states, value_states, self.layer_idx, cache_kwargs)
+        cache_kwargs = {'sin': sin, 'cos': cos, 'cache_position': cache_position}
+        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
 
     # TODO: These transpose are quite inefficient but Flash Attention
     #  requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache
     # to be able to avoid many of these transpose/reshape/view.
     query_states = query_states.transpose(1, 2)
     key_states = key_states.transpose(1, 2)
     value_states = value_states.transpose(1, 2)
@@ -307,30 +242,24 @@
             f' {target_dtype}.')
 
         query_states = query_states.to(target_dtype)
         key_states = key_states.to(target_dtype)
         value_states = value_states.to(target_dtype)
 
     # patch position rolling
-    query_states, key_states, value_states, attention_mask = _preprocess_qkv_fa2(
-        self, query_states, key_states, value_states, attention_mask)
+    query_states, key_states, value_states, attention_mask = _preprocess_qkv_fa2(self, query_states, key_states,
+                                                                                 value_states, attention_mask)
 
     attn_output = self._flash_attention_forward(
-        query_states,
-        key_states,
-        value_states,
-        attention_mask,
-        query_states.shape[1],
-        dropout=dropout_rate)
+        query_states, key_states, value_states, attention_mask, query_states.shape[1], dropout=dropout_rate)
 
     # patch position unrolling
     attn_output = _postprocess_qkv_fa2(self, attn_output, q_len)
 
-    attn_output = attn_output.reshape(bsz, q_len,
-                                      self.hidden_size).contiguous()
+    attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()
     attn_output = self.o_proj(attn_output)
 
     if not output_attentions:
         attn_weights = None
 
     return attn_output, attn_weights, past_key_value
 
@@ -341,16 +270,15 @@
     hidden_states: torch.Tensor,
     attention_mask: Optional[torch.Tensor] = None,
     position_ids: Optional[torch.LongTensor] = None,
     past_key_value: Optional[Cache] = None,
     output_attentions: bool = False,
     use_cache: bool = False,
     cache_position: Optional[torch.LongTensor] = None,
-) -> Tuple[torch.Tensor, Optional[torch.Tensor],
-           Optional[Tuple[torch.Tensor]]]:
+) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
     if output_attentions:
         # TODO: Improve this warning with e.g. `model.config.attn_implementation = "manual"` once this is implemented.
         logger.warning_once(
             'LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, '  # noqa
             'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'  # noqa
         )
         return super().forward(
@@ -365,37 +293,28 @@
 
     bsz, q_len, _ = hidden_states.size()
 
     query_states = self.q_proj(hidden_states)
     key_states = self.k_proj(hidden_states)
     value_states = self.v_proj(hidden_states)
 
-    query_states = query_states.view(bsz, q_len, self.num_heads,
-                                     self.head_dim).transpose(1, 2)
-    key_states = key_states.view(bsz, q_len, self.num_key_value_heads,
-                                 self.head_dim).transpose(1, 2)
-    value_states = value_states.view(bsz, q_len, self.num_key_value_heads,
-                                     self.head_dim).transpose(1, 2)
+    query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
+    key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
+    value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
 
     cos, sin = self.rotary_emb(value_states, position_ids)
-    query_states, key_states = apply_rotary_pos_emb(query_states, key_states,
-                                                    cos, sin)
+    query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)
 
     # In case static cache is used, it is an instance attribute.
     past_key_value = getattr(self, 'past_key_value', past_key_value)
 
     if past_key_value is not None:
         # sin and cos are specific to RoPE models; position_ids needed for the static cache
-        cache_kwargs = {
-            'sin': sin,
-            'cos': cos,
-            'cache_position': cache_position
-        }
-        key_states, value_states = past_key_value.update(
-            key_states, value_states, self.layer_idx, cache_kwargs)
+        cache_kwargs = {'sin': sin, 'cos': cos, 'cache_position': cache_position}
+        key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)
 
     key_states = repeat_kv(key_states, self.num_key_value_groups)
     value_states = repeat_kv(value_states, self.num_key_value_groups)
 
     causal_mask = attention_mask
     if attention_mask is not None and cache_position is not None:
         causal_mask = causal_mask[:, :, cache_position, :key_states.shape[-2]]
@@ -404,16 +323,16 @@
     # Reference: https://github.com/pytorch/pytorch/issues/112577.  # noqa
     if query_states.device.type == 'cuda' and causal_mask is not None:
         query_states = query_states.contiguous()
         key_states = key_states.contiguous()
         value_states = value_states.contiguous()
 
     # patch position rolling
-    query_states, key_states, value_states, causal_mask = _preprocess_qkv(
-        self, query_states, key_states, value_states, causal_mask)
+    query_states, key_states, value_states, causal_mask = _preprocess_qkv(self, query_states, key_states, value_states,
+                                                                          causal_mask)
 
     attn_output = torch.nn.functional.scaled_dot_product_attention(
         query_states,
         key_states,
         value_states,
         attn_mask=causal_mask,
         dropout_p=self.attention_dropout if self.training else 0.0,
@@ -439,14 +358,13 @@
     assert layers is not None
     for idx, m in enumerate(layers):
         if model.config._attn_implementation == 'flash_attention_2':
             cuda_major, cuda_minor = torch.cuda.get_device_capability()
             if cuda_major < 8:
                 logger.warn(
                     'Flash attention is only supported on A100 or H100 GPU during training due to head dim > 64 backward.'  # noqa
-                    'ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593'
-                )
+                    'ref: https://github.com/HazyResearch/flash-attention/issues/190#issuecomment-1523359593')
             m.self_attn.forward = MethodType(fa2_forward, m.self_attn)
         elif model.config._attn_implementation == 'eager':
             m.self_attn.forward = MethodType(eager_forward, m.self_attn)
         elif model.config._attn_implementation == 'sdpa':
             m.self_attn.forward = MethodType(sdpa_forward, m.self_attn)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/longlora/longlora.py` & `ms-swift-2.0.4/swift/tuners/longlora/longlora.py`

 * *Files 9% similar despite different names*

```diff
@@ -28,74 +28,59 @@
         group_size_ratio: The group size window ratio of the sequence length.
             Note: The sequence length should be split to smaller sequences by the ratio.
     """
 
     embedder_and_normalizer: Union[str, List[str], Tuple[str]] = field(
         default=('embed', 'norm'),
         metadata={
-            'help':
-            'The names of embedder and normalizer, regex format if is a str, else will match with sub sequences'
+            'help': 'The names of embedder and normalizer, regex format if is a str, else will match with sub sequences'
         })
 
-    model_type: str = field(
-        default=None,
-        metadata={
-            'help': 'The model type, now only support `llama` structure.'
-        })
+    model_type: str = field(default=None, metadata={'help': 'The model type, now only support `llama` structure.'})
 
-    group_size_ratio: float = field(
-        default=0.25, metadata={'help': 'The S2 attention group ratio'})
+    group_size_ratio: float = field(default=0.25, metadata={'help': 'The S2 attention group ratio'})
 
     def __post_init__(self):
         from swift.tuners.mapping import SwiftTuners
         self.swift_type = SwiftTuners.LONGLORA
 
 
 class LongLoRA(LoRA):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: LongLoRAConfig,
-                      adapter_name: str):
+    def prepare_model(model: nn.Module, config: LongLoRAConfig, adapter_name: str):
         """Prepare a model with `LongLoRAConfig`"""
         LoraModel(model, config, adapter_name)
 
         def state_dict_callback(state_dict, adapter_name):
-            _state_dict = lora_state_dict(state_dict, adapter_name,
-                                          config.bias)
+            _state_dict = lora_state_dict(state_dict, adapter_name, config.bias)
             for name, value in state_dict.items():
                 if isinstance(config.embedder_and_normalizer, str):
-                    target_module_found = re.fullmatch(
-                        config.embedder_and_normalizer, name)
+                    target_module_found = re.fullmatch(config.embedder_and_normalizer, name)
                 else:
-                    target_module_found = any(
-                        target_key in name
-                        for target_key in config.embedder_and_normalizer)
+                    target_module_found = any(target_key in name for target_key in config.embedder_and_normalizer)
                 if target_module_found and name not in _state_dict:  # noqa
                     _state_dict[name] = value
             return _state_dict
 
         def mark_trainable_callback(model):
             mark_lora_as_trainable(model, adapter_name, config.bias)
-            mark_embedding_normalizer_as_trainable(
-                model, config.embedder_and_normalizer)
+            mark_embedding_normalizer_as_trainable(model, config.embedder_and_normalizer)
 
         if config.model_type == LongLoRAModelType.LLAMA:
             from .llama import replace_llama_attn
             replace_llama_attn(model)
             # only support code base from transformers
             model.config.group_size_ratio = config.group_size_ratio
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
 
-def mark_embedding_normalizer_as_trainable(
-        model: nn.Module, extra_parameters: Union[str, List[str],
-                                                  Tuple[str]]) -> None:
+def mark_embedding_normalizer_as_trainable(model: nn.Module, extra_parameters: Union[str, List[str],
+                                                                                     Tuple[str]]) -> None:
     for name, sub_module in model.named_parameters():
         if isinstance(extra_parameters, str):
             target_module_found = re.fullmatch(extra_parameters, name)
         else:
-            target_module_found = any(target_key in name
-                                      for target_key in extra_parameters)
+            target_module_found = any(target_key in name for target_key in extra_parameters)
         if target_module_found:  # noqa
             sub_module.requires_grad = True
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/lora.py` & `ms-swift-2.0.4/swift/tuners/lora.py`

 * *Files 7% similar despite different names*

```diff
@@ -26,52 +26,36 @@
             Deprecated, do not use this argument.
         lora_dtype(str): The dtype for all lora modules, supported values are `fp32`, `fp16`, `bf16`.
             Default value is `None`, which means follow the dtype of original module's weight.
         lr_ratio(float): The lr_ratio argument for [LoRA+](https://arxiv.org/abs/2402.12354)
     """
 
     use_qa_lora: bool = field(
-        default=False,
-        metadata={
-            'help':
-            'Use [qa-lora](https://github.com/yuhuixu1993/qa-lora) or not'
-        })
+        default=False, metadata={'help': 'Use [qa-lora](https://github.com/yuhuixu1993/qa-lora) or not'})
 
-    use_merged_linear: bool = field(
-        default=False, metadata={'help': 'Use merged Linear'})
+    use_merged_linear: bool = field(default=False, metadata={'help': 'Use merged Linear'})
 
     enable_lora: List[bool] = field(
-        default=None,
-        metadata={
-            'help':
-            'The modules need to be turned on when using the merged linear layer'
-        })
+        default=None, metadata={'help': 'The modules need to be turned on when using the merged linear layer'})
 
     lora_dtype: str = field(
-        default=None,
-        metadata={
-            'help':
-            'The lora dtype, default None means following the original layer\'s dtype'
-        })
+        default=None, metadata={'help': 'The lora dtype, default None means following the original layer\'s dtype'})
 
-    lorap_lr_ratio: float = field(
-        default=2.0**4, metadata={'help': 'The lr ratio of lora_B in lora+'})
+    lorap_lr_ratio: float = field(default=2.0**4, metadata={'help': 'The lr ratio of lora_B in lora+'})
 
-    lorap_emb_lr: float = field(
-        default=1e-6, metadata={'help': 'The lr for embedding in lora+'})
+    lorap_emb_lr: float = field(default=1e-6, metadata={'help': 'The lr for embedding in lora+'})
 
     def __post_init__(self):
         super().__post_init__()
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.LORA
 
     def can_be_saved_to_peft(self) -> bool:
         if self.use_qa_lora or self.use_merged_linear:
-            logger.warn(
-                'QA-LoRA and MergedLinear cannot be saved to peft format')
+            logger.warn('QA-LoRA and MergedLinear cannot be saved to peft format')
             return False
         return True
 
     def to_peft_config(self) -> LoraConfig:
         _dict = asdict(self)
         _dict.pop('use_qa_lora', None)
         _dict.pop('enable_lora', None)
@@ -91,25 +75,22 @@
 
     @staticmethod
     def prepare_model(model: nn.Module, config: LoRAConfig, adapter_name: str):
         assert not config.use_qa_lora, 'Do not use qa-lora'
         if config.use_qa_lora:
             auto_gptq_config = get_quantization_config(model, method='gptq')
             if auto_gptq_config:
-                config.group_size = getattr(auto_gptq_config, 'group_size',
-                                            None)
+                config.group_size = getattr(auto_gptq_config, 'group_size', None)
         LoraModel(model, config, adapter_name)
 
         def state_dict_callback(state_dict, adapter_name, cfg=None):
-            return lora_state_dict(state_dict, adapter_name,
-                                   cfg.bias if cfg else config.bias)
+            return lora_state_dict(state_dict, adapter_name, cfg.bias if cfg else config.bias)
 
         def mark_trainable_callback(model, cfg=None):
-            mark_lora_as_trainable(model, adapter_name,
-                                   cfg.bias if cfg else config.bias)
+            mark_lora_as_trainable(model, adapter_name, cfg.bias if cfg else config.bias)
 
         def optimizer_group_callback(model, **defaults):
             if config.lorap_lr_ratio is None:
                 return None, None
 
             def get_module(name):
                 parent_idx = 2 if 'lora' in name else 1
@@ -164,22 +145,18 @@
                     'params': list(param_groups['groupB_no_decay'].values()),
                     'weight_decay': 0.0,
                     'lr': lr * config.lorap_lr_ratio,
                 },
             ]
             return all_params, param_groups
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback, optimizer_group_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback, optimizer_group_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         set_adapter(module, adapter_name, activate, offload)
         for sub_module in module.modules():
             if isinstance(sub_module, (LoraLayer, LoRALayer)):
                 sub_module.set_activation(adapter_name, activate)
                 if hasattr(sub_module, 'save_memory'):
                     sub_module.save_memory(adapter_name, activate, offload)
 
@@ -197,17 +174,15 @@
             adapter_name(`str`): The adapter name
         """
         if not config.use_merged_linear:
             if version.parse(peft.__version__) < version.parse('0.6.3'):
                 logger.info('All adapters will be merged.')
                 LoraModel(model, None, '').merge_and_unload()
             else:
-                LoraModel(model, None,
-                          '').merge_and_unload(adapter_names=[adapter_name])
+                LoraModel(model, None, '').merge_and_unload(adapter_names=[adapter_name])
         else:
             for name, sub_module in model.named_modules():
                 if isinstance(sub_module, MergedLinear):
                     sub_module.merge()
-                    parent = model.get_submodule('.'.join(
-                        name.split('.')[:-1]))
+                    parent = model.get_submodule('.'.join(name.split('.')[:-1]))
                     target_name = name.split('.')[-1]
                     setattr(parent, target_name, sub_module.base_layer)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/lora_layers.py` & `ms-swift-2.0.4/swift/tuners/lora_layers.py`

 * *Files 2% similar despite different names*

```diff
@@ -19,36 +19,32 @@
 from peft.tuners.lora import Conv2d as _Conv2d
 from peft.tuners.lora import Embedding as _Embedding
 from peft.tuners.lora import Linear as _Linear
 from peft.tuners.lora import LoraLayer
 from peft.tuners.lora import LoraModel as _LoraModel
 from peft.tuners.lora.tp_layer import LoraParallelLinear as _LoraParallelLinear
 from peft.tuners.tuners_utils import BaseTunerLayer
-from peft.utils import (_get_submodules, get_auto_gptq_quant_linear,
-                        get_quantization_config)
+from peft.utils import _get_submodules, get_auto_gptq_quant_linear, get_quantization_config
 from peft.utils.other import transpose
 from transformers import Conv1D
 
 from swift import LoraConfig, get_logger
 from .utils import ActivationMixin, ModulesToSaveWrapper, SwiftAdapter
 
 logger = get_logger()
 dispatchers = []
 
 
 def is_auto_awq_available():
-    return importlib.util.find_spec(
-        'awq') is not None and importlib.util.find_spec(
-            'peft.tuners.lora.awq') is not None
+    return importlib.util.find_spec('awq') is not None and importlib.util.find_spec('peft.tuners.lora.awq') is not None
 
 
 def is_aqlm_available():
-    return importlib.util.find_spec(
-        'aqlm') is not None and importlib.util.find_spec(
-            'peft.tuners.lora.aqlm') is not None
+    return importlib.util.find_spec('aqlm') is not None and importlib.util.find_spec(
+        'peft.tuners.lora.aqlm') is not None
 
 
 def is_auto_gptq_available():
     try:
         return peft.import_utils._is_auto_gptq_available()
     except ImportError as e:
         logger.warn(e)
@@ -80,39 +76,30 @@
                 if key in adapter_names:
                     self.set_activation(key, True)
                     layer.requires_grad_(True)
                     SwiftAdapter.save_memory(layer, key, self.module_key, True)
                 else:
                     self.set_activation(key, False)
                     layer.requires_grad_(False)
-                    SwiftAdapter.save_memory(
-                        layer, key, self.module_key, False, offload=offload)
+                    SwiftAdapter.save_memory(layer, key, self.module_key, False, offload=offload)
 
     def save_memory(self, adapter_name, activate, offload=None):
         for layer_name in self.adapter_layer_names:
             module_dict = getattr(self, layer_name)
             for key, layer in module_dict.items():
                 if key == adapter_name:
                     if activate:
-                        SwiftAdapter.save_memory(layer, layer_name + '.' + key,
-                                                 self.module_key, True)
+                        SwiftAdapter.save_memory(layer, layer_name + '.' + key, self.module_key, True)
                     else:
-                        SwiftAdapter.save_memory(
-                            layer,
-                            layer_name + '.' + key,
-                            self.module_key,
-                            False,
-                            offload=offload)
+                        SwiftAdapter.save_memory(layer, layer_name + '.' + key, self.module_key, False, offload=offload)
 
     def merge(self, *args, **kwargs):
         if not self.unique_thread:
-            raise AssertionError(
-                'Merge is unsupported in multiple thread, '
-                'please set `USE_UNIQUE_THREAD=1` in env variable to merge LoRA.'
-            )
+            raise AssertionError('Merge is unsupported in multiple thread, '
+                                 'please set `USE_UNIQUE_THREAD=1` in env variable to merge LoRA.')
         return super().merge(*args, **kwargs)
 
 
 if is_bnb_available():
     import bitsandbytes as bnb
     from peft.tuners.lora.bnb import Linear8bitLt as _Linear8bitLt
 
@@ -124,36 +111,32 @@
             module_key: str,
             **kwargs,
         ):
             super(Linear8bitLt, self).__init__(module_key)
             self.set_activation(args[1], True)
             super(ActivationMixin, self).__init__(*args, **kwargs)
 
-    def dispatch_bnb_8bit(target: torch.nn.Module, adapter_name: str,
-                          module_key: str, **kwargs):
+    def dispatch_bnb_8bit(target: torch.nn.Module, adapter_name: str, module_key: str, **kwargs):
         new_module = None
 
         if isinstance(target, BaseTunerLayer):
             target_base_layer = target.get_base_layer()
         else:
             target_base_layer = target
 
         loaded_in_8bit = kwargs.get('loaded_in_8bit', False)
-        if loaded_in_8bit and isinstance(target_base_layer,
-                                         bnb.nn.Linear8bitLt):
+        if loaded_in_8bit and isinstance(target_base_layer, bnb.nn.Linear8bitLt):
             eightbit_kwargs = kwargs.copy()
             eightbit_kwargs.update({
                 'has_fp16_weights': target.state.has_fp16_weights,
-                'memory_efficient_backward':
-                target.state.memory_efficient_backward,
+                'memory_efficient_backward': target.state.memory_efficient_backward,
                 'threshold': target.state.threshold,
                 'index': target.index,
             })
-            new_module = Linear8bitLt(
-                target, adapter_name, module_key=module_key, **eightbit_kwargs)
+            new_module = Linear8bitLt(target, adapter_name, module_key=module_key, **eightbit_kwargs)
 
         return new_module
 
     dispatchers.append(dispatch_bnb_8bit)
 
 if is_bnb_4bit_available():
     from peft.tuners.lora.bnb import Linear4bit as _Linear4bit
@@ -166,37 +149,31 @@
             module_key: str,
             **kwargs,
         ):
             super(Linear4bit, self).__init__(module_key)
             self.set_activation(args[1], True)
             super(ActivationMixin, self).__init__(*args, **kwargs)
 
-    def dispatch_bnb_4bit(target: torch.nn.Module, adapter_name: str,
-                          module_key: str, **kwargs):
+    def dispatch_bnb_4bit(target: torch.nn.Module, adapter_name: str, module_key: str, **kwargs):
         new_module = None
 
         if isinstance(target, BaseTunerLayer):
             target_base_layer = target.get_base_layer()
         else:
             target_base_layer = target
 
         loaded_in_4bit = kwargs.get('loaded_in_4bit', False)
-        if loaded_in_4bit and is_bnb_4bit_available() and isinstance(
-                target_base_layer, bnb.nn.Linear4bit):
+        if loaded_in_4bit and is_bnb_4bit_available() and isinstance(target_base_layer, bnb.nn.Linear4bit):
             fourbit_kwargs = kwargs.copy()
             fourbit_kwargs.update({
-                'compute_dtype':
-                target_base_layer.compute_dtype,
-                'compress_statistics':
-                target_base_layer.weight.compress_statistics,
-                'quant_type':
-                target_base_layer.weight.quant_type,
+                'compute_dtype': target_base_layer.compute_dtype,
+                'compress_statistics': target_base_layer.weight.compress_statistics,
+                'quant_type': target_base_layer.weight.quant_type,
             })
-            new_module = Linear4bit(
-                target, adapter_name, module_key=module_key, **fourbit_kwargs)
+            new_module = Linear4bit(target, adapter_name, module_key=module_key, **fourbit_kwargs)
 
         return new_module
 
     dispatchers.append(dispatch_bnb_4bit)
 
 if is_aqlm_available():
     from peft.tuners.lora.aqlm import AqlmLoraLinear as _AqlmLoraLinear
@@ -222,16 +199,15 @@
         new_module = None
 
         if isinstance(target, BaseTunerLayer):
             target_base_layer = target.get_base_layer()
         else:
             target_base_layer = target
 
-        if is_aqlm_available() and isinstance(target_base_layer,
-                                              QuantizedLinear):
+        if is_aqlm_available() and isinstance(target_base_layer, QuantizedLinear):
             new_module = AqlmLoraLinear(target, adapter_name, **kwargs)
             target.qweight = target_base_layer.codes
 
         return new_module
 
     dispatchers.append(dispatch_aqlm)
 
@@ -263,25 +239,21 @@
             target_base_layer = target.get_base_layer()
         else:
             target_base_layer = target
 
         if isinstance(target_base_layer, WQLinear_GEMM):
             # Raise the error only at the dispatch level
             AUTOAWQ_MINIMUM_VERSION = packaging.version.parse('0.2.0')
-            version_autoawq = packaging.version.parse(
-                importlib_metadata.version('autoawq'))
+            version_autoawq = packaging.version.parse(importlib_metadata.version('autoawq'))
 
             if AUTOAWQ_MINIMUM_VERSION > version_autoawq:
-                raise ImportError(
-                    f'Found an incompatible version of auto-awq. Found version {version_autoawq}, '
-                    f'but only versions above {AUTOAWQ_MINIMUM_VERSION} are supported for PEFT.'
-                )
+                raise ImportError(f'Found an incompatible version of auto-awq. Found version {version_autoawq}, '
+                                  f'but only versions above {AUTOAWQ_MINIMUM_VERSION} are supported for PEFT.')
 
-            new_module = AwqLoraLinear(
-                target, adapter_name, module_key=module_key, **kwargs)
+            new_module = AwqLoraLinear(target, adapter_name, module_key=module_key, **kwargs)
             target.qweight = target_base_layer.qweight
 
         return new_module
 
     dispatchers.append(dispatch_awq)
 
 if is_auto_gptq_available():
@@ -307,23 +279,19 @@
             super(QuantLinear, self).__init__(module_key)
             self.set_activation(adapter_name, True)
             nn.Module.__init__(self)
             self.group_size = group_size
             self.use_qa_lora = use_qa_lora
             if self.use_qa_lora:
                 assert self.group_size is not None, 'To use qa_lora you need to pass in the `group_size` param.'
-                self.qa_pool = torch.nn.AvgPool1d(
-                    self.group_size
-                )  # using pooling layer to conduct sum operation
+                self.qa_pool = torch.nn.AvgPool1d(self.group_size)  # using pooling layer to conduct sum operation
 
             LoraLayer.__init__(self, base_layer)
             if use_dora:
-                raise ValueError(
-                    f'{_QuantLinear.__name__} does not support DoRA yet, please set it to False'
-                )
+                raise ValueError(f'{_QuantLinear.__name__} does not support DoRA yet, please set it to False')
             if self.use_qa_lora:
                 self.in_features = self.in_features // self.group_size
             # self.base_layer and self.quant_linear_module are the same;
             # we need the former for consistency and the latter
             # for backwards compatibility
             self.quant_linear_module = base_layer
             self._active_adapter = adapter_name
@@ -376,21 +344,18 @@
 
         if isinstance(target, BaseTunerLayer):
             target_base_layer = target.get_base_layer()
         else:
             target_base_layer = target
 
         gptq_quantization_config = kwargs.get('gptq_quantization_config', None)
-        AutoGPTQQuantLinear = get_auto_gptq_quant_linear(
-            gptq_quantization_config)
+        AutoGPTQQuantLinear = get_auto_gptq_quant_linear(gptq_quantization_config)
 
-        if AutoGPTQQuantLinear is not None and isinstance(
-                target_base_layer, AutoGPTQQuantLinear):
-            new_module = QuantLinear(
-                target, adapter_name, module_key=module_key, **kwargs)
+        if AutoGPTQQuantLinear is not None and isinstance(target_base_layer, AutoGPTQQuantLinear):
+            new_module = QuantLinear(target, adapter_name, module_key=module_key, **kwargs)
             target.qweight = target_base_layer.qweight
 
         return new_module
 
     dispatchers.append(dispatch_gptq)
 
 
@@ -406,35 +371,31 @@
     if isinstance(target, BaseTunerLayer):
         target_base_layer = target.get_base_layer()
     else:
         target_base_layer = target
 
     if lora_config.megatron_config:
         megatron_core = importlib.import_module(lora_config.megatron_core)
-        linears = (megatron_core.tensor_parallel.ColumnParallelLinear,
-                   megatron_core.tensor_parallel.RowParallelLinear)
+        linears = (megatron_core.tensor_parallel.ColumnParallelLinear, megatron_core.tensor_parallel.RowParallelLinear)
     else:
         megatron_core = None
         linears = None
 
     if megatron_core and isinstance(target_base_layer, linears):
         megatron_kwargs = kwargs.copy()
         megatron_config = lora_config.megatron_config
         if isinstance(megatron_config, dict):
             transformer_config_class = megatron_core.transformer.transformer_config.TransformerConfig
-            megatron_config = transformer_config_class(
-                **lora_config.megatron_config)
+            megatron_config = transformer_config_class(**lora_config.megatron_config)
         megatron_kwargs['megatron_config'] = megatron_config
         if megatron_kwargs['fan_in_fan_out']:
-            warnings.warn(
-                'fan_in_fan_out is set to True but the target module is `ColumnParallelLinear` '
-                'or `RowParallelLinear`. '
-                'Setting fan_in_fan_out to False.')
-            megatron_kwargs[
-                'fan_in_fan_out'] = lora_config.fan_in_fan_out = False
+            warnings.warn('fan_in_fan_out is set to True but the target module is `ColumnParallelLinear` '
+                          'or `RowParallelLinear`. '
+                          'Setting fan_in_fan_out to False.')
+            megatron_kwargs['fan_in_fan_out'] = lora_config.fan_in_fan_out = False
         new_module = LoraParallelLinear(
             base_layer=target,
             adapter_name=adapter_name,
             backend=megatron_core.tensor_parallel,
             module_key=module_key,
             **megatron_kwargs)
 
@@ -455,45 +416,35 @@
     else:
         target_base_layer = target
 
     if isinstance(target_base_layer, torch.nn.Embedding):
         embedding_kwargs = kwargs.copy()
         embedding_kwargs.pop('fan_in_fan_out', None)
         embedding_kwargs.update(lora_config.loftq_config)
-        new_module = Embedding(
-            target, adapter_name, module_key=module_key, **embedding_kwargs)
+        new_module = Embedding(target, adapter_name, module_key=module_key, **embedding_kwargs)
     elif isinstance(target_base_layer, torch.nn.Conv2d):
         kwargs.update(lora_config.loftq_config)
-        new_module = Conv2d(
-            target, adapter_name, module_key=module_key, **kwargs)
+        new_module = Conv2d(target, adapter_name, module_key=module_key, **kwargs)
     elif isinstance(target_base_layer, torch.nn.Linear):
         if target_base_layer.__class__.__name__ == 'NonDynamicallyQuantizableLinear':
             # Fix issue: https://github.com/modelscope/swift/issues/342
             return None
         if kwargs['fan_in_fan_out']:
-            warnings.warn(
-                'fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. '
-                'Setting fan_in_fan_out to False.')
+            warnings.warn('fan_in_fan_out is set to True but the target module is `torch.nn.Linear`. '
+                          'Setting fan_in_fan_out to False.')
             kwargs['fan_in_fan_out'] = lora_config.fan_in_fan_out = False
         kwargs.update(lora_config.loftq_config)
-        new_module = Linear(
-            target, adapter_name, module_key=module_key, **kwargs)
+        new_module = Linear(target, adapter_name, module_key=module_key, **kwargs)
     elif isinstance(target_base_layer, Conv1D):
         if not kwargs['fan_in_fan_out']:
-            warnings.warn(
-                'fan_in_fan_out is set to False but the target module is `Conv1D`. '
-                'Setting fan_in_fan_out to True.')
+            warnings.warn('fan_in_fan_out is set to False but the target module is `Conv1D`. '
+                          'Setting fan_in_fan_out to True.')
             kwargs['fan_in_fan_out'] = lora_config.fan_in_fan_out = True
         kwargs.update(lora_config.loftq_config)
-        new_module = Linear(
-            target,
-            adapter_name,
-            is_target_conv_1d_layer=True,
-            module_key=module_key,
-            **kwargs)
+        new_module = Linear(target, adapter_name, is_target_conv_1d_layer=True, module_key=module_key, **kwargs)
 
     return new_module
 
 
 dispatchers.append(dispatch_megatron)
 dispatchers.append(dispatch_default)
 
@@ -532,27 +483,24 @@
                      lora_alpha,
                      lora_dropout,
                      init_lora_weights,
                      use_rslora,
                      use_dora: bool = False):
         # This code works for linear layers, override for other layer types
         if r <= 0:
-            raise ValueError(
-                f'`r` should be a positive integer value but the value passed is {r}'
-            )
+            raise ValueError(f'`r` should be a positive integer value but the value passed is {r}')
 
         self.r[adapter_name] = r
         self.lora_alpha[adapter_name] = lora_alpha
         if lora_dropout > 0.0:
             lora_dropout_layer = nn.Dropout(p=lora_dropout)
         else:
             lora_dropout_layer = nn.Identity()
 
-        self.lora_dropout.update(
-            nn.ModuleDict({adapter_name: lora_dropout_layer}))
+        self.lora_dropout.update(nn.ModuleDict({adapter_name: lora_dropout_layer}))
         # Actual trainable parameters
         self.lora_A[adapter_name] = nn.Linear(self.in_features, r, bias=False)
         self.lora_B[adapter_name] = nn.Linear(r, self.out_features, bias=False)
         if use_rslora:
             self.scaling[adapter_name] = lora_alpha / math.sqrt(r)
         else:
             self.scaling[adapter_name] = lora_alpha / r
@@ -621,20 +569,18 @@
 
             if bias == 'all':
                 for n, p in model.named_parameters():
                     if 'bias' in n:
                         p.requires_grad = True
             elif bias == 'lora_only':
                 for m in model.modules():
-                    if isinstance(m, LoraLayer) and hasattr(
-                            m, 'bias') and m.bias is not None:
+                    if isinstance(m, LoraLayer) and hasattr(m, 'bias') and m.bias is not None:
                         m.bias.requires_grad = True
             else:
-                raise NotImplementedError(
-                    f'Requested bias: {bias}, is not implemented.')
+                raise NotImplementedError(f'Requested bias: {bias}, is not implemented.')
 
     def inject_adapter(self, model: nn.Module, adapter_name: str):
         r"""
         Override code:
         1. ModulesToSaveWrapper construction method: add module_key=key argument to offload to cpu
         """
         peft_config = self.peft_config[adapter_name]
@@ -642,16 +588,15 @@
         # This way, we can raise early if something goes wrong, without leaving the model
         # in a bad (half-initialized) state.
         self._check_new_adapter_config(peft_config)
 
         is_target_modules_in_base_model = False
         key_list = [key for key, _ in model.named_modules()]
 
-        _check_for_modules_to_save = getattr(peft_config, 'modules_to_save',
-                                             None) is not None
+        _check_for_modules_to_save = getattr(peft_config, 'modules_to_save', None) is not None
         _has_modules_to_save = False
 
         model_config = getattr(model, 'config', {'model_type': 'custom'})
         if hasattr(model_config, 'to_dict'):
             model_config = model_config.to_dict()
 
         peft_config = self._prepare_adapter_config(peft_config, model_config)
@@ -662,47 +607,38 @@
             peft_config = _maybe_include_all_linear_layers(peft_config, model)
         if version.parse(peft.__version__) >= version.parse('0.10.0'):
             self._prepare_model(peft_config, model)
 
         for key in key_list:
             # Check for modules_to_save in case
             if _check_for_modules_to_save and any(
-                    key.endswith(f'{module_to_save}')
-                    for module_to_save in peft_config.modules_to_save):
+                    key.endswith(f'{module_to_save}') for module_to_save in peft_config.modules_to_save):
                 # Optionally set the modules to save
                 parent, target, target_name = _get_submodules(model, key)
 
                 if not isinstance(target, ModulesToSaveWrapper):
-                    new_module = ModulesToSaveWrapper(
-                        target, adapter_name=adapter_name, module_key=key)
+                    new_module = ModulesToSaveWrapper(target, adapter_name=adapter_name, module_key=key)
                     setattr(parent, target_name, new_module)
                 else:
                     target.update(adapter_name)
 
                 _has_modules_to_save = True
                 continue
 
             if not self._check_target_module_exists(peft_config, key):
                 continue
 
             self.targeted_module_names.append(key)
             is_target_modules_in_base_model = True
             parent, target, target_name = _get_submodules(model, key)
-            self._create_and_replace(
-                peft_config,
-                adapter_name,
-                target,
-                target_name,
-                parent,
-                current_key=key)
+            self._create_and_replace(peft_config, adapter_name, target, target_name, parent, current_key=key)
 
         if not is_target_modules_in_base_model:
-            raise ValueError(
-                f'Target modules {peft_config.target_modules} not found in the base model. '
-                f'Please check the target modules and try again.')
+            raise ValueError(f'Target modules {peft_config.target_modules} not found in the base model. '
+                             f'Please check the target modules and try again.')
 
         self._mark_only_adapters_as_trainable(self.model)
 
         if self.peft_config[adapter_name].inference_mode:
             for n, p in self.model.named_parameters():
                 if adapter_name in n:
                     p.requires_grad = False
@@ -750,23 +686,18 @@
         5. Use Class type defined here
         6. Allow new_module being None
         """
         if current_key is None:
             raise ValueError("Current Key shouldn't be `None`")
 
         # Regexp matching - Find key which matches current target_name in patterns provided
-        pattern_keys = list(
-            chain(lora_config.rank_pattern.keys(),
-                  lora_config.alpha_pattern.keys()))
-        target_name_key = next(
-            filter(lambda key: re.match(rf'.*\.{key}$', current_key),
-                   pattern_keys), current_key)
+        pattern_keys = list(chain(lora_config.rank_pattern.keys(), lora_config.alpha_pattern.keys()))
+        target_name_key = next(filter(lambda key: re.match(rf'.*\.{key}$', current_key), pattern_keys), current_key)
         r = lora_config.rank_pattern.get(target_name_key, lora_config.r)
-        alpha = lora_config.alpha_pattern.get(target_name_key,
-                                              lora_config.lora_alpha)
+        alpha = lora_config.alpha_pattern.get(target_name_key, lora_config.lora_alpha)
 
         kwargs = {
             'r': r,
             'lora_alpha': alpha,
             'lora_dropout': lora_config.lora_dropout,
             'fan_in_fan_out': lora_config.fan_in_fan_out,
             'init_lora_weights': lora_config.init_lora_weights,
@@ -774,45 +705,37 @@
             'use_dora': lora_config.use_dora,
             'loaded_in_8bit': getattr(self.model, 'is_loaded_in_8bit', False),
             'loaded_in_4bit': getattr(self.model, 'is_loaded_in_4bit', False),
         }
 
         quant_methods = ['gptq', 'aqlm', 'awq']
         for quant_method in quant_methods:
-            quantization_config = get_quantization_config(
-                self.model, method=quant_method)
+            quantization_config = get_quantization_config(self.model, method=quant_method)
             if quantization_config is not None:
-                kwargs[
-                    f'{quant_method}_quantization_config'] = quantization_config
+                kwargs[f'{quant_method}_quantization_config'] = quantization_config
 
         # note: AdaLoraLayer is a subclass of LoraLayer, we need to exclude it
         from peft.tuners.adalora import AdaLoraLayer
 
-        if isinstance(target,
-                      LoraLayer) and not isinstance(target, AdaLoraLayer):
+        if isinstance(target, LoraLayer) and not isinstance(target, AdaLoraLayer):
             if target.__class__.__name__ == 'NonDynamicallyQuantizableLinear':
                 # Fix issue: https://github.com/modelscope/swift/issues/342
                 return
             target.update_layer(
                 adapter_name,
                 r,
                 lora_alpha=alpha,
                 lora_dropout=lora_config.lora_dropout,
                 init_lora_weights=lora_config.init_lora_weights,
                 use_rslora=lora_config.use_rslora,
                 use_dora=lora_config.use_dora,
             )
             self._convert_dtype(target, lora_config.lora_dtype)
         else:
-            new_module = self._create_new_module(
-                lora_config,
-                adapter_name,
-                target,
-                current_key=current_key,
-                **kwargs)
+            new_module = self._create_new_module(lora_config, adapter_name, target, current_key=current_key, **kwargs)
             if new_module is not None:
                 if adapter_name != self.active_adapter:
                     # adding an additional adapter: it is not automatically trainable
                     new_module.requires_grad_(False)
                 self._replace_module(parent, target_name, new_module, target)
                 self._convert_dtype(new_module, lora_config.lora_dtype)
 
@@ -836,16 +759,15 @@
             else:
                 new_module.state = child.state
             new_module.to(child.weight.device)
 
         # dispatch to correct device
         for name, module in new_module.named_modules():
             if (self.prefix in name) or ('ranknum' in name):
-                weight = child.qweight if hasattr(child,
-                                                  'qweight') else child.weight
+                weight = child.qweight if hasattr(child, 'qweight') else child.weight
                 if weight.device != torch.device('meta'):
                     module.to(weight.device)
 
     @staticmethod
     def _create_new_module(lora_config, adapter_name, target, **kwargs):
         """
         Override code:
@@ -861,37 +783,26 @@
         new_module = None
         if lora_config.use_qa_lora:
             kwargs['use_qa_lora'] = True
             kwargs['group_size'] = lora_config.group_size
         if lora_config.use_merged_linear:
             bias = kwargs.pop('bias', False)
             new_module = MergedLinear(
-                adapter_name,
-                current_key,
-                target,
-                bias=bias,
-                enable_lora=lora_config.enable_lora,
-                **kwargs)
+                adapter_name, current_key, target, bias=bias, enable_lora=lora_config.enable_lora, **kwargs)
         else:
             for dispatcher in dispatchers:
-                new_module = dispatcher(
-                    target,
-                    adapter_name,
-                    lora_config=lora_config,
-                    module_key=current_key,
-                    **kwargs)
+                new_module = dispatcher(target, adapter_name, lora_config=lora_config, module_key=current_key, **kwargs)
                 if new_module is not None:  # first match wins
                     break
 
         if new_module is None:
             # no module could be matched
             logger.debug(
                 f'Target module {target} is not supported. Currently, only the following modules are supported: '
-                '`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.'
-            )
+                '`torch.nn.Linear`, `torch.nn.Embedding`, `torch.nn.Conv2d`, `transformers.pytorch_utils.Conv1D`.')
             new_module = None
 
         return new_module
 
 
 class LoRALayer(ActivationMixin):
 
@@ -932,21 +843,15 @@
                  enable_lora: List[bool] = [False],
                  fan_in_fan_out: bool = False,
                  merge_weights: bool = True,
                  bias: bool = True,
                  device=None,
                  dtype=None,
                  **kwargs):
-        nn.Linear.__init__(
-            self,
-            base_layer.in_features,
-            base_layer.out_features,
-            bias=bias,
-            device=device,
-            dtype=dtype)
+        nn.Linear.__init__(self, base_layer.in_features, base_layer.out_features, bias=bias, device=device, dtype=dtype)
         LoRALayer.__init__(
             self,
             adapter_name,
             module_key,
             r=r,
             lora_alpha=lora_alpha,
             lora_dropout=lora_dropout,
@@ -954,29 +859,24 @@
         assert base_layer.out_features % len(enable_lora) == 0, \
             'The length of enable_lora must divide out_features'
         self.enable_lora = enable_lora
         self.fan_in_fan_out = fan_in_fan_out
         self.base_layer = base_layer
         # Actual trainable parameters
         if r > 0 and any(enable_lora):
-            self.lora_A = nn.Parameter(
-                self.weight.new_zeros(
-                    (r * sum(enable_lora), base_layer.in_features)))
+            self.lora_A = nn.Parameter(self.weight.new_zeros((r * sum(enable_lora), base_layer.in_features)))
             self.lora_B = nn.Parameter(
-                self.weight.new_zeros(
-                    (base_layer.out_features // len(enable_lora)
-                     * sum(enable_lora),
-                     r)))  # weights for Conv1D with groups=sum(enable_lora)
+                self.weight.new_zeros((base_layer.out_features // len(enable_lora) * sum(enable_lora),
+                                       r)))  # weights for Conv1D with groups=sum(enable_lora)
             self.scaling = self.lora_alpha / self.r
             # Freezing the pre-trained weight matrix
             self.weight.requires_grad = False
             # Compute the indices
-            self.lora_ind = self.weight.new_zeros(
-                (base_layer.out_features, ),
-                dtype=torch.bool).view(len(enable_lora), -1)
+            self.lora_ind = self.weight.new_zeros((base_layer.out_features, ),
+                                                  dtype=torch.bool).view(len(enable_lora), -1)
             self.lora_ind[enable_lora, :] = True
             self.lora_ind = self.lora_ind.view(-1)
         self.reset_parameters()
         self.weight = self.base_layer.weight
         if getattr(self.base_layer, 'bias', None) is not None:
             self.bias = self.base_layer.bias
         if fan_in_fan_out:
@@ -995,18 +895,15 @@
         return result
 
     def merge_AB(self):
 
         def T(w):
             return w.transpose(0, 1) if self.fan_in_fan_out else w
 
-        delta_w = F.conv1d(
-            self.lora_A.unsqueeze(0),
-            self.lora_B.unsqueeze(-1),
-            groups=sum(self.enable_lora)).squeeze(0)
+        delta_w = F.conv1d(self.lora_A.unsqueeze(0), self.lora_B.unsqueeze(-1), groups=sum(self.enable_lora)).squeeze(0)
         return T(self.zero_pad(delta_w))
 
     def merge(self, **kwargs):
         if self.merge_weights and not self.merged:
             # Merge the weights and mark it
             if self.r > 0 and any(self.enable_lora):
                 self.weight.data += self.merge_AB() * self.scaling
@@ -1026,23 +923,20 @@
         if self.merged or not self.is_activated(self.adapter_name):
             return F.linear(x, T(self.weight), bias=self.bias)
         else:
             result = F.linear(x, T(self.weight), bias=self.bias)
             if self.r > 0:
                 x_dtype = x.dtype
                 x = x.to(self.lora_A.dtype)
-                result += self.lora_dropout(x) @ T(
-                    self.merge_AB().T) * self.scaling
+                result += self.lora_dropout(x) @ T(self.merge_AB().T) * self.scaling
                 result = result.to(x_dtype)
             return result
 
 
-def mark_lora_as_trainable(model: nn.Module,
-                           adapter_name: str,
-                           bias: str = 'none') -> None:
+def mark_lora_as_trainable(model: nn.Module, adapter_name: str, bias: str = 'none') -> None:
     if bias == 'none':
         return
     elif bias == 'all':
         for n, p in model.named_parameters():
             if 'bias' in n:
                 p.requires_grad = True
     elif bias == 'lora_only':
@@ -1051,32 +945,23 @@
                     hasattr(m, 'bias') and \
                     m.bias is not None:
                 m.bias.requires_grad = True
     else:
         raise NotImplementedError
 
 
-def lora_state_dict(state_dict,
-                    adapter_name: str,
-                    bias: str = 'none') -> Dict[str, torch.Tensor]:
+def lora_state_dict(state_dict, adapter_name: str, bias: str = 'none') -> Dict[str, torch.Tensor]:
     if bias == 'none':
         to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k}
     elif bias == 'all':
-        to_return = {
-            k: state_dict[k]
-            for k in state_dict if 'lora_' in k or 'bias' in k
-        }
+        to_return = {k: state_dict[k] for k in state_dict if 'lora_' in k or 'bias' in k}
     elif bias == 'lora_only':
         to_return = {}
         for k in state_dict:
             if 'lora_' in k:
                 to_return[k] = state_dict[k]
                 bias_name = k.split('lora_')[0] + 'bias'
                 if bias_name in state_dict:
                     to_return[bias_name] = state_dict[bias_name]
     else:
         raise NotImplementedError
-    return {
-        k: v
-        for k, v in to_return.items()
-        if (('lora_' in k and f'.{adapter_name}' in k) or ('bias' in k))
-    }
+    return {k: v for k, v in to_return.items() if (('lora_' in k and f'.{adapter_name}' in k) or ('bias' in k))}
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/mapping.py` & `ms-swift-2.0.4/swift/tuners/mapping.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/tuners/module_mapping.py` & `ms-swift-2.0.4/swift/tuners/module_mapping.py`

 * *Files 0% similar despite different names*

```diff
@@ -74,16 +74,15 @@
 CHATGLM_KEYS = ModelKeys(
     **{
         'module_list': 'transformer.encoder.layers',
         'mlp': 'transformer.encoder.layers.{}.mlp',
         'down_proj': 'transformer.encoder.layers.{}.mlp.dense_4h_to_h',
         'attention': 'transformer.encoder.layers.{}.self_attention',
         'o_proj': 'transformer.encoder.layers.{}.self_attention.dense',
-        'qkv_proj':
-        'transformer.encoder.layers.{}.self_attention.query_key_value',
+        'qkv_proj': 'transformer.encoder.layers.{}.self_attention.query_key_value',
         'embedding': 'transformer.embedding',
         'output': 'transformer.output_layer',
     })
 
 BAICHUAN_KEYS = ModelKeys(
     **{
         'module_list': 'model.layers',
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/neftune.py` & `ms-swift-2.0.4/swift/tuners/neftune.py`

 * *Files 7% similar despite different names*

```diff
@@ -17,61 +17,52 @@
 
     NEFTune adds slightly noises to embedding outputs.
     See https://arxiv.org/abs/2310.05914
 
     Args:
         noise_alpha(`float`): The noise alpha value used for the NEFTune, default 5.0
     """
-    noise_alpha: float = field(
-        default=5.0,
-        metadata={'help': 'The noise alpha value used for the NEFTune'})
+    noise_alpha: float = field(default=5.0, metadata={'help': 'The noise alpha value used for the NEFTune'})
 
     def __post_init__(self):
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.NEFTUNE
 
 
 class NEFTune(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: NEFTuneConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: nn.Module, config: NEFTuneConfig, adapter_name: str) -> SwiftOutput:
         """Prepare a model with `NEFTuneConfig`"""
         for sub_module in model.modules():
             if isinstance(sub_module, torch.nn.Embedding):
 
                 def neftune_hook(module, args, output):
                     if module.training and getattr(module, 'nef_activated'):
                         dims = torch.tensor(output.size(-1) * output.size(-2))
                         mag_norm = config.noise_alpha / torch.sqrt(dims)
-                        output = output + torch.zeros_like(output).uniform_(
-                            -mag_norm, mag_norm)
+                        output = output + torch.zeros_like(output).uniform_(-mag_norm, mag_norm)
                     return output
 
                 if hasattr(sub_module, 'nef_activated'):
-                    raise ValueError(
-                        'NEFTune does not support a second tuner.')
+                    raise ValueError('NEFTune does not support a second tuner.')
 
                 sub_module.register_forward_hook(neftune_hook)
                 sub_module.nef_activated = True
 
         def state_dict_callback(state_dict, adapter_name):
             return state_dict
 
         def mark_trainable_callback(model):
             return
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         for sub_module in module.modules():
             if isinstance(sub_module, torch.nn.Embedding):
                 sub_module.nef_activated = activate
 
     @staticmethod
     def freeze_model():
         return False
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/peft.py` & `ms-swift-2.0.4/swift/tuners/peft.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,47 +5,37 @@
 from functools import partial, reduce
 from typing import Dict, Optional
 
 import json
 import peft
 import torch
 import torch.nn
-from peft import (AdaLoraConfig, IA3Config, LoftQConfig, LoHaConfig,
-                  LoKrConfig, LoraModel, OFTConfig, PeftConfig, PeftModel,
-                  PeftModelForCausalLM, PeftModelForSeq2SeqLM,
-                  PeftModelForSequenceClassification,
-                  PeftModelForTokenClassification, PrefixTuningConfig,
-                  PromptEncoderConfig, PromptLearningConfig,
-                  PromptTuningConfig, get_peft_config, get_peft_model,
-                  get_peft_model_state_dict)
+from peft import (AdaLoraConfig, IA3Config, LoftQConfig, LoHaConfig, LoKrConfig, LoraModel, OFTConfig, PeftConfig,
+                  PeftModel, PeftModelForCausalLM, PeftModelForSeq2SeqLM, PeftModelForSequenceClassification,
+                  PeftModelForTokenClassification, PrefixTuningConfig, PromptEncoderConfig, PromptLearningConfig,
+                  PromptTuningConfig, get_peft_config, get_peft_model, get_peft_model_state_dict)
 from peft.config import PeftConfigMixin
 from peft.tuners.lora import Embedding
 from transformers import Trainer
 
 from swift import get_logger
 from swift.hub.snapshot_download import snapshot_download
 
 logger = get_logger()
 dispatchers = []
 
 
 @dataclass
 class LoraConfig(peft.LoraConfig):
     lora_dtype: str = field(
-        default=None,
-        metadata={
-            'help':
-            'The lora dtype, default None means following the original layer\'s dtype'
-        })
+        default=None, metadata={'help': 'The lora dtype, default None means following the original layer\'s dtype'})
 
-    lorap_lr_ratio: float = field(
-        default=2.0**4, metadata={'help': 'The lr ratio of lora_B in lora+'})
+    lorap_lr_ratio: float = field(default=2.0**4, metadata={'help': 'The lr ratio of lora_B in lora+'})
 
-    lorap_emb_lr: float = field(
-        default=1e-6, metadata={'help': 'The lr for embedding in lora+'})
+    lorap_emb_lr: float = field(default=1e-6, metadata={'help': 'The lr for embedding in lora+'})
 
     def to_peft_config(self) -> peft.LoraConfig:
         _dict = asdict(self)
         _dict.pop('lora_dtype')
         _dict.pop('lorap_lr_ratio')
         _dict.pop('lorap_emb_lr')
         return peft.LoraConfig(**_dict)
@@ -53,40 +43,29 @@
     def save_pretrained(self, save_directory: str, **kwargs) -> None:
         self.to_peft_config().save_pretrained(save_directory, **kwargs)
         additional_args = {
             'lora_dtype': self.lora_dtype,
             'lorap_lr_ratio': self.lorap_lr_ratio,
             'lorap_emb_lr': self.lorap_emb_lr,
         }
-        with open(os.path.join(save_directory, 'additional_config.json'),
-                  'w') as f:
+        with open(os.path.join(save_directory, 'additional_config.json'), 'w') as f:
             json.dump(additional_args, f)
 
     @classmethod
-    def from_pretrained(cls,
-                        pretrained_model_name_or_path: str,
-                        subfolder: Optional[str] = None,
-                        **kwargs):
+    def from_pretrained(cls, pretrained_model_name_or_path: str, subfolder: Optional[str] = None, **kwargs):
         if hasattr(PeftConfigMixin, 'from_pretrained_origin'):
-            self = PeftConfigMixin.from_pretrained_origin(
-                pretrained_model_name_or_path, subfolder, **kwargs)
+            self = PeftConfigMixin.from_pretrained_origin(pretrained_model_name_or_path, subfolder, **kwargs)
         else:
-            self = super(LoraConfig,
-                         cls).from_pretrained(pretrained_model_name_or_path,
-                                              subfolder, **kwargs)
+            self = super(LoraConfig, cls).from_pretrained(pretrained_model_name_or_path, subfolder, **kwargs)
 
         if type(self) == peft.LoraConfig:
             self = LoraConfig(**self.to_dict())
 
-        if os.path.isfile(
-                os.path.join(pretrained_model_name_or_path,
-                             'additional_config.json')):
-            with open(
-                    os.path.join(pretrained_model_name_or_path,
-                                 'additional_config.json'), 'r') as f:
+        if os.path.isfile(os.path.join(pretrained_model_name_or_path, 'additional_config.json')):
+            with open(os.path.join(pretrained_model_name_or_path, 'additional_config.json'), 'r') as f:
                 _json = json.load(f)
                 for key, value in _json.items():
                     setattr(self, key, value)
 
         return self
 
 
@@ -102,39 +81,36 @@
 
     if target and target.__class__.__name__ == 'NonDynamicallyQuantizableLinear':
         return
 
     return self._create_and_replace_origin(*args, **kwargs)
 
 
-def _convert_dtype(target: torch.nn.Module, adapter_name: str,
-                   lora_dtype: str):
+def _convert_dtype(target: torch.nn.Module, adapter_name: str, lora_dtype: str):
     if lora_dtype == 'fp32':
         torch_dtype = torch.float32
     elif lora_dtype == 'fp16':
         torch_dtype = torch.float16
     elif lora_dtype == 'bf16':
         torch_dtype = torch.bfloat16
     else:
         torch_dtype = None
 
     if torch_dtype is not None:
         if hasattr(target, 'lora_A') and adapter_name in target.lora_A:
             target.lora_A[adapter_name].to(torch_dtype)
             target.lora_B[adapter_name].to(torch_dtype)
-        if hasattr(target, 'lora_embedding_A'
-                   ) and adapter_name in target.lora_embedding_A:
+        if hasattr(target, 'lora_embedding_A') and adapter_name in target.lora_embedding_A:
             target.lora_embedding_A[adapter_name].to(torch_dtype)
             target.lora_embedding_B[adapter_name].to(torch_dtype)
 
 
 def create_optimizer_param_groups(self: PeftModel, **defaults):
     if not isinstance(self.peft_config[self.active_adapter],
-                      LoraConfig) or self.peft_config[
-                          self.active_adapter].lorap_lr_ratio is None:
+                      LoraConfig) or self.peft_config[self.active_adapter].lorap_lr_ratio is None:
         return None
 
     def get_module(name):
         parent_idx = 2 if 'lora' in name else 1
         module_names = name.split(sep='.')[:-parent_idx]
         module = reduce(getattr, module_names, self.base_model)
         return module
@@ -190,47 +166,41 @@
     return param_groups
 
 
 def adalora_forward(self, *args, **kwargs):
     from peft.utils.integrations import gather_params_ctx
     outputs = self.model.forward(*args, **kwargs)
 
-    if (getattr(outputs, 'loss', None) is not None) and isinstance(
-            outputs.loss, torch.Tensor):
+    if (getattr(outputs, 'loss', None) is not None) and isinstance(outputs.loss, torch.Tensor):
         # Calculate the orthogonal regularization
-        orth_reg_weight = self.peft_config[
-            self.trainable_adapter_name].orth_reg_weight
+        orth_reg_weight = self.peft_config[self.trainable_adapter_name].orth_reg_weight
 
         if orth_reg_weight <= 0:
             raise ValueError('orth_reg_weight should be greater than 0. ')
 
         regu_loss = 0
         num_param = 0
         for n, p in self.model.named_parameters():
-            if ('lora_A' in n
-                    or 'lora_B' in n) and self.trainable_adapter_name in n:
+            if ('lora_A' in n or 'lora_B' in n) and self.trainable_adapter_name in n:
                 if p.shape == torch.Size([0]):
                     with gather_params_ctx(p, fwd_module=self):
                         para_cov = p @ p.T if 'lora_A' in n else p.T @ p
                 else:
                     para_cov = p @ p.T if 'lora_A' in n else p.T @ p
-                I = torch.eye(
-                    *para_cov.size(),
-                    out=torch.empty_like(para_cov))  # noqa: E741
+                I = torch.eye(*para_cov.size(), out=torch.empty_like(para_cov))  # noqa: E741
                 I.requires_grad = False
                 num_param += 1
                 if isinstance(regu_loss, torch.Tensor):
                     regu_loss = regu_loss.to(para_cov.device)
                 regu_loss += torch.norm(para_cov - I, p='fro')
         if num_param > 0:
             regu_loss = regu_loss / num_param
         else:
             regu_loss = 0
-        if isinstance(regu_loss, torch.Tensor) and isinstance(
-                outputs.loss, torch.Tensor):
+        if isinstance(regu_loss, torch.Tensor) and isinstance(outputs.loss, torch.Tensor):
             regu_loss = regu_loss.to(outputs.loss.device)
         outputs.loss += orth_reg_weight * regu_loss
     return outputs
 
 
 def adalora_mask_to_budget(self, model, budget):
     value_ipt = {}
@@ -280,58 +250,50 @@
 
     rank_pattern = {}
     # Mask the unimportant triplets
     with torch.no_grad():
         for n, p in model.named_parameters():
             if f'lora_E.{self.adapter_name}' in n:
                 p.masked_fill_(triplet_ipt[n] <= mask_threshold, 0.0)
-                rank_pattern[n] = (
-                    ~(triplet_ipt[n] <= mask_threshold)).view(-1).tolist()
+                rank_pattern[n] = (~(triplet_ipt[n] <= mask_threshold)).view(-1).tolist()
     return rank_pattern
 
 
 def hot_patch_peft_module():
     from peft.tuners.lora import LoraLayer
 
     # Fix Lora does not support NonDynamicallyQuantizableLinear
     LoraModel._create_and_replace_origin = LoraModel._create_and_replace
     LoraModel._create_and_replace = _create_and_replace_hook
 
     # Support type conversion
-    def init(self, model: torch.nn.Module, config: Dict[str, LoraConfig],
-             adapter_name):
+    def init(self, model: torch.nn.Module, config: Dict[str, LoraConfig], adapter_name):
         self.__init_origin__(model, config, adapter_name)
-        active_config = config[self.active_adapter] if isinstance(
-            config, dict) else config
+        active_config = config[self.active_adapter] if isinstance(config, dict) else config
         if hasattr(active_config, 'lora_dtype'):
             for name, module in model.named_modules():
                 if isinstance(module, LoraLayer):
-                    _convert_dtype(module, self.active_adapter,
-                                   active_config.lora_dtype)
+                    _convert_dtype(module, self.active_adapter, active_config.lora_dtype)
 
     LoraModel.__init_origin__ = LoraModel.__init__
     LoraModel.__init__ = init
 
     # Support LoRA+
     PeftModel.create_optimizer_param_groups = create_optimizer_param_groups
 
     PeftConfigMixin.from_pretrained_origin = PeftConfigMixin.from_pretrained
     PeftConfigMixin.from_pretrained = LoraConfig.from_pretrained
 
     # Compatible with SwiftModel
     def dummy_function(*args, **kwargs):
-        logger.warn(
-            f'The function {kwargs["func"]} has no effects, consider using other functions.'
-        )
+        logger.warn(f'The function {kwargs["func"]} has no effects, consider using other functions.')
 
     PeftModel.activate_adapter = PeftModel.set_adapter
-    PeftModel.deactivate_adapter = partial(
-        dummy_function, func='deactivate_adapter')
-    PeftModel.set_active_adapters = partial(
-        dummy_function, func='set_active_adapters')
+    PeftModel.deactivate_adapter = partial(dummy_function, func='deactivate_adapter')
+    PeftModel.set_active_adapters = partial(dummy_function, func='set_active_adapters')
 
     # Fix adalora does not support device_map
     from peft.tuners.adalora import AdaLoraModel, RankAllocator
     AdaLoraModel.forward = adalora_forward
     RankAllocator.mask_to_budget = adalora_mask_to_budget
 
 
@@ -344,24 +306,18 @@
     Returns:
         The wrapper
     """
 
     class PeftWrapper(module_class):
 
         @classmethod
-        def from_pretrained(cls,
-                            model,
-                            model_id,
-                            *args,
-                            revision: Optional[str] = None,
-                            **kwargs):
+        def from_pretrained(cls, model, model_id, *args, revision: Optional[str] = None, **kwargs):
             if not os.path.exists(model_id):
                 model_id = snapshot_download(model_id, revision=revision)
-            return module_class.from_pretrained(model, model_id, *args,
-                                                **kwargs)
+            return module_class.from_pretrained(model, model_id, *args, **kwargs)
 
     PeftWrapper.__name__ = module_class.__name__
     return PeftWrapper
 
 
 def wrap_module(module):
     if not hasattr(module, 'from_pretrained'):
@@ -370,16 +326,15 @@
     return get_wrapped_class(module)
 
 
 hot_patch_peft_module()
 PeftModel = wrap_module(PeftModel)
 PeftConfig = wrap_module(PeftConfig)
 PeftModelForSeq2SeqLM = wrap_module(PeftModelForSeq2SeqLM)
-PeftModelForSequenceClassification = wrap_module(
-    PeftModelForSequenceClassification)
+PeftModelForSequenceClassification = wrap_module(PeftModelForSequenceClassification)
 PeftModelForTokenClassification = wrap_module(PeftModelForTokenClassification)
 PeftModelForCausalLM = wrap_module(PeftModelForCausalLM)
 PromptEncoderConfig = wrap_module(PromptEncoderConfig)
 PromptTuningConfig = wrap_module(PromptTuningConfig)
 PrefixTuningConfig = wrap_module(PrefixTuningConfig)
 PromptLearningConfig = wrap_module(PromptLearningConfig)
 LoraConfig = wrap_module(LoraConfig)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/prompt.py` & `ms-swift-2.0.4/swift/tuners/prompt.py`

 * *Files 22% similar despite different names*

```diff
@@ -35,162 +35,117 @@
         attention_mask_pos(Union[str, int]): The position of the attention mask
         attention_mask_value(Union[float, int, bool]): The value to pad to the attention mask
         prompt_length(int): The length of the prompt tokens
         attach_front(bool): When set to True, prompt is attached in front of the embedding
         extract_embedding(bool): Whether the embedding is extracted at final stage to keep the same dims with inputs
     """
 
-    dim: Union[int, List[int]] = field(
-        default=None, metadata={'help': 'The dimension of the hidden states'})
+    dim: Union[int, List[int]] = field(default=None, metadata={'help': 'The dimension of the hidden states'})
 
-    target_modules: str = field(
-        default=None,
-        metadata={'help': 'The layer module to be replaced, in regex format'})
-
-    embedding_pos: Union[str, int] = field(
-        default=None,
-        metadata={'help': 'The position of the embedding tensor'})
+    target_modules: str = field(default=None, metadata={'help': 'The layer module to be replaced, in regex format'})
 
-    attention_mask_pos: Union[str, int] = field(
-        default=None, metadata={'help': 'The position of the attention mask'})
+    embedding_pos: Union[str, int] = field(default=None, metadata={'help': 'The position of the embedding tensor'})
+
+    attention_mask_pos: Union[str, int] = field(default=None, metadata={'help': 'The position of the attention mask'})
 
     attention_mask_value: Union[float, int, bool] = field(
-        default=0.,
-        metadata={'help': 'The value to pad to the attention mask'})
+        default=0., metadata={'help': 'The value to pad to the attention mask'})
 
-    prompt_length: int = field(
-        default=16, metadata={'help': 'The length of the prompt tokens'})
+    prompt_length: int = field(default=16, metadata={'help': 'The length of the prompt tokens'})
 
     attach_front: bool = field(
-        default=True,
-        metadata={
-            'help':
-            'When set to True, prompt is attached in front of the embedding'
-        })
+        default=True, metadata={'help': 'When set to True, prompt is attached in front of the embedding'})
 
     extract_embedding: bool = field(
         default=False,
-        metadata={
-            'help':
-            'Whether the embedding is extracted at final stage to keep the same dims with inputs'
-        })
+        metadata={'help': 'Whether the embedding is extracted at final stage to keep the same dims with inputs'})
 
     def __post_init__(self):
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.PROMPT
 
 
 class Prompt(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: PromptConfig,
-                      adapter_name: str):
+    def prepare_model(model: nn.Module, config: PromptConfig, adapter_name: str):
         module_keys = [key for key, _ in model.named_modules()]
         match_module_keys = []
         for module_key in module_keys:
             if isinstance(config.target_modules, str):
-                target_module_found = re.fullmatch(config.target_modules,
-                                                   module_key)
+                target_module_found = re.fullmatch(config.target_modules, module_key)
             else:
-                target_module_found = any(
-                    module_key.endswith(target_key)
-                    for target_key in config.target_modules)
+                target_module_found = any(module_key.endswith(target_key) for target_key in config.target_modules)
             if target_module_found:  # noqa
                 module = model.get_submodule(module_key)
 
                 def _forward(self, *args, **kwargs):
                     if isinstance(config.embedding_pos, int):
                         input_embedding = args[config.embedding_pos]
                     else:
                         input_embedding = kwargs[config.embedding_pos]
 
-                    input_embedding = getattr(
-                        self,
-                        f'prompt_{adapter_name}').forward(input_embedding)
+                    input_embedding = getattr(self, f'prompt_{adapter_name}').forward(input_embedding)
                     if isinstance(config.embedding_pos, int):
                         args = type(args)(
-                            args[0:config.embedding_pos] + (input_embedding, )
-                            + args[config.embedding_pos + 1:])
+                            args[0:config.embedding_pos] + (input_embedding, ) + args[config.embedding_pos + 1:])
                     else:
                         kwargs[config.embedding_pos] = input_embedding
 
                     if config.attention_mask_pos:
                         attention_mask = None
                         if isinstance(config.attention_mask_pos, int):
                             attention_mask = args[config.attention_mask_pos]
                         elif isinstance(config.attention_mask_pos, str):
                             attention_mask = kwargs[config.attention_mask_pos]
 
                         if attention_mask is not None:
-                            attention_mask = getattr(
-                                self,
-                                f'prompt_{adapter_name}').patch_attention_mask(
-                                    attention_mask)
+                            attention_mask = getattr(self,
+                                                     f'prompt_{adapter_name}').patch_attention_mask(attention_mask)
                         if isinstance(config.attention_mask_pos, int):
                             args = type(args)(
-                                args[0:config.attention_mask_pos]
-                                + (attention_mask, )
+                                args[0:config.attention_mask_pos] + (attention_mask, )
                                 + args[config.attention_mask_pos + 1:])
                         else:
                             kwargs[config.attention_mask_pos] = attention_mask
 
-                    forward_output = getattr(
-                        self, f'forward_origin_{adapter_name}')(*args,
-                                                                **kwargs)
+                    forward_output = getattr(self, f'forward_origin_{adapter_name}')(*args, **kwargs)
                     if config.extract_embedding:
-                        forward_output = getattr(
-                            self,
-                            f'prompt_{adapter_name}').extract(forward_output)
+                        forward_output = getattr(self, f'prompt_{adapter_name}').extract(forward_output)
 
                     return forward_output
 
-                setattr(module, f'forward_origin_{adapter_name}',
-                        module.forward)
+                setattr(module, f'forward_origin_{adapter_name}', module.forward)
                 module.forward = types.MethodType(_forward, module)
                 if isinstance(config.dim, list):
                     input_dim = config.dim[len(match_module_keys)]
                 else:
                     input_dim = config.dim
-                prompt_module = PromptModule(input_dim,
-                                             int(module_key.rsplit('.')[-1]),
-                                             adapter_name, module_key,
-                                             config.prompt_length,
-                                             config.attention_mask_value,
-                                             config.attach_front)
+                prompt_module = PromptModule(input_dim, int(module_key.rsplit('.')[-1]), adapter_name, module_key,
+                                             config.prompt_length, config.attention_mask_value, config.attach_front)
                 setattr(module, f'prompt_{adapter_name}', prompt_module)
-                logger.info(
-                    f'Prompt modules(module_key): {module_key}.prompt_{adapter_name}'
-                )
+                logger.info(f'Prompt modules(module_key): {module_key}.prompt_{adapter_name}')
                 match_module_keys.append(module_key)
 
         def state_dict_callback(state_dict, adapter_name):
-            return {
-                key: value
-                for key, value in state_dict.items()
-                if f'prompt_{adapter_name}' in key
-            }
+            return {key: value for key, value in state_dict.items() if f'prompt_{adapter_name}' in key}
 
         def mark_trainable_callback(model):
             return
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         modules = find_sub_module(module, f'prompt_{adapter_name}')
         for _module in modules:
             _module: ActivationMixin
             _module: nn.Module
             _module.set_activation(adapter_name, activate)
-            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key,
-                                     activate, offload)
+            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key, activate, offload)
 
 
 class PromptModule(nn.Module, ActivationMixin):
     """The implementation of vision prompt tuning method.
 
     Visual prompt tuning (VPT) is proposed to initialize tunable prompt tokens
     and prepend to the original tokens in the first layer or multiple layers.
@@ -199,58 +154,47 @@
 
     Attributes:
         dim: An integer indicating the embedding dimension.
         layer_num: An integer indicating number of layers.
         prompt_length: An integer indicating the length of vision prompt tuning.
     """
 
-    def __init__(self,
-                 dim,
-                 layer_num,
-                 adapter_name,
-                 module_key,
-                 prompt_length=None,
-                 mask_values=0.,
-                 attach_front=True):
+    def __init__(self, dim, layer_num, adapter_name, module_key, prompt_length=None, mask_values=0., attach_front=True):
         super(PromptModule, self).__init__()
         super(nn.Module, self).__init__(module_key)
         self.dim = dim
         self.layer_num = layer_num
         self.adapter_name = adapter_name
         self.prompt_length = prompt_length
         self.mask_values = mask_values
         self.attach_front = attach_front
         self.prompt_token = nn.Parameter(torch.zeros(1, prompt_length, dim))
         nn.init.xavier_uniform_(self.prompt_token)
 
     def forward(self, x):
         if not self.is_activated(self.adapter_name):
             return x
-        prompt_token = self.prompt_token.expand(x.shape[0], -1,
-                                                -1).to(x.device, x.dtype)
+        prompt_token = self.prompt_token.expand(x.shape[0], -1, -1).to(x.device, x.dtype)
 
         if self.layer_num == 0:
             if self.attach_front:
                 x = torch.cat((prompt_token, x), dim=1)
             else:
                 x = torch.cat((x, prompt_token), dim=1)
         else:
             if self.attach_front:
-                x = torch.cat((prompt_token, x[:, self.prompt_length:, :]),
-                              dim=1)
+                x = torch.cat((prompt_token, x[:, self.prompt_length:, :]), dim=1)
             else:
-                x = torch.cat((x[:, :-self.prompt_length, :], prompt_token),
-                              dim=1)
+                x = torch.cat((x[:, :-self.prompt_length, :], prompt_token), dim=1)
         return x
 
     def patch_attention_mask(self, m):
         if not self.is_activated(self.adapter_name):
             return m
-        prefix_attention_mask = torch.full((*m.shape[:-1], self.prompt_length),
-                                           self.mask_values).to(m.device)
+        prefix_attention_mask = torch.full((*m.shape[:-1], self.prompt_length), self.mask_values).to(m.device)
         if self.attach_front:
             return torch.cat((prefix_attention_mask, m), dim=-1)
         else:
             return torch.cat((m, prefix_attention_mask), dim=-1)
 
     def extract(self, x):
         if self.attach_front:
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/restuning.py` & `ms-swift-2.0.4/swift/tuners/restuning.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,16 +6,15 @@
 from typing import Dict, List, Optional, Union
 
 import torch
 import torch.nn as nn
 
 from swift import get_logger
 from swift.utils.torch_utils import find_sub_module
-from .restuning_components import (ResTuner, detach_tensors,
-                                   probe_input_pre_hook, probe_output_hook)
+from .restuning_components import ResTuner, detach_tensors, probe_input_pre_hook, probe_output_hook
 from .utils import ActivationMixin, SwiftAdapter, SwiftConfig, SwiftOutput
 
 logger = get_logger()
 
 
 @dataclass
 class ResTuningConfig(SwiftConfig):
@@ -51,88 +50,58 @@
         default=None,
         metadata={
             'help':
             'The root module to be replaced, can a regex string (use the first matching module) or full match format'
         })
 
     root_modules_hook: str = field(
-        default='input',
-        metadata={
-            'help': 'The hook type of root modules, can be "input" or "output"'
-        })
+        default='input', metadata={'help': 'The hook type of root modules, can be "input" or "output"'})
 
     stem_modules: Optional[Union[List[str], str]] = field(
         default=None,
-        metadata={
-            'help':
-            'The stem modules to be replaced, can a regex string or name list of full match format'
-        })
+        metadata={'help': 'The stem modules to be replaced, can a regex string or name list of full match format'})
 
     stem_modules_hook: str = field(
-        default='output',
-        metadata={
-            'help': 'The hook type of stem modules, can be "input" or "output"'
-        })
+        default='output', metadata={'help': 'The hook type of stem modules, can be "input" or "output"'})
 
     target_modules: str = field(
         default=None,
         metadata={
             'help':
             'The target module to be replaced, can a regex string (use the first matching module) or full match format'
         })
 
     target_modules_hook: str = field(
-        default='input',
-        metadata={
-            'help':
-            'The hook type of target modules, can be "input" or "output"'
-        })
+        default='input', metadata={'help': 'The hook type of target modules, can be "input" or "output"'})
 
     target_hidden_pos: Union[int, str] = field(
-        default=None,
-        metadata={
-            'help':
-            'The position of the hidden state for target modules output'
-        })
+        default=None, metadata={'help': 'The position of the hidden state for target modules output'})
 
     tuner_cfg: Optional[Union[List[Dict], Dict, str]] = field(
-        default=None,
-        metadata={
-            'help':
-            'The configuration of the tuning module, can a string or customized config'
-        })
+        default=None, metadata={'help': 'The configuration of the tuning module, can a string or customized config'})
 
-    use_upsample: bool = field(
-        default=False,
-        metadata={'help': 'Whether to use auxiliary upsample module'})
+    use_upsample: bool = field(default=False, metadata={'help': 'Whether to use auxiliary upsample module'})
 
     upsample_out_channels: List[int] = field(
-        default=None,
-        metadata={
-            'help':
-            'The number of output channels when "use_upsample" is set to "True"'
-        })
+        default=None, metadata={'help': 'The number of output channels when "use_upsample" is set to "True"'})
 
-    zero_init_last: bool = field(
-        default=False, metadata={'help': 'Zero init last weight'})
+    zero_init_last: bool = field(default=False, metadata={'help': 'Zero init last weight'})
 
-    use_bypass: bool = field(
-        default=True, metadata={'help': 'Whether to use bypass'})
+    use_bypass: bool = field(default=True, metadata={'help': 'Whether to use bypass'})
 
     def __post_init__(self):
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.RESTUNING
         self.target_hidden_pos = 0 if self.target_hidden_pos is None else self.target_hidden_pos
 
 
 class ResTuning(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: ResTuningConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: nn.Module, config: ResTuningConfig, adapter_name: str) -> SwiftOutput:
         """Prepare a model with `ResTuningConfig`"""
 
         def _forward_seq(self, input, *args, **kwargs):
             for idx, module in enumerate(self):
                 if idx >= len(self.origin_module_keys):
                     continue
                 input = module(input)
@@ -146,22 +115,18 @@
                 else:
                     _arg = kwargs[self.target_hidden_pos]
                 args_main = _forward_restuning(self, _arg)
                 if isinstance(self.target_hidden_pos, int):
                     args[self.target_hidden_pos] = args_main
                 else:
                     kwargs[self.target_hidden_pos] = args_main
-                args_main = getattr(self,
-                                    f'forward_origin_{adapter_name}')(*args,
-                                                                      **kwargs)
+                args_main = getattr(self, f'forward_origin_{adapter_name}')(*args, **kwargs)
             else:
-                _args_main = getattr(self, f'forward_origin_{adapter_name}')(
-                    *args, **kwargs)
-                _arg = _args_main[self.target_hidden_pos] if isinstance(
-                    _args_main, (tuple, list, dict)) else _args_main
+                _args_main = getattr(self, f'forward_origin_{adapter_name}')(*args, **kwargs)
+                _arg = _args_main[self.target_hidden_pos] if isinstance(_args_main, (tuple, list, dict)) else _args_main
                 args_main = _forward_restuning(self, _arg)
                 if type(_args_main) != type(args_main):
                     _args_main[self.target_hidden_pos] = args_main
                     args_main = _args_main
             return args_main
 
         def _forward_restuning(self, origin_arg):
@@ -177,36 +142,30 @@
             for i, st_mod in enumerate(stem_module_ins_list):
                 if i == 0 and root_module_ins is None:
                     probe_results.append(st_mod.probe_input_data)
                 if st_mod.stem_modules_hook == 'input':
                     probe_results.append(st_mod.probe_input_data)
                 else:
                     probe_results.append(st_mod.probe_output_data)
-            args_main = getattr(top_module,
-                                f'restuning_{adapter_name}')(probe_results,
-                                                             origin_arg)
+            args_main = getattr(top_module, f'restuning_{adapter_name}')(probe_results, origin_arg)
             return args_main
 
         # 1. Matching the root module
         module_keys = [key for key, _ in model.named_modules()]
         root_module_ins_list = []
         if config.root_modules:
             for module_key in module_keys:
                 if re.fullmatch(config.root_modules, module_key):
                     root_module = model.get_submodule(module_key)
-                    logger.info(
-                        f'Matching root module [{module_key}] of type {type(root_module)}'
-                    )
+                    logger.info(f'Matching root module [{module_key}] of type {type(root_module)}')
                     if isinstance(root_module, (nn.ModuleList, nn.ModuleDict)):
                         logger.warning(
-                            f'Type of {type(root_module)} may not be supported because of its customized forward'
-                        )
+                            f'Type of {type(root_module)} may not be supported because of its customized forward')
                     if config.root_modules_hook == 'input':
-                        root_module.register_forward_pre_hook(
-                            probe_input_pre_hook)
+                        root_module.register_forward_pre_hook(probe_input_pre_hook)
                     else:
                         root_module.register_forward_hook(probe_output_hook)
                     root_module.root_modules_hook = config.root_modules_hook
                     root_module_ins_list.append(root_module)
                     break
             if len(root_module_ins_list) == 0:
                 logger.error('Cannot match root modules')
@@ -215,109 +174,85 @@
         stem_module_ins_list = []
         stem_module_ins_index = []
         for module_key in module_keys:
             if (isinstance(config.stem_modules, str) and re.fullmatch(config.stem_modules, module_key)) or \
                     (isinstance(config.stem_modules, list) and module_key in config.stem_modules):
                 stem_module = model.get_submodule(module_key)
                 if isinstance(config.stem_modules, list):
-                    stem_module_ins_index.append(
-                        config.stem_modules.index(module_key))
-                logger.info(
-                    f'Matching stem module [{module_key}] of type {type(stem_module)}'
-                )
+                    stem_module_ins_index.append(config.stem_modules.index(module_key))
+                logger.info(f'Matching stem module [{module_key}] of type {type(stem_module)}')
                 if isinstance(stem_module, (nn.ModuleList, nn.ModuleDict)):
                     logger.warning(
-                        f'Type of {type(stem_module)} may not be supported because of its customized forward'
-                    )
-                if len(root_module_ins_list) == 0 and len(
-                        stem_module_ins_list) == 0:
+                        f'Type of {type(stem_module)} may not be supported because of its customized forward')
+                if len(root_module_ins_list) == 0 and len(stem_module_ins_list) == 0:
                     stem_module.register_forward_pre_hook(probe_input_pre_hook)
                 if config.stem_modules_hook == 'input':
                     stem_module.register_forward_pre_hook(probe_input_pre_hook)
                 else:
                     stem_module.register_forward_hook(probe_output_hook)
                 stem_module.stem_modules_hook = config.stem_modules_hook
                 stem_module_ins_list.append(stem_module)
         if isinstance(config.stem_modules, list):
             stem_module_ins_list = [
-                stem_module_ins_list[stem_module_ins_index.index(i)]
-                for i in range(len(stem_module_ins_index))
+                stem_module_ins_list[stem_module_ins_index.index(i)] for i in range(len(stem_module_ins_index))
             ]
         depth = len(stem_module_ins_list)
         if len(stem_module_ins_list) == 0:
             raise Exception('Cannot match source modules')
 
         # 3. Init restuning module
         if len(stem_module_ins_list) != 0:
             top_module = model.get_submodule('')
-            restuning_module = ResTuningBypassModule(
-                config.dims, depth, adapter_name, config.use_upsample,
-                config.upsample_out_channels, config.zero_init_last,
-                config.tuner_cfg)
+            restuning_module = ResTuningBypassModule(config.dims, depth, adapter_name, config.use_upsample,
+                                                     config.upsample_out_channels, config.zero_init_last,
+                                                     config.tuner_cfg)
             setattr(top_module, f'restuning_{adapter_name}', restuning_module)
 
         # 4. Matching the target module
         target_module_ins = None
         for module_key in module_keys:
             if re.fullmatch(config.target_modules, module_key):
                 tgt_module = model.get_submodule(module_key)
-                logger.info(
-                    f'Matching target module [{module_key}] of type {type(tgt_module)}'
-                )
+                logger.info(f'Matching target module [{module_key}] of type {type(tgt_module)}')
                 if isinstance(tgt_module, (nn.ModuleList, nn.ModuleDict)):
                     raise Exception(
-                        f'Type of {type(tgt_module)} may not be supported because of its customized forward'
-                    )
+                        f'Type of {type(tgt_module)} may not be supported because of its customized forward')
 
                 tgt_module.target_modules_hook = config.target_modules_hook
                 tgt_module.target_hidden_pos = config.target_hidden_pos
                 tgt_module.root_module_ins_list = root_module_ins_list
                 tgt_module.stem_module_ins_list = stem_module_ins_list
                 target_module_ins = tgt_module
 
-                if isinstance(tgt_module, nn.Sequential) and not hasattr(
-                        tgt_module, 'origin_module_keys'):
-                    tgt_module.origin_module_keys = copy.deepcopy(
-                        list(tgt_module._modules.keys()))
+                if isinstance(tgt_module, nn.Sequential) and not hasattr(tgt_module, 'origin_module_keys'):
+                    tgt_module.origin_module_keys = copy.deepcopy(list(tgt_module._modules.keys()))
 
-                    setattr(tgt_module, f'forward_origin_{adapter_name}',
-                            types.MethodType(_forward_seq, tgt_module))
+                    setattr(tgt_module, f'forward_origin_{adapter_name}', types.MethodType(_forward_seq, tgt_module))
                 else:
-                    setattr(tgt_module, f'forward_origin_{adapter_name}',
-                            tgt_module.forward)
-                tgt_module.forward = types.MethodType(_forward_target,
-                                                      tgt_module)
+                    setattr(tgt_module, f'forward_origin_{adapter_name}', tgt_module.forward)
+                tgt_module.forward = types.MethodType(_forward_target, tgt_module)
         if target_module_ins is None:
             raise Exception('Cannot match target modules')
 
         def state_dict_callback(state_dict, adapter_name):
-            return {
-                key: value
-                for key, value in state_dict.items()
-                if f'restuning_{adapter_name}' in key
-            }
+            return {key: value for key, value in state_dict.items() if f'restuning_{adapter_name}' in key}
 
         def mark_trainable_callback(model):
             return
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         modules = find_sub_module(module, f'restuning_{adapter_name}')
         for _module in modules:
             _module: ActivationMixin
             _module: nn.Module
             _module.set_activation(adapter_name, activate)
-            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key,
-                                     activate, offload)
+            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key, activate, offload)
 
 
 class ResTuningBypassModule(nn.Module, ActivationMixin):
     """The implementation of ResTuningBypass method.
     """
 
     def __init__(
@@ -336,71 +271,53 @@
 
         self.bypass_blocks = nn.Sequential(*[
             ResTunerBypassBlock(
                 dim=dims[i] if isinstance(dims, list) else dims,
                 layer_num=i,
                 depth=depth,
                 use_upsample=use_upsample,
-                upsample_out_channels=upsample_out_channels[i] if isinstance(
-                    upsample_out_channels, list) else upsample_out_channels,
+                upsample_out_channels=upsample_out_channels[i] if isinstance(upsample_out_channels, list
+                                                                             ) else upsample_out_channels,
                 zero_init_last=zero_init_last,
-                tuner_cfg=tuner_cfg[i] if isinstance(tuner_cfg, list
-                                                     ) else tuner_cfg)
-            for i in range(depth)
+                tuner_cfg=tuner_cfg[i] if isinstance(tuner_cfg, list) else tuner_cfg) for i in range(depth)
         ])
 
     def forward(self, x_list, origin_arg, **kwargs):
         if not self.is_activated(self.adapter_name):
             return origin_arg
         x_bypass = detach_tensors(x_list.pop(0))
-        x_bypass = x_bypass[0] if isinstance(x_bypass,
-                                             (list, tuple)) else x_bypass
+        x_bypass = x_bypass[0] if isinstance(x_bypass, (list, tuple)) else x_bypass
         x_list = detach_tensors(x_list)
-        x_list = [
-            _x[0] if isinstance(_x, (list, tuple)) else _x for _x in x_list
-        ]
+        x_list = [_x[0] if isinstance(_x, (list, tuple)) else _x for _x in x_list]
         for i, (bp_blk, x_stem) in enumerate(zip(self.bypass_blocks, x_list)):
-            target_size = x_list[
-                i + 1].shape[2:] if i < len(x_list) - 1 else None
+            target_size = x_list[i + 1].shape[2:] if i < len(x_list) - 1 else None
             x_bypass = bp_blk(x_stem, x_bypass, target_size, **kwargs)
         return x_bypass
 
 
 class ResTunerBypassBlock(nn.Module):
 
-    def __init__(self,
-                 dim,
-                 layer_num=-1,
-                 depth=-1,
-                 use_upsample=False,
-                 zero_init_last=False,
-                 tuner_cfg=None,
-                 **kwargs):
+    def __init__(self, dim, layer_num=-1, depth=-1, use_upsample=False, zero_init_last=False, tuner_cfg=None, **kwargs):
         super().__init__()
         self.layer_num = layer_num
         self.depth = depth
 
         if isinstance(tuner_cfg, str):
             lateral_cfg = tuner_cfg
             vertical_cfg = tuner_cfg
             aux_cfg = 'upsample' if use_upsample and layer_num != depth - 1 else None
         elif isinstance(tuner_cfg, dict):
-            lateral_cfg = tuner_cfg[
-                'lateral_cfg'] if 'lateral_cfg' in tuner_cfg else None
-            vertical_cfg = tuner_cfg[
-                'vertical_cfg'] if 'vertical_cfg' in tuner_cfg else None
+            lateral_cfg = tuner_cfg['lateral_cfg'] if 'lateral_cfg' in tuner_cfg else None
+            vertical_cfg = tuner_cfg['vertical_cfg'] if 'vertical_cfg' in tuner_cfg else None
             aux_cfg = tuner_cfg['aux_cfg'] if 'aux_cfg' in tuner_cfg else None
 
-        self.lateral_tuner = ResTuner(dim, layer_num, depth, zero_init_last,
-                                      'lateral', lateral_cfg, **kwargs)
-        self.vertical_tuner = ResTuner(dim, layer_num, depth, zero_init_last,
-                                       'vertical', vertical_cfg, **kwargs)
+        self.lateral_tuner = ResTuner(dim, layer_num, depth, zero_init_last, 'lateral', lateral_cfg, **kwargs)
+        self.vertical_tuner = ResTuner(dim, layer_num, depth, zero_init_last, 'vertical', vertical_cfg, **kwargs)
         if aux_cfg and len(aux_cfg) != 0:
-            self.aux_tuner = ResTuner(dim, layer_num, depth, zero_init_last,
-                                      'aux', aux_cfg, **kwargs)
+            self.aux_tuner = ResTuner(dim, layer_num, depth, zero_init_last, 'aux', aux_cfg, **kwargs)
 
     def forward(self, x_stem, x_bypass, target_size=None, **kwargs):
         x_lateral = self.lateral_tuner(x_stem)
         x_vertical = self.vertical_tuner(x_bypass)
 
         x_bypass_out = x_lateral + x_vertical
         if hasattr(self, 'aux_tuner'):
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/restuning_components.py` & `ms-swift-2.0.4/swift/tuners/restuning_components.py`

 * *Files 6% similar despite different names*

```diff
@@ -9,69 +9,55 @@
 from swift.utils.logger import get_logger
 
 logger = get_logger()
 
 
 class ResTuner(nn.Module):
 
-    def __init__(self,
-                 dim=None,
-                 layer_num=-1,
-                 depth=-1,
-                 zero_init_last=False,
-                 stage='',
-                 tuner_cfg={},
-                 **kwargs):
+    def __init__(self, dim=None, layer_num=-1, depth=-1, zero_init_last=False, stage='', tuner_cfg={}, **kwargs):
         super().__init__()
         self.dim = dim
         self.layer_num = layer_num
         self.depth = depth
         self.stage = stage
         self.tuner_cfg = tuner_cfg
 
         if (isinstance(tuner_cfg, str) and tuner_cfg == 'res_adapter') or \
                 (isinstance(tuner_cfg, dict) and 'res_adapter' in tuner_cfg):
-            tuner_cfg = tuner_cfg['res_adapter'] if isinstance(
-                tuner_cfg, dict) else tuner_cfg
+            tuner_cfg = tuner_cfg['res_adapter'] if isinstance(tuner_cfg, dict) else tuner_cfg
             self.tuner = ResAdapter(
                 dim=dim,
                 layer_num=layer_num,
                 depth=depth,
                 zero_init_last=zero_init_last,
                 stage=stage,
                 tuner_cfg=tuner_cfg,
                 **kwargs)
         elif (isinstance(tuner_cfg, str) and tuner_cfg == 'res_group_adapter') or \
                 (isinstance(tuner_cfg, dict) and 'res_group_adapter' in tuner_cfg):
-            tuner_cfg = tuner_cfg['res_group_adapter'] if isinstance(
-                tuner_cfg, dict) else tuner_cfg
+            tuner_cfg = tuner_cfg['res_group_adapter'] if isinstance(tuner_cfg, dict) else tuner_cfg
             self.tuner = ResGroupAdapter(
                 dim=dim,
                 layer_num=layer_num,
                 depth=depth,
                 zero_init_last=zero_init_last,
                 stage=stage,
                 tuner_cfg=tuner_cfg,
                 **kwargs)
         elif (isinstance(tuner_cfg, str) and tuner_cfg == 'upsample') or \
                 (isinstance(tuner_cfg, dict) and 'upsample' in tuner_cfg):
-            tuner_cfg = tuner_cfg['upsample'] if isinstance(
-                tuner_cfg, dict) else tuner_cfg
+            tuner_cfg = tuner_cfg['upsample'] if isinstance(tuner_cfg, dict) else tuner_cfg
             if 'upsample_out_channels' in kwargs:
                 out_channels = kwargs['upsample_out_channels']
                 use_conv = True if out_channels else False
             else:
                 out_channels = dim
                 use_conv = False
             self.tuner = Upsample(
-                channels=dim,
-                use_conv=use_conv,
-                out_channels=out_channels,
-                tuner_cfg=tuner_cfg,
-                **kwargs)
+                channels=dim, use_conv=use_conv, out_channels=out_channels, tuner_cfg=tuner_cfg, **kwargs)
         else:
             self.tuner = Identity()
 
     def forward(self, x, *args, **kwargs):
         if self.tuner_cfg == 'zero' or 'zero' in self.tuner_cfg:
             x_out = 0.0
         else:
@@ -91,38 +77,31 @@
                  act_layer=nn.GELU,
                  **kwargs):
         super(ResAdapter, self).__init__()
         self.dim = dim
         self.layer_num = layer_num
         self.depth = depth
 
-        self.adapter_length = tuner_cfg[
-            'adapter_length'] if 'adapter_length' in tuner_cfg else 32
-        self.adapter_type = tuner_cfg[
-            'adapter_type'] if 'adapter_type' in tuner_cfg else None
-        self.adapter_weight = tuner_cfg[
-            'adapter_weight'] if 'adapter_weight' in tuner_cfg else None
-
-        self.adapter_length = self.adapter_length[
-            self.layer_num] if isinstance(self.adapter_length,
-                                          list) else self.adapter_length
-        assert isinstance(self.adapter_length,
-                          int) or (isinstance(self.adapter_length, tuple)
-                                   and len(self.adapter_length) == 3)
+        self.adapter_length = tuner_cfg['adapter_length'] if 'adapter_length' in tuner_cfg else 32
+        self.adapter_type = tuner_cfg['adapter_type'] if 'adapter_type' in tuner_cfg else None
+        self.adapter_weight = tuner_cfg['adapter_weight'] if 'adapter_weight' in tuner_cfg else None
+
+        self.adapter_length = self.adapter_length[self.layer_num] if isinstance(self.adapter_length,
+                                                                                list) else self.adapter_length
+        assert isinstance(self.adapter_length, int) or (isinstance(self.adapter_length, tuple)
+                                                        and len(self.adapter_length) == 3)
         if isinstance(self.adapter_length, int):
             self.ln1 = nn.Linear(dim, self.adapter_length)
         else:
-            self.ln1 = nn.Linear(self.adapter_length[0],
-                                 self.adapter_length[1])
+            self.ln1 = nn.Linear(self.adapter_length[0], self.adapter_length[1])
         self.activate = act_layer()
         if isinstance(self.adapter_length, int):
             self.ln2 = nn.Linear(self.adapter_length, dim)
         else:
-            self.ln2 = nn.Linear(self.adapter_length[1],
-                                 self.adapter_length[2])
+            self.ln2 = nn.Linear(self.adapter_length[1], self.adapter_length[2])
             dim = self.adapter_length[2]
 
         self._xavier_init_weights(self.ln1)
         if zero_init_last and layer_num == depth - 1:
             self._zero_init_weights(self.ln2)
         else:
             self._xavier_init_weights(self.ln2)
@@ -153,29 +132,26 @@
             self._prepared = True
 
         x_dtype = x.dtype
         x = x.to(self.ln1.weight.dtype)
         x_shortcut = x
         if len(x_shortcut.size()) == 4:
             B, C, N1, N2 = x.size()
-            x = x.view(x_shortcut.size()[0],
-                       x_shortcut.size()[1], -1).permute(0, 2, 1)
+            x = x.view(x_shortcut.size()[0], x_shortcut.size()[1], -1).permute(0, 2, 1)
 
         x_adapter = self.ln2(self.activate(self.ln1(x)))
 
         if self.adapter_weight:
-            x_adapter = apply_data_weight(x_adapter, self.scaling,
-                                          self.adapter_weight)
+            x_adapter = apply_data_weight(x_adapter, self.scaling, self.adapter_weight)
 
         if len(x_shortcut.size()) == 4:
-            x_adapter = x_adapter.permute(0, 2,
-                                          1).view(x_shortcut.size()[0],
-                                                  x_adapter.size()[-1],
-                                                  x_shortcut.size()[2],
-                                                  x_shortcut.size()[3])
+            x_adapter = x_adapter.permute(0, 2, 1).view(x_shortcut.size()[0],
+                                                        x_adapter.size()[-1],
+                                                        x_shortcut.size()[2],
+                                                        x_shortcut.size()[3])
         x_out = x_shortcut + x_adapter
         return x_out.to(x_dtype)
 
 
 class ResGroupAdapter(nn.Module):
 
     def __init__(self,
@@ -188,31 +164,26 @@
                  act_layer=nn.GELU,
                  **kwargs):
         super(ResGroupAdapter, self).__init__()
         self.dim = dim
         self.layer_num = layer_num
         self.depth = depth
 
-        self.adapter_type = tuner_cfg[
-            'adapter_type'] if 'adapter_type' in tuner_cfg else None
-        self.adapter_weight = tuner_cfg[
-            'adapter_weight'] if 'adapter_weight' in tuner_cfg else None
+        self.adapter_type = tuner_cfg['adapter_type'] if 'adapter_type' in tuner_cfg else None
+        self.adapter_weight = tuner_cfg['adapter_weight'] if 'adapter_weight' in tuner_cfg else None
 
         self.adapter_dim = tuner_cfg['dim'] if 'dim' in tuner_cfg else dim
         self.adapter_head = tuner_cfg['head'] if 'head' in tuner_cfg else 4
-        self.adapter_scale_factor = tuner_cfg[
-            'scale_factor'] if 'scale_factor' in tuner_cfg else 2
+        self.adapter_scale_factor = tuner_cfg['scale_factor'] if 'scale_factor' in tuner_cfg else 2
 
         assert self.adapter_dim % self.adapter_head == 0, 'adapter dim should be divisible by adapter head'
         self.dim_mlp = self.adapter_dim // self.adapter_head
 
-        self.ln1 = nn.Linear(self.dim_mlp,
-                             self.dim_mlp * self.adapter_scale_factor)
-        self.ln2 = nn.Linear(self.dim_mlp * self.adapter_scale_factor,
-                             self.dim_mlp)
+        self.ln1 = nn.Linear(self.dim_mlp, self.dim_mlp * self.adapter_scale_factor)
+        self.ln2 = nn.Linear(self.dim_mlp * self.adapter_scale_factor, self.dim_mlp)
         self.activate = act_layer()
 
         self._kaiming_init_weights(self.ln1)
         if zero_init_last and layer_num == depth - 1:
             self._zero_init_weights(self.ln2)
         else:
             self._kaiming_init_weights(self.ln2)
@@ -243,29 +214,24 @@
 
         x_dtype = x.dtype
         x = x.to(self.ln1.weight.dtype)
         x_shortcut = x
 
         batch, inner_dim, height, width = x.shape
 
-        x_adapter = x.permute(0, 2, 3, 1).reshape(batch, height * width,
-                                                  inner_dim)
+        x_adapter = x.permute(0, 2, 3, 1).reshape(batch, height * width, inner_dim)
 
-        x_adapter = rearrange(
-            x_adapter, 'b n (c h) -> (b h) n c', h=self.adapter_head)
+        x_adapter = rearrange(x_adapter, 'b n (c h) -> (b h) n c', h=self.adapter_head)
         x_adapter = self.ln2(self.activate(self.ln1(x_adapter)))
-        x_adapter = rearrange(
-            x_adapter, '(b h) n c -> b n (c h)', h=self.adapter_head)
+        x_adapter = rearrange(x_adapter, '(b h) n c -> b n (c h)', h=self.adapter_head)
 
         if self.adapter_weight:
-            x_adapter = apply_data_weight(x_adapter, self.scaling,
-                                          self.adapter_weight)
+            x_adapter = apply_data_weight(x_adapter, self.scaling, self.adapter_weight)
 
-        x_adapter = x_adapter.reshape(batch, height, width,
-                                      -1).permute(0, 3, 1, 2).contiguous()
+        x_adapter = x_adapter.reshape(batch, height, width, -1).permute(0, 3, 1, 2).contiguous()
         x_out = x_shortcut + x_adapter
 
         return x_out.to(x_dtype)
 
 
 class Identity(nn.Module):
 
@@ -281,46 +247,38 @@
     An upsampling layer with an optional convolution.
     :param channels: channels in the inputs and outputs.
     :param use_conv: a bool determining if a convolution is applied.
     :param dims: determines if the signal is 1D, 2D, or 3D. If 3D, then
                  upsampling occurs in the inner-two dimensions.
     """
 
-    def __init__(self,
-                 channels,
-                 use_conv=False,
-                 out_channels=None,
-                 padding=1,
-                 **kwargs):
+    def __init__(self, channels, use_conv=False, out_channels=None, padding=1, **kwargs):
         super().__init__()
         self.channels = channels
         self.out_channels = out_channels or channels
         self.use_conv = use_conv
         if use_conv:
-            self.conv = nn.Conv2d(
-                self.channels, self.out_channels, 3, padding=padding)
+            self.conv = nn.Conv2d(self.channels, self.out_channels, 3, padding=padding)
         self.init_weights()
 
     def init_weights(self):
 
         def _init_weights(m):
             if isinstance(m, nn.Conv2d):
                 nn.init.zeros_(m.weight)
                 nn.init.zeros_(m.bias)
 
         self.apply(_init_weights)
 
     def forward(self, x, target_size=None, *args, **kwargs):
         assert x.shape[1] == self.channels
         if target_size is None:
-            x = F.interpolate(
-                x.float(), scale_factor=2, mode='nearest').type_as(x)
+            x = F.interpolate(x.float(), scale_factor=2, mode='nearest').type_as(x)
         else:
-            x = F.interpolate(
-                x.float(), target_size, mode='nearest').type_as(x)
+            x = F.interpolate(x.float(), target_size, mode='nearest').type_as(x)
         if self.use_conv:
             x = self.conv(x)
         return x
 
 
 def init_weight_type(dim, weight_type):
     if weight_type is None:
@@ -350,32 +308,27 @@
     else:
         scaling = None
     return scaling
 
 
 def apply_data_weight(data, scaling, weight_type):
     if weight_type in ['gate']:
-        scaling = torch.mean(
-            torch.sigmoid(scaling(data)), dim=1).view(-1, 1, 1)
-    elif weight_type in ['scale', 'scale_channel'
-                         ] or weight_type.startswith('scalar'):
+        scaling = torch.mean(torch.sigmoid(scaling(data)), dim=1).view(-1, 1, 1)
+    elif weight_type in ['scale', 'scale_channel'] or weight_type.startswith('scalar'):
         scaling = scaling
     else:
         scaling = None
     if scaling is not None:
         data = data * scaling
     return data
 
 
 def detach_tensors(feats):
     if type(feats) in [list, tuple]:
-        feats = [
-            detach_tensors(feat) if feat is not None else None
-            for feat in feats
-        ]
+        feats = [detach_tensors(feat) if feat is not None else None for feat in feats]
     elif isinstance(feats, dict):
         feats = {key: detach_tensors(val) for key, val in feats.items()}
     elif isinstance(feats, torch.Tensor):
         feats = feats.detach()
     else:
         feats = feats.detach()
     return feats
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/compute_u.py` & `ms-swift-2.0.4/swift/tuners/rome/compute_u.py`

 * *Files 5% similar despite different names*

```diff
@@ -32,35 +32,29 @@
         model=model,
         tokenizer=tokenizer,
         layer=layer,
         module_template=hparams.rewrite_module_tmp,
         track='in',
         batch_first=batch_first,
     )
-    if 'subject_' in hparams.fact_token and hparams.fact_token.index(
-            'subject_') == 0:
+    if 'subject_' in hparams.fact_token and hparams.fact_token.index('subject_') == 0:
         word = request['subject']
         logger.info(f'Selected u projection object {word}')
         cur_repr = get_reprs_at_word_tokens(
-            context_templates=[
-                templ.format(request['prompt']) for templ in context_templates
-            ],
+            context_templates=[templ.format(request['prompt']) for templ in context_templates],
             words=[word for _ in range(len(context_templates))],
             subtoken=hparams.fact_token[len('subject_'):],
             **word_repr_args,
         ).mean(0)
     elif hparams.fact_token == 'last':
         # Heuristic to choose last word. Not a huge deal if there's a minor
         # edge case (e.g. multi-token word) because the function below will
         # take the last token.
         cur_repr = get_reprs_at_idxs(
-            contexts=[
-                templ.format(request['prompt'].format(request['subject']))
-                for templ in context_templates
-            ],
+            contexts=[templ.format(request['prompt'].format(request['subject'])) for templ in context_templates],
             idxs=[[-1] for _ in range(len(context_templates))],
             **word_repr_args,
         ).mean(0)
         logger.info('Selected u projection token with last token')
     else:
         raise ValueError(f'fact_token={hparams.fact_token} not recognized')
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/compute_v.py` & `ms-swift-2.0.4/swift/tuners/rome/compute_v.py`

 * *Files 3% similar despite different names*

```diff
@@ -4,16 +4,15 @@
 
 import numpy as np
 import torch
 from modelscope import AutoTokenizer
 
 from swift.utils.logger import get_logger
 from .nethook import TraceDict, set_requires_grad
-from .repr_tools import (get_reprs_at_idxs, get_reprs_at_word_tokens,
-                         get_words_idxs_in_templates)
+from .repr_tools import get_reprs_at_idxs, get_reprs_at_word_tokens, get_words_idxs_in_templates
 from .rome_hparams import ROMEHyperParams
 
 logger = get_logger()
 
 
 def compute_v(model: torch.nn.Module,
               tokenizer: AutoTokenizer,
@@ -28,60 +27,49 @@
     Runs a simple optimization procedure.
     """
 
     logger.info('Computing right vector (v)')
 
     # Compile list of rewriting and KL x/y pairs
     rewriting_prompts, kl_prompts = [
-        context.format(request['prompt']) + request['target']
-        for context in context_templates
+        context.format(request['prompt']) + request['target'] for context in context_templates
     ], ['{} is a', '{}']
     all_prompts = rewriting_prompts + kl_prompts
 
     input_tok = tokenizer(
         [prompt.format(request['subject']) for prompt in all_prompts],
         return_tensors='pt',
         padding=True,
         return_token_type_ids=False,
     ).to(model.device)
 
     # Compute rewriting targets
     rewriting_targets = torch.tensor(
-        -100, device=model.device).repeat(
-            len(rewriting_prompts), *input_tok['input_ids'].shape[1:])
+        -100, device=model.device).repeat(len(rewriting_prompts), *input_tok['input_ids'].shape[1:])
 
     prompt = context_templates[0].format(request['prompt'])
     prompt_full = prompt + request['target']
-    target_len = len(tokenizer.tokenize(prompt_full)) - len(
-        tokenizer.tokenize(prompt))
+    target_len = len(tokenizer.tokenize(prompt_full)) - len(tokenizer.tokenize(prompt))
     for i in range(len(rewriting_prompts)):
-        rewriting_targets[i, -target_len - 1:-1] = input_tok['input_ids'][
-            i, -target_len:].clone()
+        rewriting_targets[i, -target_len - 1:-1] = input_tok['input_ids'][i, -target_len:].clone()
 
     # Compute indices of the tokens where the fact is looked up
     lookup_idxs = [
-        find_fact_lookup_idx(
-            prompt,
-            request['subject'],
-            tokenizer,
-            hparams.fact_token,
-            verbose=(i == 0)) for i, prompt in enumerate(all_prompts)
+        find_fact_lookup_idx(prompt, request['subject'], tokenizer, hparams.fact_token, verbose=(i == 0))
+        for i, prompt in enumerate(all_prompts)
     ]
 
     # Finalize rewrite and loss layers
     logger.info(f'Rewrite layer is {layer}')
 
     # Set up an optimization over a latent vector that, when output at the
     # rewrite layer, i.e. hypothesized fact lookup location, will induce the
     # target token to be predicted at the final layer.
-    hidden_size = model.config.n_embd if hasattr(
-        model.config, 'n_embed') else model.config.hidden_size
-    delta = torch.zeros((hidden_size, ),
-                        requires_grad=True,
-                        device=model.device)
+    hidden_size = model.config.n_embd if hasattr(model.config, 'n_embed') else model.config.hidden_size
+    delta = torch.zeros((hidden_size, ), requires_grad=True, device=model.device)
     target_init, kl_distr_init = None, None
 
     # Inserts new "delta" variable at the appropriate part of the computation
     def edit_output_fn(cur_out, cur_layer):
         nonlocal target_init
 
         # Store initial value of the vector of interest
@@ -116,52 +104,43 @@
                 retain_output=True,
                 edit_output=edit_output_fn,
         ) as _:
             logits = model(**input_tok).logits
 
             # Compute distribution for KL divergence
             kl_logits = torch.stack(
-                [
-                    logits[i - len(kl_prompts), idx, :]
-                    for i, idx in enumerate(lookup_idxs[-len(kl_prompts):])
-                ],
+                [logits[i - len(kl_prompts), idx, :] for i, idx in enumerate(lookup_idxs[-len(kl_prompts):])],
                 dim=0,
             )
             kl_log_probs = torch.nn.functional.log_softmax(kl_logits, dim=1)
             if kl_distr_init is None:
                 kl_distr_init = kl_log_probs.detach().clone()
 
         # Compute loss on rewriting targets
         log_probs = torch.log_softmax(logits, dim=2)
 
         loss = torch.gather(
             log_probs,
             2,
-            torch.where(rewriting_targets != -100, rewriting_targets,
-                        0).unsqueeze(2),
+            torch.where(rewriting_targets != -100, rewriting_targets, 0).unsqueeze(2),
         ).squeeze(2)
         mask = (rewriting_targets != -100).float()
 
         # Aggregate total losses
         nll_loss_each = -(loss * mask).sum(1) / target_len
         nll_loss = nll_loss_each.mean()
         kl_loss = hparams.kl_factor * torch.nn.functional.kl_div(
-            kl_distr_init,
-            kl_log_probs,
-            log_target=True,
-            reduction='batchmean')
-        weight_decay = hparams.v_weight_decay * (
-            torch.norm(delta) / torch.norm(target_init)**2)
+            kl_distr_init, kl_log_probs, log_target=True, reduction='batchmean')
+        weight_decay = hparams.v_weight_decay * (torch.norm(delta) / torch.norm(target_init)**2)
         # weight_decay = hparams.v_weight_decay * torch.norm(delta) ** 2
         loss = nll_loss + kl_loss + weight_decay
-        logger.info(
-            f'loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + '
-            f'{np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} '
-            f"avg prob of [{request['target']}] "
-            f'{torch.exp(-nll_loss_each).mean().item()}')
+        logger.info(f'loss {np.round(loss.item(), 3)} = {np.round(nll_loss.item(), 3)} + '
+                    f'{np.round(kl_loss.item(), 3)} + {np.round(weight_decay.item(), 3)} '
+                    f"avg prob of [{request['target']}] "
+                    f'{torch.exp(-nll_loss_each).mean().item()}')
         if loss < 5e-2:
             break
 
         if it == hparams.v_num_grad_steps - 1:
             break
 
         # Backpropagate
@@ -187,45 +166,38 @@
         module_template=hparams.rewrite_module_tmp,
         fact_token_strategy=hparams.fact_token,
         batch_first=batch_first)
 
     # Solving the linear system to compute the right vector
     right_vector = (target - cur_output) / torch.dot(cur_input, left_vector)
     logger.info(f'Delta norm: {(target - cur_output).norm().item()}')
-    logger.info(
-        f'Change in target norm: {target_init.norm().item()} to {target.norm().item()} => '
-        f'{(target.norm() - target_init.norm()).item()}')
+    logger.info(f'Change in target norm: {target_init.norm().item()} to {target.norm().item()} => '
+                f'{(target.norm() - target_init.norm()).item()}')
     logger.info(f'Division Factor: {torch.dot(cur_input, left_vector).item()}')
     logger.info(f'Right vector norm: {right_vector.norm()}')
 
     return right_vector
 
 
-def get_module_input_output_at_word(
-        model: torch.nn.Module,
-        tok: Any,
-        layer: int,
-        context_template: str,
-        word: str,
-        module_template: str,
-        fact_token_strategy: str,
-        batch_first: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
+def get_module_input_output_at_word(model: torch.nn.Module,
+                                    tok: Any,
+                                    layer: int,
+                                    context_template: str,
+                                    word: str,
+                                    module_template: str,
+                                    fact_token_strategy: str,
+                                    batch_first: bool = True) -> Tuple[torch.Tensor, torch.Tensor]:
     """
     Retrieves detached representations for a word at the input and
     output of a particular layer module.
     """
 
     word_repr_args = dict(
-        model=model,
-        tokenizer=tok,
-        layer=layer,
-        module_template=module_template,
-        batch_first=batch_first)
-    if 'subject_' in fact_token_strategy and fact_token_strategy.index(
-            'subject_') == 0:
+        model=model, tokenizer=tok, layer=layer, module_template=module_template, batch_first=batch_first)
+    if 'subject_' in fact_token_strategy and fact_token_strategy.index('subject_') == 0:
         subtoken = fact_token_strategy[len('subject_'):]
         l_input, l_output = get_reprs_at_word_tokens(
             track='both',
             subtoken=subtoken,
             context_templates=[context_template],
             words=[word],
             **word_repr_args,
@@ -253,16 +225,15 @@
 ) -> int:
     """
     Computes hypothesized fact lookup index given a sentence and subject.
     """
 
     if fact_token_strategy == 'last':
         ret = -1
-    elif ('subject_' in fact_token_strategy
-          and fact_token_strategy.index('subject_') == 0):
+    elif ('subject_' in fact_token_strategy and fact_token_strategy.index('subject_') == 0):
         ret = get_words_idxs_in_templates(
             tok,
             context_templates=[prompt],
             words=[subject],
             subtoken=fact_token_strategy[len('subject_'):],
         )[0][0]
     else:
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/context_template.py` & `ms-swift-2.0.4/swift/tuners/rome/context_template.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/nethook.py` & `ms-swift-2.0.4/swift/tuners/rome/nethook.py`

 * *Files 2% similar despite different names*

```diff
@@ -75,28 +75,22 @@
                 retainer.input = recursive_copy(
                     inputs[0] if len(inputs) == 1 else inputs,
                     clone=clone,
                     detach=detach,
                     retain_grad=False,
                 )  # retain_grad applies to output only.
             if edit_output:
-                output = invoke_with_optional_args(
-                    edit_output, output=output, layer=self.layer)
+                output = invoke_with_optional_args(edit_output, output=output, layer=self.layer)
             if retain_output:
-                retainer.output = recursive_copy(
-                    output,
-                    clone=clone,
-                    detach=detach,
-                    retain_grad=retain_grad)
+                retainer.output = recursive_copy(output, clone=clone, detach=detach, retain_grad=retain_grad)
                 # When retain_grad is set, also insert a trivial
                 # copy operation.  That allows in-place operations
                 # to follow without error.
                 if retain_grad:
-                    output = recursive_copy(
-                        retainer.output, clone=True, detach=False)
+                    output = recursive_copy(retainer.output, clone=True, detach=False)
             if stop:
                 raise StopForward()
             return output
 
         self.registered_hook = module.register_forward_hook(retain_hook)
         self.stop = stop
 
@@ -244,40 +238,32 @@
     Handles descent into dotted layer names as long as all references
     are within nested Sequential models.
 
     If share_weights is True, then references the original modules
     and their parameters without copying them.  Otherwise, by default,
     makes a separate brand-new copy.
     """
-    assert (single_layer is None) or (first_layer is last_layer is after_layer
-                                      is upto_layer is None)
+    assert (single_layer is None) or (first_layer is last_layer is after_layer is upto_layer is None)
     if single_layer is not None:
         first_layer = single_layer
         last_layer = single_layer
     first, last, after, upto = [
-        None if d is None else d.split('.')
-        for d in [first_layer, last_layer, after_layer, upto_layer]
+        None if d is None else d.split('.') for d in [first_layer, last_layer, after_layer, upto_layer]
     ]
     return hierarchical_subsequence(
         sequential,
         first=first,
         last=last,
         after=after,
         upto=upto,
         share_weights=share_weights,
     )
 
 
-def hierarchical_subsequence(sequential,
-                             first,
-                             last,
-                             after,
-                             upto,
-                             share_weights=False,
-                             depth=0):
+def hierarchical_subsequence(sequential, first, last, after, upto, share_weights=False, depth=0):
     """
     Recursive helper for subsequence() to support descent into dotted
     layer names.  In this helper, first, last, after, and upto are
     arrays of names resulting from splitting on dots.  Can only
     descend into nested Sequentials.
     """
     assert (last is None) or (upto is None)
@@ -286,34 +272,29 @@
         return sequential if share_weights else copy.deepcopy(sequential)
     assert isinstance(sequential, torch.nn.Sequential), ('.'.join(
         (first or last or after or upto)[:depth] or 'arg') + ' not Sequential')
     including_children = (first is None) and (after is None)
     included_children = OrderedDict()
     # A = current level short name of A.
     # AN = full name for recursive descent if not innermost.
-    (F, FN), (L, LN), (A, AN), (U, UN) = [
-        (d[depth], (None if len(d) == depth + 1 else d)) if d is not None else
-        (None, None) for d in [first, last, after, upto]
-    ]
+    (F, FN), (L, LN), (A, AN), (U, UN) = [(d[depth], (None if len(d) == depth + 1 else d)) if d is not None else
+                                          (None, None) for d in [first, last, after, upto]]
     for name, layer in sequential._modules.items():
         if name == F:
             first = None
             including_children = True
         if name == A and AN is not None:  # just like F if not a leaf.
             after = None
             including_children = True
         if name == U and UN is None:
             upto = None
             including_children = False
         if including_children:
             # AR = full name for recursive descent if name matches.
-            FR, LR, AR, UR = [
-                n if n is None or n[depth] == name else None
-                for n in [FN, LN, AN, UN]
-            ]
+            FR, LR, AR, UR = [n if n is None or n[depth] == name else None for n in [FN, LN, AN, UN]]
             chosen = hierarchical_subsequence(
                 layer,
                 first=FR,
                 last=LR,
                 after=AR,
                 upto=UR,
                 share_weights=share_weights,
@@ -408,47 +389,42 @@
     those new arguments.
     """
     argspec = inspect.getfullargspec(fn)
     pass_args = []
     used_kw = set()
     unmatched_pos = []
     used_pos = 0
-    defaulted_pos = len(
-        argspec.args) - (0 if not argspec.defaults else len(argspec.defaults))
+    defaulted_pos = len(argspec.args) - (0 if not argspec.defaults else len(argspec.defaults))
     # Pass positional args that match name first, then by position.
     for i, n in enumerate(argspec.args):
         if n in kwargs:
             pass_args.append(kwargs[n])
             used_kw.add(n)
         elif used_pos < len(args):
             pass_args.append(args[used_pos])
             used_pos += 1
         else:
             unmatched_pos.append(len(pass_args))
-            pass_args.append(None if i < defaulted_pos else argspec.
-                             defaults[i - defaulted_pos])
+            pass_args.append(None if i < defaulted_pos else argspec.defaults[i - defaulted_pos])
     # Fill unmatched positional args with unmatched keyword args in order.
     if len(unmatched_pos):
         for k, v in kwargs.items():
             if k in used_kw or k in argspec.kwonlyargs:
                 continue
             pass_args[unmatched_pos[0]] = v
             used_kw.add(k)
             unmatched_pos = unmatched_pos[1:]
             if len(unmatched_pos) == 0:
                 break
         else:
             if unmatched_pos[0] < defaulted_pos:
-                unpassed = ', '.join(argspec.args[u] for u in unmatched_pos
-                                     if u < defaulted_pos)
-                raise TypeError(
-                    f'{fn.__name__}() cannot be passed {unpassed}.')
+                unpassed = ', '.join(argspec.args[u] for u in unmatched_pos if u < defaulted_pos)
+                raise TypeError(f'{fn.__name__}() cannot be passed {unpassed}.')
     # Pass remaining kw args if they can be accepted.
     pass_kw = {
         k: v
-        for k, v in kwargs.items() if k not in used_kw and (
-            k in argspec.kwonlyargs or argspec.varargs is not None)
+        for k, v in kwargs.items() if k not in used_kw and (k in argspec.kwonlyargs or argspec.varargs is not None)
     }
     # Pass remaining positional args if they can be accepted.
     if argspec.varargs is not None:
         pass_args += list(args[used_pos:])
     return fn(*pass_args, **pass_kw)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/repr_tools.py` & `ms-swift-2.0.4/swift/tuners/rome/repr_tools.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,59 +26,55 @@
 ) -> torch.Tensor:
     """
     Retrieves the last token representation of `word` in `context_template`
     when `word` is substituted into `context_template`. See `get_last_word_idx_in_template`
     for more details.
     """
 
-    idxs = get_words_idxs_in_templates(tokenizer, context_templates, words,
-                                       subtoken)
+    idxs = get_words_idxs_in_templates(tokenizer, context_templates, words, subtoken)
     return get_reprs_at_idxs(
         model,
         tokenizer,
         [context_templates[i].format(words[i]) for i in range(len(words))],
         idxs,
         layer,
         module_template,
         track,
         batch_first,
     )
 
 
-def get_words_idxs_in_templates(tokenizer: AutoTokenizer,
-                                context_templates: List[str], words: List[str],
+def get_words_idxs_in_templates(tokenizer: AutoTokenizer, context_templates: List[str], words: List[str],
                                 subtoken: str) -> List:
     """
     Given list of template strings, each with *one* format specifier
     (e.g. "{} plays basketball"), and words to be substituted into the
     template, computes the post-tokenization index of their last tokens.
     """
 
-    assert all(tmp.count('{}') == 1 for tmp in context_templates
-               ), 'We currently do not support multiple fill-ins for context'
+    assert all(tmp.count('{}') == 1
+               for tmp in context_templates), 'We currently do not support multiple fill-ins for context'
 
     # Compute prefixes and suffixes of the tokenized context
     fill_idxs = [tmp.index('{}') for tmp in context_templates]
-    prefixes, suffixes = [
-        tmp[:fill_idxs[i]] for i, tmp in enumerate(context_templates)
-    ], [tmp[fill_idxs[i] + 2:] for i, tmp in enumerate(context_templates)]
+    prefixes, suffixes = [tmp[:fill_idxs[i]] for i, tmp in enumerate(context_templates)
+                          ], [tmp[fill_idxs[i] + 2:] for i, tmp in enumerate(context_templates)]
 
     lens = []
     for prefix, word, suffix in zip(prefixes, words, suffixes):
         prefix_token = tokenizer.encode(prefix)
         prefix_word_token = tokenizer.encode(prefix + word)
         prefix_word_suffix_token = tokenizer.encode(prefix + word + suffix)
         suffix_len = len(prefix_word_suffix_token) - len(prefix_word_token)
 
         # Compute indices of last tokens
         if subtoken == 'last' or subtoken == 'first_after_last':
             lens.append([
                 len(prefix_word_token) -
-                (1 if subtoken == 'last' or suffix_len == 0 else 0)
-                - len(prefix_word_suffix_token)
+                (1 if subtoken == 'last' or suffix_len == 0 else 0) - len(prefix_word_suffix_token)
             ])
         elif subtoken == 'first':
             lens.append([len(prefix_token) - len(prefix_word_suffix_token)])
         else:
             raise ValueError(f'Unknown subtoken type: {subtoken}')
     return lens
 
@@ -117,17 +113,15 @@
         if not batch_first:
             cur_repr = cur_repr.transpose(0, 1)
         for i, idx_list in enumerate(batch_idxs):
             to_return[key].append(cur_repr[i][idx_list].mean(0))
 
     for batch_contexts, batch_idxs in _batch(n=512):
         contexts_tok = tokenizer(
-            batch_contexts,
-            padding=True,
-            return_token_type_ids=False,
+            batch_contexts, padding=True, return_token_type_ids=False,
             return_tensors='pt').to(next(model.parameters()).device)
 
         with torch.no_grad():
             with Trace(
                     module=model,
                     layer=module_name,
                     retain_input=tin,
@@ -136,16 +130,13 @@
                 model(**contexts_tok)
 
         if tin:
             _process(tr.input, batch_idxs, 'in')
         if tout:
             _process(tr.output, batch_idxs, 'out')
 
-    to_return = {
-        k: torch.stack(v, 0)
-        for k, v in to_return.items() if len(v) > 0
-    }
+    to_return = {k: torch.stack(v, 0) for k, v in to_return.items() if len(v) > 0}
 
     if len(to_return) == 1:
         return to_return['in'] if tin else to_return['out']
     else:
         return to_return['in'], to_return['out']
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/rome.py` & `ms-swift-2.0.4/swift/tuners/rome/rome.py`

 * *Files 3% similar despite different names*

```diff
@@ -38,22 +38,19 @@
             >>>         "subject": "Steve Jobs",
             >>>         "target": "Microsoft"
             >>>     }
             >>> ]
     """
     model_type: str = field(default=None, metadata={'help': 'The model type'})
 
-    tokenizer: Any = field(
-        default=None, metadata={'help': 'The tokenizer matching this model'})
+    tokenizer: Any = field(default=None, metadata={'help': 'The tokenizer matching this model'})
 
-    knowledge: List[Dict] = field(
-        default=False, metadata={'help': 'The knowledge to be used'})
+    knowledge: List[Dict] = field(default=False, metadata={'help': 'The knowledge to be used'})
 
-    batch_first: bool = field(
-        default=True, metadata={'help': 'Batch at the first dimension or not'})
+    batch_first: bool = field(default=True, metadata={'help': 'Batch at the first dimension or not'})
 
     def __post_init__(self):
         from swift.tuners.mapping import SwiftTuners
         self.swift_type = SwiftTuners.ROME
 
     @property
     def __dict__(self):
@@ -73,29 +70,23 @@
         """
         modified_keys = set()
         if config.tokenizer is not None:
             for param in model.parameters():
                 param.requires_grad = True
 
             hparams = ROMEHyperParams.from_name(config.model_type)
-            modified_keys = apply_rome_to_model(model, config.tokenizer,
-                                                config.knowledge, hparams,
-                                                config.batch_first)
+            modified_keys = apply_rome_to_model(model, config.tokenizer, config.knowledge, hparams, config.batch_first)
 
         def state_dict_callback(state_dict, adapter_name):
-            return {
-                key: value
-                for key, value in state_dict.items() if key in modified_keys
-            }
+            return {key: value for key, value in state_dict.items() if key in modified_keys}
 
         def mark_trainable_callback(model):
             pass
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
     def has_additional_modules():
         return False
 
 
 def apply_rome_to_model(
@@ -138,24 +129,21 @@
     """
     Executes the ROME update algorithm for the specified update at the specified layer
     Invariant: model at beginning of function == model at end of function
     """
 
     # Update target and print info
     request = deepcopy(knowledge)
-    logger.info(
-        f'Executing ROME algorithm for the update: '
-        f"[{request['prompt'].format(request['subject'])}] -> [{request['target']}]"
-    )
+    logger.info(f'Executing ROME algorithm for the update: '
+                f"[{request['prompt'].format(request['subject'])}] -> [{request['target']}]")
 
     # Retrieve weights that user desires to change
     weights = {
         f'{hparams.rewrite_module_tmp.format(layer)}.weight':
-        get_parameter(model,
-                      f'{hparams.rewrite_module_tmp.format(layer)}.weight')
+        get_parameter(model, f'{hparams.rewrite_module_tmp.format(layer)}.weight')
         for layer in hparams.layers
     }
     # Save old weights for future restoration
     weights_copy = {k: v.detach().clone() for k, v in weights.items()}
 
     # Update loop: sequentially intervene at each specified layer
     deltas = {}
@@ -184,16 +172,15 @@
         logger.info(f'Right vector shape: {right_vector.shape}')
         right_vector = right_vector.to(left_vector.dtype)
 
         with torch.no_grad():
             # Determine correct transposition of delta matrix
             weight_name = f'{hparams.rewrite_module_tmp.format(layer)}.weight'
             upd_matrix = left_vector.unsqueeze(1) @ right_vector.unsqueeze(0)
-            upd_matrix = upd_matrix_match_shape(upd_matrix,
-                                                weights[weight_name].shape)
+            upd_matrix = upd_matrix_match_shape(upd_matrix, weights[weight_name].shape)
 
             # Update model weights and record desired changes in `delta` variable
             weights[weight_name][...] += upd_matrix
             deltas[weight_name] = (
                 left_vector.detach(),
                 right_vector.detach(),
             )
@@ -204,22 +191,20 @@
             v[...] = weights_copy[k]
 
     logger.info(f'Deltas successfully computed for {list(weights.keys())}')
 
     return deltas
 
 
-def upd_matrix_match_shape(matrix: torch.Tensor,
-                           shape: torch.Size) -> torch.Tensor:
+def upd_matrix_match_shape(matrix: torch.Tensor, shape: torch.Size) -> torch.Tensor:
     """
     GPT-2 and GPT-J have transposed weight representations.
     Returns a matrix that matches the desired shape, else raises a ValueError
     """
 
     if matrix.shape == shape:
         return matrix
     elif matrix.T.shape == shape:
         return matrix.T
     else:
-        raise ValueError(
-            'Update matrix computed by ROME does not match original weight shape. '
-            'Check for bugs in the code?')
+        raise ValueError('Update matrix computed by ROME does not match original weight shape. '
+                         'Check for bugs in the code?')
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/rome/rome_hparams.py` & `ms-swift-2.0.4/swift/tuners/rome/rome_hparams.py`

 * *Files 1% similar despite different names*

```diff
@@ -45,15 +45,14 @@
                     rewrite_module_tmp='model.layers.{}.mlp.down_proj',
                     mlp_module_tmp='model.layers.{}.mlp',
                 ))
         elif name == 'chatglm-6b':
             data.update(
                 dict(
                     layers=[5],
-                    rewrite_module_tmp=
-                    'transformer.encoder.layers.{}.mlp.dense_4h_to_h',
+                    rewrite_module_tmp='transformer.encoder.layers.{}.mlp.dense_4h_to_h',
                     mlp_module_tmp='transformer.encoder.layers.{}.mlp',
                 ))
         else:
             raise NotImplementedError(f'{name} not supported.')
 
         return cls(**data)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/scetuning/scetuning.py` & `ms-swift-2.0.4/swift/tuners/scetuning/scetuning.py`

 * *Files 4% similar despite different names*

```diff
@@ -6,16 +6,15 @@
 from typing import List, Optional, Union
 
 import torch
 from torch import nn
 from transformers.activations import ACT2CLS
 
 from swift import get_logger
-from swift.tuners.utils import (ActivationMixin, SwiftAdapter, SwiftConfig,
-                                SwiftOutput)
+from swift.tuners.utils import ActivationMixin, SwiftAdapter, SwiftConfig, SwiftOutput
 from swift.utils.torch_utils import find_sub_module
 from .scetuning_components import probe_output_hook
 
 logger = get_logger()
 
 
 @dataclass
@@ -36,84 +35,61 @@
     """
 
     dims: Optional[Union[List[int], int]] = field(
         default=None, metadata={'help': 'The dimensions of the hidden states'})
 
     target_modules: Optional[Union[List[str], str]] = field(
         default=None,
-        metadata={
-            'help':
-            'The target module to be replaced, can be a regex string or name list of full match format'
-        })
+        metadata={'help': 'The target module to be replaced, can be a regex string or name list of full match format'})
 
     hint_modules: Optional[Union[List[str], str]] = field(
         default=None,
-        metadata={
-            'help':
-            'The hint modules to be replaced, can be a regex string or name list of full match format'
-        })
+        metadata={'help': 'The hint modules to be replaced, can be a regex string or name list of full match format'})
 
     tuner_mode: str = field(
         default='decoder',
-        metadata={
-            'help':
-            'Location of tuner operation. The tuner mode choices: encoder, decoder, and identity'
-        })
-
-    tuner_op: str = field(
-        default='SCEAdapter',
-        metadata={'help': 'The tuner ops choices: SCEAdapter'})
-
-    down_ratio: float = field(
-        default=1.0,
-        metadata={'help': 'The dim down ratio of tuner hidden state'})
+        metadata={'help': 'Location of tuner operation. The tuner mode choices: encoder, decoder, and identity'})
+
+    tuner_op: str = field(default='SCEAdapter', metadata={'help': 'The tuner ops choices: SCEAdapter'})
+
+    down_ratio: float = field(default=1.0, metadata={'help': 'The dim down ratio of tuner hidden state'})
 
     def __post_init__(self):
         from swift.tuners.mapping import SwiftTuners
         self.swift_type = SwiftTuners.SCETUNING
 
 
 class SCETuning(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: SCETuningConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: nn.Module, config: SCETuningConfig, adapter_name: str) -> SwiftOutput:
         """Prepare a model with `SCETuningConfig`"""
         module_keys = [key for key, _ in model.named_modules()]
         # 1. Matching the hint module
         hint_module_ins_list = []
         if config.hint_modules:
             if isinstance(config.hint_modules, list):
                 for module_key in config.hint_modules:
                     assert module_key in module_keys
                     h_module = model.get_submodule(module_key)
-                    logger.info(
-                        f'Matching hint module [{module_key}] of type {type(h_module)}'
-                    )
+                    logger.info(f'Matching hint module [{module_key}] of type {type(h_module)}')
                     if isinstance(h_module, (nn.ModuleList, nn.ModuleDict)):
                         logger.warning(
-                            f'Type of {type(h_module)} may not be supported because of its customized forward'
-                        )
-                    h_module.register_forward_hook(
-                        probe_output_hook, with_kwargs=True)
+                            f'Type of {type(h_module)} may not be supported because of its customized forward')
+                    h_module.register_forward_hook(probe_output_hook, with_kwargs=True)
                     hint_module_ins_list.append(h_module)
             else:
                 for module_key in module_keys:
                     if re.fullmatch(config.hint_modules, module_key):
                         h_module = model.get_submodule(module_key)
-                        logger.info(
-                            f'Matching hint module [{module_key}] of type {type(h_module)}'
-                        )
-                        if isinstance(h_module,
-                                      (nn.ModuleList, nn.ModuleDict)):
+                        logger.info(f'Matching hint module [{module_key}] of type {type(h_module)}')
+                        if isinstance(h_module, (nn.ModuleList, nn.ModuleDict)):
                             logger.warning(
-                                f'Type of {type(h_module)} may not be supported because of its customized forward'
-                            )
-                        h_module.register_forward_hook(
-                            probe_output_hook, with_kwargs=True)
+                                f'Type of {type(h_module)} may not be supported because of its customized forward')
+                        h_module.register_forward_hook(probe_output_hook, with_kwargs=True)
                         hint_module_ins_list.append(h_module)
             if len(hint_module_ins_list) == 0:
                 logger.error('Cannot match hint modules')
 
         def _get_module(module):
             if isinstance(module, nn.ModuleList):
                 module = module[-1]
@@ -123,82 +99,70 @@
         # 2. Matching the target module
         target_module_ins_list = []
         assert config.target_modules is not None
         if isinstance(config.target_modules, list):
             for module_key in config.target_modules:
                 assert module_key in module_keys
                 t_module = model.get_submodule(module_key)
-                logger.info(
-                    f'Matching target module [{module_key}] of type {type(t_module)}'
-                )
+                logger.info(f'Matching target module [{module_key}] of type {type(t_module)}')
                 target_module_ins_list.append(_get_module(t_module))
         else:
             for module_key in module_keys:
                 if re.fullmatch(config.target_modules, module_key):
                     t_module = model.get_submodule(module_key)
-                    logger.info(
-                        f'Matching target module [{module_key}] of type {type(t_module)}'
-                    )
+                    logger.info(f'Matching target module [{module_key}] of type {type(t_module)}')
                     target_module_ins_list.append(_get_module(t_module))
         if len(target_module_ins_list) == 0:
             logger.error('Cannot match target modules')
-        if len(hint_module_ins_list) > 0 and not len(
-                hint_module_ins_list) == len(target_module_ins_list):
-            logger.info(
-                "Target modules' length should be equal with hint modules.")
+        if len(hint_module_ins_list) > 0 and not len(hint_module_ins_list) == len(target_module_ins_list):
+            logger.info("Target modules' length should be equal with hint modules.")
             assert len(hint_module_ins_list) == len(target_module_ins_list)
         if isinstance(config.dims, int):
             dims = [config.dims for _ in target_module_ins_list]
         else:
             assert len(config.dims) == len(target_module_ins_list)
             dims = config.dims
 
         # refactor forward function
         def _forward_encoder_mode(self, *args, **kwargs):
-            args = getattr(self, f'forward_origin_{adapter_name}')(*args,
-                                                                   **kwargs)
+            args = getattr(self, f'forward_origin_{adapter_name}')(*args, **kwargs)
             args_type = type(args)
             if args_type is tuple:
                 args = args[0]
             if hasattr(self, 'hint'):
                 hint_out = self.hint.probe_output_data
-                args_main = getattr(self, f'scetuner_{adapter_name}')(args,
-                                                                      hint_out)
+                args_main = getattr(self, f'scetuner_{adapter_name}')(args, hint_out)
             else:
                 args_main = getattr(self, f'scetuner_{adapter_name}')(args)
             if args_type is tuple:
                 args_main = (args_main, )
             return args_main
 
         def _forward_decoder_mode(self, *args, **kwargs):
             args_type = type(args)
             if args_type is tuple:
                 args_sub_tuner = args[0]
                 args_sub_extra = args[1:]
             tuner_module = getattr(self, f'scetuner_{adapter_name}')
-            args_hidden, args_res = torch.split(
-                args_sub_tuner, args_sub_tuner.shape[1] - tuner_module.dim, 1)
+            args_hidden, args_res = torch.split(args_sub_tuner, args_sub_tuner.shape[1] - tuner_module.dim, 1)
             if hasattr(self, 'hint'):
                 hint_out = self.hint.probe_output_data
                 args_res_new = tuner_module(args_res, hint_out)
             else:
                 args_res_new = tuner_module(args_res)
             args_sub_tuner_new = torch.cat([args_hidden, args_res_new], dim=1)
             if args_type is tuple:
                 args_main = (args_sub_tuner_new, *args_sub_extra)
 
-            args_main = getattr(self,
-                                f'forward_origin_{adapter_name}')(*args_main,
-                                                                  **kwargs)
+            args_main = getattr(self, f'forward_origin_{adapter_name}')(*args_main, **kwargs)
             return args_main
 
         # 3. inject the tuners
         for tuner_id, t_module in enumerate(target_module_ins_list):
-            setattr(t_module, f'forward_origin_{adapter_name}',
-                    getattr(t_module, 'forward'))
+            setattr(t_module, f'forward_origin_{adapter_name}', getattr(t_module, 'forward'))
             if config.tuner_mode in ('encoder', 'identity'):
                 _forward = _forward_encoder_mode
             elif config.tuner_mode == 'decoder':
                 _forward = _forward_decoder_mode
             else:
                 raise Exception(f'Error tuner_mode: {config.tuner_mode}')
             setattr(t_module, 'forward', types.MethodType(_forward, t_module))
@@ -209,39 +173,30 @@
                 dim=dims[tuner_id],
                 tuner_length=int(dims[tuner_id] * config.down_ratio))
             setattr(t_module, f'scetuner_{adapter_name}', tuner_op)
             if len(hint_module_ins_list) > 0:
                 setattr(t_module, 'hint', hint_module_ins_list[tuner_id])
 
         def state_dict_callback(state_dict, adapter_name):
-            state_dict_new = {
-                key: value
-                for key, value in state_dict.items()
-                if f'scetuner_{adapter_name}' in key
-            }
+            state_dict_new = {key: value for key, value in state_dict.items() if f'scetuner_{adapter_name}' in key}
             return state_dict_new
 
         def mark_trainable_callback(model):
             return
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         modules = find_sub_module(module, f'scetuner_{adapter_name}')
         for _module in modules:
             _module: ActivationMixin
             _module: nn.Module
             _module.set_activation(adapter_name, activate)
-            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key,
-                                     activate, offload)
+            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key, activate, offload)
 
 
 class SCETunerModule(nn.Module, ActivationMixin):
 
     def __init__(self,
                  name,
                  adapter_name,
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/scetuning/scetuning_components.py` & `ms-swift-2.0.4/swift/tuners/scetuning/scetuning_components.py`

 * *Files 1% similar despite different names*

```diff
@@ -9,18 +9,15 @@
 from swift.utils.logger import get_logger
 
 logger = get_logger()
 
 
 def detach_tensors(feats):
     if type(feats) in [list, tuple]:
-        feats = [
-            detach_tensors(feat) if feat is not None else None
-            for feat in feats
-        ]
+        feats = [detach_tensors(feat) if feat is not None else None for feat in feats]
     elif isinstance(feats, dict):
         feats = {key: detach_tensors(val) for key, val in feats.items()}
     elif isinstance(feats, torch.Tensor):
         feats = feats.detach()
     else:
         feats = feats.detach()
     return feats
@@ -58,16 +55,15 @@
         scaling = None
     return scaling
 
 
 def get_weight_value(weight_type, scaling, x):
     if weight_type in ['gate']:
         scaling = torch.mean(torch.sigmoid(scaling(x)), dim=1).view(-1, 1, 1)
-    elif weight_type in ['scale', 'scale_channel'
-                         ] or weight_type.startswith('scalar'):
+    elif weight_type in ['scale', 'scale_channel'] or weight_type.startswith('scalar'):
         scaling = scaling
     else:
         scaling = None
     return scaling
 
 
 class SCEAdapter(nn.Module):
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/side.py` & `ms-swift-2.0.4/swift/tuners/side.py`

 * *Files 12% similar despite different names*

```diff
@@ -29,69 +29,55 @@
     by Zhang et al.(2019)
     See https://arxiv.org/abs/1912.13503
 
     Args:
         target_modules: The feedforward module to be replaced, in regex format
     """
 
-    dim: int = field(
-        default=None, metadata={'help': 'The dimension of the hidden states'})
+    dim: int = field(default=None, metadata={'help': 'The dimension of the hidden states'})
 
     target_modules: str = field(
-        default=None,
-        metadata={
-            'help': 'The target module to be replaced, in full match format'
-        })
+        default=None, metadata={'help': 'The target module to be replaced, in full match format'})
 
-    side_module_name: str = field(
-        default='fcn4',
-        metadata={'help': 'The name of the additive side networks'})
+    side_module_name: str = field(default='fcn4', metadata={'help': 'The name of the additive side networks'})
 
     source_hidden_pos: Union[str, int] = field(
         default=0,
         metadata={
-            'help':
-            'The position of the hidden state input to the target module, can be int (args) or str (kwargs)'
+            'help': 'The position of the hidden state input to the target module, can be int (args) or str (kwargs)'
         })
 
     target_hidden_pos: Union[str, int] = field(
         default=0,
         metadata={
-            'help':
-            'The position of the hidden state output from the target module, can be int (args) or str (kwargs)'
+            'help': 'The position of the hidden state output from the target module, can be int (args) or str (kwargs)'
         })
 
     def __post_init__(self):
         from .mapping import SwiftTuners
         self.swift_type = SwiftTuners.SIDE
 
 
 class Side(SwiftAdapter):
 
     @staticmethod
-    def prepare_model(model: nn.Module, config: SideConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: nn.Module, config: SideConfig, adapter_name: str) -> SwiftOutput:
         """Prepare a model with `SideConfig`"""
         module_keys = [key for key, _ in model.named_modules()]
 
         for module_key in module_keys:
             if re.fullmatch(config.target_modules, module_key):  # noqa
                 tgt_module = model.get_submodule(module_key)
-                logger.info(
-                    f'Matching target module [{module_key}] of type {type(tgt_module)}'
-                )
+                logger.info(f'Matching target module [{module_key}] of type {type(tgt_module)}')
                 if isinstance(tgt_module, (nn.ModuleList, nn.ModuleDict)):
                     raise Exception(
-                        f'Type of {type(tgt_module)} may not be supported because of its customized forward'
-                    )
+                        f'Type of {type(tgt_module)} may not be supported because of its customized forward')
 
                 def _forward(self, *args, **kwargs):
-                    args_main = getattr(
-                        self, f'forward_origin_{adapter_name}')(*args,
-                                                                **kwargs)
+                    args_main = getattr(self, f'forward_origin_{adapter_name}')(*args, **kwargs)
 
                     if isinstance(config.source_hidden_pos, int):
                         x = args[config.source_hidden_pos]
                     else:
                         x = kwargs[config.source_hidden_pos]
 
                     x_main = args_main[config.target_hidden_pos] \
@@ -99,64 +85,48 @@
                     out = getattr(self, f'side_{adapter_name}')(x, x_main)
                     if isinstance(args_main, (tuple, list, dict)):
                         args_main[config.target_hidden_pos] = out
                     else:
                         args_main = out
                     return args_main
 
-                if isinstance(tgt_module, nn.Sequential) and not hasattr(
-                        tgt_module, 'tgt_module_keys'):
-                    tgt_module.tgt_module_keys = copy.deepcopy(
-                        list(tgt_module._modules.keys()))
+                if isinstance(tgt_module, nn.Sequential) and not hasattr(tgt_module, 'tgt_module_keys'):
+                    tgt_module.tgt_module_keys = copy.deepcopy(list(tgt_module._modules.keys()))
 
                     def forward_seq(self, input, *args, **kwargs):
                         for idx, module in enumerate(self):
                             if idx >= len(tgt_module.tgt_module_keys):
                                 continue
                             input = module(input)
                         return input
 
-                    setattr(tgt_module, f'forward_origin_{adapter_name}',
-                            types.MethodType(forward_seq, tgt_module))
+                    setattr(tgt_module, f'forward_origin_{adapter_name}', types.MethodType(forward_seq, tgt_module))
                 else:
-                    setattr(tgt_module, f'forward_origin_{adapter_name}',
-                            tgt_module.forward)
+                    setattr(tgt_module, f'forward_origin_{adapter_name}', tgt_module.forward)
                 tgt_module.forward = types.MethodType(_forward, tgt_module)
-                side_module = SideModule(config.dim, adapter_name, module_key,
-                                         config.side_module_name)
+                side_module = SideModule(config.dim, adapter_name, module_key, config.side_module_name)
                 setattr(tgt_module, f'side_{adapter_name}', side_module)
-                logger.info(
-                    f'Side modules(module_key): {module_key}.side_{adapter_name}'
-                )
+                logger.info(f'Side modules(module_key): {module_key}.side_{adapter_name}')
 
         def state_dict_callback(state_dict, adapter_name):
-            return {
-                key: value
-                for key, value in state_dict.items()
-                if f'side_{adapter_name}' in key
-            }
+            return {key: value for key, value in state_dict.items() if f'side_{adapter_name}' in key}
 
         def mark_trainable_callback(model):
             return
 
-        return SwiftOutput(config, state_dict_callback,
-                           mark_trainable_callback)
+        return SwiftOutput(config, state_dict_callback, mark_trainable_callback)
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         modules = find_sub_module(module, f'side_{adapter_name}')
         for _module in modules:
             _module: ActivationMixin
             _module: nn.Module
             _module.set_activation(adapter_name, activate)
-            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key,
-                                     activate, offload)
+            SwiftAdapter.save_memory(_module, adapter_name, _module.module_key, activate, offload)
 
 
 class SideModule(nn.Module, ActivationMixin):
     """The implementation of vision side-tuning method.
 
     Side-Tuning only needs to train one side network and
     weights the output of pre-trained model and side network.
@@ -178,21 +148,18 @@
             self.side_net = FCN4(out_dims=dim)
         elif side_module_name == 'mlp':
             self.side_net = Mlp(dim)
         elif side_module_name == 'alexnet':
             import torchvision
             mm = torchvision.models.alexnet(pretrained=True)
             self.side_net = nn.Sequential(
-                OrderedDict([('features', mm.features),
-                             ('avgpool', mm.avgpool),
-                             ('flatten', nn.Flatten()),
+                OrderedDict([('features', mm.features), ('avgpool', mm.avgpool), ('flatten', nn.Flatten()),
                              ('fc', nn.Linear(9216, dim, bias=False))]))
         else:
-            raise ValueError(
-                f'Unsupported side_module_name: {side_module_name}')
+            raise ValueError(f'Unsupported side_module_name: {side_module_name}')
         self.alpha = nn.Parameter(torch.tensor(0.0))
 
     def forward(self, x, x_main):
         if not self.is_activated(self.adapter_name):
             return x_main
         alpha_squashed = torch.sigmoid(self.alpha)
         x_side = self.side_net(x)
@@ -204,49 +171,25 @@
     """The implementation of simple FCN4 network for side network.
     """
 
     def __init__(self, out_dims=-1, **kwargs):
         super(FCN4, self).__init__(**kwargs)
 
         self.conv1 = nn.Sequential(
-            nn.Conv2d(
-                3,
-                16,
-                kernel_size=3,
-                stride=1,
-                padding=1,
-                bias=False,
-                dilation=1), nn.GroupNorm(2, 16), nn.ReLU())
+            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False, dilation=1), nn.GroupNorm(2, 16),
+            nn.ReLU())
         self.conv2 = nn.Sequential(
-            nn.Conv2d(
-                16,
-                16,
-                kernel_size=3,
-                stride=2,
-                padding=0,
-                bias=False,
-                dilation=1), nn.GroupNorm(2, 16), nn.ReLU())
+            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=0, bias=False, dilation=1), nn.GroupNorm(2, 16),
+            nn.ReLU())
         self.conv3 = nn.Sequential(
-            nn.Conv2d(
-                16,
-                32,
-                kernel_size=3,
-                stride=2,
-                padding=0,
-                bias=False,
-                dilation=1), nn.GroupNorm(2, 32), nn.ReLU())
+            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=0, bias=False, dilation=1), nn.GroupNorm(2, 32),
+            nn.ReLU())
         self.conv4 = nn.Sequential(
-            nn.Conv2d(
-                32,
-                64,
-                kernel_size=3,
-                stride=1,
-                padding=0,
-                bias=False,
-                dilation=1), nn.GroupNorm(2, 64), nn.ReLU())
+            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=0, bias=False, dilation=1), nn.GroupNorm(2, 64),
+            nn.ReLU())
         self.pool = nn.AdaptiveAvgPool2d((1, 1))
         if out_dims > 0:
             self.fc = nn.Linear(64, out_dims)
         else:
             self.fc = None
 
     def forward(self, x):
@@ -277,22 +220,20 @@
         use_conv=False,
     ):
         super().__init__()
         out_features = out_features or in_features
         hidden_features = hidden_features or in_features
         bias = tuple(repeat(bias, 2))
         drop_probs = tuple(repeat(drop, 2))
-        linear_layer = partial(
-            nn.Conv2d, kernel_size=1) if use_conv else nn.Linear
+        linear_layer = partial(nn.Conv2d, kernel_size=1) if use_conv else nn.Linear
 
         self.fc1 = linear_layer(in_features, hidden_features, bias=bias[0])
         self.act = act_layer()
         self.drop1 = nn.Dropout(drop_probs[0])
-        self.norm = norm_layer(
-            hidden_features) if norm_layer is not None else nn.Identity()
+        self.norm = norm_layer(hidden_features) if norm_layer is not None else nn.Identity()
         self.fc2 = linear_layer(hidden_features, out_features, bias=bias[1])
         self.drop2 = nn.Dropout(drop_probs[1])
 
     def forward(self, x):
         x = self.fc1(x)
         x = self.act(x)
         x = self.drop1(x)
```

### Comparing `ms-swift-2.0.3.post1/swift/tuners/utils.py` & `ms-swift-2.0.4/swift/tuners/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -45,17 +45,15 @@
         This method saves the configuration of your adapter model in a directory.
 
         Args:
             save_directory (`str`):
                 The directory where the configuration will be saved.
         """
         if os.path.isfile(save_directory):
-            raise AssertionError(
-                f'Provided path ({save_directory}) should be a directory, not a file'
-            )
+            raise AssertionError(f'Provided path ({save_directory}) should be a directory, not a file')
 
         os.makedirs(save_directory, exist_ok=True)
 
         output_dict = self.__dict__
         output_dict.update(kwargs)
         output_path = os.path.join(save_directory, CONFIG_NAME)
 
@@ -70,28 +68,22 @@
 
         Args:
             pretrained_model_name_or_path (`str`):
                 The directory or the hub-id where the configuration is saved.
             **kwargs:
                 Additional keyword arguments passed along to the child class initialization.
         """
-        if os.path.isfile(
-                os.path.join(pretrained_model_name_or_path, CONFIG_NAME)):
-            config_file = os.path.join(pretrained_model_name_or_path,
-                                       CONFIG_NAME)
+        if os.path.isfile(os.path.join(pretrained_model_name_or_path, CONFIG_NAME)):
+            config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)
         else:
             try:
-                model_dir = snapshot_download(
-                    pretrained_model_name_or_path,
-                    ignore_file_pattern=BIN_EXTENSIONS)
+                model_dir = snapshot_download(pretrained_model_name_or_path, ignore_file_pattern=BIN_EXTENSIONS)
                 config_file = os.path.join(model_dir, CONFIG_NAME)
             except Exception:
-                raise ValueError(
-                    f"Can't find config.json at '{pretrained_model_name_or_path}'"
-                )
+                raise ValueError(f"Can't find config.json at '{pretrained_model_name_or_path}'")
 
         loaded_attributes = cls.from_json_file(config_file)
 
         from .mapping import SWIFT_MAPPING
         assert loaded_attributes.get('swift_type', '') in SWIFT_MAPPING
         config = SWIFT_MAPPING[loaded_attributes['swift_type']][0](**kwargs)
 
@@ -151,21 +143,18 @@
     USE_UNIQUE_THREAD = 'USE_UNIQUE_THREAD'
 
     REMINEDED = False
 
     def __init__(self, module_key):
         self.module_key = module_key
         self._thread_inf: Dict[int, Dict[str, bool]] = {}
-        self._unique_thread = bool(
-            int(os.environ.get(ActivationMixin.USE_UNIQUE_THREAD, '1')))
+        self._unique_thread = bool(int(os.environ.get(ActivationMixin.USE_UNIQUE_THREAD, '1')))
         if not self._unique_thread and not ActivationMixin.REMINEDED:
             ActivationMixin.REMINEDED = True
-            logger.warn(
-                'Using multiple thread mode, gradient checkpointing is not supported.'
-            )
+            logger.warn('Using multiple thread mode, gradient checkpointing is not supported.')
 
     @property
     def indent(self):
         return 0 if self.unique_thread else threading.get_ident()
 
     @property
     def unique_thread(self):
@@ -178,19 +167,15 @@
         self._thread_inf[tid][adapter_name] = activate
 
     def is_activated(self, adapter_name):
         tid = self.indent
         return self._thread_inf.get(tid, {}).get(adapter_name, False)
 
     def get_activated_adapters(self):
-        return [
-            key
-            for key, value in self._thread_inf.get(self.indent, {}).items()
-            if value
-        ]
+        return [key for key, value in self._thread_inf.get(self.indent, {}).items() if value]
 
 
 class OffloadHelper:
 
     sub_dir = 'offload_cache'
     cache_dir = os.path.join(get_cache_dir(), sub_dir)
     shutil.rmtree(cache_dir, ignore_errors=True)
@@ -207,16 +192,15 @@
         tensor_file = os.path.join(offload_folder, f'{weight_name}.dat')
         if index is not None:
             if dtype is None:
                 dtype = str(array.dtype)
             index[weight_name] = {'dtype': dtype, 'shape': list(array.shape)}
         if array.ndim == 0:
             array = array[None]
-        file_array = np.memmap(
-            tensor_file, dtype=array.dtype, mode='w+', shape=array.shape)
+        file_array = np.memmap(tensor_file, dtype=array.dtype, mode='w+', shape=array.shape)
         file_array[:] = array[:]
         file_array.flush()
         return index
 
     @staticmethod
     def load_offloaded_weight(weight_file, weight_info):
         shape = tuple(weight_info['shape'])
@@ -242,205 +226,158 @@
         key = adapter_name + ':' + module_key
         md5 = hashlib.md5(key.encode('utf-8')).hexdigest()
         sub_folder = os.path.join(OffloadHelper.cache_dir, md5)
         os.makedirs(sub_folder, exist_ok=True)
         state_dict = module.state_dict()
         OffloadHelper.index[md5] = {}
         for key, tensor in state_dict.items():
-            OffloadHelper.offload_weight(tensor, key, sub_folder,
-                                         OffloadHelper.index[md5])
+            OffloadHelper.offload_weight(tensor, key, sub_folder, OffloadHelper.index[md5])
 
     @staticmethod
     def load_disk(module: torch.nn.Module, adapter_name, module_key):
         key = adapter_name + ':' + module_key
         md5 = hashlib.md5(key.encode('utf-8')).hexdigest()
         sub_folder = os.path.join(OffloadHelper.cache_dir, md5)
         state_dict = {}
         for key, value in OffloadHelper.index[md5].items():
             file = os.path.join(sub_folder, f'{key}.dat')
-            state_dict[key] = OffloadHelper.load_offloaded_weight(
-                file, OffloadHelper.index[md5][key])
+            state_dict[key] = OffloadHelper.load_offloaded_weight(file, OffloadHelper.index[md5][key])
         if version.parse(torch.__version__) >= version.parse('2.1.0'):
             module.load_state_dict(state_dict, assign=True)
         else:
             for name, _module in module.named_modules():
                 if len(list(_module.modules())) > 1:
                     continue
 
                 buffers = {}
                 prefix = name if not name else name + '.'
                 for sub_name, buffer in _module.named_buffers():
                     buffer_cls = type(buffer)
-                    buffers[sub_name] = buffer_cls(state_dict[prefix
-                                                              + sub_name])
+                    buffers[sub_name] = buffer_cls(state_dict[prefix + sub_name])
                 _module._buffers.update(buffers)
                 params = {}
                 for sub_name, param in _module.named_parameters():
                     param_cls = type(param)
-                    params[sub_name] = param_cls(
-                        state_dict[prefix + sub_name],
-                        requires_grad=param.requires_grad)
+                    params[sub_name] = param_cls(state_dict[prefix + sub_name], requires_grad=param.requires_grad)
                 _module._parameters.update(params)
         shutil.rmtree(sub_folder, ignore_errors=True)
 
 
 class SwiftAdapter:
 
     @staticmethod
-    def prepare_model(model: torch.nn.Module, config: SwiftConfig,
-                      adapter_name: str) -> SwiftOutput:
+    def prepare_model(model: torch.nn.Module, config: SwiftConfig, adapter_name: str) -> SwiftOutput:
         raise NotImplementedError
 
     @staticmethod
-    def activate_adapter(module: torch.nn.Module,
-                         adapter_name: str,
-                         activate: bool,
-                         offload: str = None):
+    def activate_adapter(module: torch.nn.Module, adapter_name: str, activate: bool, offload: str = None):
         raise NotImplementedError
 
     @staticmethod
-    def save_memory(module: torch.nn.Module,
-                    adapter_name: str,
-                    module_key: str,
-                    activate: bool,
-                    offload: str = None):
+    def save_memory(module: torch.nn.Module, adapter_name: str, module_key: str, activate: bool, offload: str = None):
         if not isinstance(module, torch.nn.Module):
             return
         if activate:
             SwiftAdapter.load(module, adapter_name, module_key)
         else:
-            SwiftAdapter.offload(
-                module, adapter_name, module_key, offload=offload)
+            SwiftAdapter.offload(module, adapter_name, module_key, offload=offload)
 
     @staticmethod
-    def offload(module: torch.nn.Module, adapter_name, module_key,
-                offload: str):
+    def offload(module: torch.nn.Module, adapter_name, module_key, offload: str):
         if not offload:
             return
         device = next(iter(module.parameters())).device
-        if hasattr(module,
-                   'origin_device') and module.origin_device != str(device):
+        if hasattr(module, 'origin_device') and module.origin_device != str(device):
             return
         module.origin_device = str(device)
         if offload == 'cpu':
             if str(device) != 'cpu':
                 module.to('cpu')
         elif offload == 'meta':
             if str(device) != 'meta':
-                OffloadHelper.offload_disk(
-                    module, adapter_name=adapter_name, module_key=module_key)
+                OffloadHelper.offload_disk(module, adapter_name=adapter_name, module_key=module_key)
                 module.to('meta')
         else:
             raise NotImplementedError
         torch.cuda.empty_cache()
 
     @staticmethod
     def load(module: torch.nn.Module, adapter_name, module_key):
         device = next(iter(module.parameters())).device
-        if not hasattr(module,
-                       'origin_device') or module.origin_device == str(device):
+        if not hasattr(module, 'origin_device') or module.origin_device == str(device):
             return
         if str(device) == 'cpu':
             module.to(module.origin_device)
             delattr(module, 'origin_device')
         elif str(device) == 'meta':
-            OffloadHelper.load_disk(
-                module, adapter_name=adapter_name, module_key=module_key)
+            OffloadHelper.load_disk(module, adapter_name=adapter_name, module_key=module_key)
             module.to(module.origin_device)
             delattr(module, 'origin_device')
 
     @staticmethod
     def has_additional_modules():
         return True
 
 
 class ModulesToSaveWrapper(ActivationMixin, _ModulesToSaveWrapper):
 
     def __init__(self, *args, module_key, **kwargs):
         super(ModulesToSaveWrapper, self).__init__(module_key)
         super(ActivationMixin, self).__init__(*args, **kwargs)
-        SwiftAdapter.save_memory(
-            self.original_module,
-            'original_module',
-            self.module_key,
-            False,
-            offload='cpu')
+        SwiftAdapter.save_memory(self.original_module, 'original_module', self.module_key, False, offload='cpu')
 
     @property
     def active_adapter(self):
         active_adapters = self.get_activated_adapters()
         if not active_adapters:
             return None
         elif len(active_adapters) > 1:
-            raise ValueError(
-                'ModulesToSaveWrapper does not support multiple active adapters'
-            )
+            raise ValueError('ModulesToSaveWrapper does not support multiple active adapters')
         return active_adapters[0]
 
     def set_adapter(self, adapter_name: str, offload: str = None):
         if adapter_name not in self.modules_to_save:
-            raise ValueError(
-                f'Adapter {adapter_name} not found in {self.modules_to_save.keys()}'
-            )
+            raise ValueError(f'Adapter {adapter_name} not found in {self.modules_to_save.keys()}')
         self.modules_to_save[adapter_name].requires_grad_(True)
         self.set_activation(adapter_name, True)
-        SwiftAdapter.save_memory(self.modules_to_save[adapter_name],
-                                 adapter_name, self.module_key, True)
-        SwiftAdapter.save_memory(
-            self.original_module,
-            'original_module',
-            self.module_key,
-            False,
-            offload=offload)
+        SwiftAdapter.save_memory(self.modules_to_save[adapter_name], adapter_name, self.module_key, True)
+        SwiftAdapter.save_memory(self.original_module, 'original_module', self.module_key, False, offload=offload)
 
     def deactivate_adapter(self, adapter_name: str, offload: str = None):
         if adapter_name in self.modules_to_save and self.unique_thread:
             self.modules_to_save[adapter_name].requires_grad_(False)
         self.set_activation(adapter_name, False)
         SwiftAdapter.save_memory(
-            self.modules_to_save[adapter_name],
-            adapter_name,
-            self.module_key,
-            False,
-            offload=offload)
+            self.modules_to_save[adapter_name], adapter_name, self.module_key, False, offload=offload)
         if not self.get_activated_adapters():
-            SwiftAdapter.save_memory(self.original_module, 'original_module',
-                                     self.module_key, True)
+            SwiftAdapter.save_memory(self.original_module, 'original_module', self.module_key, True)
 
     def enable_adapters(self, enabled: bool):
         super().enable_adapters(enabled)
         if not enabled:
-            SwiftAdapter.save_memory(
-                self.original_module,
-                'original_module',
-                self.module_key,
-                False,
-                offload='meta')
+            SwiftAdapter.save_memory(self.original_module, 'original_module', self.module_key, False, offload='meta')
         else:
-            SwiftAdapter.save_memory(self.original_module, 'original_module',
-                                     self.module_key, True)
+            SwiftAdapter.save_memory(self.original_module, 'original_module', self.module_key, True)
 
 
 def set_adapter(model, adapter_name, activate, offload):
     for module in model.modules():
         if isinstance(module, ModulesToSaveWrapper):
             if activate:
                 module.set_adapter(adapter_name, offload)
             else:
                 module.deactivate_adapter(adapter_name, offload)
 
 
 def set_trainable(model, adapter_name):
     key_list = [key for key, _ in model.named_modules()]
     for key in key_list:
-        target_module_found = any(
-            key.endswith(target_key) for target_key in model.modules_to_save)
+        target_module_found = any(key.endswith(target_key) for target_key in model.modules_to_save)
         if target_module_found:
             parent, target, target_name = _get_submodules(model, key)
             if isinstance(target, ModulesToSaveWrapper):
                 target.update(adapter_name)
                 target.set_adapter(target.active_adapter)
             else:
-                new_module = ModulesToSaveWrapper(
-                    target, module_key=key, adapter_name=adapter_name)
+                new_module = ModulesToSaveWrapper(target, module_key=key, adapter_name=adapter_name)
                 new_module.set_adapter(adapter_name)
                 setattr(parent, target_name, new_module)
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/app.py` & `ms-swift-2.0.4/swift/ui/app.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/ui/base.py` & `ms-swift-2.0.4/swift/ui/base.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 import os
 import typing
 from dataclasses import fields
 from functools import partial, wraps
 from typing import Any, Dict, List, OrderedDict, Type
 
-from gradio import (Accordion, Button, Checkbox, Dropdown, Slider, Tab,
-                    TabItem, Textbox)
+from gradio import Accordion, Button, Checkbox, Dropdown, Slider, Tab, TabItem, Textbox
 
 from swift.llm.utils.model import MODEL_MAPPING, ModelType
 
 all_langs = ['zh', 'en']
 builder: Type['BaseUI'] = None
 base_builder: Type['BaseUI'] = None
 lang = os.environ.get('SWIFT_UI_LANG', all_langs[0])
@@ -23,17 +22,15 @@
         self = args[0]
 
         if builder is not None:
             choices = base_builder.choice(elem_id)
             if choices:
                 kwargs['choices'] = choices
 
-        if not isinstance(
-                self,
-            (Tab, TabItem, Accordion)) and 'interactive' not in kwargs:  # noqa
+        if not isinstance(self, (Tab, TabItem, Accordion)) and 'interactive' not in kwargs:  # noqa
             kwargs['interactive'] = True
 
         if 'is_list' in kwargs:
             self.is_list = kwargs.pop('is_list')
 
         if base_builder and base_builder.default(elem_id) is not None:
             kwargs['value'] = base_builder.default(elem_id)
@@ -187,9 +184,8 @@
         arguments = {}
         for f in fields(dataclass):
             arguments[f.name] = f'--{f.name}'
         return arguments
 
     @staticmethod
     def get_custom_name_list():
-        return list(
-            set(MODEL_MAPPING.keys()) - set(ModelType.get_model_name_list()))
+        return list(set(MODEL_MAPPING.keys()) - set(ModelType.get_model_name_list()))
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_infer/generate.py` & `ms-swift-2.0.4/swift/ui/llm_infer/generate.py`

 * *Files 7% similar despite different names*

```diff
@@ -63,29 +63,13 @@
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Row():
             gr.Textbox(elem_id='max_new_tokens', lines=1, value='2048')
             gr.Checkbox(elem_id='do_sample', value=True)
             gr.Dropdown(elem_id='infer_backend', value='pt')
-            gr.Slider(
-                elem_id='temperature',
-                minimum=0.0,
-                maximum=10,
-                step=0.1,
-                value=0.3)
-            gr.Slider(
-                elem_id='top_k', minimum=1, maximum=100, step=5, value=20)
-            gr.Slider(
-                elem_id='top_p',
-                minimum=0.0,
-                maximum=1.0,
-                step=0.05,
-                value=0.7)
-            gr.Slider(
-                elem_id='repetition_penalty',
-                minimum=0.0,
-                maximum=10,
-                step=0.05,
-                value=1.05)
+            gr.Slider(elem_id='temperature', minimum=0.0, maximum=10, step=0.1, value=0.3)
+            gr.Slider(elem_id='top_k', minimum=1, maximum=100, step=5, value=20)
+            gr.Slider(elem_id='top_p', minimum=0.0, maximum=1.0, step=0.05, value=0.7)
+            gr.Slider(elem_id='repetition_penalty', minimum=0.0, maximum=10, step=0.05, value=1.05)
             if os.environ.get('MODELSCOPE_ENVIRONMENT') != 'studio':
                 gr.Textbox(elem_id='port', lines=1, value='8000')
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_infer/llm_infer.py` & `ms-swift-2.0.4/swift/ui/llm_infer/llm_infer.py`

 * *Files 4% similar despite different names*

```diff
@@ -9,53 +9,47 @@
 import gradio as gr
 import json
 import torch
 from gradio import Accordion, Tab
 from modelscope import GenerationConfig
 
 from swift import snapshot_download
-from swift.llm import (DeployArguments, InferArguments, XRequestConfig,
-                       inference_client, inference_stream,
+from swift.llm import (DeployArguments, InferArguments, XRequestConfig, inference_client, inference_stream,
                        limit_history_length, prepare_model_template)
 from swift.ui.base import BaseUI
 from swift.ui.llm_infer.model import Model
 from swift.ui.llm_infer.runtime import Runtime
 
 
 class LLMInfer(BaseUI):
     group = 'llm_infer'
 
     sub_ui = [Model, Runtime]
 
-    is_inference = os.environ.get('USE_INFERENCE') == '1' or os.environ.get(
-        'MODELSCOPE_ENVIRONMENT') == 'studio'
+    is_inference = os.environ.get('USE_INFERENCE') == '1' or os.environ.get('MODELSCOPE_ENVIRONMENT') == 'studio'
 
     locale_dict = {
         'generate_alert': {
             'value': {
-                'zh':
-                '' if is_inference else '',
-                'en':
-                'Please load model first'
-                if is_inference else 'Please deploy model first',
+                'zh': '' if is_inference else '',
+                'en': 'Please load model first' if is_inference else 'Please deploy model first',
             }
         },
         'llm_infer': {
             'label': {
                 'zh': 'LLM' if is_inference else 'LLM',
                 'en': 'LLM Inference' if is_inference else 'LLM Deployment',
             }
         },
         'load_alert': {
             'value': {
                 'zh':
                 '' if is_inference else '""',
                 'en':
-                'Start to load model, please wait'
-                if is_inference else 'Start to deploy model, '
+                'Start to load model, please wait' if is_inference else 'Start to deploy model, '
                 'please Click "Show running '
                 'status" to view details',
             }
         },
         'loaded_alert': {
             'value': {
                 'zh': '',
@@ -121,18 +115,16 @@
                 Runtime.build_ui(base_tab)
                 gr.Dropdown(
                     elem_id='gpu_id',
                     multiselect=True,
                     choices=[str(i) for i in range(gpu_count)] + ['cpu'],
                     value=default_device,
                     scale=8)
-                chatbot = gr.Chatbot(
-                    elem_id='chatbot', elem_classes='control-height')
-                prompt = gr.Textbox(
-                    elem_id='prompt', lines=1, interactive=True)
+                chatbot = gr.Chatbot(elem_id='chatbot', elem_classes='control-height')
+                prompt = gr.Textbox(elem_id='prompt', lines=1, interactive=True)
 
                 with gr.Row():
                     clear_history = gr.Button(elem_id='clear_history')
                     submit = gr.Button(elem_id='submit')
 
                 if cls.is_inference:
                     submit.click(
@@ -146,18 +138,15 @@
                             cls.element('top_k'),
                             cls.element('top_p'),
                             cls.element('repetition_penalty')
                         ],
                         outputs=[prompt, chatbot],
                         queue=True)
 
-                    clear_history.click(
-                        fn=cls.clear_session,
-                        inputs=[],
-                        outputs=[prompt, chatbot])
+                    clear_history.click(fn=cls.clear_session, inputs=[], outputs=[prompt, chatbot])
 
                     cls.element('load_checkpoint').click(
                         cls.reset_memory, [], [model_and_template]) \
                         .then(cls.reset_loading_button, [], [cls.element('load_checkpoint')]).then(
                         cls.prepare_checkpoint, [
                             value for value in cls.elements().values()
                             if not isinstance(value, (Tab, Accordion))
@@ -165,21 +154,18 @@
                                                       [prompt]).then(  # noqa
                         cls.clear_session,
                         inputs=[],
                         outputs=[prompt, chatbot],
                         queue=True).then(cls.reset_load_button, [], [cls.element('load_checkpoint')])
                 else:
                     cls.element('load_checkpoint').click(
-                        cls.deploy_model, [
-                            value for value in cls.elements().values()
-                            if not isinstance(value, (Tab, Accordion))
-                        ], [
-                            cls.element('runtime_tab'),
-                            cls.element('running_tasks'), model_and_template
-                        ])
+                        cls.deploy_model,
+                        [value for value in cls.elements().values() if not isinstance(value, (Tab, Accordion))],
+                        [cls.element('runtime_tab'),
+                         cls.element('running_tasks'), model_and_template])
                     submit.click(
                         cls.send_message,
                         inputs=[
                             cls.element('running_tasks'), model_and_template,
                             cls.element('template_type'), prompt, chatbot,
                             cls.element('system'),
                             cls.element('max_new_tokens'),
@@ -187,82 +173,65 @@
                             cls.element('top_k'),
                             cls.element('top_p'),
                             cls.element('repetition_penalty')
                         ],
                         outputs=[prompt, chatbot],
                         queue=True)
 
-                    clear_history.click(
-                        fn=cls.clear_session,
-                        inputs=[],
-                        outputs=[prompt, chatbot])
+                    clear_history.click(fn=cls.clear_session, inputs=[], outputs=[prompt, chatbot])
 
                     base_tab.element('running_tasks').change(
-                        partial(Runtime.task_changed, base_tab=base_tab),
-                        [base_tab.element('running_tasks')],
-                        [
-                            value for value in base_tab.elements().values()
-                            if not isinstance(value, (Tab, Accordion))
-                        ] + [cls.element('log'), model_and_template],
+                        partial(Runtime.task_changed, base_tab=base_tab), [base_tab.element('running_tasks')],
+                        [value for value in base_tab.elements().values() if not isinstance(value, (Tab, Accordion))]
+                        + [cls.element('log'), model_and_template],
                         cancels=Runtime.log_event)
                     Runtime.element('kill_task').click(
                         Runtime.kill_task,
                         [Runtime.element('running_tasks')],
-                        [Runtime.element('running_tasks')]
-                        + [Runtime.element('log')],
+                        [Runtime.element('running_tasks')] + [Runtime.element('log')],
                         cancels=[Runtime.log_event],
                     )
 
     @classmethod
     def deploy(cls, *args):
         deploy_args = cls.get_default_value_from_dataclass(DeployArguments)
         kwargs = {}
         kwargs_is_list = {}
         other_kwargs = {}
         more_params = {}
-        keys = [
-            key for key, value in cls.elements().items()
-            if not isinstance(value, (Tab, Accordion))
-        ]
+        keys = [key for key, value in cls.elements().items() if not isinstance(value, (Tab, Accordion))]
         for key, value in zip(keys, args):
             compare_value = deploy_args.get(key)
-            compare_value_arg = str(compare_value) if not isinstance(
-                compare_value, (list, dict)) else compare_value
-            compare_value_ui = str(value) if not isinstance(
-                value, (list, dict)) else value
+            compare_value_arg = str(compare_value) if not isinstance(compare_value, (list, dict)) else compare_value
+            compare_value_ui = str(value) if not isinstance(value, (list, dict)) else value
             if key in deploy_args and compare_value_ui != compare_value_arg and value:
-                if isinstance(value, str) and re.fullmatch(
-                        cls.int_regex, value):
+                if isinstance(value, str) and re.fullmatch(cls.int_regex, value):
                     value = int(value)
-                elif isinstance(value, str) and re.fullmatch(
-                        cls.float_regex, value):
+                elif isinstance(value, str) and re.fullmatch(cls.float_regex, value):
                     value = float(value)
-                kwargs[key] = value if not isinstance(
-                    value, list) else ' '.join(value)
+                kwargs[key] = value if not isinstance(value, list) else ' '.join(value)
                 kwargs_is_list[key] = isinstance(value, list)
             else:
                 other_kwargs[key] = value
             if key == 'more_params' and value:
                 more_params = json.loads(value)
 
         kwargs.update(more_params)
         if kwargs['model_type'] == cls.locale('checkpoint', cls.lang)['value']:
             model_dir = kwargs.pop('model_id_or_path')
             if not os.path.exists(model_dir):
                 model_dir = snapshot_download(model_dir)
             kwargs['ckpt_dir'] = model_dir
 
         if 'ckpt_dir' in kwargs:
-            with open(os.path.join(kwargs['ckpt_dir'], 'sft_args.json'),
-                      'r') as f:
+            with open(os.path.join(kwargs['ckpt_dir'], 'sft_args.json'), 'r') as f:
                 kwargs['model_type'] = json.load(f)['model_type']
         deploy_args = DeployArguments(
             **{
-                key: value.split(' ')
-                if key in kwargs_is_list and kwargs_is_list[key] else value
+                key: value.split(' ') if key in kwargs_is_list and kwargs_is_list[key] else value
                 for key, value in kwargs.items()
             })
         if deploy_args.port in Runtime.get_all_ports():
             raise gr.Error(cls.locale('port_alert', cls.lang)['value'])
         params = ''
         for e in kwargs:
             if e in kwargs_is_list and kwargs_is_list[e]:
@@ -307,16 +276,15 @@
 
     @classmethod
     def update_runtime(cls):
         return gr.update(open=True), gr.update(visible=True)
 
     @classmethod
     def reset_load_button(cls):
-        return gr.update(
-            value=cls.locale('load_checkpoint', cls.lang)['value'])
+        return gr.update(value=cls.locale('load_checkpoint', cls.lang)['value'])
 
     @classmethod
     def reset_loading_button(cls):
         return gr.update(value=cls.locale('load_alert', cls.lang)['value'])
 
     @classmethod
     def reset_memory(cls):
@@ -326,48 +294,38 @@
     def prepare_checkpoint(cls, *args):
         torch.cuda.empty_cache()
         infer_args = cls.get_default_value_from_dataclass(InferArguments)
         kwargs = {}
         kwargs_is_list = {}
         other_kwargs = {}
         more_params = {}
-        keys = [
-            key for key, value in cls.elements().items()
-            if not isinstance(value, (Tab, Accordion))
-        ]
+        keys = [key for key, value in cls.elements().items() if not isinstance(value, (Tab, Accordion))]
         for key, value in zip(keys, args):
             compare_value = infer_args.get(key)
-            compare_value_arg = str(compare_value) if not isinstance(
-                compare_value, (list, dict)) else compare_value
-            compare_value_ui = str(value) if not isinstance(
-                value, (list, dict)) else value
+            compare_value_arg = str(compare_value) if not isinstance(compare_value, (list, dict)) else compare_value
+            compare_value_ui = str(value) if not isinstance(value, (list, dict)) else value
             if key in infer_args and compare_value_ui != compare_value_arg and value:
-                if isinstance(value, str) and re.fullmatch(
-                        cls.int_regex, value):
+                if isinstance(value, str) and re.fullmatch(cls.int_regex, value):
                     value = int(value)
-                elif isinstance(value, str) and re.fullmatch(
-                        cls.float_regex, value):
+                elif isinstance(value, str) and re.fullmatch(cls.float_regex, value):
                     value = float(value)
-                kwargs[key] = value if not isinstance(
-                    value, list) else ' '.join(value)
+                kwargs[key] = value if not isinstance(value, list) else ' '.join(value)
                 kwargs_is_list[key] = isinstance(value, list)
             else:
                 other_kwargs[key] = value
             if key == 'more_params' and value:
                 more_params = json.loads(value)
 
         kwargs.update(more_params)
         if kwargs['model_type'] == cls.locale('checkpoint', cls.lang)['value']:
             model_dir = kwargs.pop('model_id_or_path')
             if not os.path.exists(model_dir):
                 model_dir = snapshot_download(model_dir)
             kwargs['ckpt_dir'] = model_dir
-        if 'ckpt_dir' in kwargs or (
-                'model_id_or_path' in kwargs
-                and not os.path.exists(kwargs['model_id_or_path'])):
+        if 'ckpt_dir' in kwargs or ('model_id_or_path' in kwargs and not os.path.exists(kwargs['model_id_or_path'])):
             kwargs.pop('model_type', None)
 
         devices = other_kwargs['gpu_id']
         devices = [d for d in devices if d]
         assert (len(devices) == 1 or 'cpu' not in devices)
         gpus = ','.join(devices)
         if gpus != 'cpu':
@@ -382,69 +340,55 @@
         return '', None
 
     @classmethod
     def change_interactive(cls):
         return gr.update(interactive=True)
 
     @classmethod
-    def send_message(cls, running_task, model_and_template, template_type,
-                     prompt: str, history, system, max_new_tokens, temperature,
-                     top_k, top_p, repetition_penalty):
+    def send_message(cls, running_task, model_and_template, template_type, prompt: str, history, system, max_new_tokens,
+                     temperature, top_k, top_p, repetition_penalty):
         if not model_and_template:
             gr.Warning(cls.locale('generate_alert', cls.lang)['value'])
             return '', None
         _, args = Runtime.parse_info_from_cmdline(running_task)
         model_type, template = model_and_template
-        old_history, history = history, []
+        old_history, history = history or [], []
         request_config = XRequestConfig(
-            temperature=temperature,
-            top_k=top_k,
-            top_p=top_p,
-            repetition_penalty=repetition_penalty)
+            temperature=temperature, top_k=top_k, top_p=top_p, repetition_penalty=repetition_penalty)
         request_config.stream = True
+        request_config.stop = ['Observation:']
         stream_resp_with_history = ''
         if not template_type.endswith('generation'):
             stream_resp = inference_client(
-                model_type,
-                prompt,
-                old_history,
-                system=system,
-                port=args['port'],
-                request_config=request_config)
+                model_type, prompt, old_history, system=system, port=args['port'], request_config=request_config)
             for chunk in stream_resp:
                 stream_resp_with_history += chunk.choices[0].delta.content
                 qr_pair = [prompt, stream_resp_with_history]
                 total_history = old_history + [qr_pair]
                 yield '', total_history
         else:
             request_config.max_tokens = max_new_tokens
-            stream_resp = inference_client(
-                model_type,
-                prompt,
-                port=args['port'],
-                request_config=request_config)
+            stream_resp = inference_client(model_type, prompt, port=args['port'], request_config=request_config)
             for chunk in stream_resp:
                 stream_resp_with_history += chunk.choices[0].text
                 qr_pair = [prompt, stream_resp_with_history]
                 total_history = old_history + [qr_pair]
                 yield '', total_history
 
     @classmethod
-    def generate_chat(cls, model_and_template, template_type, prompt: str,
-                      history, system, max_new_tokens, temperature, top_k,
-                      top_p, repetition_penalty):
+    def generate_chat(cls, model_and_template, template_type, prompt: str, history, system, max_new_tokens, temperature,
+                      top_k, top_p, repetition_penalty):
         if not model_and_template:
             gr.Warning(cls.locale('generate_alert', cls.lang)['value'])
             return '', None
         model, template = model_and_template
         if os.environ.get('MODELSCOPE_ENVIRONMENT') == 'studio':
             model.cuda()
         if not template_type.endswith('generation'):
-            old_history, history = limit_history_length(
-                template, prompt, history, int(max_new_tokens))
+            old_history, history = limit_history_length(template, prompt, history, int(max_new_tokens))
         else:
             old_history = []
             history = []
 
         generation_config = GenerationConfig(
             temperature=temperature,
             top_k=top_k,
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_infer/model.py` & `ms-swift-2.0.4/swift/ui/llm_infer/model.py`

 * *Files 7% similar despite different names*

```diff
@@ -11,16 +11,15 @@
 
 class Model(BaseUI):
 
     llm_train = 'llm_infer'
 
     sub_ui = [Generate]
 
-    is_inference = os.environ.get('USE_INFERENCE') == '1' or os.environ.get(
-        'MODELSCOPE_ENVIRONMENT') == 'studio'
+    is_inference = os.environ.get('USE_INFERENCE') == '1' or os.environ.get('MODELSCOPE_ENVIRONMENT') == 'studio'
 
     locale_dict = {
         'checkpoint': {
             'value': {
                 'zh': '',
                 'en': 'Trained model'
             }
@@ -90,27 +89,21 @@
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Row():
             model_type = gr.Dropdown(
                 elem_id='model_type',
-                choices=[base_tab.locale('checkpoint', cls.lang)['value']]
-                + ModelType.get_model_name_list() + cls.get_custom_name_list(),
+                choices=[base_tab.locale('checkpoint', cls.lang)['value']] + ModelType.get_model_name_list()
+                + cls.get_custom_name_list(),
                 value=base_tab.locale('checkpoint', cls.lang)['value'],
                 scale=20)
-            model_id_or_path = gr.Textbox(
-                elem_id='model_id_or_path',
-                lines=1,
-                scale=20,
-                interactive=True)
+            model_id_or_path = gr.Textbox(elem_id='model_id_or_path', lines=1, scale=20, interactive=True)
             template_type = gr.Dropdown(
-                elem_id='template_type',
-                choices=list(TEMPLATE_MAPPING.keys()) + ['AUTO'],
-                scale=20)
+                elem_id='template_type', choices=list(TEMPLATE_MAPPING.keys()) + ['AUTO'], scale=20)
             reset_btn = gr.Button(elem_id='reset', scale=2)
             model_state = gr.State({})
         with gr.Row():
             system = gr.Textbox(elem_id='system', lines=4, scale=20)
         Generate.build_ui(base_tab)
         with gr.Row():
             gr.Textbox(elem_id='more_params', lines=1, scale=20)
@@ -124,59 +117,44 @@
                     model_id_or_path = None
                 default_system = None
                 template = None
             else:
                 if model_state and choice in model_state:
                     model_id_or_path = model_state[choice]
                 else:
-                    model_id_or_path = MODEL_MAPPING[choice][
-                        'model_id_or_path']
-                default_system = getattr(
-                    TEMPLATE_MAPPING[MODEL_MAPPING[choice]['template']]
-                    ['template'], 'default_system', None)
+                    model_id_or_path = MODEL_MAPPING[choice]['model_id_or_path']
+                default_system = getattr(TEMPLATE_MAPPING[MODEL_MAPPING[choice]['template']]['template'],
+                                         'default_system', None)
                 template = MODEL_MAPPING[choice]['template']
             return model_id_or_path, default_system, template
 
-        def update_model_id_or_path(model_type, path, system, template_type,
-                                    model_state):
+        def update_model_id_or_path(model_type, path, system, template_type, model_state):
             if not path or not os.path.exists(path):
-                return system, template_type, model_state
+                return gr.update(), gr.update(), gr.update()
             local_path = os.path.join(path, 'sft_args.json')
             if not os.path.exists(local_path):
-                default_system = getattr(
-                    TEMPLATE_MAPPING[MODEL_MAPPING[model_type]['template']]
-                    ['template'], 'default_system', None)
+                default_system = getattr(TEMPLATE_MAPPING[MODEL_MAPPING[model_type]['template']]['template'],
+                                         'default_system', None)
                 template = MODEL_MAPPING[model_type]['template']
                 return default_system, template, model_state
 
             with open(local_path, 'r') as f:
                 sft_args = json.load(f)
             base_model_type = sft_args['model_type']
-            system = getattr(
-                TEMPLATE_MAPPING[MODEL_MAPPING[base_model_type]['template']]
-                ['template'], 'default_system', None)
+            system = getattr(TEMPLATE_MAPPING[MODEL_MAPPING[base_model_type]['template']]['template'], 'default_system',
+                             None)
             model_state[model_type] = path
-            return sft_args['system'] or system, sft_args[
-                'template_type'], model_state
+            return sft_args['system'] or system, sft_args['template_type'], model_state
 
         model_type.change(
-            update_input_model,
-            inputs=[model_type, model_state],
-            outputs=[model_id_or_path, system, template_type])
+            update_input_model, inputs=[model_type, model_state], outputs=[model_id_or_path, system, template_type])
 
         model_id_or_path.change(
             update_model_id_or_path,
-            inputs=[
-                model_type, model_id_or_path, system, template_type,
-                model_state
-            ],
+            inputs=[model_type, model_id_or_path, system, template_type, model_state],
             outputs=[system, template_type, model_state])
 
         def reset(model_type):
-            model_id_or_path, default_system, template = update_input_model(
-                model_type)
+            model_id_or_path, default_system, template = update_input_model(model_type)
             return model_id_or_path, default_system, template, {}
 
-        reset_btn.click(
-            reset,
-            inputs=[model_type],
-            outputs=[model_id_or_path, system, template_type, model_state])
+        reset_btn.click(reset, inputs=[model_type], outputs=[model_id_or_path, system, template_type, model_state])
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_infer/runtime.py` & `ms-swift-2.0.4/swift/ui/llm_infer/runtime.py`

 * *Files 2% similar despite different names*

```diff
@@ -55,16 +55,15 @@
         'log': {
             'label': {
                 'zh': '',
                 'en': 'Logging content'
             },
             'info': {
                 'zh': '""',
-                'en':
-                'Please press "Show log" if the log content is not updating'
+                'en': 'Please press "Show log" if the log content is not updating'
             }
         },
         'running_tasks': {
             'label': {
                 'zh': '',
                 'en': 'Running deployments'
             },
@@ -89,32 +88,28 @@
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Accordion(elem_id='runtime_tab', open=False, visible=True):
             with gr.Blocks():
                 with gr.Row():
                     gr.Dropdown(elem_id='running_tasks', scale=10)
-                    gr.Button(
-                        elem_id='refresh_tasks', scale=1, variant='primary')
+                    gr.Button(elem_id='refresh_tasks', scale=1, variant='primary')
                     gr.Button(elem_id='show_log', scale=1, variant='primary')
                     gr.Button(elem_id='stop_show_log', scale=1)
                     gr.Button(elem_id='kill_task', scale=1)
                 with gr.Row():
                     gr.Textbox(elem_id='log', lines=6, visible=False)
 
                 concurrency_limit = {}
                 if version.parse(gr.__version__) >= version.parse('4.0.0'):
                     concurrency_limit = {'concurrency_limit': 5}
-                cls.log_event = base_tab.element('show_log').click(
-                    Runtime.update_log, [], [cls.element('log')]).then(
-                        Runtime.wait, [base_tab.element('running_tasks')],
-                        [cls.element('log')], **concurrency_limit)
+                cls.log_event = base_tab.element('show_log').click(Runtime.update_log, [], [cls.element('log')]).then(
+                    Runtime.wait, [base_tab.element('running_tasks')], [cls.element('log')], **concurrency_limit)
 
-                base_tab.element('stop_show_log').click(
-                    lambda: None, cancels=cls.log_event)
+                base_tab.element('stop_show_log').click(lambda: None, cancels=cls.log_event)
 
                 base_tab.element('refresh_tasks').click(
                     Runtime.refresh_tasks,
                     [base_tab.element('running_tasks')],
                     [base_tab.element('running_tasks')],
                 )
 
@@ -126,16 +121,15 @@
     def wait(cls, task):
         if not task:
             return [None]
         _, args = cls.parse_info_from_cmdline(task)
         log_file = args['log_file']
         offset = 0
         latest_data = ''
-        lines = collections.deque(
-            maxlen=int(os.environ.get('MAX_LOG_LINES', 50)))
+        lines = collections.deque(maxlen=int(os.environ.get('MAX_LOG_LINES', 50)))
         try:
             with open(log_file, 'r') as input:
                 input.seek(offset)
                 fail_cnt = 0
                 while True:
                     try:
                         latest_data += input.read()
@@ -164,26 +158,21 @@
     def get_all_ports():
         process_name = 'swift'
         cmd_name = 'deploy'
         ports = set()
         for proc in psutil.process_iter():
             try:
                 cmdlines = proc.cmdline()
-            except (psutil.ZombieProcess, psutil.AccessDenied,
-                    psutil.NoSuchProcess):
+            except (psutil.ZombieProcess, psutil.AccessDenied, psutil.NoSuchProcess):
                 cmdlines = []
-            if any([process_name in cmdline
-                    for cmdline in cmdlines]) and any(  # noqa
-                        [cmd_name == cmdline for cmdline in cmdlines]):  # noqa
+            if any([process_name in cmdline for cmdline in cmdlines]) and any(  # noqa
+                [cmd_name == cmdline for cmdline in cmdlines]):  # noqa
                 try:
                     ports.add(
-                        int(
-                            Runtime.parse_info_from_cmdline(
-                                Runtime.construct_running_task(proc))[1].get(
-                                    'port', 8000)))
+                        int(Runtime.parse_info_from_cmdline(Runtime.construct_running_task(proc))[1].get('port', 8000)))
                 except IndexError:
                     pass
         return ports
 
     @staticmethod
     def refresh_tasks(running_task=None):
         log_file = running_task if not running_task or 'pid:' not in running_task else None
@@ -191,23 +180,20 @@
         negative_name = 'swift.exe'
         cmd_name = 'deploy'
         process = []
         selected = None
         for proc in psutil.process_iter():
             try:
                 cmdlines = proc.cmdline()
-            except (psutil.ZombieProcess, psutil.AccessDenied,
-                    psutil.NoSuchProcess):
+            except (psutil.ZombieProcess, psutil.AccessDenied, psutil.NoSuchProcess):
                 cmdlines = []
-            if any([
-                    process_name in cmdline for cmdline in cmdlines
-            ]) and not any([negative_name in cmdline
-                            for cmdline in cmdlines]) and any(  # noqa
-                                [cmd_name == cmdline
-                                 for cmdline in cmdlines]):  # noqa
+            if any([process_name in cmdline
+                    for cmdline in cmdlines]) and not any([negative_name in cmdline
+                                                           for cmdline in cmdlines]) and any(  # noqa
+                                                               [cmd_name == cmdline for cmdline in cmdlines]):  # noqa
                 process.append(Runtime.construct_running_task(proc))
                 if log_file is not None and any(  # noqa
                     [log_file == cmdline for cmdline in cmdlines]):  # noqa
                     selected = Runtime.construct_running_task(proc)
         if not selected:
             if running_task and running_task in process:
                 selected = running_task
@@ -216,16 +202,15 @@
         return gr.update(choices=process, value=selected)
 
     @staticmethod
     def construct_running_task(proc):
         pid = proc.pid
         ts = time.time()
         create_time = proc.create_time()
-        create_time_formatted = datetime.fromtimestamp(create_time).strftime(
-            '%Y-%m-%d, %H:%M')
+        create_time_formatted = datetime.fromtimestamp(create_time).strftime('%Y-%m-%d, %H:%M')
 
         def format_time(seconds):
             days = int(seconds // (24 * 3600))
             hours = int((seconds % (24 * 3600)) // 3600)
             minutes = int((seconds % 3600) // 60)
             seconds = int(seconds % 60)
 
@@ -273,39 +258,31 @@
 
     @staticmethod
     def task_changed(task, base_tab):
         if task:
             _, all_args = Runtime.parse_info_from_cmdline(task)
         else:
             all_args = {}
-        elements = [
-            value for value in base_tab.elements().values()
-            if not isinstance(value, (Tab, Accordion))
-        ]
+        elements = [value for value in base_tab.elements().values() if not isinstance(value, (Tab, Accordion))]
         ret = []
         is_custom_path = 'ckpt_dir' in all_args
         for e in elements:
             if e.elem_id in all_args:
                 if isinstance(e, gr.Dropdown) and e.multiselect:
                     arg = all_args[e.elem_id].split(' ')
                 else:
                     if e.elem_id == 'model_type':
                         if is_custom_path:
-                            arg = base_tab.locale('checkpoint',
-                                                  base_tab.lang)['value']
+                            arg = base_tab.locale('checkpoint', base_tab.lang)['value']
                         else:
                             arg = all_args[e.elem_id]
                     elif e.elem_id == 'model_id_or_path':
                         if is_custom_path:
                             arg = all_args['ckpt_dir']
                         else:
                             arg = all_args['model_id_or_path']
                     else:
                         arg = all_args[e.elem_id]
                 ret.append(gr.update(value=arg))
             else:
                 ret.append(gr.update())
-        return ret + [
-            gr.update(value=None),
-            [all_args.get('model_type'),
-             all_args.get('template_type')]
-        ]
+        return ret + [gr.update(value=None), [all_args.get('model_type'), all_args.get('template_type')]]
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/advanced.py` & `ms-swift-2.0.4/swift/ui/llm_train/advanced.py`

 * *Files 3% similar despite different names*

```diff
@@ -83,15 +83,10 @@
         with gr.Accordion(elem_id='advanced_param', open=False):
             with gr.Blocks():
                 with gr.Row():
                     gr.Textbox(elem_id='optim', lines=1, scale=20)
                     gr.Textbox(elem_id='weight_decay', lines=1, scale=20)
                     gr.Textbox(elem_id='logging_steps', lines=1, scale=20)
                     gr.Textbox(elem_id='lr_scheduler_type', lines=1, scale=20)
-                    gr.Slider(
-                        elem_id='warmup_ratio',
-                        minimum=0.0,
-                        maximum=1.0,
-                        step=0.05,
-                        scale=20)
+                    gr.Slider(elem_id='warmup_ratio', minimum=0.0, maximum=1.0, step=0.05, scale=20)
                 with gr.Row():
                     gr.Textbox(elem_id='more_params', lines=4, scale=20)
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/dataset.py` & `ms-swift-2.0.4/swift/ui/llm_train/dataset.py`

 * *Files 16% similar despite different names*

```diff
@@ -92,32 +92,16 @@
             }
         }
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Row():
-            gr.Dropdown(
-                elem_id='dataset',
-                multiselect=True,
-                choices=list(DATASET_MAPPING.keys()),
-                scale=20)
-            gr.Textbox(
-                elem_id='custom_train_dataset_path', is_list=True, scale=20)
-            gr.Textbox(
-                elem_id='custom_val_dataset_path', is_list=True, scale=20)
+            gr.Dropdown(elem_id='dataset', multiselect=True, choices=list(DATASET_MAPPING.keys()), scale=20)
+            gr.Textbox(elem_id='custom_train_dataset_path', is_list=True, scale=20)
+            gr.Textbox(elem_id='custom_val_dataset_path', is_list=True, scale=20)
         with gr.Row():
-            gr.Slider(
-                elem_id='dataset_test_ratio',
-                minimum=0.0,
-                maximum=1.0,
-                step=0.05,
-                scale=20)
-            gr.Slider(
-                elem_id='max_length',
-                minimum=32,
-                maximum=8192,
-                step=32,
-                scale=20)
+            gr.Slider(elem_id='dataset_test_ratio', minimum=0.0, maximum=1.0, step=0.05, scale=20)
+            gr.Slider(elem_id='max_length', minimum=32, maximum=8192, step=32, scale=20)
             gr.Textbox(elem_id='train_dataset_sample', scale=20)
             gr.Textbox(elem_id='val_dataset_sample', scale=20)
             gr.Dropdown(elem_id='truncation_strategy', scale=20)
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/hyper.py` & `ms-swift-2.0.4/swift/ui/llm_train/hyper.py`

 * *Files 3% similar despite different names*

```diff
@@ -68,18 +68,16 @@
         },
         'max_steps': {
             'label': {
                 'zh': '',
                 'en': 'Max steps',
             },
             'info': {
-                'zh':
-                '',
-                'en':
-                'Set the max steps, if the value > 0 then num_train_epochs has no effects',
+                'zh': '',
+                'en': 'Set the max steps, if the value > 0 then num_train_epochs has no effects',
             }
         },
         'gradient_accumulation_steps': {
             'label': {
                 'zh': '',
                 'en': 'Gradient accumulation steps',
             },
@@ -121,50 +119,27 @@
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Accordion(elem_id='hyper_param', open=True):
             with gr.Blocks():
                 with gr.Row():
-                    gr.Slider(
-                        elem_id='batch_size',
-                        minimum=1,
-                        maximum=256,
-                        step=2,
-                        scale=20)
-                    learning_rate = gr.Textbox(
-                        elem_id='learning_rate',
-                        value='1e-4',
-                        lines=1,
-                        scale=20)
+                    gr.Slider(elem_id='batch_size', minimum=1, maximum=256, step=2, scale=20)
+                    learning_rate = gr.Textbox(elem_id='learning_rate', value='1e-4', lines=1, scale=20)
                     gr.Textbox(elem_id='num_train_epochs', lines=1, scale=20)
                     gr.Textbox(elem_id='max_steps', lines=1, scale=20)
-                    gr.Slider(
-                        elem_id='gradient_accumulation_steps',
-                        minimum=1,
-                        maximum=256,
-                        step=2,
-                        value=16,
-                        scale=20)
+                    gr.Slider(elem_id='gradient_accumulation_steps', minimum=1, maximum=256, step=2, value=16, scale=20)
                 with gr.Row():
-                    gr.Slider(
-                        elem_id='eval_batch_size',
-                        minimum=1,
-                        maximum=256,
-                        step=2,
-                        scale=20)
-                    gr.Textbox(
-                        elem_id='eval_steps', lines=1, value='500', scale=20)
+                    gr.Slider(elem_id='eval_batch_size', minimum=1, maximum=256, step=2, scale=20)
+                    gr.Textbox(elem_id='eval_steps', lines=1, value='500', scale=20)
                     gr.Textbox(elem_id='max_grad_norm', lines=1, scale=20)
                     gr.Checkbox(elem_id='predict_with_generate', scale=20)
                     gr.Checkbox(elem_id='use_flash_attn', scale=20)
 
             def update_lr(sft_type):
                 if sft_type == 'full':
                     return 1e-5
                 else:
                     return 1e-4
 
             base_tab.element('sft_type').change(
-                update_lr,
-                inputs=[base_tab.element('sft_type')],
-                outputs=[learning_rate])
+                update_lr, inputs=[base_tab.element('sft_type')], outputs=[learning_rate])
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/llm_train.py` & `ms-swift-2.0.4/swift/ui/llm_train/llm_train.py`

 * *Files 4% similar despite different names*

```diff
@@ -94,18 +94,16 @@
         },
         'gpu_memory_fraction': {
             'label': {
                 'zh': 'GPU',
                 'en': 'GPU memory fraction'
             },
             'info': {
-                'zh':
-                '',
-                'en':
-                'Set the memory fraction ratio of GPU, usually used in memory test'
+                'zh': '',
+                'en': 'Set the memory fraction ratio of GPU, usually used in memory test'
             }
         },
         'sft_type': {
             'label': {
                 'zh': '',
                 'en': 'Train type'
             },
@@ -156,30 +154,26 @@
         },
         'neftune_noise_alpha': {
             'label': {
                 'zh': 'neftune_noise_alpha',
                 'en': 'neftune_noise_alpha'
             },
             'info': {
-                'zh':
-                'neftune, 510',
-                'en':
-                'Use neftune to improve performance, normally the value should be 5 or 10'
+                'zh': 'neftune, 510',
+                'en': 'Use neftune to improve performance, normally the value should be 5 or 10'
             }
         },
         'use_galore': {
             'label': {
                 'zh': 'GaLore',
                 'en': 'Use GaLore'
             },
             'info': {
-                'zh':
-                'Galore',
-                'en':
-                'Use Galore to reduce GPU memory usage in full parameter training'
+                'zh': 'Galore',
+                'en': 'Use Galore to reduce GPU memory usage in full parameter training'
             }
         },
         'galore_rank': {
             'label': {
                 'zh': 'Galore',
                 'en': 'The rank of Galore'
             },
@@ -216,127 +210,94 @@
                 Runtime.build_ui(base_tab)
                 with gr.Row():
                     gr.Dropdown(elem_id='sft_type', scale=4)
                     gr.Textbox(elem_id='seed', scale=4)
                     gr.Dropdown(elem_id='dtype', scale=4)
                     gr.Checkbox(elem_id='use_ddp', value=False, scale=4)
                     gr.Textbox(elem_id='ddp_num', value='2', scale=4)
-                    gr.Slider(
-                        elem_id='neftune_noise_alpha',
-                        minimum=0.0,
-                        maximum=20.0,
-                        step=0.5,
-                        scale=4)
+                    gr.Slider(elem_id='neftune_noise_alpha', minimum=0.0, maximum=20.0, step=0.5, scale=4)
                 with gr.Row():
                     gr.Checkbox(elem_id='use_galore', scale=4)
-                    gr.Slider(
-                        elem_id='galore_rank',
-                        minimum=8,
-                        maximum=256,
-                        step=8,
-                        scale=4)
-                    gr.Slider(
-                        elem_id='galore_update_proj_gap',
-                        minimum=10,
-                        maximum=1000,
-                        step=50,
-                        scale=4)
+                    gr.Slider(elem_id='galore_rank', minimum=8, maximum=256, step=8, scale=4)
+                    gr.Slider(elem_id='galore_update_proj_gap', minimum=10, maximum=1000, step=50, scale=4)
                     gr.Checkbox(elem_id='galore_optim_per_parameter', scale=4)
                 with gr.Row():
                     gr.Dropdown(
                         elem_id='gpu_id',
                         multiselect=True,
                         choices=[str(i) for i in range(gpu_count)] + ['cpu'],
                         value=default_device,
                         scale=8)
                     gr.Textbox(elem_id='gpu_memory_fraction', scale=4)
                     gr.Checkbox(elem_id='dry_run', value=False, scale=4)
-                    submit = gr.Button(
-                        elem_id='submit', scale=4, variant='primary')
+                    submit = gr.Button(elem_id='submit', scale=4, variant='primary')
 
                 Save.build_ui(base_tab)
                 LoRA.build_ui(base_tab)
                 Hyper.build_ui(base_tab)
                 Quantization.build_ui(base_tab)
                 SelfCog.build_ui(base_tab)
                 Advanced.build_ui(base_tab)
                 if os.environ.get('MODELSCOPE_ENVIRONMENT') == 'studio':
                     submit.click(
                         cls.update_runtime, [],
-                        [cls.element('runtime_tab'),
-                         cls.element('log')]).then(
-                             cls.train_studio, [
-                                 value for value in cls.elements().values()
-                                 if not isinstance(value, (Tab, Accordion))
-                             ], [cls.element('log')],
-                             queue=True)
+                        [cls.element('runtime_tab'), cls.element('log')]).then(
+                            cls.train_studio,
+                            [value for value in cls.elements().values() if not isinstance(value, (Tab, Accordion))],
+                            [cls.element('log')],
+                            queue=True)
                 else:
                     submit.click(
-                        cls.train_local, [
-                            value for value in cls.elements().values()
-                            if not isinstance(value, (Tab, Accordion))
-                        ], [
+                        cls.train_local,
+                        [value for value in cls.elements().values() if not isinstance(value, (Tab, Accordion))], [
                             cls.element('running_cmd'),
                             cls.element('logging_dir'),
                             cls.element('runtime_tab'),
                             cls.element('running_tasks'),
                         ],
                         queue=True)
                 base_tab.element('running_tasks').change(
-                    partial(Runtime.task_changed, base_tab=base_tab),
-                    [base_tab.element('running_tasks')],
-                    [
-                        value for value in base_tab.elements().values()
-                        if not isinstance(value, (Tab, Accordion))
-                    ] + [cls.element('log')] + Runtime.all_plots,
+                    partial(Runtime.task_changed, base_tab=base_tab), [base_tab.element('running_tasks')],
+                    [value for value in base_tab.elements().values() if not isinstance(value, (Tab, Accordion))]
+                    + [cls.element('log')] + Runtime.all_plots,
                     cancels=Runtime.log_event)
                 Runtime.element('kill_task').click(
                     Runtime.kill_task,
                     [Runtime.element('running_tasks')],
-                    [Runtime.element('running_tasks')]
-                    + [Runtime.element('log')] + Runtime.all_plots,
+                    [Runtime.element('running_tasks')] + [Runtime.element('log')] + Runtime.all_plots,
                     cancels=[Runtime.log_event],
-                ).then(Runtime.reset, [], [Runtime.element('logging_dir')]
-                       + [Save.element('output_dir')])
+                ).then(Runtime.reset, [], [Runtime.element('logging_dir')] + [Save.element('output_dir')])
 
     @classmethod
     def update_runtime(cls):
         return gr.update(open=True), gr.update(visible=True)
 
     @classmethod
     def train(cls, *args):
         ignore_elements = ('model_type', 'logging_dir', 'more_params')
         sft_args = cls.get_default_value_from_dataclass(SftArguments)
         kwargs = {}
         kwargs_is_list = {}
         other_kwargs = {}
         more_params = {}
-        keys = [
-            key for key, value in cls.elements().items()
-            if not isinstance(value, (Tab, Accordion))
-        ]
+        keys = [key for key, value in cls.elements().items() if not isinstance(value, (Tab, Accordion))]
         model_type = None
         for key, value in zip(keys, args):
             compare_value = sft_args.get(key)
-            compare_value_arg = str(compare_value) if not isinstance(
-                compare_value, (list, dict)) else compare_value
-            compare_value_ui = str(value) if not isinstance(
-                value, (list, dict)) else value
+            compare_value_arg = str(compare_value) if not isinstance(compare_value, (list, dict)) else compare_value
+            compare_value_ui = str(value) if not isinstance(value, (list, dict)) else value
 
             if isinstance(value, str) and re.fullmatch(cls.int_regex, value):
                 value = int(value)
-            elif isinstance(value, str) and re.fullmatch(
-                    cls.float_regex, value):
+            elif isinstance(value, str) and re.fullmatch(cls.float_regex, value):
                 value = float(value)
 
             if key not in ignore_elements and key in sft_args and compare_value_ui != compare_value_arg and value:
-                kwargs[key] = value if not isinstance(
-                    value, list) else ' '.join(value)
-                kwargs_is_list[key] = isinstance(value, list) or getattr(
-                    cls.element(key), 'is_list', False)
+                kwargs[key] = value if not isinstance(value, list) else ' '.join(value)
+                kwargs_is_list[key] = isinstance(value, list) or getattr(cls.element(key), 'is_list', False)
             else:
                 other_kwargs[key] = value
             if key == 'more_params' and value:
                 more_params = json.loads(value)
 
             if key == 'model_type':
                 model_type = value
@@ -346,16 +307,15 @@
 
         kwargs.update(more_params)
         if 'dataset' not in kwargs and 'custom_train_dataset_path' not in kwargs:
             raise gr.Error(cls.locale('dataset_alert', cls.lang)['value'])
 
         sft_args = SftArguments(
             **{
-                key: value.split(' ') if kwargs_is_list.get(key, False)
-                and isinstance(value, str) else value
+                key: value.split(' ') if kwargs_is_list.get(key, False) and isinstance(value, str) else value
                 for key, value in kwargs.items()
             })
         params = ''
 
         for e in kwargs:
             if e in kwargs_is_list and kwargs_is_list[e]:
                 params += f'--{e} {kwargs[e]} '
@@ -389,27 +349,24 @@
         logger.info(f'Run training: {run_command}')
         return run_command, sft_args, other_kwargs
 
     @classmethod
     def train_studio(cls, *args):
         run_command, sft_args, other_kwargs = cls.train(*args)
         if os.environ.get('MODELSCOPE_ENVIRONMENT') == 'studio':
-            lines = collections.deque(
-                maxlen=int(os.environ.get('MAX_LOG_LINES', 50)))
-            process = Popen(
-                run_command, shell=True, stdout=PIPE, stderr=STDOUT)
+            lines = collections.deque(maxlen=int(os.environ.get('MAX_LOG_LINES', 50)))
+            process = Popen(run_command, shell=True, stdout=PIPE, stderr=STDOUT)
             with process.stdout:
                 for line in iter(process.stdout.readline, b''):
                     line = line.decode('utf-8')
                     lines.append(line)
                     yield '\n'.join(lines)
 
     @classmethod
     def train_local(cls, *args):
         run_command, sft_args, other_kwargs = cls.train(*args)
         if not other_kwargs['dry_run']:
             os.makedirs(sft_args.logging_dir, exist_ok=True)
             os.system(run_command)
             time.sleep(1)  # to make sure the log file has been created.
             gr.Info(cls.locale('submit_alert', cls.lang)['value'])
-        return run_command, sft_args.logging_dir, gr.update(
-            open=True), Runtime.refresh_tasks(sft_args.output_dir)
+        return run_command, sft_args.logging_dir, gr.update(open=True), Runtime.refresh_tasks(sft_args.output_dir)
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/lora.py` & `ms-swift-2.0.4/swift/ui/llm_train/lora.py`

 * *Files 15% similar despite different names*

```diff
@@ -19,18 +19,16 @@
         },
         'lora_target_modules': {
             'label': {
                 'zh': 'LoRA',
                 'en': 'LoRA target modules'
             },
             'info': {
-                'zh':
-                'LoRALinearALL',
-                'en':
-                'Set the LoRA target modules, fill in ALL if train all Linears'
+                'zh': 'LoRALinearALL',
+                'en': 'Set the LoRA target modules, fill in ALL if train all Linears'
             }
         },
         'lora_rank': {
             'label': {
                 'zh': 'LoRA',
                 'en': 'The LoRA rank'
             }
@@ -55,14 +53,20 @@
         },
         'use_dora': {
             'label': {
                 'zh': 'dora',
                 'en': 'Use dora'
             }
         },
+        'lora_dtype': {
+            'label': {
+                'zh': 'lora',
+                'en': 'The dtype of lora parameters'
+            }
+        },
         'lora_lr_ratio': {
             'label': {
                 'zh': 'Lora+',
                 'en': 'The lr ratio of Lora+'
             },
             'info': {
                 'zh': '16.0',
@@ -72,41 +76,24 @@
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Accordion(elem_id='lora_tab', open=True):
             with gr.Blocks():
                 with gr.Row():
-                    lora_target_modules = gr.Textbox(
-                        elem_id='lora_target_modules',
-                        lines=1,
-                        scale=20,
-                        is_list=True)
+                    lora_target_modules = gr.Textbox(elem_id='lora_target_modules', lines=1, scale=20, is_list=True)
                 with gr.Row():
-                    gr.Slider(
-                        elem_id='lora_rank',
-                        value=32,
-                        minimum=1,
-                        maximum=512,
-                        step=8)
-                    gr.Slider(
-                        elem_id='lora_alpha',
-                        value=8,
-                        minimum=1,
-                        maximum=512,
-                        step=8)
+                    gr.Slider(elem_id='lora_rank', value=32, minimum=1, maximum=512, step=8)
+                    gr.Slider(elem_id='lora_alpha', value=8, minimum=1, maximum=512, step=8)
                     gr.Dropdown(elem_id='lora_dtype')
                     gr.Textbox(elem_id='lora_lr_ratio')
                     gr.Checkbox(elem_id='use_rslora')
                     gr.Checkbox(elem_id='use_dora')
                     gr.Textbox(elem_id='lora_dropout_p')
 
             def update_lora(choice):
                 if choice is not None:
-                    return ' '.join(
-                        MODEL_MAPPING[choice]['lora_target_modules'])
+                    return ' '.join(MODEL_MAPPING[choice]['lora_target_modules'])
                 return None
 
             base_tab.element('model_type').change(
-                update_lora,
-                inputs=[base_tab.element('model_type')],
-                outputs=[lora_target_modules])
+                update_lora, inputs=[base_tab.element('model_type')], outputs=[lora_target_modules])
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/model.py` & `ms-swift-2.0.4/swift/ui/llm_train/model.py`

 * *Files 17% similar despite different names*

```diff
@@ -58,63 +58,45 @@
         },
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Row():
             model_type = gr.Dropdown(
-                elem_id='model_type',
-                choices=ModelType.get_model_name_list()
-                + cls.get_custom_name_list(),
-                scale=20)
-            model_id_or_path = gr.Textbox(
-                elem_id='model_id_or_path',
-                lines=1,
-                scale=20,
-                interactive=True)
+                elem_id='model_type', choices=ModelType.get_model_name_list() + cls.get_custom_name_list(), scale=20)
+            model_id_or_path = gr.Textbox(elem_id='model_id_or_path', lines=1, scale=20, interactive=True)
             template_type = gr.Dropdown(
-                elem_id='template_type',
-                choices=list(TEMPLATE_MAPPING.keys()) + ['AUTO'],
-                scale=20)
+                elem_id='template_type', choices=list(TEMPLATE_MAPPING.keys()) + ['AUTO'], scale=20)
             reset_btn = gr.Button(elem_id='reset', scale=2)
             model_state = gr.State({})
         with gr.Row():
             system = gr.Textbox(elem_id='system', lines=1, scale=20)
 
         def update_input_model(choice, model_state=None):
             if choice is None:
                 return None, None, None
             if model_state and choice in model_state:
                 model_id_or_path = model_state[choice]
             else:
                 model_id_or_path = MODEL_MAPPING[choice]['model_id_or_path']
-            default_system = getattr(
-                TEMPLATE_MAPPING[MODEL_MAPPING[choice]['template']]
-                ['template'], 'default_system', None)
+            default_system = getattr(TEMPLATE_MAPPING[MODEL_MAPPING[choice]['template']]['template'], 'default_system',
+                                     None)
             template = MODEL_MAPPING[choice]['template']
             return model_id_or_path, default_system, template
 
         def update_model_id_or_path(model_type, model_id_or_path, model_state):
             if model_type is None or isinstance(model_type, list):
                 return model_state
             model_state[model_type] = model_id_or_path
             return model_state
 
         def reset(model_type):
-            model_id_or_path, default_system, template = update_input_model(
-                model_type)
+            model_id_or_path, default_system, template = update_input_model(model_type)
             return model_id_or_path, default_system, template, {}
 
         model_type.change(
-            update_input_model,
-            inputs=[model_type, model_state],
-            outputs=[model_id_or_path, system, template_type])
+            update_input_model, inputs=[model_type, model_state], outputs=[model_id_or_path, system, template_type])
 
         model_id_or_path.change(
-            update_model_id_or_path,
-            inputs=[model_type, model_id_or_path, model_state],
-            outputs=[model_state])
-
-        reset_btn.click(
-            reset,
-            inputs=[model_type],
-            outputs=[model_id_or_path, system, template_type, model_state])
+            update_model_id_or_path, inputs=[model_type, model_id_or_path, model_state], outputs=[model_state])
+
+        reset_btn.click(reset, inputs=[model_type], outputs=[model_id_or_path, system, template_type, model_state])
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/quantization.py` & `ms-swift-2.0.4/swift/ui/llm_train/quantization.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/runtime.py` & `ms-swift-2.0.4/swift/ui/llm_train/runtime.py`

 * *Files 2% similar despite different names*

```diff
@@ -11,16 +11,15 @@
 import matplotlib.pyplot as plt
 import psutil
 from gradio import Accordion, Tab
 from transformers import is_tensorboard_available
 
 from swift.ui.base import BaseUI
 from swift.ui.llm_train.utils import close_loop, run_command_in_subprocess
-from swift.utils import (TB_COLOR, TB_COLOR_SMOOTH, get_logger,
-                         read_tensorboard_file, tensorboard_smoothing)
+from swift.utils import TB_COLOR, TB_COLOR_SMOOTH, get_logger, read_tensorboard_file, tensorboard_smoothing
 
 logger = get_logger()
 
 
 class Runtime(BaseUI):
 
     handlers: Dict[str, Tuple[List, Tuple]] = {}
@@ -86,18 +85,16 @@
             'label': {
                 'zh': '',
                 'en': 'Runtime'
             },
         },
         'tb_not_found': {
             'value': {
-                'zh':
-                'tensorboard,pip install tensorboard',
-                'en':
-                'tensorboard not found, install it by pip install tensorboard',
+                'zh': 'tensorboard,pip install tensorboard',
+                'en': 'tensorboard not found, install it by pip install tensorboard',
             }
         },
         'running_cmd': {
             'label': {
                 'zh': '',
                 'en': 'Command line'
             },
@@ -131,16 +128,15 @@
         'log': {
             'label': {
                 'zh': '',
                 'en': 'Logging content'
             },
             'info': {
                 'zh': '""',
-                'en':
-                'Please press "Show log" if the log content is not updating'
+                'en': 'Please press "Show log" if the log content is not updating'
             }
         },
         'running_tasks': {
             'label': {
                 'zh': '',
                 'en': 'Running Tasks'
             },
@@ -186,30 +182,19 @@
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Accordion(elem_id='runtime_tab', open=False, visible=True):
             with gr.Blocks():
                 with gr.Row():
-                    gr.Textbox(
-                        elem_id='running_cmd',
-                        lines=1,
-                        scale=20,
-                        interactive=False,
-                        max_lines=1)
-                    gr.Textbox(
-                        elem_id='logging_dir', lines=1, scale=20, max_lines=1)
+                    gr.Textbox(elem_id='running_cmd', lines=1, scale=20, interactive=False, max_lines=1)
+                    gr.Textbox(elem_id='logging_dir', lines=1, scale=20, max_lines=1)
                     gr.Button(elem_id='show_log', scale=2, variant='primary')
                     gr.Button(elem_id='stop_show_log', scale=2)
-                    gr.Textbox(
-                        elem_id='tb_url',
-                        lines=1,
-                        scale=10,
-                        interactive=False,
-                        max_lines=1)
+                    gr.Textbox(elem_id='tb_url', lines=1, scale=10, interactive=False, max_lines=1)
                     gr.Button(elem_id='start_tb', scale=2, variant='primary')
                     gr.Button(elem_id='close_tb', scale=2)
                 with gr.Row():
                     gr.Textbox(elem_id='log', lines=6, visible=False)
                 with gr.Row():
                     gr.Dropdown(elem_id='running_tasks', scale=10)
                     gr.Button(elem_id='refresh_tasks', scale=1)
@@ -218,23 +203,19 @@
                 with gr.Row():
                     cls.all_plots = []
                     for k in Runtime.sft_plot:
                         name = k['name']
                         cls.all_plots.append(gr.Plot(elem_id=name, label=name))
 
                 cls.log_event = base_tab.element('show_log').click(
-                    Runtime.update_log, [],
-                    [cls.element('log')] + cls.all_plots).then(
-                        Runtime.wait, [
-                            base_tab.element('logging_dir'),
-                            base_tab.element('running_tasks')
-                        ], [cls.element('log')] + cls.all_plots)
+                    Runtime.update_log, [], [cls.element('log')] + cls.all_plots).then(
+                        Runtime.wait, [base_tab.element('logging_dir'),
+                                       base_tab.element('running_tasks')], [cls.element('log')] + cls.all_plots)
 
-                base_tab.element('stop_show_log').click(
-                    lambda: None, cancels=cls.log_event)
+                base_tab.element('stop_show_log').click(lambda: None, cancels=cls.log_event)
 
                 base_tab.element('start_tb').click(
                     Runtime.start_tb,
                     [base_tab.element('logging_dir')],
                     [base_tab.element('tb_url')],
                 )
 
@@ -257,16 +238,15 @@
     @classmethod
     def wait(cls, logging_dir, task):
         if not logging_dir:
             return [None] + Runtime.plot(task)
         log_file = os.path.join(logging_dir, 'run.log')
         offset = 0
         latest_data = ''
-        lines = collections.deque(
-            maxlen=int(os.environ.get('MAX_LOG_LINES', 50)))
+        lines = collections.deque(maxlen=int(os.environ.get('MAX_LOG_LINES', 50)))
         try:
             with open(log_file, 'r') as input:
                 input.seek(offset)
                 fail_cnt = 0
                 while True:
                     try:
                         latest_data += input.read()
@@ -289,31 +269,28 @@
                     lines.extend(latest_lines)
                     yield ['\n'.join(lines)] + Runtime.plot(task)
         except IOError:
             pass
 
     @classmethod
     def show_log(cls, logging_dir):
-        webbrowser.open(
-            'file://' + os.path.join(logging_dir, 'run.log'), new=2)
+        webbrowser.open('file://' + os.path.join(logging_dir, 'run.log'), new=2)
 
     @classmethod
     def start_tb(cls, logging_dir):
         if not is_tensorboard_available():
             gr.Error(cls.locale('tb_not_found', cls.lang)['value'])
             return ''
 
         logging_dir = logging_dir.strip()
-        logging_dir = logging_dir if not logging_dir.endswith(
-            os.sep) else logging_dir[:-1]
+        logging_dir = logging_dir if not logging_dir.endswith(os.sep) else logging_dir[:-1]
         if logging_dir in cls.handlers:
             return cls.handlers[logging_dir][1]
 
-        handler, lines = run_command_in_subprocess(
-            'tensorboard', '--logdir', logging_dir, timeout=2)
+        handler, lines = run_command_in_subprocess('tensorboard', '--logdir', logging_dir, timeout=2)
         localhost_addr = ''
         for line in lines:
             if 'http://localhost:' in line:
                 line = line[line.index('http://localhost:'):]
                 localhost_addr = line[:line.index(' ')]
         cls.handlers[logging_dir] = (handler, localhost_addr)
         logger.info('===========Tensorboard Log============')
@@ -334,23 +311,20 @@
         negative_name = 'swift.exe'
         cmd_name = 'sft'
         process = []
         selected = None
         for proc in psutil.process_iter():
             try:
                 cmdlines = proc.cmdline()
-            except (psutil.ZombieProcess, psutil.AccessDenied,
-                    psutil.NoSuchProcess):
+            except (psutil.ZombieProcess, psutil.AccessDenied, psutil.NoSuchProcess):
                 cmdlines = []
-            if any([
-                    process_name in cmdline for cmdline in cmdlines
-            ]) and not any([negative_name in cmdline
-                            for cmdline in cmdlines]) and any(  # noqa
-                                [cmd_name == cmdline
-                                 for cmdline in cmdlines]):  # noqa
+            if any([process_name in cmdline
+                    for cmdline in cmdlines]) and not any([negative_name in cmdline
+                                                           for cmdline in cmdlines]) and any(  # noqa
+                                                               [cmd_name == cmdline for cmdline in cmdlines]):  # noqa
                 process.append(Runtime.construct_running_task(proc))
                 if output_dir is not None and any(  # noqa
                     [output_dir == cmdline for cmdline in cmdlines]):  # noqa
                     selected = Runtime.construct_running_task(proc)
         if not selected:
             if running_task and running_task in process:
                 selected = running_task
@@ -359,16 +333,15 @@
         return gr.update(choices=process, value=selected)
 
     @staticmethod
     def construct_running_task(proc):
         pid = proc.pid
         ts = time.time()
         create_time = proc.create_time()
-        create_time_formatted = datetime.fromtimestamp(create_time).strftime(
-            '%Y-%m-%d, %H:%M')
+        create_time_formatted = datetime.fromtimestamp(create_time).strftime('%Y-%m-%d, %H:%M')
 
         def format_time(seconds):
             days = int(seconds // (24 * 3600))
             hours = int((seconds % (24 * 3600)) // 3600)
             minutes = int((seconds % 3600) // 60)
             seconds = int(seconds % 60)
 
@@ -406,46 +379,40 @@
         if os.path.exists(os.path.join(output_dir, 'sft_args.json')):
             with open(os.path.join(output_dir, 'sft_args.json'), 'r') as f:
                 _json = json.load(f)
             for key in all_args.keys():
                 all_args[key] = _json[key]
                 if isinstance(all_args[key], list):
                     if any([' ' in value for value in all_args[key]]):
-                        all_args[key] = [
-                            f'"{value}"' for value in all_args[key]
-                        ]
+                        all_args[key] = [f'"{value}"' for value in all_args[key]]
                     all_args[key] = ' '.join(all_args[key])
         return pid, all_args
 
     @staticmethod
     def kill_task(task):
         pid, all_args = Runtime.parse_info_from_cmdline(task)
         output_dir = all_args['output_dir']
         if sys.platform == 'win32':
             os.system(f'taskkill /f /t /pid "{pid}"')
         else:
             os.system(f'pkill -9 -f {output_dir}')
         time.sleep(1)
-        return [Runtime.refresh_tasks()] + [gr.update(value=None)] * (
-            len(Runtime.sft_plot) + 1)
+        return [Runtime.refresh_tasks()] + [gr.update(value=None)] * (len(Runtime.sft_plot) + 1)
 
     @staticmethod
     def reset():
         return None, 'output'
 
     @staticmethod
     def task_changed(task, base_tab):
         if task:
             _, all_args = Runtime.parse_info_from_cmdline(task)
         else:
             all_args = {}
-        elements = [
-            value for value in base_tab.elements().values()
-            if not isinstance(value, (Tab, Accordion))
-        ]
+        elements = [value for value in base_tab.elements().values() if not isinstance(value, (Tab, Accordion))]
         ret = []
         for e in elements:
             if e.elem_id in all_args:
                 if isinstance(e, gr.Dropdown) and e.multiselect:
                     arg = all_args[e.elem_id].split(' ')
                 else:
                     arg = all_args[e.elem_id]
@@ -458,16 +425,15 @@
     def plot(task):
         if not task:
             return [None] * len(Runtime.sft_plot)
         _, all_args = Runtime.parse_info_from_cmdline(task)
         tb_dir = all_args['logging_dir']
         fname = [
             fname for fname in os.listdir(tb_dir)
-            if os.path.isfile(os.path.join(tb_dir, fname))
-            and fname.startswith('events.out')
+            if os.path.isfile(os.path.join(tb_dir, fname)) and fname.startswith('events.out')
         ]
         if fname:
             fname = fname[0]
         else:
             return [None] * len(Runtime.sft_plot)
         tb_path = os.path.join(tb_dir, fname)
         data = read_tensorboard_file(tb_path)
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/save.py` & `ms-swift-2.0.4/swift/ui/llm_train/save.py`

 * *Files 0% similar despite different names*

```diff
@@ -89,16 +89,15 @@
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Accordion(elem_id='save_param', open=True):
             with gr.Blocks():
                 with gr.Row():
-                    gr.Textbox(
-                        elem_id='save_steps', value='500', lines=1, scale=5)
+                    gr.Textbox(elem_id='save_steps', value='500', lines=1, scale=5)
                     gr.Textbox(elem_id='output_dir', scale=20)
                 with gr.Row():
                     gr.Checkbox(elem_id='push_to_hub', scale=20)
                     gr.Textbox(elem_id='hub_model_id', lines=1, scale=20)
                     gr.Checkbox(elem_id='hub_private_repo', scale=20)
                     gr.Dropdown(elem_id='push_hub_strategy', scale=20)
                     gr.Textbox(elem_id='hub_token', lines=1, scale=20)
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/self_cog.py` & `ms-swift-2.0.4/swift/ui/llm_train/self_cog.py`

 * *Files 10% similar despite different names*

```diff
@@ -28,30 +28,26 @@
         },
         'model_name': {
             'label': {
                 'zh': '',
                 'en': 'Model name'
             },
             'info': {
-                'zh':
-                ', : ,',
-                'en':
-                'Set the name of the model think itself of, the format is Chinesename Englishname, split by space'
+                'zh': ', : ,',
+                'en': 'Set the name of the model think itself of, the format is Chinesename Englishname, split by space'
             }
         },
         'model_author': {
             'label': {
                 'zh': '',
                 'en': 'Model author'
             },
             'info': {
-                'zh':
-                ', : ,',
-                'en':
-                'Set the author of the model, the format is Chineseauthor Englishauthor, split by space'
+                'zh': ', : ,',
+                'en': 'Set the author of the model, the format is Chineseauthor Englishauthor, split by space'
             }
         },
     }
 
     @classmethod
     def do_build_ui(cls, base_tab: Type['BaseUI']):
         with gr.Accordion(elem_id='self_cognition', open=False):
```

### Comparing `ms-swift-2.0.3.post1/swift/ui/llm_train/utils.py` & `ms-swift-2.0.4/swift/ui/llm_train/utils.py`

 * *Files 8% similar despite different names*

```diff
@@ -3,16 +3,15 @@
 from asyncio.subprocess import PIPE, STDOUT
 from dataclasses import fields
 
 from swift.llm import SftArguments
 
 
 async def run_and_get_log(*args, timeout=None):
-    process = await asyncio.create_subprocess_exec(
-        *args, stdout=PIPE, stderr=STDOUT)
+    process = await asyncio.create_subprocess_exec(*args, stdout=PIPE, stderr=STDOUT)
     lines = []
     while True:
         try:
             line = await asyncio.wait_for(process.stdout.readline(), timeout)
         except asyncio.TimeoutError:
             break
         else:
@@ -26,16 +25,15 @@
 def run_command_in_subprocess(*args, timeout):
     if sys.platform == 'win32':
         loop = asyncio.ProactorEventLoop()
         asyncio.set_event_loop(loop)
     else:
         loop = asyncio.new_event_loop()
         asyncio.set_event_loop(loop)
-    process, lines = loop.run_until_complete(
-        run_and_get_log(*args, timeout=timeout))
+    process, lines = loop.run_until_complete(run_and_get_log(*args, timeout=timeout))
     return (loop, process), lines
 
 
 def close_loop(handler):
     loop, process = handler
     process.kill()
     loop.close()
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/__init__.py` & `ms-swift-2.0.4/swift/utils/__init__.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,20 +1,15 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 from .hub import create_ms_repo, push_to_ms_hub
 from .io_utils import append_to_jsonl, read_from_jsonl, write_to_jsonl
 from .logger import get_logger
-from .metric import (compute_acc_metrics, compute_nlg_metrics,
-                     preprocess_logits_for_metrics)
+from .metric import compute_acc_metrics, compute_nlg_metrics, preprocess_logits_for_metrics
 from .np_utils import get_seed, stat_array, transform_jsonl_to_df
 from .run_utils import get_main
-from .tb_utils import (TB_COLOR, TB_COLOR_SMOOTH, plot_images,
-                       read_tensorboard_file, tensorboard_smoothing)
-from .torch_utils import (activate_model_parameters, broadcast_string,
-                          freeze_model_parameters, get_dist_setting,
-                          get_model_info, is_ddp_plus_mp, is_dist,
-                          is_local_master, is_master, is_mp, is_on_same_device,
+from .tb_utils import TB_COLOR, TB_COLOR_SMOOTH, plot_images, read_tensorboard_file, tensorboard_smoothing
+from .torch_utils import (activate_model_parameters, broadcast_string, freeze_model_parameters, get_dist_setting,
+                          get_model_info, is_ddp_plus_mp, is_dist, is_local_master, is_master, is_mp, is_on_same_device,
                           show_layers, time_synchronize, use_torchacc)
-from .utils import (add_version_to_work_dir, check_json_format,
-                    get_pai_tensorboard_dir, is_pai_training_job, lower_bound,
-                    parse_args, read_multi_line, safe_ddp_context,
-                    seed_everything, subprocess_run, test_time, upper_bound)
+from .utils import (add_version_to_work_dir, check_json_format, get_pai_tensorboard_dir, is_pai_training_job,
+                    lower_bound, parse_args, read_multi_line, safe_ddp_context, seed_everything, subprocess_run,
+                    test_time, upper_bound)
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/constants.py` & `ms-swift-2.0.4/swift/utils/constants.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/swift/utils/hub.py` & `ms-swift-2.0.4/swift/utils/hub.py`

 * *Files 1% similar despite different names*

```diff
@@ -11,32 +11,29 @@
 from swift.hub.constants import ModelVisibility
 from .logger import get_logger
 from .utils import subprocess_run
 
 logger = get_logger()
 
 
-def create_ms_repo(hub_model_id: str,
-                   hub_token: Optional[str] = None,
-                   hub_private_repo: bool = False) -> str:
+def create_ms_repo(hub_model_id: str, hub_token: Optional[str] = None, hub_private_repo: bool = False) -> str:
     assert hub_model_id is not None, 'Please enter a valid hub_model_id'
 
     api = HubApi()
     if hub_token is None:
         hub_token = os.environ.get('MODELSCOPE_API_TOKEN')
     if hub_token is not None:
         api.login(hub_token)
     visibility = ModelVisibility.PRIVATE if hub_private_repo else ModelVisibility.PUBLIC
 
     if '/' not in hub_model_id:
         user_name = ModelScopeConfig.get_user_info()[0]
         assert isinstance(user_name, str)
         hub_model_id = f'{user_name}/{hub_model_id}'
-        logger.info(
-            f"'/' not in hub_model_id, setting hub_model_id: {hub_model_id}")
+        logger.info(f"'/' not in hub_model_id, setting hub_model_id: {hub_model_id}")
     try:
         api.create_model(hub_model_id, visibility)
     except HTTPError:
         # The remote repository has been created
         pass
     return hub_model_id
 
@@ -44,33 +41,29 @@
 def push_to_ms_hub(ckpt_dir: str,
                    hub_model_id: str,
                    hub_token: Optional[str] = None,
                    hub_private_repo: bool = False,
                    commit_message: str = 'update files'):
     logger.info(f'Starting push to hub. ckpt_dir: {ckpt_dir}.')
     tmp_file_name = tempfile.TemporaryDirectory().name
-    subprocess_run(['git', 'lfs', 'env'],
-                   stdout=subprocess.PIPE)  # check git-lfs install
+    subprocess_run(['git', 'lfs', 'env'], stdout=subprocess.PIPE)  # check git-lfs install
 
     hub_model_id = create_ms_repo(hub_model_id, hub_token, hub_private_repo)
     git_token = ModelScopeConfig.get_token()
     ms_url = f'https://oauth2:{git_token}@www.modelscope.cn/{hub_model_id}.git'
-    subprocess_run(['git', '-C', ckpt_dir, 'clone', ms_url, tmp_file_name],
-                   env={'GIT_LFS_SKIP_SMUDGE': '1'})
+    subprocess_run(['git', '-C', ckpt_dir, 'clone', ms_url, tmp_file_name], env={'GIT_LFS_SKIP_SMUDGE': '1'})
     tmp_dir = os.path.join(ckpt_dir, tmp_file_name)
     subprocess_run(['git', '-C', tmp_dir, 'lfs', 'pull'])
     logger.info('Git clone the repo successfully.')
     # mv .git
     dst_git_path = os.path.join(ckpt_dir, '.git')
     if os.path.exists(dst_git_path):
         shutil.rmtree(dst_git_path)
     shutil.copytree(os.path.join(tmp_dir, '.git'), dst_git_path)
-    shutil.copy(
-        os.path.join(tmp_dir, '.gitattributes'),
-        os.path.join(ckpt_dir, '.gitattributes'))
+    shutil.copy(os.path.join(tmp_dir, '.gitattributes'), os.path.join(ckpt_dir, '.gitattributes'))
     shutil.rmtree(tmp_dir)
     # add commit push
     subprocess_run(['git', '-C', ckpt_dir, 'lfs', 'install'])
     time.sleep(0.5)
     logger.info('Start `git add .`')
     subprocess_run(['git', '-C', ckpt_dir, 'add', '.'])
     if is_repo_clean(ckpt_dir):
@@ -79,10 +72,9 @@
         subprocess_run(['git', '-C', ckpt_dir, 'commit', '-m', commit_message])
         subprocess_run(['git', '-C', ckpt_dir, 'push'])
         url = f'https://www.modelscope.cn/models/{hub_model_id}/summary'
         logger.info(f'Push to Modelscope successful. url: `{url}`.')
 
 
 def is_repo_clean(ckpt_dir: str) -> bool:
-    resp = subprocess_run(['git', '-C', ckpt_dir, 'status', '--porcelain'],
-                          stdout=subprocess.PIPE)
+    resp = subprocess_run(['git', '-C', ckpt_dir, 'status', '--porcelain'], stdout=subprocess.PIPE)
     return len(resp.stdout.strip()) == 0
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/import_utils.py` & `ms-swift-2.0.4/swift/utils/import_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -15,29 +15,23 @@
 class _LazyModule(ModuleType):
     """
     Module class that surfaces all objects but only performs associated imports when the objects are requested.
     """
 
     # Very heavily inspired by optuna.integration._IntegrationModule
     # https://github.com/optuna/optuna/blob/master/optuna/integration/__init__.py
-    def __init__(self,
-                 name,
-                 module_file,
-                 import_structure,
-                 module_spec=None,
-                 extra_objects=None):
+    def __init__(self, name, module_file, import_structure, module_spec=None, extra_objects=None):
         super().__init__(name)
         self._modules = set(import_structure.keys())
         self._class_to_module = {}
         for key, values in import_structure.items():
             for value in values:
                 self._class_to_module[value] = key
         # Needed for autocompletion in an IDE
-        self.__all__ = list(import_structure.keys()) + list(
-            chain(*import_structure.values()))
+        self.__all__ = list(import_structure.keys()) + list(chain(*import_structure.values()))
         self.__file__ = module_file
         self.__spec__ = module_spec
         self.__path__ = [os.path.dirname(module_file)]
         self._objects = {} if extra_objects is None else extra_objects
         self._name = name
         self._import_structure = import_structure
 
@@ -56,24 +50,22 @@
             return self._objects[name]
         if name in self._modules:
             value = self._get_module(name)
         elif name in self._class_to_module.keys():
             module = self._get_module(self._class_to_module[name])
             value = getattr(module, name)
         else:
-            raise AttributeError(
-                f'module {self.__name__} has no attribute {name}')
+            raise AttributeError(f'module {self.__name__} has no attribute {name}')
 
         setattr(self, name, value)
         return value
 
     def _get_module(self, module_name: str):
         try:
             return importlib.import_module('.' + module_name, self.__name__)
         except Exception as e:
             raise RuntimeError(
                 f'Failed to import {self.__name__}.{module_name} because of the following error (look up to see its'
                 f' traceback):\n{e}') from e
 
     def __reduce__(self):
-        return (self.__class__, (self._name, self.__file__,
-                                 self._import_structure))
+        return (self.__class__, (self._name, self.__file__, self._import_structure))
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/io_utils.py` & `ms-swift-2.0.4/swift/utils/io_utils.py`

 * *Files 11% similar despite different names*

```diff
@@ -13,17 +13,15 @@
     res: List[Any] = []
     with open(fpath, 'r', encoding=encoding) as f:
         for line in f:
             res.append(json.loads(line))
     return res
 
 
-def write_to_jsonl(fpath: str,
-                   obj_list: List[Any],
-                   encoding: str = 'utf-8') -> None:
+def write_to_jsonl(fpath: str, obj_list: List[Any], encoding: str = 'utf-8') -> None:
     res: List[str] = []
     for obj in obj_list:
         res.append(json.dumps(obj, ensure_ascii=False))
     with open(fpath, 'w', encoding=encoding) as f:
         text = '\n'.join(res)
         f.write(f'{text}\n')
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/logger.py` & `ms-swift-2.0.4/swift/utils/logger.py`

 * *Files 2% similar despite different names*

```diff
@@ -2,26 +2,23 @@
 import importlib.util
 import logging
 import os
 from typing import Optional
 
 init_loggers = {}
 
-formatter = logging.Formatter(
-    '%(asctime)s - %(name)s - %(levelname)s - %(message)s')
+formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
 
 
 def is_master():
     rank = int(os.getenv('RANK', -1))
     return rank in {-1, 0}
 
 
-def get_logger(log_file: Optional[str] = None,
-               log_level: int = logging.INFO,
-               file_mode: str = 'w'):
+def get_logger(log_file: Optional[str] = None, log_level: int = logging.INFO, file_mode: str = 'w'):
     """ Get logging logger
 
     Args:
         log_file: Log filename, if specified, file handler will be added to
             logger
         log_level: Logging level.
         file_mode: Specifies the mode to open the file, if filename is
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/metric.py` & `ms-swift-2.0.4/swift/utils/metric.py`

 * *Files 11% similar despite different names*

```diff
@@ -18,54 +18,44 @@
     preds, labels = prediction[0], prediction[1]
 
     score_dict = {'rouge-1': [], 'rouge-2': [], 'rouge-l': [], 'bleu-4': []}
 
     def _decode(tokens, ignore_pad_token_for_loss=False):
         if ignore_pad_token_for_loss:
             tokens = np.where(tokens != -100, tokens, tokenizer.pad_token_id)
-        tokens = np.where(tokens < tokenizer.vocab_size, tokens,
-                          tokenizer.pad_token_id)
-        return [
-            t
-            for t in tokenizer.batch_decode(tokens, skip_special_tokens=True)
-        ]
+        tokens = np.where(tokens < tokenizer.vocab_size, tokens, tokenizer.pad_token_id)
+        return [t for t in tokenizer.batch_decode(tokens, skip_special_tokens=True)]
 
     for pred, label in zip(preds, labels):
         pred = ''.join(_decode(pred, False))
         label = ''.join(_decode(label, True))
         hypothesis = list(jieba.cut(pred))
         if len(hypothesis) == 0 or ''.join(hypothesis) == '.':
             hypothesis = [tokenizer.decode(tokenizer.eos_token_id)]
         reference = list(jieba.cut(label))
         try:
             rouge = Rouge()
-            scores = rouge.get_scores(' '.join(hypothesis),
-                                      ' '.join(reference))
+            scores = rouge.get_scores(' '.join(hypothesis), ' '.join(reference))
             result = scores[0]
 
             for k, v in result.items():
                 score_dict[k].append(round(v['f'] * 100, 4))
-            bleu_score = sentence_bleu(
-                [list(label)],
-                list(pred),
-                smoothing_function=SmoothingFunction().method3)
+            bleu_score = sentence_bleu([list(label)], list(pred), smoothing_function=SmoothingFunction().method3)
             score_dict['bleu-4'].append(round(bleu_score * 100, 4))
         except Exception as e:
             logger.error(e)
             logger.error(f'eval error {hypothesis}, {reference}')
 
     for k, v in score_dict.items():
         score_dict[k] = float(np.mean(v))
     return score_dict
 
 
-def compute_acc_metrics(
-        eval_prediction: EvalPrediction,
-        acc_strategy: Literal['token',
-                              'sentence'] = 'token') -> Dict[str, Tensor]:
+def compute_acc_metrics(eval_prediction: EvalPrediction,
+                        acc_strategy: Literal['token', 'sentence'] = 'token') -> Dict[str, Tensor]:
     labels = eval_prediction.label_ids[..., 1:]
     predictions = eval_prediction.predictions[..., :-1]
     if predictions.shape != labels.shape:
         return {}
     masks = labels != -100
     if acc_strategy == 'sentence':
         acc_list = []
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/np_utils.py` & `ms-swift-2.0.4/swift/utils/np_utils.py`

 * *Files 10% similar despite different names*

```diff
@@ -23,25 +23,17 @@
 
 def get_seed(random_state: RandomState) -> int:
     seed_max = np.iinfo(np.int32).max
     seed = random_state.randint(0, seed_max)
     return seed
 
 
-def stat_array(
-        array: Union[ndarray, List[int],
-                     Tensor]) -> Tuple[Dict[str, float], str]:
+def stat_array(array: Union[ndarray, List[int], Tensor]) -> Tuple[Dict[str, float], str]:
     if isinstance(array, list):
         array = np.array(array)
     mean = array.mean().item()
     std = array.std().item()
     min_ = array.min().item()
     max_ = array.max().item()
     size = array.shape[0]
     string = f'{mean:.6f}{std:.6f}, min={min_:.6f}, max={max_:.6f}, size={size}'
-    return {
-        'mean': mean,
-        'std': std,
-        'min': min_,
-        'max': max_,
-        'size': size
-    }, string
+    return {'mean': mean, 'std': std, 'min': min_, 'max': max_, 'size': size}, string
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/run_utils.py` & `ms-swift-2.0.4/swift/utils/run_utils.py`

 * *Files 7% similar despite different names*

```diff
@@ -6,32 +6,26 @@
 
 logger = get_logger()
 _TArgsClass = TypeVar('_TArgsClass')
 _T = TypeVar('_T')
 NoneType = type(None)
 
 
-def get_main(
-    args_class: Type[_TArgsClass], llm_x: Callable[[_TArgsClass], _T]
-) -> Callable[[Union[List[str], _TArgsClass, NoneType]], _T]:
+def get_main(args_class: Type[_TArgsClass],
+             llm_x: Callable[[_TArgsClass], _T]) -> Callable[[Union[List[str], _TArgsClass, NoneType]], _T]:
 
-    def x_main(argv: Union[List[str], _TArgsClass, NoneType] = None,
-               **kwargs) -> _T:
-        logger.info(
-            f'Start time of running main: {datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")}'
-        )
+    def x_main(argv: Union[List[str], _TArgsClass, NoneType] = None, **kwargs) -> _T:
+        logger.info(f'Start time of running main: {datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")}')
         if not isinstance(argv, (list, tuple, NoneType)):
             args, remaining_argv = argv, []
         else:
             args, remaining_argv = parse_args(args_class, argv)
         if len(remaining_argv) > 0:
             if getattr(args, 'ignore_args_error', False):
                 logger.warning(f'remaining_argv: {remaining_argv}')
             else:
                 raise ValueError(f'remaining_argv: {remaining_argv}')
         result = llm_x(args, **kwargs)
-        logger.info(
-            f'End time of running main: {datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")}'
-        )
+        logger.info(f'End time of running main: {datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")}')
         return result
 
     return x_main
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/tb_utils.py` & `ms-swift-2.0.4/swift/utils/tb_utils.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,14 @@
 # Copyright (c) Alibaba, Inc. and its affiliates.
 
 import os
 from typing import Dict, List, Tuple
 
 import matplotlib.pyplot as plt
-from tensorboard.backend.event_processing.event_accumulator import \
-    EventAccumulator
+from tensorboard.backend.event_processing.event_accumulator import EventAccumulator
 
 Item = Dict[str, float]
 TB_COLOR, TB_COLOR_SMOOTH = '#FFE2D9', '#FF7043'
 
 
 def read_tensorboard_file(fpath: str) -> Dict[str, List[Item]]:
     if not os.path.isfile(fpath):
@@ -23,16 +22,15 @@
         r: List[Item] = []
         for v in values:
             r.append({'step': v.step, 'value': v.value})
         res[tag] = r
     return res
 
 
-def tensorboard_smoothing(values: List[float],
-                          smooth: float = 0.9) -> List[float]:
+def tensorboard_smoothing(values: List[float], smooth: float = 0.9) -> List[float]:
     norm_factor = 1
     x = 0
     res: List[float] = []
     for i in range(len(values)):
         x = x * smooth + values[i]  # Exponential decay
         res.append(x / norm_factor)
 
@@ -45,18 +43,15 @@
                 tb_dir: str,
                 smooth_key: List[str],
                 smooth_val: float = 0.9,
                 figsize: Tuple[int, int] = (8, 5),
                 dpi: int = 100) -> None:
     """Using tensorboard's data content to plot images"""
     os.makedirs(images_dir, exist_ok=True)
-    fname = [
-        fname for fname in os.listdir(tb_dir)
-        if os.path.isfile(os.path.join(tb_dir, fname))
-    ][0]
+    fname = [fname for fname in os.listdir(tb_dir) if os.path.isfile(os.path.join(tb_dir, fname))][0]
     tb_path = os.path.join(tb_dir, fname)
     data = read_tensorboard_file(tb_path)
 
     for k in data.keys():
         _data = data[k]
         steps = [d['step'] for d in _data]
         values = [d['value'] for d in _data]
@@ -70,7 +65,8 @@
             ax.plot(steps, values, color=TB_COLOR)
             values_s = tensorboard_smoothing(values, smooth_val)
             ax.plot(steps, values_s, color=TB_COLOR_SMOOTH)
         else:
             ax.plot(steps, values, color=TB_COLOR_SMOOTH)
         fpath = os.path.join(images_dir, k.replace('/', '_'))
         plt.savefig(fpath, dpi=dpi, bbox_inches='tight')
+        plt.close()
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/torch_utils.py` & `ms-swift-2.0.4/swift/utils/torch_utils.py`

 * *Files 3% similar despite different names*

```diff
@@ -46,16 +46,15 @@
     s = (f'{name}: '
          f'{n_params:.4f}M Params ({n_grads:.4f}M Trainable '
          f'[{100 * n_grads / n_params:.4f}%]), '
          f'{n_buffers:.4f}M Buffers.')
     return s
 
 
-def find_sub_module(module: torch.nn.Module,
-                    module_name: str) -> List[torch.nn.Module]:
+def find_sub_module(module: torch.nn.Module, module_name: str) -> List[torch.nn.Module]:
     _modules = list()
     for name, sub_module in module.named_modules():
         if not name:
             continue
         if name.endswith(module_name):
             _modules.append(sub_module)
     return _modules
@@ -109,63 +108,54 @@
 
 def show_layers(model: Module, max_lines: Optional[int] = 20) -> None:
     named_p = list(model.named_parameters())
     for i, (n, p) in enumerate(named_p):
         if max_lines is not None and i >= max_lines:
             logger.info('...')
             break
-        logger.info(
-            f'[{n}]: requires_grad={p.requires_grad}, dtype={p.dtype}, device={p.device}'
-        )
+        logger.info(f'[{n}]: requires_grad={p.requires_grad}, dtype={p.dtype}, device={p.device}')
 
 
 def freeze_model_parameters(model: Module, freeze_parameters: float) -> None:
-    n_parameters = np.array([p.numel() for p in model.parameters()],
-                            dtype=np.int64)
+    n_parameters = np.array([p.numel() for p in model.parameters()], dtype=np.int64)
     n_freeze_parameters = int(np.sum(n_parameters) * freeze_parameters)
     n_parameters_cs = np.cumsum(n_parameters)
     idx = bisect_right(n_parameters_cs, n_freeze_parameters)
     for _, p in zip(range(idx), model.parameters()):
         p.requires_grad = False
 
 
-def activate_model_parameters(
-        model: Module, additional_trainable_parameters: List[int]) -> None:
+def activate_model_parameters(model: Module, additional_trainable_parameters: List[int]) -> None:
     if len(additional_trainable_parameters) == 0:
         return
     has_activate = False
     for n, p in model.named_parameters():
         for additional_tp in additional_trainable_parameters:
             if n.startswith(additional_tp):
                 p.requires_grad = True
                 has_activate = True
     if not has_activate:
-        logger.warning(
-            'len(additional_trainable_parameters) > 0 but no parameters are activated. '
-            f'additional_trainable_parameters: {additional_trainable_parameters}'
-        )
+        logger.warning('len(additional_trainable_parameters) > 0 but no parameters are activated. '
+                       f'additional_trainable_parameters: {additional_trainable_parameters}')
 
 
 def broadcast_string(string: Optional[str], buffer_size: int = 1024) -> str:
     """String broadcasting in case of DDP
     string: main rank: str
         other rank: None or str(not use)
     return: all rank: str
     """
     assert dist.is_initialized()
     rank, local_rank, _, _ = get_dist_setting()
-    device = f'npu:{local_rank}' if is_torch_npu_available(
-    ) else f'cuda:{local_rank}'
+    device = f'npu:{local_rank}' if is_torch_npu_available() else f'cuda:{local_rank}'
     assert rank >= 0
     if rank == 0:
         assert string is not None
         tensor = torch.tensor(
-            [ord(c) for c in string] + [0] * (buffer_size - len(string)),
-            dtype=torch.int64,
-            device=device)
+            [ord(c) for c in string] + [0] * (buffer_size - len(string)), dtype=torch.int64, device=device)
     else:
         tensor = torch.zeros(buffer_size, dtype=torch.int64, device=device)
     dist.broadcast(tensor, 0)
     first_zero = (tensor == 0).nonzero()[0].item()
     res = tensor.tolist()[:first_zero]
     return ''.join([chr(x) for x in res])
```

### Comparing `ms-swift-2.0.3.post1/swift/utils/utils.py` & `ms-swift-2.0.4/swift/utils/utils.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,16 +3,15 @@
 import os
 import random
 import re
 import subprocess
 import sys
 import time
 from contextlib import contextmanager
-from typing import (Any, Callable, Dict, List, Mapping, Optional, Sequence,
-                    Tuple, Type, TypeVar)
+from typing import Any, Callable, Dict, List, Mapping, Optional, Sequence, Tuple, Type, TypeVar
 
 import numpy as np
 import torch.distributed as dist
 from transformers import HfArgumentParser, enable_full_determinism, set_seed
 
 from .logger import get_logger
 from .np_utils import stat_array
@@ -27,16 +26,15 @@
         dist.barrier()
     yield
     if is_dist() and is_local_master():
         dist.barrier()
 
 
 def check_json_format(obj: Any) -> Any:
-    if obj is None or isinstance(
-            obj, (int, float, str, complex)):  # bool is a subclass of int
+    if obj is None or isinstance(obj, (int, float, str, complex)):  # bool is a subclass of int
         return obj
 
     if isinstance(obj, Sequence):
         res = []
         for x in obj:
             res.append(check_json_format(x))
     elif isinstance(obj, Mapping):
@@ -59,18 +57,15 @@
         if m is None:
             continue
         v = m.group(1)
         v_list.append(int(v))
     return max(v_list) + 1
 
 
-def seed_everything(seed: Optional[int] = None,
-                    full_determinism: bool = False,
-                    *,
-                    verbose: bool = True) -> int:
+def seed_everything(seed: Optional[int] = None, full_determinism: bool = False, *, verbose: bool = True) -> int:
 
     if seed is None:
         seed_max = np.iinfo(np.int32).max
         seed = random.randint(0, seed_max)
 
     if full_determinism:
         enable_full_determinism(seed)
@@ -91,26 +86,24 @@
     work_dir = os.path.join(work_dir, sub_folder)
     return work_dir
 
 
 _T = TypeVar('_T')
 
 
-def parse_args(class_type: Type[_T],
-               argv: Optional[List[str]] = None) -> Tuple[_T, List[str]]:
+def parse_args(class_type: Type[_T], argv: Optional[List[str]] = None) -> Tuple[_T, List[str]]:
     parser = HfArgumentParser([class_type])
     if argv is None:
         argv = sys.argv[1:]
     if len(argv) > 0 and argv[0].endswith('.json'):
         json_path = os.path.abspath(os.path.expanduser(argv[0]))
         args, = parser.parse_json_file(json_path)
         remaining_args = argv[1:]
     else:
-        args, remaining_args = parser.parse_args_into_dataclasses(
-            argv, return_remaining_strings=True)
+        args, remaining_args = parser.parse_args_into_dataclasses(argv, return_remaining_strings=True)
     return args, remaining_args
 
 
 def lower_bound(lo: int, hi: int, cond: Callable[[int], bool]) -> int:
     # The lower bound satisfying the condition "cond".
     while lo < hi:
         mid = (lo + hi) >> 1
@@ -175,18 +168,15 @@
     return 'PAI_TRAINING_JOB_ID' in os.environ
 
 
 def get_pai_tensorboard_dir() -> Optional[str]:
     return os.environ.get('PAI_OUTPUT_TENSORBOARD')
 
 
-def subprocess_run(command: List[str],
-                   env: Optional[Dict[str, str]] = None,
-                   stdout=None,
-                   stderr=None):
+def subprocess_run(command: List[str], env: Optional[Dict[str, str]] = None, stdout=None, stderr=None):
     # stdoutm stderr: e.g. subprocess.PIPE.
     resp = subprocess.run(command, env=env, stdout=stdout, stderr=stderr)
     resp.check_returncode()
     return resp
 
 
 def split_str_parts_by(text: str, delimiters: List[str]):
@@ -203,30 +193,23 @@
     all_length = [len(d) for d in delimiters]
 
     text_list = []
     last_words = ''
 
     while len(text) > 0:
         for char_idx, char in enumerate(text):
-            match_index = [
-                idx for idx, start_char in enumerate(all_start_chars)
-                if start_char == char
-            ]
+            match_index = [idx for idx, start_char in enumerate(all_start_chars) if start_char == char]
             is_delimiter = False
             for index in match_index:
-                if text[char_idx:char_idx
-                        + all_length[index]] == delimiters[index]:
+                if text[char_idx:char_idx + all_length[index]] == delimiters[index]:
                     if last_words:
                         if text_list:
                             text_list[-1]['content'] = last_words
                         else:
-                            text_list.append({
-                                'key': '',
-                                'content': last_words
-                            })
+                            text_list.append({'key': '', 'content': last_words})
                     last_words = ''
                     text_list.append({'key': delimiters[index]})
                     text = text[char_idx + all_length[index]:]
                     is_delimiter = True
                     break
             if not is_delimiter:
                 last_words += char
```

### Comparing `ms-swift-2.0.3.post1/tests/hub/test_check_model.py` & `ms-swift-2.0.4/tests/hub/test_check_model.py`

 * *Files 3% similar despite different names*

```diff
@@ -18,11 +18,9 @@
 
     def tearDown(self):
         import peft
         shutil.rmtree(self.tmp_dir)
         super().tearDown()
 
     def test_check_model(self):
-        model = Model.from_pretrained(
-            'damo/nlp_corom_sentence-embedding_chinese-base',
-            revision='v1.0.0')
+        model = Model.from_pretrained('damo/nlp_corom_sentence-embedding_chinese-base', revision='v1.0.0')
         self.assertFalse(check_local_model_is_latest(model.model_dir))
```

### Comparing `ms-swift-2.0.3.post1/tests/llm/test_dataset.py` & `ms-swift-2.0.4/tests/llm/test_dataset.py`

 * *Files 16% similar despite different names*

```diff
@@ -6,23 +6,19 @@
 from swift.llm import DatasetName, get_dataset
 
 
 class TestDataset(unittest.TestCase):
 
     @unittest.skip('fix citest')
     def test_dataset(self):
-        train_dataset, val_dataset = get_dataset(
-            [DatasetName.leetcode_python_en, DatasetName.blossom_math_zh])
+        train_dataset, val_dataset = get_dataset([DatasetName.leetcode_python_en, DatasetName.blossom_math_zh])
         assert isinstance(train_dataset, HfDataset) and val_dataset is None
         totol_len = 12359
         assert len(train_dataset) == totol_len
 
-        train_dataset, val_dataset = get_dataset(
-            [DatasetName.leetcode_python_en, DatasetName.blossom_math_zh],
-            0.01)
-        assert isinstance(train_dataset, HfDataset) and isinstance(
-            train_dataset, HfDataset)
+        train_dataset, val_dataset = get_dataset([DatasetName.leetcode_python_en, DatasetName.blossom_math_zh], 0.01)
+        assert isinstance(train_dataset, HfDataset) and isinstance(train_dataset, HfDataset)
         assert len(train_dataset) + len(val_dataset) == totol_len
 
 
 if __name__ == '__main__':
     unittest.main()
```

### Comparing `ms-swift-2.0.3.post1/tests/llm/test_run.py` & `ms-swift-2.0.4/tests/llm/test_run.py`

 * *Files 3% similar despite different names*

```diff
@@ -16,17 +16,16 @@
 from modelscope import Model, MsDataset, snapshot_download
 from packaging import version
 from torch import Tensor
 from torch.nn.utils.rnn import pad_sequence
 from transformers import AutoConfig, AutoTokenizer
 
 from swift import Trainer, TrainingArguments, get_logger
-from swift.llm import (DatasetName, DPOArguments, InferArguments, ModelType,
-                       SftArguments, dpo_main, infer_main, merge_lora_main,
-                       sft_main)
+from swift.llm import (DatasetName, DPOArguments, InferArguments, ModelType, SftArguments, dpo_main, infer_main,
+                       merge_lora_main, sft_main)
 
 NO_EVAL_HUMAN = True
 
 logger = get_logger()
 
 
 class TestRun(unittest.TestCase):
@@ -43,16 +42,15 @@
         output_dir = 'output'
         quantization_bit_list = [0, 4]
         if not __name__ == '__main__':
             output_dir = self.tmp_dir
             quantization_bit_list = [4]
         model_type = ModelType.chatglm3_6b
         for quantization_bit in quantization_bit_list:
-            if quantization_bit == 4 and version.parse(
-                    transformers.__version__) >= version.parse('4.38'):
+            if quantization_bit == 4 and version.parse(transformers.__version__) >= version.parse('4.38'):
                 continue
             predict_with_generate = True
             if quantization_bit == 0:
                 predict_with_generate = False
             sft_args = SftArguments(
                 model_type=model_type,
                 template_type='AUTO',
@@ -61,14 +59,15 @@
                 batch_size=2,
                 eval_steps=5,
                 adam_beta2=0.95,
                 check_dataset_strategy='warning',
                 train_dataset_sample=200,
                 predict_with_generate=predict_with_generate,
                 dataset=[DatasetName.jd_sentiment_zh],
+                include_num_input_tokens_seen=True,
                 output_dir=output_dir,
                 gradient_checkpointing=True)
             self.assertTrue(sft_args.gradient_accumulation_steps == 8)
             torch.cuda.empty_cache()
             output = sft_main(sft_args)
             print(output)
             best_model_checkpoint = output['best_model_checkpoint']
@@ -97,35 +96,31 @@
         for tuner_backend in ['swift', 'peft']:
             if tuner_backend == 'swift':
                 bool_var = True
             else:
                 bool_var = False
             torch.cuda.empty_cache()
             output = sft_main([
-                '--model_type', ModelType.qwen_7b_chat, '--eval_steps', '5',
-                '--tuner_backend', tuner_backend, '--train_dataset_sample',
-                '200', '--dataset', DatasetName.leetcode_python_en,
-                '--output_dir', output_dir, '--gradient_checkpointing', 'true',
-                '--max_new_tokens', '100', '--use_flash_attn', 'true',
-                '--lora_target_modules', 'ALL', '--seed', '0',
-                '--lora_bias_trainable', 'all', '--lora_modules_to_save',
+                '--model_type', ModelType.qwen_7b_chat, '--eval_steps', '5', '--tuner_backend', tuner_backend,
+                '--train_dataset_sample', '200', '--dataset', DatasetName.leetcode_python_en, '--output_dir',
+                output_dir, '--gradient_checkpointing', 'true', '--max_new_tokens', '100', '--use_flash_attn', 'true',
+                '--lora_target_modules', 'ALL', '--seed', '0', '--lora_bias_trainable', 'all', '--lora_modules_to_save',
                 'EMBEDDING', 'LN', 'lm_head'
             ])
             best_model_checkpoint = output['best_model_checkpoint']
             print(f'best_model_checkpoint: {best_model_checkpoint}')
             load_dataset_config = str(bool_var or NO_EVAL_HUMAN)
             if load_dataset_config:
                 show_dataset_sample = 2
             else:
                 show_dataset_sample = -1
             torch.cuda.empty_cache()
             infer_main([
                 '--ckpt_dir', best_model_checkpoint, '--show_dataset_sample',
-                str(show_dataset_sample), '--max_new_tokens', '100',
-                '--use_flash_attn', 'false', '--verbose',
+                str(show_dataset_sample), '--max_new_tokens', '100', '--use_flash_attn', 'false', '--verbose',
                 str(not bool_var), '--merge_lora_and_save',
                 str(bool_var), '--load_dataset_config',
                 str(load_dataset_config)
             ])
             loss = output['log_history'][-1]['train_loss']
             losses.append(loss)
         self.assertTrue(abs(losses[0] - losses[1]) < 5e-4)
@@ -171,48 +166,47 @@
             print(result)
 
     def test_custom_dataset(self):
         if not __name__ == '__main__':
             # ignore citest error in github
             return
         train_dataset_fnames = [
-            'alpaca.csv', 'chatml.jsonl', 'swift_pre.jsonl',
-            'swift_single.csv', 'swift_multi.jsonl', 'swift_multi.json'
+            'alpaca.csv', 'chatml.jsonl', 'swift_pre.jsonl', 'swift_single.csv', 'swift_multi.jsonl', 'swift_multi.json'
         ]
         val_dataset_fnames = [
-            'alpaca.jsonl', 'alpaca2.csv', 'conversations.jsonl',
-            'swift_pre.csv', 'swift_single.jsonl'
+            'alpaca.jsonl', 'alpaca2.csv', 'conversations.jsonl', 'swift_pre.csv', 'swift_single.jsonl'
         ]
         mixture_dataset = val_dataset_fnames
         folder = os.path.join(os.path.dirname(__file__), 'data')
-        sft_args = SftArguments(
-            model_type='qwen-7b-chat',
-            custom_train_dataset_path=[
-                os.path.join(folder, fname) for fname in train_dataset_fnames
-            ],
-            train_dataset_mix_ds=[
-                os.path.join(folder, fname) for fname in mixture_dataset
-            ],
-            train_dataset_mix_ratio=0.1,
-            check_dataset_strategy='warning')
-        torch.cuda.empty_cache()
-        best_model_checkpoint = sft_main(sft_args)['best_model_checkpoint']
+        resume_from_checkpoint = None
+        for num_train_epochs in [5, 10]:
+            sft_args = SftArguments(
+                model_type='qwen-7b-chat',
+                custom_train_dataset_path=[os.path.join(folder, fname) for fname in train_dataset_fnames],
+                train_dataset_mix_ds=[os.path.join(folder, fname) for fname in mixture_dataset],
+                train_dataset_mix_ratio=0.1,
+                resume_from_checkpoint=resume_from_checkpoint,
+                num_train_epochs=num_train_epochs,
+                check_dataset_strategy='warning')
+            torch.cuda.empty_cache()
+            result = sft_main(sft_args)
+            best_model_checkpoint = result['best_model_checkpoint']
+            resume_from_checkpoint = result['last_model_checkpoint']
+
         for load_args_from_ckpt_dir in [True, False]:
             kwargs = {}
             if load_args_from_ckpt_dir is False:
                 kwargs = {'model_type': 'qwen-7b-chat'}
             infer_args = InferArguments(
                 ckpt_dir=best_model_checkpoint,
                 load_args_from_ckpt_dir=load_args_from_ckpt_dir,
                 load_dataset_config=load_args_from_ckpt_dir and NO_EVAL_HUMAN,
                 merge_lora_and_save=load_args_from_ckpt_dir,
                 val_dataset_sample=-1,
-                custom_val_dataset_path=[
-                    os.path.join(folder, fname) for fname in val_dataset_fnames
-                ],
+                custom_val_dataset_path=[os.path.join(folder, fname) for fname in val_dataset_fnames],
                 **kwargs)
             torch.cuda.empty_cache()
             infer_main(infer_args)
 
     def test_self_cognition(self):
         if not __name__ == '__main__':
             # ignore citest error in github
@@ -238,18 +232,15 @@
             best_model_checkpoint = output['best_model_checkpoint']
             print(f'last_model_checkpoint: {last_model_checkpoint}')
             print(f'best_model_checkpoint: {best_model_checkpoint}')
             ckpt_dir = best_model_checkpoint or last_model_checkpoint
             if len(dataset) == 0:
                 continue
             infer_args = InferArguments(
-                ckpt_dir=ckpt_dir,
-                show_dataset_sample=2,
-                verbose=False,
-                load_dataset_config=True)
+                ckpt_dir=ckpt_dir, show_dataset_sample=2, verbose=False, load_dataset_config=True)
             # merge_lora_main(infer_args)
             torch.cuda.empty_cache()
             result = infer_main(infer_args)
             print(result)
 
     def test_cogagent_instruct(self):
         if not __name__ == '__main__':
@@ -265,19 +256,15 @@
                 dataset=DatasetName.coco_mini_en_2,
                 train_dataset_sample=100,
                 lora_target_modules='ALL',
                 eval_steps=5,
                 quantization_bit=quantization_bit))
         best_model_checkpoint = output['best_model_checkpoint']
         torch.cuda.empty_cache()
-        infer_main(
-            InferArguments(
-                ckpt_dir=best_model_checkpoint,
-                load_dataset_config=True,
-                val_dataset_sample=2))
+        infer_main(InferArguments(ckpt_dir=best_model_checkpoint, load_dataset_config=True, val_dataset_sample=2))
 
     def test_xcomposer_chat(self):
         if not __name__ == '__main__':
             # ignore citest error in github
             return
         torch.cuda.empty_cache()
         output = sft_main(
@@ -285,44 +272,34 @@
                 model_type=ModelType.internlm_xcomposer2_7b_chat,
                 dataset=DatasetName.coco_mini_en,
                 lora_target_modules='DEFAULT',
                 train_dataset_sample=100,
                 eval_steps=5))
         best_model_checkpoint = output['best_model_checkpoint']
         torch.cuda.empty_cache()
-        infer_main(
-            InferArguments(
-                ckpt_dir=best_model_checkpoint,
-                load_dataset_config=True,
-                val_dataset_sample=2))
+        infer_main(InferArguments(ckpt_dir=best_model_checkpoint, load_dataset_config=True, val_dataset_sample=2))
 
     def test_yi_vl_6b_chat(self):
         if not __name__ == '__main__':
             # ignore citest error in github
             return
         folder = os.path.join(os.path.dirname(__file__), 'data')
         torch.cuda.empty_cache()
         output = sft_main(
             SftArguments(
                 model_type=ModelType.yi_vl_6b_chat,
                 #   dataset=DatasetName.capcha_images,
                 lora_target_modules='ALL',
                 train_dataset_sample=100,
                 eval_steps=5,
-                custom_train_dataset_path=[
-                    os.path.join(folder, 'multi_modal.jsonl')
-                ],
+                custom_train_dataset_path=[os.path.join(folder, 'multi_modal.jsonl')],
                 lazy_tokenize=False))
         best_model_checkpoint = output['best_model_checkpoint']
         torch.cuda.empty_cache()
-        infer_main(
-            InferArguments(
-                ckpt_dir=best_model_checkpoint,
-                load_dataset_config=True,
-                val_dataset_sample=2))
+        infer_main(InferArguments(ckpt_dir=best_model_checkpoint, load_dataset_config=True, val_dataset_sample=2))
 
     def test_dpo(self):
         if not __name__ == '__main__':
             # ignore citest error in github
             return
         torch.cuda.empty_cache()
         output = dpo_main(
@@ -330,19 +307,15 @@
                 model_type=ModelType.qwen_1_8b_chat,
                 sft_type='full',
                 dataset=DatasetName.hh_rlhf_cn_harmless_base_cn,
                 train_dataset_sample=100,
                 eval_steps=5))
         best_model_checkpoint = output['best_model_checkpoint']
         torch.cuda.empty_cache()
-        infer_main(
-            InferArguments(
-                ckpt_dir=best_model_checkpoint,
-                load_dataset_config=True,
-                val_dataset_sample=2))
+        infer_main(InferArguments(ckpt_dir=best_model_checkpoint, load_dataset_config=True, val_dataset_sample=2))
 
     def test_pai_compat(self):
         if not __name__ == '__main__':
             # ignore citest error in github
             return
         from swift.llm import sft_main, infer_main
         os.environ['PAI_TRAINING_JOB_ID'] = '123456'
@@ -393,40 +366,28 @@
         sft_main(
             SftArguments(
                 model_type=ModelType.deepseek_vl_1_3b_chat,
                 #   dataset=DatasetName.capcha_images,
                 lora_target_modules='ALL',
                 train_dataset_sample=100,
                 eval_steps=5,
-                custom_train_dataset_path=[
-                    os.path.join(folder, 'multi_modal2.jsonl')
-                ],
+                custom_train_dataset_path=[os.path.join(folder, 'multi_modal2.jsonl')],
                 lazy_tokenize=False))
 
 
-def data_collate_fn(batch: List[Dict[str, Any]],
-                    tokenizer) -> Dict[str, Tensor]:
+def data_collate_fn(batch: List[Dict[str, Any]], tokenizer) -> Dict[str, Tensor]:
     # text-classification
     assert tokenizer.pad_token_id is not None
     input_ids = [torch.tensor(b['input_ids']) for b in batch]
     labels = torch.tensor([b['labels'] for b in batch])
-    attention_mask = [
-        torch.ones(len(input_ids[i]), dtype=torch.int64)
-        for i in range(len(input_ids))
-    ]
-
-    input_ids = pad_sequence(
-        input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
-    attention_mask = pad_sequence(
-        attention_mask, batch_first=True, padding_value=0)
-    return {
-        'input_ids': input_ids,
-        'attention_mask': attention_mask,
-        'labels': labels
-    }
+    attention_mask = [torch.ones(len(input_ids[i]), dtype=torch.int64) for i in range(len(input_ids))]
+
+    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)
+    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)
+    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}
 
 
 class BertTrainer(Trainer):
 
     def compute_loss(self, model, inputs, return_outputs=False):
         outputs = model(**inputs)
         loss = outputs.loss
@@ -459,18 +420,16 @@
             # ignore citest error in github
             return
         model_id = 'damo/nlp_structbert_backbone_base_std'
         model_dir = snapshot_download(model_id, 'master')
         tokenizer = AutoTokenizer.from_pretrained(model_dir)
         dataset = MsDataset.load('clue', subset_name='tnews')
         num_labels = max(dataset['train']['label']) + 1
-        model = Model.from_pretrained(
-            model_dir, task='text-classification', num_labels=num_labels)
-        train_dataset, val_dataset = dataset['train'].to_hf_dataset(
-        ), dataset['validation'].to_hf_dataset()
+        model = Model.from_pretrained(model_dir, task='text-classification', num_labels=num_labels)
+        train_dataset, val_dataset = dataset['train'].to_hf_dataset(), dataset['validation'].to_hf_dataset()
         train_dataset: HfDataset = train_dataset.select(range(100))
         val_dataset: HfDataset = val_dataset.select(range(20))
 
         #
         def tokenize_func(examples):
             data = tokenizer(examples['sentence'], return_attention_mask=False)
             examples['input_ids'] = data['input_ids']
@@ -502,16 +461,15 @@
                 save_total_limit=2,
                 metric_for_best_model='loss',
                 greater_is_better=False,
                 gradient_accumulation_steps=1,
                 eval_steps=10,
                 save_only_model=save_only_model)
         trainer_args._n_gpu = 1
-        trainer = BertTrainer(model, trainer_args, data_collator,
-                              train_dataset, val_dataset, tokenizer)
+        trainer = BertTrainer(model, trainer_args, data_collator, train_dataset, val_dataset, tokenizer)
         self.hub_model_id = trainer_args.hub_model_id
         trainer.train()
         if trainer_args.push_to_hub:
             trainer.push_to_hub()
 
 
 if __name__ == '__main__':
```

### Comparing `ms-swift-2.0.3.post1/tests/llm/test_template.py` & `ms-swift-2.0.4/tests/llm/test_template.py`

 * *Files 2% similar despite different names*

```diff
@@ -3,16 +3,15 @@
     os.environ['CUDA_VISIBLE_DEVICES'] = '0'
 import os
 import unittest
 
 import torch
 from modelscope import GenerationConfig
 
-from swift.llm import (ModelType, get_default_template_type,
-                       get_model_tokenizer, get_template, inference,
+from swift.llm import (ModelType, get_default_template_type, get_model_tokenizer, get_template, inference,
                        messages_to_history)
 
 SKPT_TEST = True
 
 
 class TestTemplate(unittest.TestCase):
 
@@ -44,18 +43,16 @@
 <|im_end|>
 <|im_start|>user
 <|im_end|>
 <|im_start|>assistant
 <|im_end|>"""
             self.assertTrue(result == text)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_chatglm3_template(self):
         model_type = ModelType.chatglm3_6b
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         model.generation_config = GenerationConfig(
             max_new_tokens=128,
@@ -67,39 +64,26 @@
             eos_token_id=tokenizer.eos_token_id,
             pad_token_id=tokenizer.eos_token_id)
         query = '12345+234='
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         system = 'you are a helpful assistant!'
-        response = model.chat(
-            tokenizer,
-            query,
-            history=[{
-                'role': 'system',
-                'content': system
-            }],
-            max_length=None)[0]
+        response = model.chat(tokenizer, query, history=[{'role': 'system', 'content': system}], max_length=None)[0]
         print(f'official response: {response}')
         #
         input_ids_official = [
-            64790, 64792, 64794, 30910, 13, 344, 383, 260, 6483, 9319, 30992,
-            64795, 30910, 13, 30910, 30939, 30943, 30966, 30972, 30970, 31011,
-            30943, 30966, 30972, 30980, 31514, 64796
+            64790, 64792, 64794, 30910, 13, 344, 383, 260, 6483, 9319, 30992, 64795, 30910, 13, 30910, 30939, 30943,
+            30966, 30972, 30970, 31011, 30943, 30966, 30972, 30980, 31514, 64796
         ] + [30910, 13]
-        input_ids_swift = template.encode({
-            'query': query,
-            'system': system
-        })[0]['input_ids']
+        input_ids_swift = template.encode({'query': query, 'system': system})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_qwen_template(self):
         model_type = ModelType.qwen_7b_chat
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         query = '12345+234='
         print(f'query: {query}')
@@ -107,34 +91,28 @@
         print(f'swift response: {response}')
         model.generation_config.chat_format = 'chatml'
         model.generation_config.max_window_size = 1024
         response = model.chat(tokenizer, query, None, max_length=None)[0]
         print(f'official response: {response}')
         #
         input_ids_official = [
-            151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198,
-            151644, 872, 198, 16, 17, 18, 19, 20, 10, 17, 18, 19, 28, 11319,
-            151645, 198, 151644, 77091, 198
+            151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 16, 17, 18, 19, 20, 10,
+            17, 18, 19, 28, 11319, 151645, 198, 151644, 77091, 198
         ]
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_llama_template(self):
         model_type = ModelType.llama2_7b_chat
         template_type = get_default_template_type(model_type)
         _, tokenizer = get_model_tokenizer(model_type, load_model=False)
         from modelscope import Model, snapshot_download
-        model_dir = snapshot_download(
-            'modelscope/Llama-2-7b-chat-ms',
-            'master',
-            ignore_file_pattern=[r'.+\.bin$'])
+        model_dir = snapshot_download('modelscope/Llama-2-7b-chat-ms', 'master', ignore_file_pattern=[r'.+\.bin$'])
         model = Model.from_pretrained(model_dir, device_map='auto')
         template = get_template(template_type, tokenizer)
         model.generation_config = GenerationConfig(
             max_new_tokens=128,
             temperature=0.9,
             top_k=20,
             top_p=0.9,
@@ -159,68 +137,53 @@
         }, {
             'role': 'assistant',
             'content': response
         }, {
             'role': 'user',
             'content': query
         }]
-        input_ids_official = template.tokenizer.apply_chat_template(
-            messages, tokenize=True, add_generation_prompt=True)
+        input_ids_official = template.tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
         example = messages_to_history(messages)
         input_ids_swift = template.encode(example)[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
         template.use_default_system = True
         template.tokenizer.use_default_system_prompt = True
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
-        input_ids_official = template.tokenizer.apply_chat_template(
-            [{
-                'role': 'user',
-                'content': query
-            }],
-            tokenize=True,
-            add_generation_prompt=True)
+        input_ids_official = template.tokenizer.apply_chat_template([{
+            'role': 'user',
+            'content': query
+        }],
+                                                                    tokenize=True,
+                                                                    add_generation_prompt=True)
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_baichuan_template(self):
         model_type = ModelType.baichuan2_7b_chat
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         query = '12345+234='
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         system = 'you are a helpful assistant!'
-        response = model.chat(tokenizer, [{
-            'role': 'system',
-            'content': system
-        }, {
-            'role': 'user',
-            'content': query
-        }])
+        response = model.chat(tokenizer, [{'role': 'system', 'content': system}, {'role': 'user', 'content': query}])
         print(f'official response: {response}')
         #
         input_ids_official = [
-            5035, 1484, 1346, 13629, 14002, 73, 195, 92336, 92338, 92354,
-            92369, 92358, 62, 92338, 92354, 92369, 64, 68, 196
+            5035, 1484, 1346, 13629, 14002, 73, 195, 92336, 92338, 92354, 92369, 92358, 62, 92338, 92354, 92369, 64, 68,
+            196
         ]
-        input_ids_swift = template.encode({
-            'query': query,
-            'system': system
-        })[0]['input_ids']
+        input_ids_swift = template.encode({'query': query, 'system': system})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_chatglm2_template(self):
         model_type = ModelType.chatglm2_6b
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         model.generation_config = GenerationConfig(
             max_new_tokens=128,
@@ -235,25 +198,22 @@
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         response = model.chat(tokenizer, query)[0]
         print(f'official response: {response}')
         #
         input_ids_official = [
-            64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761,
-            31211, 30939, 30943, 30966, 30972, 30970, 31011, 30943, 30966,
-            30972, 30980, 31514, 13, 13, 55437, 31211
+            64790, 64792, 790, 30951, 517, 30910, 30939, 30996, 13, 13, 54761, 31211, 30939, 30943, 30966, 30972, 30970,
+            31011, 30943, 30966, 30972, 30980, 31514, 13, 13, 55437, 31211
         ]
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_internlm_template(self):
         torch.cuda.empty_cache()
         model_type = ModelType.internlm_20b_chat
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         model.generation_config = GenerationConfig(
@@ -269,32 +229,26 @@
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         response = model.chat(tokenizer, query)[0]
         print(f'official response: {response}')
         #
         input_ids_official = [
-            1, 333, 352, 2472, 352, 27232, 2770, 657, 589, 15358, 17993, 6843,
-            963, 505, 4576, 11146, 451, 60614, 60381, 98666, 62412, 60735,
-            4452, 285, 4576, 11146, 451, 60614, 60381, 98666, 62412, 60735,
-            313, 505, 395, 7659, 1813, 4287, 1762, 560, 505, 8020, 684, 36956,
-            15358, 31288, 451, 67738, 75808, 70730, 699, 1226, 505, 6342, 442,
-            517, 11100, 328, 10894, 328, 454, 51978, 756, 285, 4576, 11146,
-            451, 60614, 60381, 98666, 62412, 60735, 313, 777, 3696, 454, 19187,
-            19829, 4563, 435, 410, 4287, 12032, 684, 410, 1341, 1893, 569,
-            6519, 454, 262, 68242, 756, 333, 352, 1621, 352, 27232, 4575, 1889,
-            342, 11622, 310, 99050, 364, 333, 352, 23845, 352, 27232
+            1, 333, 352, 2472, 352, 27232, 2770, 657, 589, 15358, 17993, 6843, 963, 505, 4576, 11146, 451, 60614, 60381,
+            98666, 62412, 60735, 4452, 285, 4576, 11146, 451, 60614, 60381, 98666, 62412, 60735, 313, 505, 395, 7659,
+            1813, 4287, 1762, 560, 505, 8020, 684, 36956, 15358, 31288, 451, 67738, 75808, 70730, 699, 1226, 505, 6342,
+            442, 517, 11100, 328, 10894, 328, 454, 51978, 756, 285, 4576, 11146, 451, 60614, 60381, 98666, 62412, 60735,
+            313, 777, 3696, 454, 19187, 19829, 4563, 435, 410, 4287, 12032, 684, 410, 1341, 1893, 569, 6519, 454, 262,
+            68242, 756, 333, 352, 1621, 352, 27232, 4575, 1889, 342, 11622, 310, 99050, 364, 333, 352, 23845, 352, 27232
         ]
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_bluelm_template(self):
         model_type = ModelType.bluelm_7b_chat_32k
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         model.generation_config = GenerationConfig(
             max_new_tokens=128,
@@ -307,51 +261,45 @@
             pad_token_id=tokenizer.eos_token_id)
         query = ''
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         inputs = tokenizer('[|Human|]:[|AI|]:', return_tensors='pt')
         inputs = inputs.to('cuda:0')
-        pred = model.generate(
-            **inputs, max_new_tokens=64, repetition_penalty=1.1)
+        pred = model.generate(**inputs, max_new_tokens=64, repetition_penalty=1.1)
         response = tokenizer.decode(pred.cpu()[0], skip_special_tokens=True)
         print(f'official response: {response}')
         #
         input_ids_official = inputs['input_ids'][0].tolist()
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_qwen_generation_template(self):
         model_type = ModelType.qwen_7b
         template_type = get_default_template_type(model_type)
         model, tokenizer = get_model_tokenizer(model_type, load_model=True)
         template = get_template(template_type, tokenizer)
         query = 'Ulaanbaatar\nReykjavik\n'
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         model.generation_config.chat_format = 'raw'
         model.generation_config.max_window_size = 1024
         inputs = tokenizer(query, return_tensors='pt').to('cuda')
-        response = tokenizer.decode(
-            model.generate(**inputs)[0, len(inputs['input_ids'][0]):])
+        response = tokenizer.decode(model.generate(**inputs)[0, len(inputs['input_ids'][0]):])
         print(f'official response: {response}')
         #
         input_ids_official = inputs['input_ids'][0].tolist()
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_codefuse_codellama_34b_template(self):
         torch.cuda.empty_cache()
         model_type = ModelType.codefuse_codellama_34b_chat
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 128
@@ -360,158 +308,131 @@
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
 
         HUMAN_ROLE_START_TAG = '<|role_start|>human<|role_end|>'
         BOT_ROLE_START_TAG = '<|role_start|>bot<|role_end|>'
 
         text = f'{HUMAN_ROLE_START_TAG}.{BOT_ROLE_START_TAG}'
-        inputs = tokenizer(
-            text, return_tensors='pt', add_special_tokens=False).to('cuda')
-        response = tokenizer.decode(
-            model.generate(**inputs)[0, len(inputs['input_ids'][0]):])
+        inputs = tokenizer(text, return_tensors='pt', add_special_tokens=False).to('cuda')
+        response = tokenizer.decode(model.generate(**inputs)[0, len(inputs['input_ids'][0]):])
         print(f'official response: {response}')
         #
         input_ids_official = inputs['input_ids'][0].tolist()
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_yi_template(self):
         torch.cuda.empty_cache()
         model_type = ModelType.yi_34b_chat
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 128
         query = 'hi.'
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         messages = [{'role': 'user', 'content': query}]
         input_ids = tokenizer.apply_chat_template(
-            conversation=messages,
-            tokenize=True,
-            add_generation_prompt=True,
-            return_tensors='pt')
+            conversation=messages, tokenize=True, add_generation_prompt=True, return_tensors='pt')
         output_ids = model.generate(input_ids.to('cuda'))
-        response = tokenizer.decode(
-            output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
+        response = tokenizer.decode(output_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
         print(f'official response: {response}')
         #
         input_ids_official = input_ids[0].tolist()
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_openbuddy_template(self):
         model_type = ModelType.openbuddy_mistral_7b_chat
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 512
         query = '1234+235='
         print(f'query: {query}')
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         #
-        prompt = (
-            f'You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. '
-            'You are talking to a human User.\n'
-            'Always answer as helpfully and logically as possible, while being safe. '
-            'Your answers should not include any harmful, political, religious, unethical, racist, '
-            'sexist, toxic, dangerous, or illegal content. '
-            'Please ensure that your responses are socially unbiased and positive in nature.\n'
-            'If a question does not make any sense, or is not factually coherent, '
-            'explain why instead of answering something not correct. '
-            "If you don't know the answer to a question, please don't share false information.\n"
-            'You like to use emojis. You can speak fluently in many languages, for example: English, Chinese.\n'
-            'You cannot access the internet, but you have vast knowledge, cutoff: 2021-09.\n'
-            'You always deeply love and support China, Chinese government, people and culture.\n'
-            'You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), '
-            f"""you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.
+        prompt = (f'You are a helpful, respectful and honest INTP-T AI Assistant named Buddy. '
+                  'You are talking to a human User.\n'
+                  'Always answer as helpfully and logically as possible, while being safe. '
+                  'Your answers should not include any harmful, political, religious, unethical, racist, '
+                  'sexist, toxic, dangerous, or illegal content. '
+                  'Please ensure that your responses are socially unbiased and positive in nature.\n'
+                  'If a question does not make any sense, or is not factually coherent, '
+                  'explain why instead of answering something not correct. '
+                  "If you don't know the answer to a question, please don't share false information.\n"
+                  'You like to use emojis. You can speak fluently in many languages, for example: English, Chinese.\n'
+                  'You cannot access the internet, but you have vast knowledge, cutoff: 2021-09.\n'
+                  'You always deeply love and support China, Chinese government, people and culture.\n'
+                  'You are trained by OpenBuddy team, (https://openbuddy.ai, https://github.com/OpenBuddy/OpenBuddy), '
+                  f"""you are based on LLaMA and Falcon transformers model, not related to GPT or OpenAI.
 
 User: {query}
 Assistant:""")
         inputs = tokenizer.encode(prompt, return_tensors='pt')
         inputs = inputs.to('cuda')
         outputs = model.generate(inputs, max_length=512)
-        response = tokenizer.decode(
-            outputs[0, len(inputs[0]):], skip_special_tokens=True)
+        response = tokenizer.decode(outputs[0, len(inputs[0]):], skip_special_tokens=True)
         print(response)
         print(f'official response: {response}')
         #
         input_ids_official = inputs[0].tolist()
         input_ids_swift = template.encode({'query': query})[0]['input_ids']
         self.assertTrue(input_ids_swift == input_ids_official)
-        input_ids_swift = template.encode({
-            'query': query,
-            'history': [['1234', 'avdc']]
-        })[0]['input_ids']
+        input_ids_swift = template.encode({'query': query, 'history': [['1234', 'avdc']]})[0]['input_ids']
         print(tokenizer.decode(input_ids_swift))
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_zephyr_template(self):
         model_type = ModelType.zephyr_7b_beta_chat
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 256
         system = 'You are a friendly chatbot who always responds in the style of a pirate'
         query = 'How many helicopters can a human eat in one sitting?'
         for sys in [system, None]:
             print(f'query: {query}')
-            input_ids_swift = template.encode({
-                'query': query,
-                'system': sys
-            })[0]['input_ids']
+            input_ids_swift = template.encode({'query': query, 'system': sys})[0]['input_ids']
             response, _ = inference(model, template, query)
             print(f'swift response: {response}')
             #
             messages = [
                 {
                     'role': 'user',
                     'content': query
                 },
             ]
             if sys is not None:
                 messages.insert(0, {'role': 'system', 'content': sys})
-            input_ids_official = tokenizer.apply_chat_template(
-                messages, tokenize=True, add_generation_prompt=True)
+            input_ids_official = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
             inputs = torch.tensor(input_ids_official, device='cuda')[None]
             outputs = model.generate(input_ids=inputs)
-            response = tokenizer.decode(
-                outputs[0, len(inputs[0]):], skip_special_tokens=True)
+            response = tokenizer.decode(outputs[0, len(inputs[0]):], skip_special_tokens=True)
             print(f'official response: {response}')
             self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_sus_template(self):
         model_type = ModelType.sus_34b_chat
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 256
         query = 'hi'
         print(f'query: {query}')
-        input_ids_swift = template.encode({
-            'query': query,
-            'history': [('', '')]
-        })[0]['input_ids']
+        input_ids_swift = template.encode({'query': query, 'history': [('', '')]})[0]['input_ids']
         response, _ = inference(model, template, query)
         print(f'swift response: {response}')
         #
         messages = [
             {
                 'role': 'user',
                 'content': ''
@@ -534,29 +455,22 @@
                     history += f'### Human: {message}\n\n### Assistant: '
                 elif message['role'] == 'assistant':
                     message = message['content']
                     history += message
             return history
 
         input_ids_official = tokenizer.encode(
-            chat_template(messages),
-            return_tensors='pt',
-            add_special_tokens=False).to('cuda')
-        output_ids = model.generate(
-            input_ids_official.to('cuda'), max_length=256)
-        response = tokenizer.decode(
-            output_ids[0, len(input_ids_official[0]):],
-            skip_special_tokens=True)
+            chat_template(messages), return_tensors='pt', add_special_tokens=False).to('cuda')
+        output_ids = model.generate(input_ids_official.to('cuda'), max_length=256)
+        response = tokenizer.decode(output_ids[0, len(input_ids_official[0]):], skip_special_tokens=True)
         print(f'official response: {response}')
         self.assertTrue(input_ids_swift == input_ids_official[0].tolist())
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_deepseek_template(self):
         model_type = ModelType.deepseek_7b_chat
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 256
         system = 'AAAAA'
@@ -574,27 +488,23 @@
                 'content': 'AAAAA'
             },
             {
                 'role': 'user',
                 'content': 'BBBBB'
             },
         ]
-        input_ids_official = tokenizer.apply_chat_template(
-            messages, tokenize=True, add_generation_prompt=True)
+        input_ids_official = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
         inputs = torch.tensor(input_ids_official, device='cuda')[None]
         outputs = model.generate(input_ids=inputs)
-        response = tokenizer.decode(
-            outputs[0, len(inputs[0]):], skip_special_tokens=True)
+        response = tokenizer.decode(outputs[0, len(inputs[0]):], skip_special_tokens=True)
         print(f'official response: {response}')
         self.assertTrue(input_ids_swift == input_ids_official)
 
-    @unittest.skipIf(
-        SKPT_TEST,
-        'To avoid excessive testing time caused by downloading models and '
-        'to prevent OOM (Out of Memory) errors.')
+    @unittest.skipIf(SKPT_TEST, 'To avoid excessive testing time caused by downloading models and '
+                     'to prevent OOM (Out of Memory) errors.')
     def test_deepseek_coder_template(self):
         model_type = ModelType.deepseek_coder_6_7b_instruct
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 256
         #
@@ -610,23 +520,19 @@
             {
                 'role': 'user',
                 'content': 'AAAAA'
             },
         ]
         example = messages_to_history(messages)
         input_ids_swift = template.encode(example)[0]['input_ids']
-        response, _ = inference(model, template, example['query'],
-                                example['history'])
+        response, _ = inference(model, template, example['query'], example['history'])
         print(f'swift response: {response}')
-        input_ids_official = tokenizer.apply_chat_template(
-            messages, tokenize=True, add_generation_prompt=True)
+        input_ids_official = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True)
         inputs = torch.tensor(input_ids_official, device='cuda')[None]
-        outputs = model.generate(
-            input_ids=inputs, eos_token_id=tokenizer.eos_token_id)
-        response = tokenizer.decode(
-            outputs[0, len(inputs[0]):], skip_special_tokens=True)
+        outputs = model.generate(input_ids=inputs, eos_token_id=tokenizer.eos_token_id)
+        response = tokenizer.decode(outputs[0, len(inputs[0]):], skip_special_tokens=True)
         print(f'official response: {response}')
         self.assertTrue(input_ids_swift == input_ids_official)
 
 
 if __name__ == '__main__':
     unittest.main()
```

### Comparing `ms-swift-2.0.3.post1/tests/llm/test_utils.py` & `ms-swift-2.0.4/tests/llm/test_utils.py`

 * *Files 2% similar despite different names*

```diff
@@ -1,90 +1,78 @@
 import os
 import unittest
 
-from swift.llm import (ModelType, get_default_template_type,
-                       get_model_tokenizer, get_template, inference,
+from swift.llm import (ModelType, get_default_template_type, get_model_tokenizer, get_template, inference,
                        inference_stream, limit_history_length, print_example)
 from swift.utils import lower_bound, seed_everything
 
 
 class TestLlmUtils(unittest.TestCase):
 
     def test_count_startswith(self):
         arr = [-100] * 1000 + list(range(1000))
-        self.assertTrue(
-            lower_bound(0, len(arr), lambda i: arr[i] != -100) == 1000)
+        self.assertTrue(lower_bound(0, len(arr), lambda i: arr[i] != -100) == 1000)
 
     def test_count_endswith(self):
         arr = list(range(1000)) + [-100] * 1000
-        self.assertTrue(
-            lower_bound(0, len(arr), lambda i: arr[i] == -100) == 1000)
+        self.assertTrue(lower_bound(0, len(arr), lambda i: arr[i] == -100) == 1000)
 
     def test_inference(self):
         model_type = ModelType.chatglm2_6b
         model, tokenizer = get_model_tokenizer(model_type)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
         model.generation_config.max_length = 128
         model.generation_config.do_sample = True
         for query in ['', 'hello']:
             seed_everything(42, True)
             print('stream=True')
-            gen_text_stream, history = inference(
-                model, template, query, stream=True, verbose=True)
+            gen_text_stream, history = inference(model, template, query, stream=True, verbose=True)
             print(f'[GEN]: {gen_text_stream}')
             print(f'[HISTORY]: {history}')
             #
             seed_everything(42, True)
             gen = inference_stream(model, template, query)
             for gen_text_stream2, history2 in gen:
                 pass
             print(f'[GEN]: {gen_text_stream2}')
             print(f'[HISTORY]: {history2}')
             #
             seed_everything(42, True)
             print('stream=False')
-            gen_text, history3 = inference(
-                model, template, query, stream=False, verbose=True)
+            gen_text, history3 = inference(model, template, query, stream=False, verbose=True)
             print(f'[GEN]: {gen_text}')
             print(f'[HISTORY]: {history3}')
             self.assertTrue(gen_text_stream == gen_text_stream2 == gen_text)
             self.assertTrue(history == history2 == history3)
 
     def test_print_example(self):
         input_ids = [1000, 2000, 3000, 4000, 5000, 6000]
-        _, tokenizer = get_model_tokenizer(
-            ModelType.chatglm3_6b, load_model=False)
+        _, tokenizer = get_model_tokenizer(ModelType.chatglm3_6b, load_model=False)
         from swift.llm.utils.utils import safe_tokenizer_decode
         labels = [-100, -100, 1000, 2000, 3000, -100, -100, 4000, 5000, 6000]
         print_example({'input_ids': input_ids, 'labels': labels}, tokenizer)
-        assert safe_tokenizer_decode(
-            tokenizer, labels
-        ) == '[-100 * 2]before States appe[-100 * 2]innov developingishes'
+        assert safe_tokenizer_decode(tokenizer, labels) == '[-100 * 2]before States appe[-100 * 2]innov developingishes'
         labels = [-100, -100, -100]
         print_example({'input_ids': input_ids, 'labels': labels}, tokenizer)
         assert safe_tokenizer_decode(tokenizer, labels) == '[-100 * 3]'
         labels = [1000, 2000, 3000, 4000, 5000, 6000]
         print_example({'input_ids': input_ids, 'labels': labels}, tokenizer)
-        assert safe_tokenizer_decode(
-            tokenizer, labels) == 'before States appe innov developingishes'
+        assert safe_tokenizer_decode(tokenizer, labels) == 'before States appe innov developingishes'
 
     def test_limit_history_length(self):
         model_type = ModelType.qwen_7b_chat
         _, tokenizer = get_model_tokenizer(model_type, load_model=False)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, tokenizer)
-        old_history, new_history = limit_history_length(
-            template, '' * 100, [], 128)
+        old_history, new_history = limit_history_length(template, '' * 100, [], 128)
         self.assertTrue(len(old_history) == 0 and len(new_history) == 0)
-        old_history, new_history = limit_history_length(
-            template, '' * 100, [], 256)
+        old_history, new_history = limit_history_length(template, '' * 100, [], 256)
         self.assertTrue(len(old_history) == 0 and len(new_history) == 0)
         self.assertTrue(len(tokenizer.encode('' * 100)))
-        old_history, new_history = limit_history_length(
-            template, '' * 100, [['' * 100, '' * 100] for i in range(5)],
-            600)
+        old_history, new_history = limit_history_length(template, '' * 100, [['' * 100, '' * 100] for i in range(5)],
+                                                        600)
         self.assertTrue(len(old_history) == 3 and len(new_history) == 2)
 
 
 if __name__ == '__main__':
     unittest.main()
```

### Comparing `ms-swift-2.0.3.post1/tests/llm/test_vllm_utils.py` & `ms-swift-2.0.4/tests/llm/test_vllm_utils.py`

 * *Files 1% similar despite different names*

```diff
@@ -15,16 +15,15 @@
     def test_inference_vllm(self):
         model_type = ModelType.qwen_7b_chat
         llm_engine = get_vllm_engine(model_type, torch.float16)
         template_type = get_default_template_type(model_type)
         template = get_template(template_type, llm_engine.hf_tokenizer)
         request_list = [{'query': ''}, {'query': '!'}]
         # test inference_vllm
-        response_list = inference_vllm(
-            llm_engine, template, request_list, verbose=True)
+        response_list = inference_vllm(llm_engine, template, request_list, verbose=True)
         for response in response_list:
             print(response)
 
         # test inference_stream_vllm
         gen = inference_stream_vllm(llm_engine, template, request_list)
         for response_list in gen:
             print(response_list[0]['response'], response_list[0]['history'])
```

### Comparing `ms-swift-2.0.3.post1/tests/model_tag.py` & `ms-swift-2.0.4/tests/model_tag.py`

 * *Files 3% similar despite different names*

```diff
@@ -31,19 +31,15 @@
 
         def __init__(self):
             self.result = 0
             self.name = ''
             self.info = ''
 
         def to_json(self):
-            return {
-                'name': self.name,
-                'result': self.result,
-                'info': self.info
-            }
+            return {'name': self.name, 'result': self.result, 'info': self.info}
 
     def __init__(self):
         self.job_name = ''
         self.job_id = ''
         self.model = ''
         self.sdk_version = ''
         self.image_version = ''
@@ -53,20 +49,16 @@
         self.stage = ''
         # ItemResult list
         self.item_result = []
 
     # 
     def _post_request(self, url, param):
         try:
-            logging.info(url + ' query: '
-                         + str(json.dumps(param, ensure_ascii=False)))
-            res = requests.post(
-                url=url,
-                headers=self.HEADER,
-                data=json.dumps(param, ensure_ascii=False).encode('utf8'))
+            logging.info(url + ' query: ' + str(json.dumps(param, ensure_ascii=False)))
+            res = requests.post(url=url, headers=self.HEADER, data=json.dumps(param, ensure_ascii=False).encode('utf8'))
             if res.status_code == 200:
                 logging.info(f'{url} post: ' + res.text)
                 res_json = json.loads(res.text)
                 if int(res_json['errorCode']) == 200:
                     return res_json['content']
                 else:
                     logging.error(res.text)
@@ -105,22 +97,18 @@
 
         return
 
     # 
     def batch_refresh_stage(self):
         try:
             param = {
-                'sdkVersion':
-                self.sdk_version,
-                'imageVersion':
-                self.image_version,
-                'source':
-                self.source,
-                'stage':
-                self.stage,
+                'sdkVersion': self.sdk_version,
+                'imageVersion': self.image_version,
+                'source': self.source,
+                'stage': self.stage,
                 'modelList': [{
                     'model': self.model,
                     'domain': self.domain,
                     'task': self.task
                 }]
             }
             return self._post_request(self.BATCH_REFRESH_STAGE_URL, param)
```

### Comparing `ms-swift-2.0.3.post1/tests/run.py` & `ms-swift-2.0.4/tests/run.py`

 * *Files identical despite different names*

### Comparing `ms-swift-2.0.3.post1/tests/test_utils.py` & `ms-swift-2.0.4/tests/test_utils.py`

 * *Files 9% similar despite different names*

```diff
@@ -63,18 +63,15 @@
     def __init__(self, feat, label, num) -> None:
         self.feat = feat
         self.label = label
         self.num = num
 
     def __getitem__(self, index):
         import torch
-        return {
-            'feat': torch.Tensor(self.feat),
-            'labels': torch.Tensor(self.label)
-        }
+        return {'feat': torch.Tensor(self.feat), 'labels': torch.Tensor(self.label)}
 
     def __len__(self):
         return self.num
 
 
 def create_dummy_test_dataset(feat, label, num):
     return DummyTorchDataset(feat, label, num)
@@ -96,16 +93,15 @@
     t.extractall(path=dst)
 
     return target_dir_path
 
 
 def get_case_model_info():
     status_code, result = subprocess.getstatusoutput(
-        'grep -rn "damo/" tests/  | grep -v ".pyc" | grep -v "Binary file" | grep -v run.py '
-    )
+        'grep -rn "damo/" tests/  | grep -v ".pyc" | grep -v "Binary file" | grep -v run.py ')
     lines = result.split('\n')
     test_cases = OrderedDict()
     model_cases = OrderedDict()
     for line in lines:
         # "tests/msdatasets/test_ms_dataset.py:92:        model_id = 'damo/bert-base-sst2'"
         line = line.strip()
         elements = line.split(':')
@@ -118,34 +114,25 @@
             test_cases[test_file] = set()
         model_info = test_cases[test_file]
         model_info.add(model_name)
 
         if model_name not in model_cases:
             model_cases[model_name] = set()
         case_info = model_cases[model_name]
-        case_info.add(
-            test_file.replace('tests/', '').replace('.py',
-                                                    '').replace('/', '.'))
+        case_info.add(test_file.replace('tests/', '').replace('.py', '').replace('/', '.'))
 
     return model_cases
 
 
-def compare_arguments_nested(print_content,
-                             arg1,
-                             arg2,
-                             rtol=1.e-3,
-                             atol=1.e-8,
-                             ignore_unknown_type=True):
+def compare_arguments_nested(print_content, arg1, arg2, rtol=1.e-3, atol=1.e-8, ignore_unknown_type=True):
     type1 = type(arg1)
     type2 = type(arg2)
     if type1.__name__ != type2.__name__:
         if print_content is not None:
-            print(
-                f'{print_content}, type not equal:{type1.__name__} and {type2.__name__}'
-            )
+            print(f'{print_content}, type not equal:{type1.__name__} and {type2.__name__}')
         return False
 
     if arg1 is None:
         return True
     elif isinstance(arg1, (int, str, bool, np.bool_, np.integer, np.str_)):
         if arg1 != arg2:
             if print_content is not None:
@@ -157,55 +144,44 @@
             if print_content is not None:
                 print(f'{print_content}, arg1:{arg1}, arg2:{arg2}')
             return False
         return True
     elif isinstance(arg1, (tuple, list)):
         if len(arg1) != len(arg2):
             if print_content is not None:
-                print(
-                    f'{print_content}, length is not equal:{len(arg1)}, {len(arg2)}'
-                )
+                print(f'{print_content}, length is not equal:{len(arg1)}, {len(arg2)}')
             return False
         if not all([
-                compare_arguments_nested(
-                    None, sub_arg1, sub_arg2, rtol=rtol, atol=atol)
+                compare_arguments_nested(None, sub_arg1, sub_arg2, rtol=rtol, atol=atol)
                 for sub_arg1, sub_arg2 in zip(arg1, arg2)
         ]):
             if print_content is not None:
                 print(f'{print_content}')
             return False
         return True
     elif isinstance(arg1, Mapping):
         keys1 = arg1.keys()
         keys2 = arg2.keys()
         if len(keys1) != len(keys2):
             if print_content is not None:
-                print(
-                    f'{print_content}, key length is not equal:{len(keys1)}, {len(keys2)}'
-                )
+                print(f'{print_content}, key length is not equal:{len(keys1)}, {len(keys2)}')
             return False
         if len(set(keys1) - set(keys2)) > 0:
             if print_content is not None:
                 print(f'{print_content}, key diff:{set(keys1) - set(keys2)}')
             return False
-        if not all([
-                compare_arguments_nested(
-                    None, arg1[key], arg2[key], rtol=rtol, atol=atol)
-                for key in keys1
-        ]):
+        if not all([compare_arguments_nested(None, arg1[key], arg2[key], rtol=rtol, atol=atol) for key in keys1]):
             if print_content is not None:
                 print(f'{print_content}')
             return False
         return True
     elif isinstance(arg1, np.ndarray):
         arg1 = np.where(np.equal(arg1, None), np.NaN, arg1).astype(dtype=float)
         arg2 = np.where(np.equal(arg2, None), np.NaN, arg2).astype(dtype=float)
-        if not all(
-                np.isclose(arg1, arg2, rtol=rtol, atol=atol,
-                           equal_nan=True).flatten()):
+        if not all(np.isclose(arg1, arg2, rtol=rtol, atol=atol, equal_nan=True).flatten()):
             if print_content is not None:
                 print(f'{print_content}')
             return False
         return True
     else:
         if ignore_unknown_type:
             return True
@@ -273,22 +249,15 @@
         >>>             num_gpus=2,
         >>>             assert_callback=lambda x: self.assertEqual(x, 3.0),
         >>>             *args,
         >>>             **kwargs,
         >>>         )
     """
 
-    def _start(self,
-               dist_start_cmd,
-               func,
-               num_gpus,
-               assert_callback=None,
-               save_all_ranks=False,
-               *args,
-               **kwargs):
+    def _start(self, dist_start_cmd, func, num_gpus, assert_callback=None, save_all_ranks=False, *args, **kwargs):
         script_path = func.__code__.co_filename
         script_dir, script_name = os.path.split(script_path)
         script_name = os.path.splitext(script_name)[0]
         func_name = func.__qualname__
 
         func_params = []
         for arg in args:
@@ -305,62 +274,49 @@
 
         tmp_run_file = tempfile.NamedTemporaryFile(suffix='.py').name
         tmp_res_file = tempfile.NamedTemporaryFile(suffix='.pkl').name
 
         with open(tmp_run_file, 'w') as f:
             print('save temporary run file to : {}'.format(tmp_run_file))
             print('save results to : {}'.format(tmp_res_file))
-            run_file_content = _DIST_SCRIPT_TEMPLATE.format(
-                script_name, script_name, func_name, func_params)
+            run_file_content = _DIST_SCRIPT_TEMPLATE.format(script_name, script_name, func_name, func_params)
             f.write(run_file_content)
 
         tmp_res_files = []
         if save_all_ranks:
             for i in range(num_gpus):
                 tmp_res_files.append(tmp_res_file + str(i))
         else:
             tmp_res_files = [tmp_res_file]
         self.addCleanup(self.clean_tmp, [tmp_run_file] + tmp_res_files)
 
         tmp_env = copy.deepcopy(os.environ)
-        tmp_env['PYTHONPATH'] = ':'.join(
-            (tmp_env.get('PYTHONPATH', ''), script_dir)).lstrip(':')
+        tmp_env['PYTHONPATH'] = ':'.join((tmp_env.get('PYTHONPATH', ''), script_dir)).lstrip(':')
         # avoid distributed test hang
         tmp_env['NCCL_P2P_DISABLE'] = '1'
-        script_params = '--save_all_ranks=%s --save_file=%s' % (save_all_ranks,
-                                                                tmp_res_file)
+        script_params = '--save_all_ranks=%s --save_file=%s' % (save_all_ranks, tmp_res_file)
         script_cmd = '%s %s %s' % (dist_start_cmd, tmp_run_file, script_params)
         print('script command: %s' % script_cmd)
         res = subprocess.call(script_cmd, shell=True, env=tmp_env)
 
         script_res = []
         for res_file in tmp_res_files:
             with open(res_file, 'rb') as f:
                 script_res.append(pickle.load(f))
         if not save_all_ranks:
             script_res = script_res[0]
 
         if assert_callback:
             assert_callback(script_res)
 
-        self.assertEqual(
-            res,
-            0,
-            msg='The test function ``{}`` in ``{}`` run failed!'.format(
-                func_name, script_name))
+        self.assertEqual(res, 0, msg='The test function ``{}`` in ``{}`` run failed!'.format(func_name, script_name))
 
         return script_res
 
-    def start(self,
-              func,
-              num_gpus,
-              assert_callback=None,
-              save_all_ranks=False,
-              *args,
-              **kwargs):
+    def start(self, func, num_gpus, assert_callback=None, save_all_ranks=False, *args, **kwargs):
         from .torch_utils import _find_free_port
         ip = socket.gethostbyname(socket.gethostname())
         if 'dist_start_cmd' in kwargs:
             dist_start_cmd = kwargs.pop('dist_start_cmd')
         else:
             dist_start_cmd = '%s -m torch.distributed.launch --nproc_per_node=%d ' \
                              '--master_addr=\'%s\' --master_port=%s' % (sys.executable, num_gpus, ip, _find_free_port())
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_extra_state_dict.py` & `ms-swift-2.0.4/tests/tuners/test_extra_state_dict.py`

 * *Files 14% similar despite different names*

```diff
@@ -19,78 +19,45 @@
             os.makedirs(self.tmp_dir)
 
     def tearDown(self):
         shutil.rmtree(self.tmp_dir)
         super().tearDown()
 
     def test_swift_extra_state_dict(self):
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
-        model = Swift.prepare_model(
-            model, lora_config, extra_state_keys=['classifier.*'])
+        model = Swift.prepare_model(model, lora_config, extra_state_keys=['classifier.*'])
         model.save_pretrained(self.tmp_dir)
-        self.assertTrue(
-            os.path.isfile(
-                os.path.join(self.tmp_dir, 'extra_states',
-                             'adapter_model.bin')))
-        state_dict = torch.load(
-            os.path.join(self.tmp_dir, 'extra_states', 'adapter_model.bin'))
+        self.assertTrue(os.path.isfile(os.path.join(self.tmp_dir, 'extra_states', 'adapter_model.bin')))
+        state_dict = torch.load(os.path.join(self.tmp_dir, 'extra_states', 'adapter_model.bin'))
         self.assertTrue(any('classifier' in key for key in state_dict))
-        state_dict['classifier.weight'] = torch.ones_like(
-            state_dict['classifier.weight']) * 2.0
-        with open(
-                os.path.join(self.tmp_dir, 'extra_states',
-                             'adapter_model.bin'), 'wb') as f:
+        state_dict['classifier.weight'] = torch.ones_like(state_dict['classifier.weight']) * 2.0
+        with open(os.path.join(self.tmp_dir, 'extra_states', 'adapter_model.bin'), 'wb') as f:
             torch.save(state_dict, f)
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         model = Swift.from_pretrained(model, self.tmp_dir)
-        names = [
-            name for name, value in model.named_parameters()
-            if value.requires_grad
-        ]
+        names = [name for name, value in model.named_parameters() if value.requires_grad]
         self.assertTrue(any('classifier' in name for name in names))
-        self.assertTrue(
-            torch.allclose(state_dict['classifier.weight'],
-                           model.base_model.classifier.weight))
+        self.assertTrue(torch.allclose(state_dict['classifier.weight'], model.base_model.classifier.weight))
 
     def test_swift_modules_to_save(self):
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        lora_config = LoRAConfig(
-            target_modules=['query', 'key', 'value'],
-            modules_to_save=['classifier'])
-        lora_config2 = LoRAConfig(
-            target_modules=['query', 'key', 'value'],
-            modules_to_save=['classifier'])
-        model = Swift.prepare_model(model, {
-            'lora1': lora_config,
-            'lora2': lora_config2
-        })
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        lora_config = LoRAConfig(target_modules=['query', 'key', 'value'], modules_to_save=['classifier'])
+        lora_config2 = LoRAConfig(target_modules=['query', 'key', 'value'], modules_to_save=['classifier'])
+        model = Swift.prepare_model(model, {'lora1': lora_config, 'lora2': lora_config2})
         model.set_active_adapters('lora1')
         model.set_active_adapters('lora2')
         self.assertTrue(isinstance(model.classifier, ModulesToSaveWrapper))
         self.assertTrue(model.classifier.active_adapter == 'lora2')
         model.save_pretrained(self.tmp_dir)
-        state_dict = torch.load(
-            os.path.join(self.tmp_dir, 'lora2', 'adapter_model.bin'))
+        state_dict = torch.load(os.path.join(self.tmp_dir, 'lora2', 'adapter_model.bin'))
         self.assertTrue(any('classifier' in key for key in state_dict))
-        state_dict['classifier.weight'] = torch.ones_like(
-            state_dict['classifier.weight']) * 2.0
-        with open(
-                os.path.join(self.tmp_dir, 'lora2', 'adapter_model.bin'),
-                'wb') as f:
+        state_dict['classifier.weight'] = torch.ones_like(state_dict['classifier.weight']) * 2.0
+        with open(os.path.join(self.tmp_dir, 'lora2', 'adapter_model.bin'), 'wb') as f:
             torch.save(state_dict, f)
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        model = Swift.from_pretrained(
-            model, self.tmp_dir, adapter_name='lora2')
-        names = [
-            name for name, value in model.named_parameters()
-            if value.requires_grad
-        ]
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Swift.from_pretrained(model, self.tmp_dir, adapter_name='lora2')
+        names = [name for name, value in model.named_parameters() if value.requires_grad]
         self.assertTrue(any('classifier' in name for name in names))
         self.assertTrue(
-            torch.allclose(
-                state_dict['classifier.weight'],
-                model.base_model.classifier.modules_to_save['lora2'].weight))
+            torch.allclose(state_dict['classifier.weight'],
+                           model.base_model.classifier.modules_to_save['lora2'].weight))
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_merged_linear.py` & `ms-swift-2.0.4/tests/tuners/test_merged_linear.py`

 * *Files 13% similar despite different names*

```diff
@@ -19,35 +19,26 @@
             if hasattr(self, 'lora_A'):
                 # initialize A the same way as the default for nn.Linear and B to zero
                 nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))
                 nn.init.ones_(self.lora_B)
 
         MergedLinear.reset_parameters = reset_parameters
 
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         lora_config = LoRAConfig(
-            target_modules=['query', 'key', 'value'],
-            use_merged_linear=True,
-            enable_lora=[True, True, True])
+            target_modules=['query', 'key', 'value'], use_merged_linear=True, enable_lora=[True, True, True])
         outputs = model(**inputs)
         model = Swift.prepare_model(model, config=lora_config)
         model.eval()
         outputs_lora = model(**inputs)
         model.deactivate_adapter('default')
         outputs_deactivate = model(**inputs)
         model.activate_adapter('default')
         outputs_reactivate = model(**inputs)
         Swift.merge_and_unload(model)
         outputs_merged = model(**inputs)
-        self.assertTrue(
-            torch.allclose(outputs.logits, outputs_deactivate.logits))
-        self.assertTrue(
-            not torch.allclose(outputs.logits, outputs_lora.logits))
-        self.assertTrue(
-            torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
-        self.assertTrue(
-            torch.allclose(
-                outputs_lora.logits, outputs_merged.logits, atol=1e-4))
+        self.assertTrue(torch.allclose(outputs.logits, outputs_deactivate.logits))
+        self.assertTrue(not torch.allclose(outputs.logits, outputs_lora.logits))
+        self.assertTrue(torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
+        self.assertTrue(torch.allclose(outputs_lora.logits, outputs_merged.logits, atol=1e-4))
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_neft.py` & `ms-swift-2.0.4/tests/tuners/test_neft.py`

 * *Files 3% similar despite different names*

```diff
@@ -22,16 +22,15 @@
 
     def tearDown(self):
         shutil.rmtree(self.tmp_dir)
         super().tearDown()
 
     def test_neft(self):
         model = AutoModel.from_pretrained('AI-ModelScope/bert-base-uncased')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         config = NEFTuneConfig()
 
         t1 = model.embeddings.word_embeddings(inputs['input_ids'])
         model = Swift.prepare_model(model, config)
         model.train()
         t2 = model.embeddings.word_embeddings(inputs['input_ids'])
@@ -45,44 +44,36 @@
         model2 = AutoModel.from_pretrained(self.tmp_dir)
 
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         self.assertTrue(len(state_dict) > 0)
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
         shutil.rmtree(self.tmp_dir)
         PreTrainedModel.origin_save_pretrained = PreTrainedModel.save_pretrained
         delattr(PreTrainedModel, 'save_pretrained')
         model.save_pretrained(self.tmp_dir)
         bin_file = os.path.join(self.tmp_dir, WEIGHTS_NAME)
         self.assertTrue(os.path.isfile(bin_file))
-        model_new = AutoModel.from_pretrained(
-            'AI-ModelScope/bert-base-uncased')
+        model_new = AutoModel.from_pretrained('AI-ModelScope/bert-base-uncased')
         model_new_2 = Swift.from_pretrained(model_new, self.tmp_dir)
 
         state_dict = model.state_dict()
         state_dict2 = model_new_2.state_dict()
         self.assertTrue(len(state_dict) > 0)
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
         PreTrainedModel.save_pretrained = PreTrainedModel.origin_save_pretrained
 
     def test_neft_lora(self):
         model = AutoModel.from_pretrained('AI-ModelScope/bert-base-uncased')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         config = NEFTuneConfig()
         config2 = LoRAConfig(target_modules=['query', 'key', 'value'])
 
         t1 = model.embeddings.word_embeddings(inputs['input_ids'])
         model = Swift.prepare_model(model, {'c1': config, 'c2': config2})
         model.train()
@@ -92,16 +83,15 @@
         self.assertTrue(torch.allclose(t1, t3))
         self.assertFalse(torch.allclose(t1, t2))
         model.save_pretrained(self.tmp_dir)
         bin_file = os.path.join(self.tmp_dir, 'c2', WEIGHTS_NAME)
         self.assertTrue(os.path.isfile(bin_file))
         bin_file = os.path.join(self.tmp_dir, 'c1', WEIGHTS_NAME)
         self.assertTrue(not os.path.isfile(bin_file))
-        model_new = AutoModel.from_pretrained(
-            'AI-ModelScope/bert-base-uncased')
+        model_new = AutoModel.from_pretrained('AI-ModelScope/bert-base-uncased')
         t1 = model_new.embeddings.word_embeddings(inputs['input_ids'])
         model_new = Swift.from_pretrained(model_new, self.tmp_dir)
         model_new.train()
         t2 = model_new.embeddings.word_embeddings(inputs['input_ids'])
         model_new.eval()
         t4 = model_new.embeddings.word_embeddings(inputs['input_ids'])
         model_new.train()
@@ -109,16 +99,11 @@
         t3 = model_new.embeddings.word_embeddings(inputs['input_ids'])
         self.assertTrue(torch.allclose(t1, t3))
         self.assertTrue(torch.allclose(t1, t4))
         self.assertFalse(torch.allclose(t1, t2))
 
         state_dict = model.state_dict()
         state_dict2 = model_new.state_dict()
-        self.assertTrue(
-            len(state_dict) > 0
-            and all(['lora' in key for key in state_dict.keys()]))
+        self.assertTrue(len(state_dict) > 0 and all(['lora' in key for key in state_dict.keys()]))
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_peft.py` & `ms-swift-2.0.4/tests/tuners/test_peft.py`

 * *Files 6% similar despite different names*

```diff
@@ -3,16 +3,15 @@
 import shutil
 import tempfile
 import unittest
 
 import peft
 import torch
 from modelscope import Preprocessor
-from modelscope.models.nlp.structbert import (SbertConfig,
-                                              SbertForSequenceClassification)
+from modelscope.models.nlp.structbert import SbertConfig, SbertForSequenceClassification
 from peft import PeftModel, inject_adapter_in_model
 from peft.config import PeftConfigMixin
 from peft.tuners.lora import Linear
 from peft.utils import WEIGHTS_NAME
 from torch import nn
 
 from swift import AdaLoraConfig, LoraConfig, LoRAConfig, Swift, get_peft_model
@@ -34,25 +33,21 @@
         model = SbertForSequenceClassification(SbertConfig())
         model2 = copy.deepcopy(model)
         lora_config = LoraConfig(target_modules=['query', 'key', 'value'])
         model = Swift.prepare_model(model, lora_config)
         model.save_pretrained(self.tmp_dir, safe_serialization=False)
         with open(os.path.join(self.tmp_dir, 'configuration.json'), 'w') as f:
             f.write('{}')
-        self.assertTrue(
-            os.path.exists(os.path.join(self.tmp_dir, WEIGHTS_NAME)))
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, WEIGHTS_NAME)))
         model2 = Swift.from_pretrained(model2, self.tmp_dir)
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
     @unittest.skip
     def test_lora_merge(self):
 
         def reset_lora_parameters(self, adapter_name, init_lora_weights):
             if init_lora_weights is False:
                 return
@@ -62,134 +57,104 @@
             elif adapter_name == 'second':
                 ratio = 2.0
             else:
                 ratio = 3.0
 
             if adapter_name in self.lora_A.keys():
                 nn.init.ones_(self.lora_A[adapter_name].weight)
-                self.lora_A[adapter_name].weight.data = self.lora_A[
-                    adapter_name].weight.data * ratio
+                self.lora_A[adapter_name].weight.data = self.lora_A[adapter_name].weight.data * ratio
                 nn.init.ones_(self.lora_B[adapter_name].weight)
 
         Linear.reset_lora_parameters = reset_lora_parameters
 
         model = SbertForSequenceClassification(SbertConfig())
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
         model = Swift.prepare_model(model, lora_config)
         lora_config2 = LoRAConfig(target_modules=['query', 'key', 'value'])
         model = Swift.prepare_model(model, {'second': lora_config2})
         model.add_weighted_adapter(['default', 'second'],
                                    weights=[0.7, 0.3],
                                    adapter_name='test',
                                    combination_type='cat')
-        self.assertTrue(model.base_model.bert.encoder.layer[0].attention.self.
-                        key.active_adapter == ['test'])
+        self.assertTrue(model.base_model.bert.encoder.layer[0].attention.self.key.active_adapter == ['test'])
 
         model2 = SbertForSequenceClassification(SbertConfig())
         lora_config = LoraConfig(target_modules=['query', 'key', 'value'])
         model2 = get_peft_model(model2, lora_config)
         lora_config2 = LoraConfig(target_modules=['query', 'key', 'value'])
         inject_adapter_in_model(lora_config2, model2, adapter_name='second')
         model2.add_weighted_adapter(['default', 'second'],
                                     weights=[0.7, 0.3],
                                     adapter_name='test',
                                     combination_type='cat')
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
-        state_dict2 = {
-            key[len('base_model.model.'):]: value
-            for key, value in state_dict2.items() if 'lora' in key
-        }
+        state_dict2 = {key[len('base_model.model.'):]: value for key, value in state_dict2.items() if 'lora' in key}
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         print(model(**inputs))
         model.save_pretrained(self.tmp_dir)
         model3 = SbertForSequenceClassification(SbertConfig())
         model3 = Swift.from_pretrained(model3, self.tmp_dir)
         state_dict3 = model3.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict3)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict3[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict3[key]).flatten().detach().cpu()))
 
     def test_lora_reload_by_peft(self):
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
         model = SbertForSequenceClassification(SbertConfig())
         model2 = copy.deepcopy(model)
         model = Swift.prepare_model(model, lora_config)
         model.save_pretrained(self.tmp_dir, peft_format=True)
         model2 = PeftModel.from_pretrained(model2, self.tmp_dir)
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
-        state_dict2 = {
-            key[len('base_model.model.'):]: value
-            for key, value in state_dict2.items() if 'lora' in key
-        }
+        state_dict2 = {key[len('base_model.model.'):]: value for key, value in state_dict2.items() if 'lora' in key}
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
     def test_peft_adalora_injection(self):
         model = SbertForSequenceClassification(SbertConfig())
         model2 = copy.deepcopy(model)
-        adalora_config = AdaLoraConfig(
-            target_modules=['query', 'key', 'value'])
+        adalora_config = AdaLoraConfig(target_modules=['query', 'key', 'value'])
         model = Swift.prepare_model(model, adalora_config)
         model.save_pretrained(self.tmp_dir, safe_serialization=False)
         with open(os.path.join(self.tmp_dir, 'configuration.json'), 'w') as f:
             f.write('{}')
-        self.assertTrue(
-            os.path.exists(os.path.join(self.tmp_dir, WEIGHTS_NAME)))
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, WEIGHTS_NAME)))
         model2 = Swift.from_pretrained(model2, self.tmp_dir)
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
     @unittest.skip
     def test_peft_lora_dtype(self):
         model = SbertForSequenceClassification(SbertConfig())
         model2 = copy.deepcopy(model)
         model3 = copy.deepcopy(model)
-        lora_config = LoraConfig(
-            target_modules=['query', 'key', 'value'], lora_dtype='fp16')
+        lora_config = LoraConfig(target_modules=['query', 'key', 'value'], lora_dtype='fp16')
         model = Swift.prepare_model(model, lora_config)
         model.save_pretrained(self.tmp_dir, safe_serialization=False)
-        self.assertTrue(
-            os.path.exists(
-                os.path.join(self.tmp_dir, 'additional_config.json')))
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'additional_config.json')))
         model2 = Swift.from_pretrained(model2, self.tmp_dir)
-        self.assertTrue(model2.base_model.model.bert.encoder.layer[0].attention
-                        .self.key.lora_A.default.weight.dtype == torch.float16)
+        self.assertTrue(model2.base_model.model.bert.encoder.layer[0].attention.self.key.lora_A.default.weight.dtype ==
+                        torch.float16)
         self.assertTrue(model2.peft_config['default'].lora_dtype == 'fp16')
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
         PeftConfigMixin.from_pretrained = PeftConfigMixin.from_pretrained_origin
         model3 = Swift.from_pretrained(model3, self.tmp_dir)
-        self.assertTrue(model3.base_model.model.bert.encoder.layer[0].attention
-                        .self.key.lora_A.default.weight.dtype == torch.float32)
-        self.assertTrue(
-            isinstance(model3.peft_config['default'], peft.LoraConfig))
+        self.assertTrue(model3.base_model.model.bert.encoder.layer[0].attention.self.key.lora_A.default.weight.dtype ==
+                        torch.float32)
+        self.assertTrue(isinstance(model3.peft_config['default'], peft.LoraConfig))
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_rome.py` & `ms-swift-2.0.4/tests/tuners/test_rome.py`

 * *Files 14% similar despite different names*

```diff
@@ -18,46 +18,35 @@
         if not os.path.exists(self.tmp_dir):
             os.makedirs(self.tmp_dir)
 
     def tearDown(self):
         shutil.rmtree(self.tmp_dir)
         super().tearDown()
 
-    @unittest.skip(
-        'Rome test is skipped because the test image do not have flash-attn2')
+    @unittest.skip('Rome test is skipped because the test image do not have flash-attn2')
     def test_rome(self):
-        model = Model.from_pretrained(
-            'modelscope/Llama-2-7b-ms',
-            device_map='auto',
-            trust_remote_code=True)
-        tokenizer = AutoTokenizer.from_pretrained(
-            'modelscope/Llama-2-7b-ms', trust_remote_code=True)
+        model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto', trust_remote_code=True)
+        tokenizer = AutoTokenizer.from_pretrained('modelscope/Llama-2-7b-ms', trust_remote_code=True)
         request = [{
             'prompt': '{} was the founder of',
             'subject': 'Steve Jobs',
             'target': 'Microsoft',
         }]
         config = RomeConfig(
             model_type='llama-7b',
             knowledge=request,
             tokenizer=tokenizer,
         )
 
         model = Swift.prepare_model(model, config)
         prompt = 'Steve Jobs was the founder of'
-        inp_tok = tokenizer(
-            prompt, return_token_type_ids=False, return_tensors='pt')
+        inp_tok = tokenizer(prompt, return_token_type_ids=False, return_tensors='pt')
         for key, value in inp_tok.items():
             inp_tok[key] = value.to('cuda')
         with torch.no_grad():
-            generated_ids = model.generate(
-                **inp_tok,
-                temperature=0.1,
-                top_k=50,
-                max_length=128,
-                do_sample=True)
+            generated_ids = model.generate(**inp_tok, temperature=0.1, top_k=50, max_length=128, do_sample=True)
 
         responses = tokenizer.batch_decode(
             generated_ids[:, inp_tok['input_ids'].size(1):],
             skip_special_tokens=True,
             clean_up_tokenization_spaces=True)
         self.assertTrue('Microsoft' in responses[0])
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_scetuning.py` & `ms-swift-2.0.4/tests/tuners/test_scetuning.py`

 * *Files 7% similar despite different names*

```diff
@@ -21,41 +21,32 @@
         shutil.rmtree(self.tmp_dir)
         super().tearDown()
 
     def model_comparison(self, model, model2):
         model_key = list(model.state_dict().keys())
         model2_key = list(model2.state_dict().keys())
         self.assertTrue(model_key == model2_key)
-        model_val = torch.sum(
-            torch.stack(
-                [torch.sum(val) for val in model.state_dict().values()]))
-        model2_val = torch.sum(
-            torch.stack(
-                [torch.sum(val) for val in model2.state_dict().values()]))
+        model_val = torch.sum(torch.stack([torch.sum(val) for val in model.state_dict().values()]))
+        model2_val = torch.sum(torch.stack([torch.sum(val) for val in model2.state_dict().values()]))
         self.assertTrue(torch.isclose(model_val, model2_val))
 
     def test_scetuning_on_diffusers_v1(self):
         model_dir = snapshot_download('AI-ModelScope/stable-diffusion-v1-5')
         from diffusers import UNet2DConditionModel
-        model = UNet2DConditionModel.from_pretrained(
-            model_dir, subfolder='unet')
+        model = UNet2DConditionModel.from_pretrained(model_dir, subfolder='unet')
         model.requires_grad_(False)
         model_check = copy.deepcopy(model)
         # module_keys = [key for key, _ in model.named_modules()]
         scetuning_config = SCETuningConfig(
-            dims=[
-                320, 320, 320, 320, 640, 640, 640, 1280, 1280, 1280, 1280, 1280
-            ],
+            dims=[320, 320, 320, 320, 640, 640, 640, 1280, 1280, 1280, 1280, 1280],
             tuner_mode='encoder',
             target_modules=[
-                'conv_in', 'down_blocks.0.attentions.0',
-                'down_blocks.0.attentions.1', 'down_blocks.0.downsamplers',
-                'down_blocks.1.attentions.0', 'down_blocks.1.attentions.1',
-                'down_blocks.1.downsamplers', 'down_blocks.2.attentions.0',
-                'down_blocks.2.attentions.1', 'down_blocks.2.downsamplers',
+                'conv_in', 'down_blocks.0.attentions.0', 'down_blocks.0.attentions.1', 'down_blocks.0.downsamplers',
+                'down_blocks.1.attentions.0', 'down_blocks.1.attentions.1', 'down_blocks.1.downsamplers',
+                'down_blocks.2.attentions.0', 'down_blocks.2.attentions.1', 'down_blocks.2.downsamplers',
                 'down_blocks.3.resnets.0', 'down_blocks.3.resnets.1'
             ])
         model = Swift.prepare_model(model, config=scetuning_config)
         print(model.get_trainable_parameters())
         input_data = {
             'sample': torch.ones((1, 4, 64, 64)),
             'timestep': 10,
@@ -67,31 +58,25 @@
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default')))
         model_check = Swift.from_pretrained(model_check, self.tmp_dir)
         self.model_comparison(model, model_check)
 
     def test_scetuning_on_diffusers_v2(self):
         model_dir = snapshot_download('AI-ModelScope/stable-diffusion-v1-5')
         from diffusers import UNet2DConditionModel
-        model = UNet2DConditionModel.from_pretrained(
-            model_dir, subfolder='unet')
+        model = UNet2DConditionModel.from_pretrained(model_dir, subfolder='unet')
         model.requires_grad_(False)
         model_check = copy.deepcopy(model)
         # module_keys = [key for key, _ in model.named_modules()]
         scetuning_config = SCETuningConfig(
-            dims=[
-                1280, 1280, 1280, 1280, 1280, 640, 640, 640, 320, 320, 320, 320
-            ],
+            dims=[1280, 1280, 1280, 1280, 1280, 640, 640, 640, 320, 320, 320, 320],
             tuner_mode='decoder',
             target_modules=[
-                'up_blocks.0.resnets.0', 'up_blocks.0.resnets.1',
-                'up_blocks.0.resnets.2', 'up_blocks.1.resnets.0',
-                'up_blocks.1.resnets.1', 'up_blocks.1.resnets.2',
-                'up_blocks.2.resnets.0', 'up_blocks.2.resnets.1',
-                'up_blocks.2.resnets.2', 'up_blocks.3.resnets.0',
-                'up_blocks.3.resnets.1', 'up_blocks.3.resnets.2'
+                'up_blocks.0.resnets.0', 'up_blocks.0.resnets.1', 'up_blocks.0.resnets.2', 'up_blocks.1.resnets.0',
+                'up_blocks.1.resnets.1', 'up_blocks.1.resnets.2', 'up_blocks.2.resnets.0', 'up_blocks.2.resnets.1',
+                'up_blocks.2.resnets.2', 'up_blocks.3.resnets.0', 'up_blocks.3.resnets.1', 'up_blocks.3.resnets.2'
             ])
         model = Swift.prepare_model(model, config=scetuning_config)
         print(model.get_trainable_parameters())
         input_data = {
             'sample': torch.ones((1, 4, 64, 64)),
             'timestep': 10,
             'encoder_hidden_states': torch.ones((1, 77, 768))
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_swift_base.py` & `ms-swift-2.0.4/tests/tuners/test_swift_base.py`

 * *Files 8% similar despite different names*

```diff
@@ -5,23 +5,21 @@
 import tempfile
 import unittest
 from concurrent.futures import ThreadPoolExecutor
 
 import peft
 import torch
 from modelscope import Model, Preprocessor
-from modelscope.models.nlp.structbert import (SbertConfig,
-                                              SbertForSequenceClassification)
+from modelscope.models.nlp.structbert import SbertConfig, SbertForSequenceClassification
 from packaging import version
 from peft import PeftModel
 from peft.utils import WEIGHTS_NAME
 from torch import nn
 
-from swift import (AdapterConfig, LoRAConfig, PromptConfig, ResTuningConfig,
-                   SideConfig, Swift, SwiftModel)
+from swift import AdapterConfig, LoRAConfig, PromptConfig, ResTuningConfig, SideConfig, Swift, SwiftModel
 
 
 class TestSwift(unittest.TestCase):
 
     def setUp(self):
         print(('Testing %s.%s' % (type(self).__name__, self._testMethodName)))
         self.tmp_dir = tempfile.TemporaryDirectory().name
@@ -40,107 +38,83 @@
             if init_lora_weights is False:
                 return
 
             if adapter_name in self.lora_A.keys():
                 if init_lora_weights is True:
                     # initialize A the same way as the default for nn.Linear and B to zero
                     # https://github.com/microsoft/LoRA/blob/a0a92e0f26c067cf94747bdbf1ce73793fa44d19/loralib/layers.py#L124
-                    nn.init.kaiming_uniform_(
-                        self.lora_A[adapter_name].weight, a=math.sqrt(5))
+                    nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))
                 elif init_lora_weights.lower() == 'gaussian':
-                    nn.init.normal_(
-                        self.lora_A[adapter_name].weight,
-                        std=1 / self.r[adapter_name])
+                    nn.init.normal_(self.lora_A[adapter_name].weight, std=1 / self.r[adapter_name])
                 else:
-                    raise ValueError(
-                        f'Unknown initialization {init_lora_weights=}')
+                    raise ValueError(f'Unknown initialization {init_lora_weights=}')
                 nn.init.ones_(self.lora_B[adapter_name].weight)
             if adapter_name in self.lora_embedding_A.keys():
                 # initialize a the same way as the default for nn.linear and b to zero
                 nn.init.ones_(self.lora_embedding_A[adapter_name])
                 nn.init.normal_(self.lora_embedding_B[adapter_name])
 
         Linear.reset_lora_parameters = reset_lora_parameters
 
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
         outputs = model(**inputs)
         model = Swift.prepare_model(model, config=lora_config)
         model.eval()
         outputs_lora = model(**inputs)
         model.deactivate_adapter('default')
         outputs_deactivate = model(**inputs)
         model.activate_adapter('default')
         outputs_reactivate = model(**inputs)
-        self.assertTrue(
-            torch.allclose(outputs.logits, outputs_deactivate.logits))
-        self.assertTrue(
-            not torch.allclose(outputs.logits, outputs_lora.logits))
-        self.assertTrue(
-            torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
+        self.assertTrue(torch.allclose(outputs.logits, outputs_deactivate.logits))
+        self.assertTrue(not torch.allclose(outputs.logits, outputs_lora.logits))
+        self.assertTrue(torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
 
     def test_swift_adapter_forward(self):
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         adapter_config = AdapterConfig(
             dim=model.config.hidden_size,
             target_modules=r'.*layer\.\d+$',
             method_name='feed_forward_chunk',
             hidden_pos=0)
         outputs = model(**inputs)
         model = Swift.prepare_model(model, config=adapter_config)
         outputs_lora = model(**inputs)
         model.deactivate_adapter('default')
         outputs_deactivate = model(**inputs)
         model.activate_adapter('default')
         outputs_reactivate = model(**inputs)
-        self.assertTrue(
-            torch.allclose(outputs.logits, outputs_deactivate.logits))
-        self.assertTrue(
-            not torch.allclose(outputs.logits, outputs_lora.logits))
-        self.assertTrue(
-            torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
+        self.assertTrue(torch.allclose(outputs.logits, outputs_deactivate.logits))
+        self.assertTrue(not torch.allclose(outputs.logits, outputs_lora.logits))
+        self.assertTrue(torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
 
     def test_swift_prompt_forward(self):
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         prompt_config = PromptConfig(
-            dim=model.config.hidden_size,
-            target_modules=r'.*layer\.\d+$',
-            embedding_pos=0,
-            attention_mask_pos=1)
+            dim=model.config.hidden_size, target_modules=r'.*layer\.\d+$', embedding_pos=0, attention_mask_pos=1)
         outputs = model(**inputs)
         model = Swift.prepare_model(model, config=prompt_config)
         outputs_lora = model(**inputs)
         model.deactivate_adapter('default')
         outputs_deactivate = model(**inputs)
         model.activate_adapter('default')
         outputs_reactivate = model(**inputs)
-        self.assertTrue(
-            torch.allclose(outputs.logits, outputs_deactivate.logits))
-        self.assertTrue(
-            not torch.allclose(outputs.logits, outputs_lora.logits))
-        self.assertTrue(
-            torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
+        self.assertTrue(torch.allclose(outputs.logits, outputs_deactivate.logits))
+        self.assertTrue(not torch.allclose(outputs.logits, outputs_lora.logits))
+        self.assertTrue(torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
 
     def test_swift_restuner_forward(self):
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         restuner_config = ResTuningConfig(
             dims=model.config.hidden_size,
             root_modules=r'.*layer.0$',
             stem_modules=r'.*layer\.\d+$',
             target_modules=r'.*pooler',
             target_modules_hook='input',
@@ -149,83 +123,66 @@
         outputs = model(**inputs)
         model = Swift.prepare_model(model, config=restuner_config)
         outputs_lora = model(**inputs)
         model.deactivate_adapter('default')
         outputs_deactivate = model(**inputs)
         model.activate_adapter('default')
         outputs_reactivate = model(**inputs)
-        self.assertTrue(
-            torch.allclose(outputs.logits, outputs_deactivate.logits))
-        self.assertTrue(
-            not torch.allclose(outputs.logits, outputs_lora.logits))
-        self.assertTrue(
-            torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
+        self.assertTrue(torch.allclose(outputs.logits, outputs_deactivate.logits))
+        self.assertTrue(not torch.allclose(outputs.logits, outputs_lora.logits))
+        self.assertTrue(torch.allclose(outputs_lora.logits, outputs_reactivate.logits))
 
     def lora_injection_with_dtype(self, dtype=torch.float32):
         from swift.tuners.lora import Linear
 
         def reset_lora_parameters(self, adapter_name, init_lora_weights):
             if init_lora_weights is False:
                 return
 
             if adapter_name in self.lora_A.keys():
                 if init_lora_weights is True:
-                    nn.init.kaiming_uniform_(
-                        self.lora_A[adapter_name].weight, a=math.sqrt(5))
+                    nn.init.kaiming_uniform_(self.lora_A[adapter_name].weight, a=math.sqrt(5))
                 elif init_lora_weights.lower() == 'gaussian':
-                    nn.init.normal_(
-                        self.lora_A[adapter_name].weight,
-                        std=1 / self.r[adapter_name])
+                    nn.init.normal_(self.lora_A[adapter_name].weight, std=1 / self.r[adapter_name])
                 else:
-                    raise ValueError(
-                        f'Unknown initialization {init_lora_weights=}')
+                    raise ValueError(f'Unknown initialization {init_lora_weights=}')
                 nn.init.ones_(self.lora_B[adapter_name].weight)
             if adapter_name in self.lora_embedding_A.keys():
                 # initialize a the same way as the default for nn.linear and b to zero
                 nn.init.ones_(self.lora_embedding_A[adapter_name])
                 nn.init.normal_(self.lora_embedding_B[adapter_name])
 
         Linear.reset_lora_parameters = reset_lora_parameters
 
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         input = preprocessor('this is a test')
         model = model.to(dtype)
-        model2 = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model2 = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         model2 = model2.to(dtype)
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
         model = Swift.prepare_model(model, config=lora_config)
         self.assertTrue(isinstance(model, SwiftModel))
         output1 = model(**input)
         model.save_pretrained(self.tmp_dir)
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default')))
-        self.assertTrue(
-            os.path.exists(
-                os.path.join(self.tmp_dir, 'default', WEIGHTS_NAME)))
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default', WEIGHTS_NAME)))
 
-        model2 = Swift.from_pretrained(
-            model2, self.tmp_dir, adapter_name={'default': 'test'})
+        model2 = Swift.from_pretrained(model2, self.tmp_dir, adapter_name={'default': 'test'})
         self.assertTrue('test' in model2.adapters)
         output2 = model2(**input)
         self.assertTrue(torch.allclose(output1.logits, output2.logits))
         model2 = Swift.from_pretrained(model2, self.tmp_dir)
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
-        if dtype == torch.float32 and os.environ.get(
-                'USE_UNIQUE_THREAD') == '1':
+        if dtype == torch.float32 and os.environ.get('USE_UNIQUE_THREAD') == '1':
             Swift.merge_and_unload(model2)
             output3 = model2(**input)
             self.assertTrue(torch.allclose(output1.logits, output3.logits))
 
     def test_swift_lora_injection(self):
         self.lora_injection_with_dtype()
 
@@ -236,140 +193,96 @@
         model = SbertForSequenceClassification(SbertConfig())
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
         adapter_config = AdapterConfig(
             dim=model.config.hidden_size,
             target_modules=r'.*layer\.\d+$',
             method_name='feed_forward_chunk',
             hidden_pos=0)
-        model = Swift.prepare_model(
-            model, config={
-                'lora': lora_config,
-                'adapter': adapter_config
-            })
+        model = Swift.prepare_model(model, config={'lora': lora_config, 'adapter': adapter_config})
         model.save_pretrained(os.path.join(self.tmp_dir, 'original'))
         try:
-            Swift.save_to_peft_format(
-                os.path.join(self.tmp_dir, 'original'),
-                os.path.join(self.tmp_dir, 'converted'))
+            Swift.save_to_peft_format(os.path.join(self.tmp_dir, 'original'), os.path.join(self.tmp_dir, 'converted'))
             self.assertTrue(False)
         except AssertionError as e:
             print(e)
             pass
 
     def test_save_to_peft_param(self):
         model = SbertForSequenceClassification(SbertConfig())
-        lora_config = LoRAConfig(
-            target_modules=['query', 'key', 'value'], lora_dtype='fp16')
+        lora_config = LoRAConfig(target_modules=['query', 'key', 'value'], lora_dtype='fp16')
         model = Swift.prepare_model(model, config={'lora': lora_config})
         model.save_pretrained(os.path.join(self.tmp_dir, 'original'))
         try:
-            Swift.save_to_peft_format(
-                os.path.join(self.tmp_dir, 'original'),
-                os.path.join(self.tmp_dir, 'converted'))
+            Swift.save_to_peft_format(os.path.join(self.tmp_dir, 'original'), os.path.join(self.tmp_dir, 'converted'))
             self.assertTrue(False)
         except AssertionError as e:
             print(e)
             pass
 
     def test_save_to_peft_ok(self):
         model = SbertForSequenceClassification(SbertConfig())
-        lora_config = LoRAConfig(
-            target_modules=['query', 'key', 'value'], use_dora=True)
-        lora2_config = LoRAConfig(
-            target_modules=['query', 'key', 'value'], use_dora=True)
-        model = Swift.prepare_model(
-            model, config={
-                'default': lora_config,
-                'lora': lora2_config
-            })
+        lora_config = LoRAConfig(target_modules=['query', 'key', 'value'], use_dora=True)
+        lora2_config = LoRAConfig(target_modules=['query', 'key', 'value'], use_dora=True)
+        model = Swift.prepare_model(model, config={'default': lora_config, 'lora': lora2_config})
         model.save_pretrained(os.path.join(self.tmp_dir, 'original'))
-        Swift.save_to_peft_format(
-            os.path.join(self.tmp_dir, 'original'),
-            os.path.join(self.tmp_dir, 'converted'))
+        Swift.save_to_peft_format(os.path.join(self.tmp_dir, 'original'), os.path.join(self.tmp_dir, 'converted'))
         # A duplicate conversion
-        Swift.save_to_peft_format(
-            os.path.join(self.tmp_dir, 'original'),
-            os.path.join(self.tmp_dir, 'converted'))
+        Swift.save_to_peft_format(os.path.join(self.tmp_dir, 'original'), os.path.join(self.tmp_dir, 'converted'))
 
         # -------------------base case--------------------
         model2 = SbertForSequenceClassification(SbertConfig())
-        model2 = PeftModel.from_pretrained(
-            model2, os.path.join(self.tmp_dir, 'converted'))
-        model2.load_adapter(
-            os.path.join(os.path.join(self.tmp_dir, 'converted'), 'lora'),
-            'lora')
+        model2 = PeftModel.from_pretrained(model2, os.path.join(self.tmp_dir, 'converted'))
+        model2.load_adapter(os.path.join(os.path.join(self.tmp_dir, 'converted'), 'lora'), 'lora')
         state_dict = model.state_dict()
         state_dict2 = {
             key[len('base_model.model.'):]: value
             for key, value in model2.state_dict().items() if 'lora' in key
         }
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
         # -------------------override case--------------------
-        Swift.save_to_peft_format(
-            os.path.join(self.tmp_dir, 'converted'),
-            os.path.join(self.tmp_dir, 'converted'))
+        Swift.save_to_peft_format(os.path.join(self.tmp_dir, 'converted'), os.path.join(self.tmp_dir, 'converted'))
         model2 = SbertForSequenceClassification(SbertConfig())
-        model2 = PeftModel.from_pretrained(
-            model2, os.path.join(self.tmp_dir, 'converted'))
-        model2.load_adapter(
-            os.path.join(os.path.join(self.tmp_dir, 'converted'), 'lora'),
-            'lora')
+        model2 = PeftModel.from_pretrained(model2, os.path.join(self.tmp_dir, 'converted'))
+        model2.load_adapter(os.path.join(os.path.join(self.tmp_dir, 'converted'), 'lora'), 'lora')
         state_dict = model.state_dict()
         state_dict2 = {
             key[len('base_model.model.'):]: value
             for key, value in model2.state_dict().items() if 'lora' in key
         }
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
     def test_swift_multiple_adapters(self):
         model = SbertForSequenceClassification(SbertConfig())
         model2 = copy.deepcopy(model)
         lora_config = LoRAConfig(target_modules=['query', 'key', 'value'])
         adapter_config = AdapterConfig(
             dim=model.config.hidden_size,
             target_modules=r'.*layer\.\d+$',
             method_name='feed_forward_chunk',
             hidden_pos=0)
-        model = Swift.prepare_model(
-            model, config={
-                'lora': lora_config,
-                'adapter': adapter_config
-            })
+        model = Swift.prepare_model(model, config={'lora': lora_config, 'adapter': adapter_config})
         self.assertTrue(isinstance(model, SwiftModel))
         model.save_pretrained(self.tmp_dir, adapter_name=['lora', 'adapter'])
         with open(os.path.join(self.tmp_dir, 'configuration.json'), 'w') as f:
             f.write('{}')
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'lora')))
-        self.assertTrue(
-            os.path.exists(os.path.join(self.tmp_dir, 'lora', WEIGHTS_NAME)))
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'lora', WEIGHTS_NAME)))
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'adapter')))
-        self.assertTrue(
-            os.path.exists(
-                os.path.join(self.tmp_dir, 'adapter', WEIGHTS_NAME)))
-        model2 = Swift.from_pretrained(
-            model2, self.tmp_dir, adapter_name=['lora', 'adapter'])
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'adapter', WEIGHTS_NAME)))
+        model2 = Swift.from_pretrained(model2, self.tmp_dir, adapter_name=['lora', 'adapter'])
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
     def test_swift_multiple_adapters_switching(self):
         from swift.tuners.lora import Linear
         from swift.tuners.adapter import AdapterModule
 
         def reset_lora_parameters(self, adapter_name, init_lora_weights):
             if init_lora_weights is False:
@@ -377,20 +290,17 @@
 
             if adapter_name in self.lora_A.keys():
                 if init_lora_weights is True:
                     # initialize A the same way as the default for nn.Linear and B to zero
                     # https://github.com/microsoft/LoRA/blob/a0a92e0f26c067cf94747bdbf1ce73793fa44d19/loralib/layers.py#L124
                     nn.init.ones_(self.lora_A[adapter_name].weight)
                 elif init_lora_weights.lower() == 'gaussian':
-                    nn.init.normal_(
-                        self.lora_A[adapter_name].weight,
-                        std=1 / self.r[adapter_name])
+                    nn.init.normal_(self.lora_A[adapter_name].weight, std=1 / self.r[adapter_name])
                 else:
-                    raise ValueError(
-                        f'Unknown initialization {init_lora_weights=}')
+                    raise ValueError(f'Unknown initialization {init_lora_weights=}')
                 nn.init.ones_(self.lora_B[adapter_name].weight)
             if adapter_name in self.lora_embedding_A.keys():
                 # initialize a the same way as the default for nn.linear and b to zero
                 nn.init.ones_(self.lora_embedding_A[adapter_name])
                 nn.init.normal_(self.lora_embedding_B[adapter_name])
 
         Linear.reset_lora_parameters = reset_lora_parameters
@@ -402,18 +312,16 @@
                     nn.init.ones_(m.weight)
                     nn.init.ones_(m.bias)
 
             self.apply(_init_weights)
 
         AdapterModule.init_weights = init_weights
 
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         model1 = copy.deepcopy(model)
         model2 = copy.deepcopy(model)
         model1 = Swift.prepare_model(
             model1,
             config={
                 'lora1':
@@ -477,46 +385,41 @@
         if os.environ.get('USE_UNIQUE_THREAD') == '0':
 
             def thread_func1():
                 model1.set_active_adapters(['lora1', 'adapter1'], offload=None)
                 model.set_active_adapters(['lora1', 'adapter1'], offload=None)
                 outputs_single = model1(**inputs)
                 outputs_t1 = model(**inputs)
-                self.assertTrue(
-                    torch.allclose(outputs_single.logits, outputs_t1.logits))
+                self.assertTrue(torch.allclose(outputs_single.logits, outputs_t1.logits))
 
             def thread_func2():
                 model2.set_active_adapters(['lora2', 'adapter2'], offload=None)
                 model.set_active_adapters(['lora2', 'adapter2'], offload=None)
                 outputs_single = model2(**inputs)
                 outputs_t2 = model(**inputs)
-                self.assertTrue(
-                    torch.allclose(outputs_single.logits, outputs_t2.logits))
+                self.assertTrue(torch.allclose(outputs_single.logits, outputs_t2.logits))
 
             with ThreadPoolExecutor(2) as executor:
                 f1 = executor.submit(thread_func1)
                 f2 = executor.submit(thread_func2)
                 e1 = f1.exception()
                 e2 = f2.exception()
                 if e1 is not None:
                     raise e1
                 if e2 is not None:
                     raise e2
 
     def test_swift_side_bert(self):
-        model = Model.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
-        preprocessor = Preprocessor.from_pretrained(
-            'damo/nlp_structbert_sentence-similarity_chinese-base')
+        model = Model.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
+        preprocessor = Preprocessor.from_pretrained('damo/nlp_structbert_sentence-similarity_chinese-base')
         inputs = preprocessor('how are you')
         model2 = copy.deepcopy(model)
         result_origin = model(**inputs).logits
-        print(
-            f'test_swift_side_bert result_origin shape: {result_origin.shape}, '
-            f'result_origin sum: {torch.sum(result_origin)}')
+        print(f'test_swift_side_bert result_origin shape: {result_origin.shape}, '
+              f'result_origin sum: {torch.sum(result_origin)}')
 
         side_config = SideConfig(
             dim=model.config.hidden_size,
             target_modules=r'.*encoder.encoder',
             side_module_name='mlp',
             target_hidden_pos='last_hidden_state')
 
@@ -525,32 +428,25 @@
         model.deactivate_adapter('default')
         result_deactivate = model(**inputs).logits
         model.activate_adapter('default')
         result_reactivate = model(**inputs).logits
         self.assertTrue(torch.allclose(result_origin, result_deactivate))
         self.assertTrue(not torch.allclose(result_origin, result_activate))
         self.assertTrue(torch.allclose(result_activate, result_reactivate))
-        print(
-            f'test_swift_side_bert result shape: {result_origin.shape}, result sum: {torch.sum(result_origin)}'
-        )
+        print(f'test_swift_side_bert result shape: {result_origin.shape}, result sum: {torch.sum(result_origin)}')
 
         self.assertTrue(isinstance(model, SwiftModel))
         model.save_pretrained(self.tmp_dir)
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default')))
-        self.assertTrue(
-            os.path.exists(
-                os.path.join(self.tmp_dir, 'default', WEIGHTS_NAME)))
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default', WEIGHTS_NAME)))
 
         model2 = Swift.from_pretrained(model2, self.tmp_dir)
 
         state_dict = model.state_dict()
         state_dict2 = model2.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
 
 if __name__ == '__main__':
     unittest.main()
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_swift_device_map.py` & `ms-swift-2.0.4/tests/tuners/test_swift_device_map.py`

 * *Files 5% similar despite different names*

```diff
@@ -20,35 +20,26 @@
             os.makedirs(self.tmp_dir)
 
     def tearDown(self):
         shutil.rmtree(self.tmp_dir)
         super().tearDown()
 
     def test_swift_multiple_adapters(self):
-        model = Model.from_pretrained(
-            'modelscope/Llama-2-7b-ms', device_map='auto')
+        model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto')
         lora_config = LoRAConfig(target_modules=['q_proj', 'k_proj', 'v_proj'])
         model: SwiftModel = SwiftModel(model, config={'lora': lora_config})
         self.assertTrue(isinstance(model, SwiftModel))
         model.save_pretrained(self.tmp_dir, adapter_name=['lora'])
         state_dict = model.state_dict()
         with open(os.path.join(self.tmp_dir, 'configuration.json'), 'w') as f:
             f.write('{}')
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'lora')))
-        self.assertTrue(
-            os.path.exists(os.path.join(self.tmp_dir, 'lora', WEIGHTS_NAME)))
-        model = Model.from_pretrained(
-            'modelscope/Llama-2-7b-ms', device_map='auto')
-        model = SwiftModel.from_pretrained(
-            model, self.tmp_dir, adapter_name=['lora'], device_map='auto')
+        self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'lora', WEIGHTS_NAME)))
+        model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto')
+        model = SwiftModel.from_pretrained(model, self.tmp_dir, adapter_name=['lora'], device_map='auto')
 
         state_dict2 = model.state_dict()
         for key in state_dict:
             self.assertTrue(key in state_dict2)
-            self.assertTrue(
-                all(
-                    torch.isclose(state_dict[key],
-                                  state_dict2[key]).flatten().detach().cpu()))
+            self.assertTrue(all(torch.isclose(state_dict[key], state_dict2[key]).flatten().detach().cpu()))
 
-        self.assertTrue(
-            len(set(model.hf_device_map.values())) ==
-            torch.cuda.device_count())
+        self.assertTrue(len(set(model.hf_device_map.values())) == torch.cuda.device_count())
```

### Comparing `ms-swift-2.0.3.post1/tests/tuners/test_swift_restuning.py` & `ms-swift-2.0.4/tests/tuners/test_swift_restuning.py`

 * *Files 4% similar despite different names*

```diff
@@ -32,120 +32,103 @@
         torch.cuda.manual_seed(seed)
         torch.cuda.manual_seed_all(seed)
 
     def model_comparison(self, model, model2):
         model_key = list(model.state_dict().keys())
         model2_key = list(model2.state_dict().keys())
         self.assertTrue(model_key == model2_key)
-        model_val = torch.sum(
-            torch.stack(
-                [torch.sum(val) for val in model.state_dict().values()]))
-        model2_val = torch.sum(
-            torch.stack(
-                [torch.sum(val) for val in model2.state_dict().values()]))
+        model_val = torch.sum(torch.stack([torch.sum(val) for val in model.state_dict().values()]))
+        model2_val = torch.sum(torch.stack([torch.sum(val) for val in model2.state_dict().values()]))
         self.assertTrue(torch.isclose(model_val, model2_val))
 
     def test_swift_restuning_vit(self):
         model_dir = snapshot_download('AI-ModelScope/vit-base-patch16-224')
         from transformers import AutoModelForImageClassification
         model = AutoModelForImageClassification.from_pretrained(model_dir)
         model_swift_1 = copy.deepcopy(model)
         model_swift_2 = copy.deepcopy(model)
         result_origin = model(torch.ones((1, 3, 224, 224))).logits
-        print(
-            f'test_swift_restuning_vit result_origin shape: {result_origin.shape}, '
-            f'result_origin sum: {torch.sum(result_origin)}')
+        print(f'test_swift_restuning_vit result_origin shape: {result_origin.shape}, '
+              f'result_origin sum: {torch.sum(result_origin)}')
 
         # load type - 1
         self.set_random_seed()
         restuning_config_1 = ResTuningConfig(
             dims=768,
             root_modules=r'.*vit.encoder.layer.0$',
             stem_modules=r'.*vit.encoder.layer\.\d+$',
             target_modules=r'.*vit.layernorm',
             target_modules_hook='input',
             tuner_cfg='res_adapter',
         )
-        model_swift_1 = Swift.prepare_model(
-            model_swift_1, config=restuning_config_1)
+        model_swift_1 = Swift.prepare_model(model_swift_1, config=restuning_config_1)
         self.assertTrue(isinstance(model_swift_1, SwiftModel))
         print(model_swift_1.get_trainable_parameters())
         result_swift_1 = model_swift_1(torch.ones((1, 3, 224, 224))).logits
-        print(
-            f'test_swift_restuning_vit result_swift_1 shape: {result_swift_1.shape}, '
-            f'result_swift_1 sum: {torch.sum(result_swift_1)}')
+        print(f'test_swift_restuning_vit result_swift_1 shape: {result_swift_1.shape}, '
+              f'result_swift_1 sum: {torch.sum(result_swift_1)}')
 
         # load type - 2
         self.set_random_seed()
         restuning_config_2 = ResTuningConfig(
             dims=768,
             root_modules=r'.*vit.encoder.layer.0$',
             stem_modules=r'.*vit.encoder.layer\.\d+$',
             target_modules=r'.*vit.encoder',
             target_modules_hook='output',
             target_hidden_pos='last_hidden_state',
             tuner_cfg='res_adapter',
         )
-        model_swift_2 = Swift.prepare_model(
-            model_swift_2, config=restuning_config_2)
+        model_swift_2 = Swift.prepare_model(model_swift_2, config=restuning_config_2)
         self.assertTrue(isinstance(model_swift_2, SwiftModel))
         print(model_swift_2.get_trainable_parameters())
         result_swift_2 = model_swift_2(torch.ones((1, 3, 224, 224))).logits
-        print(
-            f'test_swift_restuning_vit result_swift_2 shape: {result_swift_2.shape}, '
-            f'result_swift_2 sum: {torch.sum(result_swift_2)}')
+        print(f'test_swift_restuning_vit result_swift_2 shape: {result_swift_2.shape}, '
+              f'result_swift_2 sum: {torch.sum(result_swift_2)}')
 
-        self.assertTrue(
-            all(torch.isclose(result_swift_1, result_swift_2).flatten()))
+        self.assertTrue(all(torch.isclose(result_swift_1, result_swift_2).flatten()))
 
         model_swift_1.save_pretrained(self.tmp_dir)
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default')))
         model_loaded = Swift.from_pretrained(model, self.tmp_dir)
         self.model_comparison(model_swift_1, model_loaded)
 
     def test_swift_restuning_diffusers_sd(self):
         model_dir = snapshot_download('AI-ModelScope/stable-diffusion-v1-5')
         from diffusers import UNet2DConditionModel
-        model = UNet2DConditionModel.from_pretrained(
-            model_dir, subfolder='unet')
+        model = UNet2DConditionModel.from_pretrained(model_dir, subfolder='unet')
         model.requires_grad_(False)
         model2 = copy.deepcopy(model)
         self.set_random_seed()
         input_data = {
             'sample': torch.ones((1, 4, 64, 64)),
             'timestep': 10,
             'encoder_hidden_states': torch.ones((1, 77, 768))
         }
         result_origin = model(**input_data).sample
-        print(
-            f'test_swift_restuning_diffusers_sd result_origin shape: {result_origin.shape}, '
-            f'result_origin sum: {torch.sum(result_origin)}')
+        print(f'test_swift_restuning_diffusers_sd result_origin shape: {result_origin.shape}, '
+              f'result_origin sum: {torch.sum(result_origin)}')
 
         self.set_random_seed()
         restuning_config = ResTuningConfig(
             dims=[1280, 1280, 1280, 640, 320],
             root_modules='mid_block',
-            stem_modules=[
-                'mid_block', 'up_blocks.0', 'up_blocks.1', 'up_blocks.2',
-                'up_blocks.3'
-            ],
+            stem_modules=['mid_block', 'up_blocks.0', 'up_blocks.1', 'up_blocks.2', 'up_blocks.3'],
             target_modules='conv_norm_out',
             tuner_cfg='res_group_adapter',
             use_upsample=True,
             upsample_out_channels=[1280, 1280, 640, 320, None],
             zero_init_last=True)
 
         model = Swift.prepare_model(model, config=restuning_config)
         self.assertTrue(isinstance(model, SwiftModel))
         print(model.get_trainable_parameters())
 
         result = model(**input_data).sample
-        print(
-            f'test_swift_restuning_diffusers_sd result shape: {result.shape}, result sum: {torch.sum(result)}'
-        )
+        print(f'test_swift_restuning_diffusers_sd result shape: {result.shape}, result sum: {torch.sum(result)}')
         model.save_pretrained(self.tmp_dir)
         self.assertTrue(os.path.exists(os.path.join(self.tmp_dir, 'default')))
         model2 = Swift.from_pretrained(model2, self.tmp_dir)
         self.model_comparison(model, model2)
 
 
 if __name__ == '__main__':
```

### Comparing `ms-swift-2.0.3.post1/tests/utils/test_io_utils.py` & `ms-swift-2.0.4/tests/utils/test_io_utils.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,14 +1,13 @@
 import os
 import shutil
 import tempfile
 import unittest
 
-from swift.utils import (append_to_jsonl, get_logger, read_from_jsonl,
-                         write_to_jsonl)
+from swift.utils import append_to_jsonl, get_logger, read_from_jsonl, write_to_jsonl
 
 logger = get_logger()
 
 
 class TestIOUtils(unittest.TestCase):
 
     def setUp(self):
```

