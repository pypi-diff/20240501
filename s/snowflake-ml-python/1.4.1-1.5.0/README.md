# Comparing `tmp/snowflake_ml_python-1.4.1-py3-none-any.whl.zip` & `tmp/snowflake_ml_python-1.5.0-py3-none-any.whl.zip`

## zipinfo {}

```diff
@@ -1,374 +1,382 @@
-Zip file size: 1907332 bytes, number of entries: 372
--rw-r--r--  2.0 unx      360 b- defN 24-Apr-22 20:48 snowflake/cortex/__init__.py
--rw-r--r--  2.0 unx     1226 b- defN 24-Apr-22 20:48 snowflake/cortex/_complete.py
--rw-r--r--  2.0 unx     1358 b- defN 24-Apr-22 20:48 snowflake/cortex/_extract_answer.py
--rw-r--r--  2.0 unx     1149 b- defN 24-Apr-22 20:48 snowflake/cortex/_sentiment.py
--rw-r--r--  2.0 unx     1075 b- defN 24-Apr-22 20:48 snowflake/cortex/_summarize.py
--rw-r--r--  2.0 unx     1420 b- defN 24-Apr-22 20:48 snowflake/cortex/_translate.py
--rw-r--r--  2.0 unx     1557 b- defN 24-Apr-22 20:48 snowflake/cortex/_util.py
--rwxr-xr-x  2.0 unx       16 b- defN 24-Apr-22 20:49 snowflake/ml/version.py
--rw-r--r--  2.0 unx      161 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/env.py
--rw-r--r--  2.0 unx    26212 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/env_utils.py
--rw-r--r--  2.0 unx    13622 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/file_utils.py
--rw-r--r--  2.0 unx     2696 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/init_utils.py
--rw-r--r--  2.0 unx      161 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/migrator_utils.py
--rw-r--r--  2.0 unx    22763 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/telemetry.py
--rw-r--r--  2.0 unx     2168 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/type_utils.py
--rw-r--r--  2.0 unx     3291 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/container_services/image_registry/credential.py
--rw-r--r--  2.0 unx     5214 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/container_services/image_registry/http_client.py
--rw-r--r--  2.0 unx    14537 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/container_services/image_registry/imagelib.py
--rw-r--r--  2.0 unx     9115 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/container_services/image_registry/registry_client.py
--rw-r--r--  2.0 unx     5317 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/exceptions/error_codes.py
--rw-r--r--  2.0 unx       47 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/exceptions/error_messages.py
--rw-r--r--  2.0 unx     1653 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/exceptions/exceptions.py
--rw-r--r--  2.0 unx      419 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/exceptions/fileset_error_messages.py
--rw-r--r--  2.0 unx     1040 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/exceptions/fileset_errors.py
--rw-r--r--  2.0 unx      665 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/exceptions/modeling_error_messages.py
--rw-r--r--  2.0 unx      804 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/human_readable_id/adjectives.txt
--rw-r--r--  2.0 unx      837 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/human_readable_id/animals.txt
--rw-r--r--  2.0 unx     1341 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/human_readable_id/hrid_generator.py
--rw-r--r--  2.0 unx     4519 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/human_readable_id/hrid_generator_base.py
--rw-r--r--  2.0 unx     3676 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/formatting.py
--rw-r--r--  2.0 unx    10925 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/identifier.py
--rw-r--r--  2.0 unx     2068 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/import_utils.py
--rw-r--r--  2.0 unx     1001 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/log_stream_processor.py
--rw-r--r--  2.0 unx     4534 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/parallelize.py
--rw-r--r--  2.0 unx     5857 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/pkg_version_utils.py
--rw-r--r--  2.0 unx    10668 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/query_result_checker.py
--rw-r--r--  2.0 unx     2405 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/result.py
--rw-r--r--  2.0 unx     1321 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/retryable_http.py
--rw-r--r--  2.0 unx     1727 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/session_token_manager.py
--rw-r--r--  2.0 unx     2927 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/snowflake_env.py
--rw-r--r--  2.0 unx     5403 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/snowpark_dataframe_utils.py
--rw-r--r--  2.0 unx     4292 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/spcs_attribution_utils.py
--rw-r--r--  2.0 unx     3270 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/sql_identifier.py
--rw-r--r--  2.0 unx     4384 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/table_manager.py
--rw-r--r--  2.0 unx     1402 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/temp_file_utils.py
--rw-r--r--  2.0 unx     2828 b- defN 24-Apr-22 20:48 snowflake/ml/_internal/utils/uri.py
--rw-r--r--  2.0 unx     6078 b- defN 24-Apr-22 20:48 snowflake/ml/dataset/dataset.py
--rw-r--r--  2.0 unx      298 b- defN 24-Apr-22 20:48 snowflake/ml/feature_store/__init__.py
--rw-r--r--  2.0 unx     3378 b- defN 24-Apr-22 20:48 snowflake/ml/feature_store/entity.py
--rw-r--r--  2.0 unx    71962 b- defN 24-Apr-22 20:48 snowflake/ml/feature_store/feature_store.py
--rw-r--r--  2.0 unx    17544 b- defN 24-Apr-22 20:48 snowflake/ml/feature_store/feature_view.py
--rw-r--r--  2.0 unx    26201 b- defN 24-Apr-22 20:48 snowflake/ml/fileset/fileset.py
--rw-r--r--  2.0 unx     6884 b- defN 24-Apr-22 20:48 snowflake/ml/fileset/parquet_parser.py
--rw-r--r--  2.0 unx    15596 b- defN 24-Apr-22 20:48 snowflake/ml/fileset/sfcfs.py
--rw-r--r--  2.0 unx    18595 b- defN 24-Apr-22 20:48 snowflake/ml/fileset/stage_fs.py
--rw-r--r--  2.0 unx     3849 b- defN 24-Apr-22 20:48 snowflake/ml/fileset/tf_dataset.py
--rw-r--r--  2.0 unx     2386 b- defN 24-Apr-22 20:48 snowflake/ml/fileset/torch_datapipe.py
--rw-r--r--  2.0 unx      367 b- defN 24-Apr-22 20:48 snowflake/ml/model/__init__.py
--rw-r--r--  2.0 unx    21595 b- defN 24-Apr-22 20:48 snowflake/ml/model/_api.py
--rw-r--r--  2.0 unx     8267 b- defN 24-Apr-22 20:48 snowflake/ml/model/custom_model.py
--rw-r--r--  2.0 unx      144 b- defN 24-Apr-22 20:48 snowflake/ml/model/deploy_platforms.py
--rw-r--r--  2.0 unx    29342 b- defN 24-Apr-22 20:48 snowflake/ml/model/model_signature.py
--rw-r--r--  2.0 unx    12705 b- defN 24-Apr-22 20:48 snowflake/ml/model/type_hints.py
--rw-r--r--  2.0 unx    12525 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/model/model_impl.py
--rw-r--r--  2.0 unx    11160 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/model/model_version_impl.py
--rw-r--r--  2.0 unx     4094 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/ops/metadata_ops.py
--rw-r--r--  2.0 unx    20264 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/ops/model_ops.py
--rw-r--r--  2.0 unx     5386 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/sql/model.py
--rw-r--r--  2.0 unx    10172 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/sql/model_version.py
--rw-r--r--  2.0 unx     1497 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/sql/stage.py
--rw-r--r--  2.0 unx     4646 b- defN 24-Apr-22 20:48 snowflake/ml/model/_client/sql/tag.py
--rw-r--r--  2.0 unx      302 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
--rw-r--r--  2.0 unx    10681 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
--rw-r--r--  2.0 unx     6129 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
--rw-r--r--  2.0 unx     1578 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
--rw-r--r--  2.0 unx     9947 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py
--rw-r--r--  2.0 unx    11148 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
--rw-r--r--  2.0 unx     1752 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
--rw-r--r--  2.0 unx     1225 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/templates/image_build_job_spec_template
--rw-r--r--  2.0 unx     3633 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/image_builds/templates/kaniko_shell_script_template
--rw-r--r--  2.0 unx    29141 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/snowservice/deploy.py
--rw-r--r--  2.0 unx     5989 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
--rw-r--r--  2.0 unx      232 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/snowservice/instance_types.py
--rw-r--r--  2.0 unx      734 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
--rw-r--r--  2.0 unx      540 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template_with_model
--rw-r--r--  2.0 unx     1841 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/utils/constants.py
--rw-r--r--  2.0 unx    13284 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
--rw-r--r--  2.0 unx     8358 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/warehouse/deploy.py
--rw-r--r--  2.0 unx     3011 b- defN 24-Apr-22 20:48 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
--rw-r--r--  2.0 unx     6337 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_composer.py
--rw-r--r--  2.0 unx     4722 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_manifest/model_manifest.py
--rw-r--r--  2.0 unx     2291 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py
--rw-r--r--  2.0 unx     1837 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_method/function_generator.py
--rw-r--r--  2.0 unx     2291 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_method/infer_function.py_template
--rw-r--r--  2.0 unx     2170 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_method/infer_table_function.py_template
--rw-r--r--  2.0 unx     6682 b- defN 24-Apr-22 20:48 snowflake/ml/model/_model_composer/model_method/model_method.py
--rw-r--r--  2.0 unx     2642 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handler.py
--rw-r--r--  2.0 unx     5958 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_packager.py
--rw-r--r--  2.0 unx    16640 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_env/model_env.py
--rw-r--r--  2.0 unx     6033 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/_base.py
--rw-r--r--  2.0 unx     2622 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/_utils.py
--rw-r--r--  2.0 unx     8142 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/catboost.py
--rw-r--r--  2.0 unx     7325 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/custom.py
--rw-r--r--  2.0 unx    19985 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/huggingface_pipeline.py
--rw-r--r--  2.0 unx     8445 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/lightgbm.py
--rw-r--r--  2.0 unx    10772 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/llm.py
--rw-r--r--  2.0 unx     8993 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/mlflow.py
--rw-r--r--  2.0 unx     8033 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/pytorch.py
--rw-r--r--  2.0 unx     9051 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/sentence_transformers.py
--rw-r--r--  2.0 unx     8218 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/sklearn.py
--rw-r--r--  2.0 unx     7967 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/snowmlmodel.py
--rw-r--r--  2.0 unx     8179 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/tensorflow.py
--rw-r--r--  2.0 unx     8104 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/torchscript.py
--rw-r--r--  2.0 unx     8874 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers/xgboost.py
--rw-r--r--  2.0 unx     1409 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_handlers_migrator/base_migrator.py
--rwxr-xr-x  2.0 unx      274 b- defN 24-Apr-22 20:49 snowflake/ml/model/_packager/model_meta/_core_requirements.py
--rwxr-xr-x  2.0 unx       44 b- defN 24-Apr-22 20:49 snowflake/ml/model/_packager/model_meta/_packaging_requirements.py
--rw-r--r--  2.0 unx     1818 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_meta/model_blob_meta.py
--rw-r--r--  2.0 unx    17266 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_meta/model_meta.py
--rw-r--r--  2.0 unx     2380 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_meta/model_meta_schema.py
--rw-r--r--  2.0 unx     1311 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_meta_migrator/base_migrator.py
--rw-r--r--  2.0 unx     1041 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_meta_migrator/migrator_plans.py
--rw-r--r--  2.0 unx     2070 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_meta_migrator/migrator_v1.py
--rwxr-xr-x  2.0 unx      248 b- defN 24-Apr-22 20:49 snowflake/ml/model/_packager/model_runtime/_snowml_inference_alternative_requirements.py
--rw-r--r--  2.0 unx     5872 b- defN 24-Apr-22 20:48 snowflake/ml/model/_packager/model_runtime/model_runtime.py
--rw-r--r--  2.0 unx     1304 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/base_handler.py
--rw-r--r--  2.0 unx     2706 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/builtins_handler.py
--rw-r--r--  2.0 unx    17972 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/core.py
--rw-r--r--  2.0 unx     5858 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/numpy_handler.py
--rw-r--r--  2.0 unx     8057 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/pandas_handler.py
--rw-r--r--  2.0 unx     4568 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/pytorch_handler.py
--rw-r--r--  2.0 unx     6036 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/snowpark_handler.py
--rw-r--r--  2.0 unx     6082 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/tensorflow_handler.py
--rw-r--r--  2.0 unx    12687 b- defN 24-Apr-22 20:48 snowflake/ml/model/_signatures/utils.py
--rw-r--r--  2.0 unx    10221 b- defN 24-Apr-22 20:48 snowflake/ml/model/models/huggingface_pipeline.py
--rw-r--r--  2.0 unx     3616 b- defN 24-Apr-22 20:48 snowflake/ml/model/models/llm.py
--rw-r--r--  2.0 unx       45 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/constants.py
--rw-r--r--  2.0 unx     9215 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/estimator_utils.py
--rw-r--r--  2.0 unx     4801 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/model_specifications.py
--rw-r--r--  2.0 unx      694 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/model_trainer.py
--rw-r--r--  2.0 unx     7269 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/model_trainer_builder.py
--rw-r--r--  2.0 unx     3364 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/model_transformer_builder.py
--rw-r--r--  2.0 unx     6482 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/transformer_protocols.py
--rw-r--r--  2.0 unx     7831 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/local_implementations/pandas_handlers.py
--rw-r--r--  2.0 unx     3169 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py
--rw-r--r--  2.0 unx     4850 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py
--rw-r--r--  2.0 unx     2535 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_trainer.py
--rw-r--r--  2.0 unx    54368 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py
--rw-r--r--  2.0 unx    13616 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py
--rw-r--r--  2.0 unx    21699 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py
--rw-r--r--  2.0 unx    17208 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/_internal/snowpark_implementations/xgboost_external_memory_trainer.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/calibration/__init__.py
--rwxr-xr-x  2.0 unx    51017 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/__init__.py
--rwxr-xr-x  2.0 unx    48846 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/affinity_propagation.py
--rwxr-xr-x  2.0 unx    50883 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/agglomerative_clustering.py
--rwxr-xr-x  2.0 unx    48571 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/birch.py
--rwxr-xr-x  2.0 unx    51270 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/bisecting_k_means.py
--rwxr-xr-x  2.0 unx    48933 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/dbscan.py
--rwxr-xr-x  2.0 unx    51401 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/feature_agglomeration.py
--rwxr-xr-x  2.0 unx    50826 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/k_means.py
--rwxr-xr-x  2.0 unx    49143 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/mean_shift.py
--rwxr-xr-x  2.0 unx    52188 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/mini_batch_k_means.py
--rwxr-xr-x  2.0 unx    52247 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/optics.py
--rwxr-xr-x  2.0 unx    49142 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/spectral_biclustering.py
--rwxr-xr-x  2.0 unx    52334 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/spectral_clustering.py
--rwxr-xr-x  2.0 unx    48275 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/cluster/spectral_coclustering.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/compose/__init__.py
--rwxr-xr-x  2.0 unx    50841 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/compose/column_transformer.py
--rwxr-xr-x  2.0 unx    48829 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/compose/transformed_target_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/__init__.py
--rwxr-xr-x  2.0 unx    49167 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/elliptic_envelope.py
--rwxr-xr-x  2.0 unx    46975 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/empirical_covariance.py
--rwxr-xr-x  2.0 unx    48839 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/graphical_lasso.py
--rwxr-xr-x  2.0 unx    50004 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
--rwxr-xr-x  2.0 unx    47113 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/ledoit_wolf.py
--rwxr-xr-x  2.0 unx    47868 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/min_cov_det.py
--rwxr-xr-x  2.0 unx    46754 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/oas.py
--rwxr-xr-x  2.0 unx    47130 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/covariance/shrunk_covariance.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/__init__.py
--rwxr-xr-x  2.0 unx    51838 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/dictionary_learning.py
--rwxr-xr-x  2.0 unx    49543 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/factor_analysis.py
--rwxr-xr-x  2.0 unx    49484 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/fast_ica.py
--rwxr-xr-x  2.0 unx    47837 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/incremental_pca.py
--rwxr-xr-x  2.0 unx    51836 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/kernel_pca.py
--rwxr-xr-x  2.0 unx    52882 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
--rwxr-xr-x  2.0 unx    50174 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
--rwxr-xr-x  2.0 unx    51103 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/pca.py
--rwxr-xr-x  2.0 unx    49006 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/sparse_pca.py
--rwxr-xr-x  2.0 unx    48594 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/decomposition/truncated_svd.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/discriminant_analysis/__init__.py
--rwxr-xr-x  2.0 unx    51309 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
--rwxr-xr-x  2.0 unx    49398 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/__init__.py
--rwxr-xr-x  2.0 unx    50216 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
--rwxr-xr-x  2.0 unx    49107 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
--rwxr-xr-x  2.0 unx    51127 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/bagging_classifier.py
--rwxr-xr-x  2.0 unx    50363 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/bagging_regressor.py
--rwxr-xr-x  2.0 unx    56047 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
--rwxr-xr-x  2.0 unx    54651 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
--rwxr-xr-x  2.0 unx    57502 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
--rwxr-xr-x  2.0 unx    57095 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
--rwxr-xr-x  2.0 unx    57336 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
--rwxr-xr-x  2.0 unx    55821 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
--rwxr-xr-x  2.0 unx    50315 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/isolation_forest.py
--rwxr-xr-x  2.0 unx    56030 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/random_forest_classifier.py
--rwxr-xr-x  2.0 unx    54622 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/random_forest_regressor.py
--rwxr-xr-x  2.0 unx    50060 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/stacking_regressor.py
--rwxr-xr-x  2.0 unx    49631 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/voting_classifier.py
--rwxr-xr-x  2.0 unx    48160 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/ensemble/voting_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/__init__.py
--rwxr-xr-x  2.0 unx    47471 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
--rwxr-xr-x  2.0 unx    47114 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/select_fdr.py
--rwxr-xr-x  2.0 unx    47108 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/select_fpr.py
--rwxr-xr-x  2.0 unx    47116 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/select_fwe.py
--rwxr-xr-x  2.0 unx    47201 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/select_k_best.py
--rwxr-xr-x  2.0 unx    47241 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/select_percentile.py
--rwxr-xr-x  2.0 unx    49834 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
--rwxr-xr-x  2.0 unx    46794 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/feature_selection/variance_threshold.py
--rw-r--r--  2.0 unx    10203 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/framework/_utils.py
--rw-r--r--  2.0 unx    30470 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/framework/base.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/gaussian_process/__init__.py
--rwxr-xr-x  2.0 unx    52768 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
--rwxr-xr-x  2.0 unx    51833 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
--rw-r--r--  2.0 unx      298 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/impute/__init__.py
--rwxr-xr-x  2.0 unx    53331 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/impute/iterative_imputer.py
--rwxr-xr-x  2.0 unx    49088 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/impute/knn_imputer.py
--rwxr-xr-x  2.0 unx    47916 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/impute/missing_indicator.py
--rw-r--r--  2.0 unx    18499 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/impute/simple_imputer.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_approximation/__init__.py
--rwxr-xr-x  2.0 unx    46907 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
--rwxr-xr-x  2.0 unx    48711 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_approximation/nystroem.py
--rwxr-xr-x  2.0 unx    47920 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
--rwxr-xr-x  2.0 unx    47307 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
--rwxr-xr-x  2.0 unx    47334 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_ridge/__init__.py
--rwxr-xr-x  2.0 unx    49147 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/lightgbm/__init__.py
--rwxr-xr-x  2.0 unx    48715 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
--rwxr-xr-x  2.0 unx    48218 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/__init__.py
--rwxr-xr-x  2.0 unx    49092 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/ard_regression.py
--rwxr-xr-x  2.0 unx    49508 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/bayesian_ridge.py
--rwxr-xr-x  2.0 unx    50080 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/elastic_net.py
--rwxr-xr-x  2.0 unx    51348 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/elastic_net_cv.py
--rwxr-xr-x  2.0 unx    49160 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/gamma_regressor.py
--rwxr-xr-x  2.0 unx    48357 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/huber_regressor.py
--rwxr-xr-x  2.0 unx    49580 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lars.py
--rwxr-xr-x  2.0 unx    49801 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lars_cv.py
--rwxr-xr-x  2.0 unx    49686 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lasso.py
--rwxr-xr-x  2.0 unx    50471 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lasso_cv.py
--rwxr-xr-x  2.0 unx    50716 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lasso_lars.py
--rwxr-xr-x  2.0 unx    50677 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
--rwxr-xr-x  2.0 unx    50023 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
--rwxr-xr-x  2.0 unx    47899 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/linear_regression.py
--rwxr-xr-x  2.0 unx    54154 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/logistic_regression.py
--rwxr-xr-x  2.0 unx    55194 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
--rwxr-xr-x  2.0 unx    49371 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
--rwxr-xr-x  2.0 unx    51009 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
--rwxr-xr-x  2.0 unx    48919 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/multi_task_lasso.py
--rwxr-xr-x  2.0 unx    50181 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
--rwxr-xr-x  2.0 unx    48526 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
--rwxr-xr-x  2.0 unx    51882 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
--rwxr-xr-x  2.0 unx    50949 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
--rwxr-xr-x  2.0 unx    51266 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/perceptron.py
--rwxr-xr-x  2.0 unx    49205 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/poisson_regressor.py
--rwxr-xr-x  2.0 unx    52324 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/ransac_regressor.py
--rwxr-xr-x  2.0 unx    51230 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/ridge.py
--rwxr-xr-x  2.0 unx    51618 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/ridge_classifier.py
--rwxr-xr-x  2.0 unx    49613 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
--rwxr-xr-x  2.0 unx    50314 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/ridge_cv.py
--rwxr-xr-x  2.0 unx    56691 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/sgd_classifier.py
--rwxr-xr-x  2.0 unx    51489 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
--rwxr-xr-x  2.0 unx    54162 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/sgd_regressor.py
--rwxr-xr-x  2.0 unx    49640 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
--rwxr-xr-x  2.0 unx    50596 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/linear_model/tweedie_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/manifold/__init__.py
--rwxr-xr-x  2.0 unx    49600 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/manifold/isomap.py
--rwxr-xr-x  2.0 unx    48814 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/manifold/mds.py
--rwxr-xr-x  2.0 unx    49638 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/manifold/spectral_embedding.py
--rwxr-xr-x  2.0 unx    52597 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/manifold/tsne.py
--rw-r--r--  2.0 unx      524 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/__init__.py
--rw-r--r--  2.0 unx    66499 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/classification.py
--rw-r--r--  2.0 unx     4803 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/correlation.py
--rw-r--r--  2.0 unx     4672 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/covariance.py
--rw-r--r--  2.0 unx    13114 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/metrics_utils.py
--rw-r--r--  2.0 unx    17569 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/ranking.py
--rw-r--r--  2.0 unx    25845 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/metrics/regression.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/mixture/__init__.py
--rwxr-xr-x  2.0 unx    54396 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
--rwxr-xr-x  2.0 unx    52297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/mixture/gaussian_mixture.py
--rw-r--r--  2.0 unx      298 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/model_selection/__init__.py
--rw-r--r--  2.0 unx    38453 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/model_selection/grid_search_cv.py
--rw-r--r--  2.0 unx    38692 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/model_selection/randomized_search_cv.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/multiclass/__init__.py
--rwxr-xr-x  2.0 unx    47899 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
--rwxr-xr-x  2.0 unx    48833 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
--rwxr-xr-x  2.0 unx    48169 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/multiclass/output_code_classifier.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/naive_bayes/__init__.py
--rwxr-xr-x  2.0 unx    48438 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
--rwxr-xr-x  2.0 unx    48773 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/naive_bayes/categorical_nb.py
--rwxr-xr-x  2.0 unx    48453 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/naive_bayes/complement_nb.py
--rwxr-xr-x  2.0 unx    47582 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
--rwxr-xr-x  2.0 unx    48218 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/__init__.py
--rwxr-xr-x  2.0 unx    51287 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
--rwxr-xr-x  2.0 unx    50758 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
--rwxr-xr-x  2.0 unx    49116 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/kernel_density.py
--rwxr-xr-x  2.0 unx    51694 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/local_outlier_factor.py
--rwxr-xr-x  2.0 unx    47776 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/nearest_centroid.py
--rwxr-xr-x  2.0 unx    49585 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/nearest_neighbors.py
--rwxr-xr-x  2.0 unx    50965 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
--rwxr-xr-x  2.0 unx    51700 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
--rwxr-xr-x  2.0 unx    50581 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neural_network/__init__.py
--rwxr-xr-x  2.0 unx    48080 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
--rwxr-xr-x  2.0 unx    55660 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neural_network/mlp_classifier.py
--rwxr-xr-x  2.0 unx    54929 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/neural_network/mlp_regressor.py
--rw-r--r--  2.0 unx      221 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/parameters/disable_distributed_hpo.py
--rw-r--r--  2.0 unx      298 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/pipeline/__init__.py
--rw-r--r--  2.0 unx    25456 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/pipeline/pipeline.py
--rw-r--r--  2.0 unx      298 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/__init__.py
--rw-r--r--  2.0 unx     6999 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/binarizer.py
--rw-r--r--  2.0 unx    20970 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
--rw-r--r--  2.0 unx     7405 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/label_encoder.py
--rw-r--r--  2.0 unx     8768 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
--rw-r--r--  2.0 unx    12200 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/min_max_scaler.py
--rw-r--r--  2.0 unx     6584 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/normalizer.py
--rw-r--r--  2.0 unx    71579 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
--rw-r--r--  2.0 unx    33276 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
--rwxr-xr-x  2.0 unx    47991 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/preprocessing/polynomial_features.py
--rw-r--r--  2.0 unx    12398 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/robust_scaler.py
--rw-r--r--  2.0 unx    11395 b- defN 24-Apr-22 20:48 snowflake/ml/modeling/preprocessing/standard_scaler.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/semi_supervised/__init__.py
--rwxr-xr-x  2.0 unx    48675 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/semi_supervised/label_propagation.py
--rwxr-xr-x  2.0 unx    49024 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/semi_supervised/label_spreading.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/__init__.py
--rwxr-xr-x  2.0 unx    51485 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/linear_svc.py
--rwxr-xr-x  2.0 unx    49838 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/linear_svr.py
--rwxr-xr-x  2.0 unx    51797 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/nu_svc.py
--rwxr-xr-x  2.0 unx    48876 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/nu_svr.py
--rwxr-xr-x  2.0 unx    51946 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/svc.py
--rwxr-xr-x  2.0 unx    49065 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/svm/svr.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/tree/__init__.py
--rwxr-xr-x  2.0 unx    54222 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/tree/decision_tree_classifier.py
--rwxr-xr-x  2.0 unx    52921 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/tree/decision_tree_regressor.py
--rwxr-xr-x  2.0 unx    53564 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/tree/extra_tree_classifier.py
--rwxr-xr-x  2.0 unx    52272 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/tree/extra_tree_regressor.py
--rwxr-xr-x  2.0 unx      297 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/xgboost/__init__.py
--rwxr-xr-x  2.0 unx    59220 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/xgboost/xgb_classifier.py
--rwxr-xr-x  2.0 unx    58719 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/xgboost/xgb_regressor.py
--rwxr-xr-x  2.0 unx    59396 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
--rwxr-xr-x  2.0 unx    58922 b- defN 24-Apr-22 20:49 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
--rw-r--r--  2.0 unx     7135 b- defN 24-Apr-22 20:48 snowflake/ml/monitoring/monitor.py
--rw-r--r--  2.0 unx     3597 b- defN 24-Apr-22 20:48 snowflake/ml/monitoring/shap.py
--rw-r--r--  2.0 unx       76 b- defN 24-Apr-22 20:48 snowflake/ml/registry/__init__.py
--rw-r--r--  2.0 unx     5544 b- defN 24-Apr-22 20:48 snowflake/ml/registry/_artifact_manager.py
--rw-r--r--  2.0 unx     5316 b- defN 24-Apr-22 20:48 snowflake/ml/registry/_initial_schema.py
--rw-r--r--  2.0 unx     3166 b- defN 24-Apr-22 20:48 snowflake/ml/registry/_schema.py
--rw-r--r--  2.0 unx     4209 b- defN 24-Apr-22 20:48 snowflake/ml/registry/_schema_upgrade_plans.py
--rw-r--r--  2.0 unx     6922 b- defN 24-Apr-22 20:48 snowflake/ml/registry/_schema_version_manager.py
--rw-r--r--  2.0 unx     1263 b- defN 24-Apr-22 20:48 snowflake/ml/registry/artifact.py
--rw-r--r--  2.0 unx    91007 b- defN 24-Apr-22 20:48 snowflake/ml/registry/model_registry.py
--rw-r--r--  2.0 unx    10949 b- defN 24-Apr-22 20:48 snowflake/ml/registry/registry.py
--rw-r--r--  2.0 unx     5809 b- defN 24-Apr-22 20:48 snowflake/ml/registry/_manager/model_manager.py
--rw-r--r--  2.0 unx     8005 b- defN 24-Apr-22 20:48 snowflake/ml/utils/connection_params.py
--rw-r--r--  2.0 unx     3817 b- defN 24-Apr-22 20:48 snowflake/ml/utils/sparse.py
--rw-r--r--  2.0 unx    11365 b- defN 24-Apr-22 20:49 snowflake_ml_python-1.4.1.dist-info/LICENSE.txt
--rw-r--r--  2.0 unx    47072 b- defN 24-Apr-22 20:49 snowflake_ml_python-1.4.1.dist-info/METADATA
--rw-r--r--  2.0 unx       92 b- defN 24-Apr-22 20:49 snowflake_ml_python-1.4.1.dist-info/WHEEL
--rw-r--r--  2.0 unx       10 b- defN 24-Apr-22 20:49 snowflake_ml_python-1.4.1.dist-info/top_level.txt
--rw-rw-r--  2.0 unx    40360 b- defN 24-Apr-22 20:49 snowflake_ml_python-1.4.1.dist-info/RECORD
-372 files, 9457780 bytes uncompressed, 1840906 bytes compressed:  80.5%
+Zip file size: 1897128 bytes, number of entries: 380
+-rw-r--r--  2.0 unx      360 b- defN 24-May-01 19:19 snowflake/cortex/__init__.py
+-rw-r--r--  2.0 unx     1226 b- defN 24-May-01 19:19 snowflake/cortex/_complete.py
+-rw-r--r--  2.0 unx     1358 b- defN 24-May-01 19:19 snowflake/cortex/_extract_answer.py
+-rw-r--r--  2.0 unx     1149 b- defN 24-May-01 19:19 snowflake/cortex/_sentiment.py
+-rw-r--r--  2.0 unx     1075 b- defN 24-May-01 19:19 snowflake/cortex/_summarize.py
+-rw-r--r--  2.0 unx     1420 b- defN 24-May-01 19:19 snowflake/cortex/_translate.py
+-rw-r--r--  2.0 unx     1557 b- defN 24-May-01 19:19 snowflake/cortex/_util.py
+-rwxr-xr-x  2.0 unx       16 b- defN 24-May-01 19:20 snowflake/ml/version.py
+-rw-r--r--  2.0 unx      161 b- defN 24-May-01 19:19 snowflake/ml/_internal/env.py
+-rw-r--r--  2.0 unx    27629 b- defN 24-May-01 19:19 snowflake/ml/_internal/env_utils.py
+-rw-r--r--  2.0 unx    13622 b- defN 24-May-01 19:19 snowflake/ml/_internal/file_utils.py
+-rw-r--r--  2.0 unx     2696 b- defN 24-May-01 19:19 snowflake/ml/_internal/init_utils.py
+-rw-r--r--  2.0 unx      161 b- defN 24-May-01 19:19 snowflake/ml/_internal/migrator_utils.py
+-rw-r--r--  2.0 unx    22763 b- defN 24-May-01 19:19 snowflake/ml/_internal/telemetry.py
+-rw-r--r--  2.0 unx     2168 b- defN 24-May-01 19:19 snowflake/ml/_internal/type_utils.py
+-rw-r--r--  2.0 unx     3291 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/credential.py
+-rw-r--r--  2.0 unx     5214 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/http_client.py
+-rw-r--r--  2.0 unx    14537 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/imagelib.py
+-rw-r--r--  2.0 unx     9115 b- defN 24-May-01 19:19 snowflake/ml/_internal/container_services/image_registry/registry_client.py
+-rw-r--r--  2.0 unx      285 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/dataset_error_messages.py
+-rw-r--r--  2.0 unx      762 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/dataset_errors.py
+-rw-r--r--  2.0 unx     5453 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/error_codes.py
+-rw-r--r--  2.0 unx       47 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/error_messages.py
+-rw-r--r--  2.0 unx     1653 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/exceptions.py
+-rw-r--r--  2.0 unx      419 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/fileset_error_messages.py
+-rw-r--r--  2.0 unx     1040 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/fileset_errors.py
+-rw-r--r--  2.0 unx      665 b- defN 24-May-01 19:19 snowflake/ml/_internal/exceptions/modeling_error_messages.py
+-rw-r--r--  2.0 unx      804 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/adjectives.txt
+-rw-r--r--  2.0 unx      837 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/animals.txt
+-rw-r--r--  2.0 unx     1341 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/hrid_generator.py
+-rw-r--r--  2.0 unx     4519 b- defN 24-May-01 19:19 snowflake/ml/_internal/human_readable_id/hrid_generator_base.py
+-rw-r--r--  2.0 unx      214 b- defN 24-May-01 19:19 snowflake/ml/_internal/lineage/data_source.py
+-rw-r--r--  2.0 unx     1712 b- defN 24-May-01 19:19 snowflake/ml/_internal/lineage/dataset_dataframe.py
+-rw-r--r--  2.0 unx     3676 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/formatting.py
+-rw-r--r--  2.0 unx    10925 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/identifier.py
+-rw-r--r--  2.0 unx     2068 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/import_utils.py
+-rw-r--r--  2.0 unx     1001 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/log_stream_processor.py
+-rw-r--r--  2.0 unx     4534 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/parallelize.py
+-rw-r--r--  2.0 unx     5857 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/pkg_version_utils.py
+-rw-r--r--  2.0 unx    10668 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/query_result_checker.py
+-rw-r--r--  2.0 unx     2405 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/result.py
+-rw-r--r--  2.0 unx     1321 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/retryable_http.py
+-rw-r--r--  2.0 unx     1727 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/session_token_manager.py
+-rw-r--r--  2.0 unx     2927 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/snowflake_env.py
+-rw-r--r--  2.0 unx     5403 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/snowpark_dataframe_utils.py
+-rw-r--r--  2.0 unx     4292 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/spcs_attribution_utils.py
+-rw-r--r--  2.0 unx     3270 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/sql_identifier.py
+-rw-r--r--  2.0 unx     4384 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/table_manager.py
+-rw-r--r--  2.0 unx     1402 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/temp_file_utils.py
+-rw-r--r--  2.0 unx     2828 b- defN 24-May-01 19:19 snowflake/ml/_internal/utils/uri.py
+-rw-r--r--  2.0 unx      236 b- defN 24-May-01 19:19 snowflake/ml/dataset/__init__.py
+-rw-r--r--  2.0 unx    20863 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset.py
+-rw-r--r--  2.0 unx     1712 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset_factory.py
+-rw-r--r--  2.0 unx     3992 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset_metadata.py
+-rw-r--r--  2.0 unx     8205 b- defN 24-May-01 19:19 snowflake/ml/dataset/dataset_reader.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/feature_store/__init__.py
+-rw-r--r--  2.0 unx     3378 b- defN 24-May-01 19:19 snowflake/ml/feature_store/entity.py
+-rw-r--r--  2.0 unx    76062 b- defN 24-May-01 19:19 snowflake/ml/feature_store/feature_store.py
+-rw-r--r--  2.0 unx    18624 b- defN 24-May-01 19:19 snowflake/ml/feature_store/feature_view.py
+-rw-r--r--  2.0 unx     5740 b- defN 24-May-01 19:19 snowflake/ml/fileset/embedded_stage_fs.py
+-rw-r--r--  2.0 unx    26201 b- defN 24-May-01 19:19 snowflake/ml/fileset/fileset.py
+-rw-r--r--  2.0 unx     6884 b- defN 24-May-01 19:19 snowflake/ml/fileset/parquet_parser.py
+-rw-r--r--  2.0 unx    15344 b- defN 24-May-01 19:19 snowflake/ml/fileset/sfcfs.py
+-rw-r--r--  2.0 unx     6914 b- defN 24-May-01 19:19 snowflake/ml/fileset/snowfs.py
+-rw-r--r--  2.0 unx    18408 b- defN 24-May-01 19:19 snowflake/ml/fileset/stage_fs.py
+-rw-r--r--  2.0 unx     3849 b- defN 24-May-01 19:19 snowflake/ml/fileset/tf_dataset.py
+-rw-r--r--  2.0 unx     2386 b- defN 24-May-01 19:19 snowflake/ml/fileset/torch_datapipe.py
+-rw-r--r--  2.0 unx      393 b- defN 24-May-01 19:19 snowflake/ml/model/__init__.py
+-rw-r--r--  2.0 unx    22329 b- defN 24-May-01 19:19 snowflake/ml/model/_api.py
+-rw-r--r--  2.0 unx     8267 b- defN 24-May-01 19:19 snowflake/ml/model/custom_model.py
+-rw-r--r--  2.0 unx      144 b- defN 24-May-01 19:19 snowflake/ml/model/deploy_platforms.py
+-rw-r--r--  2.0 unx    29342 b- defN 24-May-01 19:19 snowflake/ml/model/model_signature.py
+-rw-r--r--  2.0 unx    12705 b- defN 24-May-01 19:19 snowflake/ml/model/type_hints.py
+-rw-r--r--  2.0 unx    13623 b- defN 24-May-01 19:19 snowflake/ml/model/_client/model/model_impl.py
+-rw-r--r--  2.0 unx    17320 b- defN 24-May-01 19:19 snowflake/ml/model/_client/model/model_version_impl.py
+-rw-r--r--  2.0 unx     4094 b- defN 24-May-01 19:19 snowflake/ml/model/_client/ops/metadata_ops.py
+-rw-r--r--  2.0 unx    23490 b- defN 24-May-01 19:19 snowflake/ml/model/_client/ops/model_ops.py
+-rw-r--r--  2.0 unx     5687 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/model.py
+-rw-r--r--  2.0 unx    14290 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/model_version.py
+-rw-r--r--  2.0 unx     1497 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/stage.py
+-rw-r--r--  2.0 unx     4646 b- defN 24-May-01 19:19 snowflake/ml/model/_client/sql/tag.py
+-rw-r--r--  2.0 unx      302 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py
+-rw-r--r--  2.0 unx    10681 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py
+-rw-r--r--  2.0 unx     6129 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/docker_context.py
+-rw-r--r--  2.0 unx     1578 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh
+-rw-r--r--  2.0 unx    10095 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py
+-rw-r--r--  2.0 unx    11148 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py
+-rw-r--r--  2.0 unx     1783 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template
+-rw-r--r--  2.0 unx     1225 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/templates/image_build_job_spec_template
+-rw-r--r--  2.0 unx     3633 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/image_builds/templates/kaniko_shell_script_template
+-rw-r--r--  2.0 unx    29286 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/deploy.py
+-rw-r--r--  2.0 unx     5989 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py
+-rw-r--r--  2.0 unx      232 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/instance_types.py
+-rw-r--r--  2.0 unx      734 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template
+-rw-r--r--  2.0 unx      540 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template_with_model
+-rw-r--r--  2.0 unx     1696 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/utils/constants.py
+-rw-r--r--  2.0 unx    11671 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/utils/snowservice_client.py
+-rw-r--r--  2.0 unx     8358 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/warehouse/deploy.py
+-rw-r--r--  2.0 unx     3011 b- defN 24-May-01 19:19 snowflake/ml/model/_deploy_client/warehouse/infer_template.py
+-rw-r--r--  2.0 unx     7325 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_composer.py
+-rw-r--r--  2.0 unx     5667 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_manifest/model_manifest.py
+-rw-r--r--  2.0 unx     2530 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py
+-rw-r--r--  2.0 unx     1837 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/function_generator.py
+-rw-r--r--  2.0 unx     2291 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/infer_function.py_template
+-rw-r--r--  2.0 unx     2170 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/infer_table_function.py_template
+-rw-r--r--  2.0 unx     6682 b- defN 24-May-01 19:19 snowflake/ml/model/_model_composer/model_method/model_method.py
+-rw-r--r--  2.0 unx     2642 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handler.py
+-rw-r--r--  2.0 unx     5836 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_packager.py
+-rw-r--r--  2.0 unx    18312 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_env/model_env.py
+-rw-r--r--  2.0 unx     6033 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/_base.py
+-rw-r--r--  2.0 unx     2622 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/_utils.py
+-rw-r--r--  2.0 unx     8142 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/catboost.py
+-rw-r--r--  2.0 unx     7325 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/custom.py
+-rw-r--r--  2.0 unx    19985 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/huggingface_pipeline.py
+-rw-r--r--  2.0 unx     8445 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/lightgbm.py
+-rw-r--r--  2.0 unx    10772 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/llm.py
+-rw-r--r--  2.0 unx     8993 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/mlflow.py
+-rw-r--r--  2.0 unx     8033 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/pytorch.py
+-rw-r--r--  2.0 unx     9051 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/sentence_transformers.py
+-rw-r--r--  2.0 unx     8218 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/sklearn.py
+-rw-r--r--  2.0 unx     7967 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/snowmlmodel.py
+-rw-r--r--  2.0 unx     8179 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/tensorflow.py
+-rw-r--r--  2.0 unx     8104 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/torchscript.py
+-rw-r--r--  2.0 unx     8874 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers/xgboost.py
+-rw-r--r--  2.0 unx     1409 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_handlers_migrator/base_migrator.py
+-rwxr-xr-x  2.0 unx      274 b- defN 24-May-01 19:20 snowflake/ml/model/_packager/model_meta/_core_requirements.py
+-rwxr-xr-x  2.0 unx       44 b- defN 24-May-01 19:20 snowflake/ml/model/_packager/model_meta/_packaging_requirements.py
+-rw-r--r--  2.0 unx     1818 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta/model_blob_meta.py
+-rw-r--r--  2.0 unx    17203 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta/model_meta.py
+-rw-r--r--  2.0 unx     2380 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta/model_meta_schema.py
+-rw-r--r--  2.0 unx     1311 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta_migrator/base_migrator.py
+-rw-r--r--  2.0 unx     1041 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta_migrator/migrator_plans.py
+-rw-r--r--  2.0 unx     2070 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_meta_migrator/migrator_v1.py
+-rwxr-xr-x  2.0 unx      248 b- defN 24-May-01 19:20 snowflake/ml/model/_packager/model_runtime/_snowml_inference_alternative_requirements.py
+-rw-r--r--  2.0 unx     5872 b- defN 24-May-01 19:19 snowflake/ml/model/_packager/model_runtime/model_runtime.py
+-rw-r--r--  2.0 unx     1304 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/base_handler.py
+-rw-r--r--  2.0 unx     2706 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/builtins_handler.py
+-rw-r--r--  2.0 unx    17972 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/core.py
+-rw-r--r--  2.0 unx     5858 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/numpy_handler.py
+-rw-r--r--  2.0 unx     8057 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/pandas_handler.py
+-rw-r--r--  2.0 unx     4568 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/pytorch_handler.py
+-rw-r--r--  2.0 unx     6036 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/snowpark_handler.py
+-rw-r--r--  2.0 unx     6082 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/tensorflow_handler.py
+-rw-r--r--  2.0 unx    12687 b- defN 24-May-01 19:19 snowflake/ml/model/_signatures/utils.py
+-rw-r--r--  2.0 unx    10221 b- defN 24-May-01 19:19 snowflake/ml/model/models/huggingface_pipeline.py
+-rw-r--r--  2.0 unx     3616 b- defN 24-May-01 19:19 snowflake/ml/model/models/llm.py
+-rw-r--r--  2.0 unx       45 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/constants.py
+-rw-r--r--  2.0 unx     9215 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/estimator_utils.py
+-rw-r--r--  2.0 unx     4801 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_specifications.py
+-rw-r--r--  2.0 unx      923 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_trainer.py
+-rw-r--r--  2.0 unx     8422 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_trainer_builder.py
+-rw-r--r--  2.0 unx     3364 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/model_transformer_builder.py
+-rw-r--r--  2.0 unx     6482 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/transformer_protocols.py
+-rw-r--r--  2.0 unx     7831 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/local_implementations/pandas_handlers.py
+-rw-r--r--  2.0 unx     5737 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py
+-rw-r--r--  2.0 unx     5544 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py
+-rw-r--r--  2.0 unx     2535 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_trainer.py
+-rw-r--r--  2.0 unx    54368 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py
+-rw-r--r--  2.0 unx    14539 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py
+-rw-r--r--  2.0 unx    31819 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py
+-rw-r--r--  2.0 unx    17208 b- defN 24-May-01 19:19 snowflake/ml/modeling/_internal/snowpark_implementations/xgboost_external_memory_trainer.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/calibration/__init__.py
+-rwxr-xr-x  2.0 unx    51278 b- defN 24-May-01 19:20 snowflake/ml/modeling/calibration/calibrated_classifier_cv.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/__init__.py
+-rwxr-xr-x  2.0 unx    49107 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/affinity_propagation.py
+-rwxr-xr-x  2.0 unx    51144 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/agglomerative_clustering.py
+-rwxr-xr-x  2.0 unx    49034 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/birch.py
+-rwxr-xr-x  2.0 unx    51793 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/bisecting_k_means.py
+-rwxr-xr-x  2.0 unx    49194 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/dbscan.py
+-rwxr-xr-x  2.0 unx    51909 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/feature_agglomeration.py
+-rwxr-xr-x  2.0 unx    51322 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/k_means.py
+-rwxr-xr-x  2.0 unx    49404 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/mean_shift.py
+-rwxr-xr-x  2.0 unx    52711 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/mini_batch_k_means.py
+-rwxr-xr-x  2.0 unx    52508 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/optics.py
+-rwxr-xr-x  2.0 unx    49403 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/spectral_biclustering.py
+-rwxr-xr-x  2.0 unx    52595 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/spectral_clustering.py
+-rwxr-xr-x  2.0 unx    48536 b- defN 24-May-01 19:20 snowflake/ml/modeling/cluster/spectral_coclustering.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/compose/__init__.py
+-rwxr-xr-x  2.0 unx    51374 b- defN 24-May-01 19:20 snowflake/ml/modeling/compose/column_transformer.py
+-rwxr-xr-x  2.0 unx    49090 b- defN 24-May-01 19:20 snowflake/ml/modeling/compose/transformed_target_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/__init__.py
+-rwxr-xr-x  2.0 unx    49428 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/elliptic_envelope.py
+-rwxr-xr-x  2.0 unx    47236 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/empirical_covariance.py
+-rwxr-xr-x  2.0 unx    49100 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/graphical_lasso.py
+-rwxr-xr-x  2.0 unx    50265 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/graphical_lasso_cv.py
+-rwxr-xr-x  2.0 unx    47374 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/ledoit_wolf.py
+-rwxr-xr-x  2.0 unx    48129 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/min_cov_det.py
+-rwxr-xr-x  2.0 unx    47015 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/oas.py
+-rwxr-xr-x  2.0 unx    47391 b- defN 24-May-01 19:20 snowflake/ml/modeling/covariance/shrunk_covariance.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/__init__.py
+-rwxr-xr-x  2.0 unx    52388 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/dictionary_learning.py
+-rwxr-xr-x  2.0 unx    50051 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/factor_analysis.py
+-rwxr-xr-x  2.0 unx    49985 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/fast_ica.py
+-rwxr-xr-x  2.0 unx    48345 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/incremental_pca.py
+-rwxr-xr-x  2.0 unx    52343 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/kernel_pca.py
+-rwxr-xr-x  2.0 unx    53429 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py
+-rwxr-xr-x  2.0 unx    50694 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py
+-rwxr-xr-x  2.0 unx    51612 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/pca.py
+-rwxr-xr-x  2.0 unx    49499 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/sparse_pca.py
+-rwxr-xr-x  2.0 unx    49122 b- defN 24-May-01 19:20 snowflake/ml/modeling/decomposition/truncated_svd.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/discriminant_analysis/__init__.py
+-rwxr-xr-x  2.0 unx    51877 b- defN 24-May-01 19:20 snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py
+-rwxr-xr-x  2.0 unx    49659 b- defN 24-May-01 19:20 snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/__init__.py
+-rwxr-xr-x  2.0 unx    50477 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/ada_boost_classifier.py
+-rwxr-xr-x  2.0 unx    49368 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/ada_boost_regressor.py
+-rwxr-xr-x  2.0 unx    51388 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/bagging_classifier.py
+-rwxr-xr-x  2.0 unx    50624 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/bagging_regressor.py
+-rwxr-xr-x  2.0 unx    56308 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/extra_trees_classifier.py
+-rwxr-xr-x  2.0 unx    54912 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/extra_trees_regressor.py
+-rwxr-xr-x  2.0 unx    57763 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py
+-rwxr-xr-x  2.0 unx    57356 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py
+-rwxr-xr-x  2.0 unx    57597 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py
+-rwxr-xr-x  2.0 unx    56082 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py
+-rwxr-xr-x  2.0 unx    50576 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/isolation_forest.py
+-rwxr-xr-x  2.0 unx    56291 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/random_forest_classifier.py
+-rwxr-xr-x  2.0 unx    54883 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/random_forest_regressor.py
+-rwxr-xr-x  2.0 unx    50602 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/stacking_regressor.py
+-rwxr-xr-x  2.0 unx    50155 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/voting_classifier.py
+-rwxr-xr-x  2.0 unx    48681 b- defN 24-May-01 19:20 snowflake/ml/modeling/ensemble/voting_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/__init__.py
+-rwxr-xr-x  2.0 unx    48018 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/generic_univariate_select.py
+-rwxr-xr-x  2.0 unx    47619 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_fdr.py
+-rwxr-xr-x  2.0 unx    47613 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_fpr.py
+-rwxr-xr-x  2.0 unx    47621 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_fwe.py
+-rwxr-xr-x  2.0 unx    47712 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_k_best.py
+-rwxr-xr-x  2.0 unx    47767 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/select_percentile.py
+-rwxr-xr-x  2.0 unx    50387 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/sequential_feature_selector.py
+-rwxr-xr-x  2.0 unx    47323 b- defN 24-May-01 19:20 snowflake/ml/modeling/feature_selection/variance_threshold.py
+-rw-r--r--  2.0 unx    10203 b- defN 24-May-01 19:19 snowflake/ml/modeling/framework/_utils.py
+-rw-r--r--  2.0 unx    31359 b- defN 24-May-01 19:19 snowflake/ml/modeling/framework/base.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/gaussian_process/__init__.py
+-rwxr-xr-x  2.0 unx    53029 b- defN 24-May-01 19:20 snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py
+-rwxr-xr-x  2.0 unx    52094 b- defN 24-May-01 19:20 snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/impute/__init__.py
+-rwxr-xr-x  2.0 unx    53847 b- defN 24-May-01 19:20 snowflake/ml/modeling/impute/iterative_imputer.py
+-rwxr-xr-x  2.0 unx    49563 b- defN 24-May-01 19:20 snowflake/ml/modeling/impute/knn_imputer.py
+-rwxr-xr-x  2.0 unx    48420 b- defN 24-May-01 19:20 snowflake/ml/modeling/impute/missing_indicator.py
+-rw-r--r--  2.0 unx    18499 b- defN 24-May-01 19:19 snowflake/ml/modeling/impute/simple_imputer.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/__init__.py
+-rwxr-xr-x  2.0 unx    47451 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py
+-rwxr-xr-x  2.0 unx    49222 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/nystroem.py
+-rwxr-xr-x  2.0 unx    48470 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py
+-rwxr-xr-x  2.0 unx    47824 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/rbf_sampler.py
+-rwxr-xr-x  2.0 unx    47872 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_ridge/__init__.py
+-rwxr-xr-x  2.0 unx    49408 b- defN 24-May-01 19:20 snowflake/ml/modeling/kernel_ridge/kernel_ridge.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/lightgbm/__init__.py
+-rwxr-xr-x  2.0 unx    48976 b- defN 24-May-01 19:20 snowflake/ml/modeling/lightgbm/lgbm_classifier.py
+-rwxr-xr-x  2.0 unx    48479 b- defN 24-May-01 19:20 snowflake/ml/modeling/lightgbm/lgbm_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/__init__.py
+-rwxr-xr-x  2.0 unx    49353 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ard_regression.py
+-rwxr-xr-x  2.0 unx    49769 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/bayesian_ridge.py
+-rwxr-xr-x  2.0 unx    50341 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/elastic_net.py
+-rwxr-xr-x  2.0 unx    51609 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/elastic_net_cv.py
+-rwxr-xr-x  2.0 unx    49421 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/gamma_regressor.py
+-rwxr-xr-x  2.0 unx    48618 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/huber_regressor.py
+-rwxr-xr-x  2.0 unx    49841 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lars.py
+-rwxr-xr-x  2.0 unx    50062 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lars_cv.py
+-rwxr-xr-x  2.0 unx    49947 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso.py
+-rwxr-xr-x  2.0 unx    50732 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_cv.py
+-rwxr-xr-x  2.0 unx    50977 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_lars.py
+-rwxr-xr-x  2.0 unx    50938 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_lars_cv.py
+-rwxr-xr-x  2.0 unx    50284 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/lasso_lars_ic.py
+-rwxr-xr-x  2.0 unx    48160 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/linear_regression.py
+-rwxr-xr-x  2.0 unx    54415 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/logistic_regression.py
+-rwxr-xr-x  2.0 unx    55455 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/logistic_regression_cv.py
+-rwxr-xr-x  2.0 unx    49632 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_elastic_net.py
+-rwxr-xr-x  2.0 unx    51270 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py
+-rwxr-xr-x  2.0 unx    49180 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_lasso.py
+-rwxr-xr-x  2.0 unx    50442 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py
+-rwxr-xr-x  2.0 unx    48787 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py
+-rwxr-xr-x  2.0 unx    52143 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py
+-rwxr-xr-x  2.0 unx    51210 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py
+-rwxr-xr-x  2.0 unx    51527 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/perceptron.py
+-rwxr-xr-x  2.0 unx    49466 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/poisson_regressor.py
+-rwxr-xr-x  2.0 unx    52585 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ransac_regressor.py
+-rwxr-xr-x  2.0 unx    51491 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge.py
+-rwxr-xr-x  2.0 unx    51879 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge_classifier.py
+-rwxr-xr-x  2.0 unx    49874 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge_classifier_cv.py
+-rwxr-xr-x  2.0 unx    50575 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/ridge_cv.py
+-rwxr-xr-x  2.0 unx    56952 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/sgd_classifier.py
+-rwxr-xr-x  2.0 unx    51750 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/sgd_one_class_svm.py
+-rwxr-xr-x  2.0 unx    54423 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/sgd_regressor.py
+-rwxr-xr-x  2.0 unx    49901 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/theil_sen_regressor.py
+-rwxr-xr-x  2.0 unx    50857 b- defN 24-May-01 19:20 snowflake/ml/modeling/linear_model/tweedie_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/__init__.py
+-rwxr-xr-x  2.0 unx    50084 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/isomap.py
+-rwxr-xr-x  2.0 unx    49304 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/mds.py
+-rwxr-xr-x  2.0 unx    50155 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/spectral_embedding.py
+-rwxr-xr-x  2.0 unx    53094 b- defN 24-May-01 19:20 snowflake/ml/modeling/manifold/tsne.py
+-rw-r--r--  2.0 unx      524 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/__init__.py
+-rw-r--r--  2.0 unx    66499 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/classification.py
+-rw-r--r--  2.0 unx     4803 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/correlation.py
+-rw-r--r--  2.0 unx     4672 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/covariance.py
+-rw-r--r--  2.0 unx    13114 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/metrics_utils.py
+-rw-r--r--  2.0 unx    17569 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/ranking.py
+-rw-r--r--  2.0 unx    25845 b- defN 24-May-01 19:19 snowflake/ml/modeling/metrics/regression.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/mixture/__init__.py
+-rwxr-xr-x  2.0 unx    54657 b- defN 24-May-01 19:20 snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py
+-rwxr-xr-x  2.0 unx    52558 b- defN 24-May-01 19:20 snowflake/ml/modeling/mixture/gaussian_mixture.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/model_selection/__init__.py
+-rw-r--r--  2.0 unx    38125 b- defN 24-May-01 19:19 snowflake/ml/modeling/model_selection/grid_search_cv.py
+-rw-r--r--  2.0 unx    38867 b- defN 24-May-01 19:19 snowflake/ml/modeling/model_selection/randomized_search_cv.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/__init__.py
+-rwxr-xr-x  2.0 unx    48160 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/one_vs_one_classifier.py
+-rwxr-xr-x  2.0 unx    49094 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py
+-rwxr-xr-x  2.0 unx    48430 b- defN 24-May-01 19:20 snowflake/ml/modeling/multiclass/output_code_classifier.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/__init__.py
+-rwxr-xr-x  2.0 unx    48699 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/bernoulli_nb.py
+-rwxr-xr-x  2.0 unx    49034 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/categorical_nb.py
+-rwxr-xr-x  2.0 unx    48714 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/complement_nb.py
+-rwxr-xr-x  2.0 unx    47843 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/gaussian_nb.py
+-rwxr-xr-x  2.0 unx    48479 b- defN 24-May-01 19:20 snowflake/ml/modeling/naive_bayes/multinomial_nb.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/__init__.py
+-rwxr-xr-x  2.0 unx    51548 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/k_neighbors_classifier.py
+-rwxr-xr-x  2.0 unx    51019 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/k_neighbors_regressor.py
+-rwxr-xr-x  2.0 unx    49377 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/kernel_density.py
+-rwxr-xr-x  2.0 unx    51955 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/local_outlier_factor.py
+-rwxr-xr-x  2.0 unx    48037 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/nearest_centroid.py
+-rwxr-xr-x  2.0 unx    49846 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/nearest_neighbors.py
+-rwxr-xr-x  2.0 unx    51509 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py
+-rwxr-xr-x  2.0 unx    51961 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py
+-rwxr-xr-x  2.0 unx    50842 b- defN 24-May-01 19:20 snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/__init__.py
+-rwxr-xr-x  2.0 unx    48585 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/bernoulli_rbm.py
+-rwxr-xr-x  2.0 unx    55921 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/mlp_classifier.py
+-rwxr-xr-x  2.0 unx    55190 b- defN 24-May-01 19:20 snowflake/ml/modeling/neural_network/mlp_regressor.py
+-rw-r--r--  2.0 unx      221 b- defN 24-May-01 19:19 snowflake/ml/modeling/parameters/disable_distributed_hpo.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/pipeline/__init__.py
+-rw-r--r--  2.0 unx    45360 b- defN 24-May-01 19:19 snowflake/ml/modeling/pipeline/pipeline.py
+-rw-r--r--  2.0 unx      298 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/__init__.py
+-rw-r--r--  2.0 unx     6999 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/binarizer.py
+-rw-r--r--  2.0 unx    20970 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py
+-rw-r--r--  2.0 unx     7405 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/label_encoder.py
+-rw-r--r--  2.0 unx     8768 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/max_abs_scaler.py
+-rw-r--r--  2.0 unx    12200 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/min_max_scaler.py
+-rw-r--r--  2.0 unx     6584 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/normalizer.py
+-rw-r--r--  2.0 unx    72322 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/one_hot_encoder.py
+-rw-r--r--  2.0 unx    33276 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/ordinal_encoder.py
+-rwxr-xr-x  2.0 unx    48511 b- defN 24-May-01 19:20 snowflake/ml/modeling/preprocessing/polynomial_features.py
+-rw-r--r--  2.0 unx    12398 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/robust_scaler.py
+-rw-r--r--  2.0 unx    11395 b- defN 24-May-01 19:19 snowflake/ml/modeling/preprocessing/standard_scaler.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/semi_supervised/__init__.py
+-rwxr-xr-x  2.0 unx    48936 b- defN 24-May-01 19:20 snowflake/ml/modeling/semi_supervised/label_propagation.py
+-rwxr-xr-x  2.0 unx    49285 b- defN 24-May-01 19:20 snowflake/ml/modeling/semi_supervised/label_spreading.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/__init__.py
+-rwxr-xr-x  2.0 unx    51746 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/linear_svc.py
+-rwxr-xr-x  2.0 unx    50099 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/linear_svr.py
+-rwxr-xr-x  2.0 unx    52058 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/nu_svc.py
+-rwxr-xr-x  2.0 unx    49137 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/nu_svr.py
+-rwxr-xr-x  2.0 unx    52207 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/svc.py
+-rwxr-xr-x  2.0 unx    49326 b- defN 24-May-01 19:20 snowflake/ml/modeling/svm/svr.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/__init__.py
+-rwxr-xr-x  2.0 unx    54483 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/decision_tree_classifier.py
+-rwxr-xr-x  2.0 unx    53182 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/decision_tree_regressor.py
+-rwxr-xr-x  2.0 unx    53825 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/extra_tree_classifier.py
+-rwxr-xr-x  2.0 unx    52533 b- defN 24-May-01 19:20 snowflake/ml/modeling/tree/extra_tree_regressor.py
+-rwxr-xr-x  2.0 unx      297 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/__init__.py
+-rwxr-xr-x  2.0 unx    59481 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgb_classifier.py
+-rwxr-xr-x  2.0 unx    58980 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgb_regressor.py
+-rwxr-xr-x  2.0 unx    59657 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgbrf_classifier.py
+-rwxr-xr-x  2.0 unx    59183 b- defN 24-May-01 19:20 snowflake/ml/modeling/xgboost/xgbrf_regressor.py
+-rw-r--r--  2.0 unx     7135 b- defN 24-May-01 19:19 snowflake/ml/monitoring/monitor.py
+-rw-r--r--  2.0 unx     3597 b- defN 24-May-01 19:19 snowflake/ml/monitoring/shap.py
+-rw-r--r--  2.0 unx       76 b- defN 24-May-01 19:19 snowflake/ml/registry/__init__.py
+-rw-r--r--  2.0 unx     5316 b- defN 24-May-01 19:19 snowflake/ml/registry/_initial_schema.py
+-rw-r--r--  2.0 unx     3166 b- defN 24-May-01 19:19 snowflake/ml/registry/_schema.py
+-rw-r--r--  2.0 unx     4209 b- defN 24-May-01 19:19 snowflake/ml/registry/_schema_upgrade_plans.py
+-rw-r--r--  2.0 unx     6922 b- defN 24-May-01 19:19 snowflake/ml/registry/_schema_version_manager.py
+-rw-r--r--  2.0 unx    84795 b- defN 24-May-01 19:19 snowflake/ml/registry/model_registry.py
+-rw-r--r--  2.0 unx    10949 b- defN 24-May-01 19:19 snowflake/ml/registry/registry.py
+-rw-r--r--  2.0 unx     5809 b- defN 24-May-01 19:19 snowflake/ml/registry/_manager/model_manager.py
+-rw-r--r--  2.0 unx     8005 b- defN 24-May-01 19:19 snowflake/ml/utils/connection_params.py
+-rw-r--r--  2.0 unx     3817 b- defN 24-May-01 19:19 snowflake/ml/utils/sparse.py
+-rw-r--r--  2.0 unx    11365 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/LICENSE.txt
+-rw-r--r--  2.0 unx    50050 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/METADATA
+-rw-r--r--  2.0 unx       92 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/WHEEL
+-rw-r--r--  2.0 unx       10 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/top_level.txt
+-rw-rw-r--  2.0 unx    41165 b- defN 24-May-01 19:20 snowflake_ml_python-1.5.0.dist-info/RECORD
+380 files, 9604806 bytes uncompressed, 1829392 bytes compressed:  81.0%
```

## zipnote {}

```diff
@@ -51,14 +51,20 @@
 
 Filename: snowflake/ml/_internal/container_services/image_registry/imagelib.py
 Comment: 
 
 Filename: snowflake/ml/_internal/container_services/image_registry/registry_client.py
 Comment: 
 
+Filename: snowflake/ml/_internal/exceptions/dataset_error_messages.py
+Comment: 
+
+Filename: snowflake/ml/_internal/exceptions/dataset_errors.py
+Comment: 
+
 Filename: snowflake/ml/_internal/exceptions/error_codes.py
 Comment: 
 
 Filename: snowflake/ml/_internal/exceptions/error_messages.py
 Comment: 
 
 Filename: snowflake/ml/_internal/exceptions/exceptions.py
@@ -81,14 +87,20 @@
 
 Filename: snowflake/ml/_internal/human_readable_id/hrid_generator.py
 Comment: 
 
 Filename: snowflake/ml/_internal/human_readable_id/hrid_generator_base.py
 Comment: 
 
+Filename: snowflake/ml/_internal/lineage/data_source.py
+Comment: 
+
+Filename: snowflake/ml/_internal/lineage/dataset_dataframe.py
+Comment: 
+
 Filename: snowflake/ml/_internal/utils/formatting.py
 Comment: 
 
 Filename: snowflake/ml/_internal/utils/identifier.py
 Comment: 
 
 Filename: snowflake/ml/_internal/utils/import_utils.py
@@ -132,38 +144,56 @@
 
 Filename: snowflake/ml/_internal/utils/temp_file_utils.py
 Comment: 
 
 Filename: snowflake/ml/_internal/utils/uri.py
 Comment: 
 
+Filename: snowflake/ml/dataset/__init__.py
+Comment: 
+
 Filename: snowflake/ml/dataset/dataset.py
 Comment: 
 
+Filename: snowflake/ml/dataset/dataset_factory.py
+Comment: 
+
+Filename: snowflake/ml/dataset/dataset_metadata.py
+Comment: 
+
+Filename: snowflake/ml/dataset/dataset_reader.py
+Comment: 
+
 Filename: snowflake/ml/feature_store/__init__.py
 Comment: 
 
 Filename: snowflake/ml/feature_store/entity.py
 Comment: 
 
 Filename: snowflake/ml/feature_store/feature_store.py
 Comment: 
 
 Filename: snowflake/ml/feature_store/feature_view.py
 Comment: 
 
+Filename: snowflake/ml/fileset/embedded_stage_fs.py
+Comment: 
+
 Filename: snowflake/ml/fileset/fileset.py
 Comment: 
 
 Filename: snowflake/ml/fileset/parquet_parser.py
 Comment: 
 
 Filename: snowflake/ml/fileset/sfcfs.py
 Comment: 
 
+Filename: snowflake/ml/fileset/snowfs.py
+Comment: 
+
 Filename: snowflake/ml/fileset/stage_fs.py
 Comment: 
 
 Filename: snowflake/ml/fileset/tf_dataset.py
 Comment: 
 
 Filename: snowflake/ml/fileset/torch_datapipe.py
@@ -1062,32 +1092,26 @@
 
 Filename: snowflake/ml/monitoring/shap.py
 Comment: 
 
 Filename: snowflake/ml/registry/__init__.py
 Comment: 
 
-Filename: snowflake/ml/registry/_artifact_manager.py
-Comment: 
-
 Filename: snowflake/ml/registry/_initial_schema.py
 Comment: 
 
 Filename: snowflake/ml/registry/_schema.py
 Comment: 
 
 Filename: snowflake/ml/registry/_schema_upgrade_plans.py
 Comment: 
 
 Filename: snowflake/ml/registry/_schema_version_manager.py
 Comment: 
 
-Filename: snowflake/ml/registry/artifact.py
-Comment: 
-
 Filename: snowflake/ml/registry/model_registry.py
 Comment: 
 
 Filename: snowflake/ml/registry/registry.py
 Comment: 
 
 Filename: snowflake/ml/registry/_manager/model_manager.py
@@ -1095,23 +1119,23 @@
 
 Filename: snowflake/ml/utils/connection_params.py
 Comment: 
 
 Filename: snowflake/ml/utils/sparse.py
 Comment: 
 
-Filename: snowflake_ml_python-1.4.1.dist-info/LICENSE.txt
+Filename: snowflake_ml_python-1.5.0.dist-info/LICENSE.txt
 Comment: 
 
-Filename: snowflake_ml_python-1.4.1.dist-info/METADATA
+Filename: snowflake_ml_python-1.5.0.dist-info/METADATA
 Comment: 
 
-Filename: snowflake_ml_python-1.4.1.dist-info/WHEEL
+Filename: snowflake_ml_python-1.5.0.dist-info/WHEEL
 Comment: 
 
-Filename: snowflake_ml_python-1.4.1.dist-info/top_level.txt
+Filename: snowflake_ml_python-1.5.0.dist-info/top_level.txt
 Comment: 
 
-Filename: snowflake_ml_python-1.4.1.dist-info/RECORD
+Filename: snowflake_ml_python-1.5.0.dist-info/RECORD
 Comment: 
 
 Zip file comment:
```

## snowflake/ml/version.py

```diff
@@ -1 +1 @@
-VERSION="1.4.1"
+VERSION="1.5.0"
```

## snowflake/ml/_internal/env_utils.py

```diff
@@ -9,18 +9,14 @@
 from typing import Any, DefaultDict, Dict, List, Optional, Tuple
 
 import yaml
 from packaging import requirements, specifiers, utils as packaging_utils, version
 
 import snowflake.connector
 from snowflake.ml._internal import env as snowml_env
-from snowflake.ml._internal.exceptions import (
-    error_codes,
-    exceptions as snowml_exceptions,
-)
 from snowflake.ml._internal.utils import query_result_checker
 from snowflake.snowpark import context, exceptions, session
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 
 class CONDA_OS(Enum):
     LINUX_64 = "linux-64"
@@ -233,14 +229,80 @@
             stacklevel=2,
             category=UserWarning,
         )
         return pip_req
     return new_pip_req
 
 
+class IncorrectLocalEnvironmentError(Exception):
+    ...
+
+
+def validate_local_installed_version_of_pip_package(pip_req: requirements.Requirement) -> None:
+    """Validate if the package is locally installed, and the local version meet the specifier of the requirements.
+
+    Args:
+        pip_req: A requirements.Requirement object showing the requirement.
+
+    Raises:
+        IncorrectLocalEnvironmentError: Raised when cannot find the local installation of the requested package.
+        IncorrectLocalEnvironmentError: Raised when the local installed version cannot meet the requirement.
+    """
+    try:
+        local_dist = importlib_metadata.distribution(pip_req.name)
+        local_dist_version = version.parse(local_dist.version)
+    except importlib_metadata.PackageNotFoundError:
+        raise IncorrectLocalEnvironmentError(f"Cannot find the local installation of the requested package {pip_req}.")
+
+    if not pip_req.specifier.contains(local_dist_version):
+        raise IncorrectLocalEnvironmentError(
+            f"The local installed version {local_dist_version} cannot meet the requirement {pip_req}."
+        )
+
+
+CONDA_PKG_NAME_TO_PYPI_MAP = {"pytorch": "torch"}
+
+
+def try_convert_conda_requirement_to_pip(conda_req: requirements.Requirement) -> requirements.Requirement:
+    """Return a new requirements.Requirement object whose name has been attempted to convert to name in pypi from conda.
+
+    Args:
+        conda_req: A requirements.Requirement object showing the requirement in conda.
+
+    Returns:
+        A new requirements.Requirement object showing the requirement in pypi.
+    """
+    pip_req = copy.deepcopy(conda_req)
+    pip_req.name = CONDA_PKG_NAME_TO_PYPI_MAP.get(conda_req.name, conda_req.name)
+    return pip_req
+
+
+def validate_py_runtime_version(provided_py_version_str: str) -> None:
+    """Validate the provided python version string with python version in current runtime.
+        If the major or minor is different, errors out.
+
+    Args:
+        provided_py_version_str: the provided python version string.
+
+    Raises:
+        IncorrectLocalEnvironmentError: Raised when the provided python version has different major or minor.
+    """
+    if provided_py_version_str != snowml_env.PYTHON_VERSION:
+        provided_py_version = version.parse(provided_py_version_str)
+        current_py_version = version.parse(snowml_env.PYTHON_VERSION)
+        if (
+            provided_py_version.major != current_py_version.major
+            or provided_py_version.minor != current_py_version.minor
+        ):
+            raise IncorrectLocalEnvironmentError(
+                f"Requested python version is {provided_py_version_str} "
+                f"while current Python version is {snowml_env.PYTHON_VERSION}. "
+            )
+
+
 def get_package_spec_with_supported_ops_only(req: requirements.Requirement) -> requirements.Requirement:
     """Get the package spec with supported ops only including ==, >=, <=, > and <
 
     Args:
         req: A requirements.Requirement object showing the requirement.
 
     Returns:
@@ -564,41 +626,14 @@
             return ver
         else:
             # "python" only, no specifier
             return ""
     return None
 
 
-def validate_py_runtime_version(provided_py_version_str: str) -> None:
-    """Validate the provided python version string with python version in current runtime.
-        If the major or minor is different, errors out.
-
-    Args:
-        provided_py_version_str: the provided python version string.
-
-    Raises:
-        SnowflakeMLException: Raised when the provided python version has different major or minor.
-    """
-    if provided_py_version_str != snowml_env.PYTHON_VERSION:
-        provided_py_version = version.parse(provided_py_version_str)
-        current_py_version = version.parse(snowml_env.PYTHON_VERSION)
-        if (
-            provided_py_version.major != current_py_version.major
-            or provided_py_version.minor != current_py_version.minor
-        ):
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.LOCAL_ENVIRONMENT_ERROR,
-                original_exception=RuntimeError(
-                    f"Unable to load model which is saved with Python {provided_py_version_str} "
-                    f"while current Python version is {snowml_env.PYTHON_VERSION}. "
-                    "To load model metadata only, set meta_only to True."
-                ),
-            )
-
-
 def _find_conda_dep_spec(
     conda_chan_deps: DefaultDict[str, List[requirements.Requirement]], pkg_name: str
 ) -> Optional[Tuple[str, requirements.Requirement]]:
     for channel in conda_chan_deps:
         spec = next(filter(lambda req: req.name == pkg_name, conda_chan_deps[channel]), None)
         if spec:
             return channel, spec
```

## snowflake/ml/_internal/exceptions/error_codes.py

```diff
@@ -101,7 +101,10 @@
 # Incorrect local Python environment when trying to do some actions that require specific dependency or versions.
 LOCAL_ENVIRONMENT_ERROR = "2501"
 # Unfeasible dependencies requirement when trying to do some actions that require specific environments.
 UNFEASIBLE_ENVIRONMENT_ERROR = "2502"
 
 # Missing required client side dependency.
 CLIENT_DEPENDENCY_MISSING_ERROR = "2511"
+
+# Current client side snowpark-ml-python version is outdated and may have forward compatibility issue
+SNOWML_PACKAGE_OUTDATED = "2700"
```

## snowflake/ml/dataset/dataset.py

```diff
@@ -1,161 +1,486 @@
 import json
-import time
-from dataclasses import dataclass
-from typing import Any, Dict, List, Optional
-
-from snowflake.ml.registry.artifact import Artifact, ArtifactType
-from snowflake.snowpark import DataFrame, Session
-
-
-def _get_val_or_null(val: Any) -> Any:
-    return val if val is not None else "null"
-
-
-def _wrap_embedded_str(s: str) -> str:
-    s = s.replace("\\", "\\\\")
-    s = s.replace('"', '\\"')
-    return s
-
-
-DATASET_SCHEMA_VERSION = "1"
-
-
-@dataclass(frozen=True)
-class FeatureStoreMetadata:
-    """
-    Feature store metadata.
-
-    Properties:
-        spine_query: The input query on source table which will be joined with features.
-        connection_params: a config contains feature store metadata.
-        features: A list of feature serialized object in the feature store.
-
-    """
-
-    spine_query: str
-    connection_params: Dict[str, str]
-    features: List[str]
-
-    def to_json(self) -> str:
-        state_dict = {
-            # TODO(zhe): Additional wrap is needed because ml_.artifact.ad_artifact takes a dict
-            # but we retrieve it as an object. Snowpark serialization is inconsistent with
-            # our deserialization. A fix is let artifact table stores string and callers
-            # handles both serialization and deserialization.
-            "spine_query": self.spine_query,
-            "connection_params": json.dumps(self.connection_params),
-            "features": json.dumps(self.features),
-        }
-        return json.dumps(state_dict)
-
-    @classmethod
-    def from_json(cls, json_str: str) -> "FeatureStoreMetadata":
-        json_dict = json.loads(json_str)
-        return cls(
-            spine_query=json_dict["spine_query"],
-            connection_params=json.loads(json_dict["connection_params"]),
-            features=json.loads(json_dict["features"]),
+import warnings
+from datetime import datetime
+from typing import Any, Dict, List, Optional, Tuple, Union
+
+from snowflake import snowpark
+from snowflake.ml._internal import telemetry
+from snowflake.ml._internal.exceptions import (
+    dataset_error_messages,
+    dataset_errors,
+    error_codes,
+    exceptions as snowml_exceptions,
+)
+from snowflake.ml._internal.lineage import data_source
+from snowflake.ml._internal.utils import (
+    formatting,
+    identifier,
+    query_result_checker,
+    snowpark_dataframe_utils,
+)
+from snowflake.ml.dataset import dataset_metadata, dataset_reader
+from snowflake.snowpark import exceptions as snowpark_exceptions, functions
+
+_PROJECT = "Dataset"
+_TELEMETRY_STATEMENT_PARAMS = telemetry.get_function_usage_statement_params(_PROJECT)
+_METADATA_MAX_QUERY_LENGTH = 10000
+_DATASET_VERSION_NAME_COL = "version"
+
+
+class DatasetVersion:
+    """Represents a version of a Snowflake Dataset"""
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def __init__(
+        self,
+        dataset: "Dataset",
+        version: str,
+    ) -> None:
+        """Initialize a DatasetVersion object.
+
+        Args:
+            dataset: The parent Snowflake Dataset.
+            version: Dataset version name.
+        """
+        self._parent = dataset
+        self._version = version
+        self._session: snowpark.Session = self._parent._session
+
+        self._properties: Optional[Dict[str, Any]] = None
+        self._raw_metadata: Optional[Dict[str, Any]] = None
+        self._metadata: Optional[dataset_metadata.DatasetMetadata] = None
+
+    @property
+    def name(self) -> str:
+        return self._version
+
+    @property
+    def created_on(self) -> datetime:
+        timestamp = self._get_property("created_on")
+        assert isinstance(timestamp, datetime)
+        return timestamp
+
+    @property
+    def comment(self) -> Optional[str]:
+        comment: Optional[str] = self._get_property("comment")
+        return comment
+
+    def _get_property(self, property_name: str, default: Any = None) -> Any:
+        if self._properties is None:
+            sql_result = (
+                query_result_checker.SqlResultValidator(
+                    self._session,
+                    f"SHOW VERSIONS LIKE '{self._version}' IN DATASET {self._parent.fully_qualified_name}",
+                    statement_params=_TELEMETRY_STATEMENT_PARAMS,
+                )
+                .has_dimensions(expected_rows=1)
+                .validate()
+            )
+            self._properties = sql_result[0].as_dict(True)
+        return self._properties.get(property_name, default)
+
+    def _get_metadata(self) -> Optional[dataset_metadata.DatasetMetadata]:
+        if self._raw_metadata is None:
+            self._raw_metadata = json.loads(self._get_property("metadata", "{}"))
+            try:
+                self._metadata = (
+                    dataset_metadata.DatasetMetadata.from_json(self._raw_metadata) if self._raw_metadata else None
+                )
+            except ValueError as e:
+                warnings.warn(f"Metadata parsing failed with error: {e}", UserWarning, stacklevel=2)
+        return self._metadata
+
+    def _get_exclude_cols(self) -> List[str]:
+        metadata = self._get_metadata()
+        if metadata is None:
+            return []
+        cols = []
+        if metadata.exclude_cols:
+            cols.extend(metadata.exclude_cols)
+        if metadata.label_cols:
+            cols.extend(metadata.label_cols)
+        return cols
+
+    def url(self) -> str:
+        """Returns the URL of the DatasetVersion contents in Snowflake.
+
+        Returns:
+            Snowflake URL string.
+        """
+        path = f"snow://dataset/{self._parent.fully_qualified_name}/versions/{self._version}/"
+        return path
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def list_files(self, subdir: Optional[str] = None) -> List[snowpark.Row]:
+        """Get the list of remote file paths for the current DatasetVersion."""
+        return self._session.sql(f"LIST {self.url()}{subdir or ''}").collect(
+            statement_params=_TELEMETRY_STATEMENT_PARAMS
         )
 
+    def __repr__(self) -> str:
+        return f"{self.__class__.__name__}(dataset='{self._parent.fully_qualified_name}', version='{self.name}')"
 
-class Dataset(Artifact):
-    """Metadata of dataset."""
 
+class Dataset:
+    """Represents a Snowflake Dataset which is organized into versions."""
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
     def __init__(
         self,
-        session: Session,
-        df: DataFrame,
-        generation_timestamp: Optional[float] = None,
-        materialized_table: Optional[str] = None,
-        snapshot_table: Optional[str] = None,
-        timestamp_col: Optional[str] = None,
-        label_cols: Optional[List[str]] = None,
-        feature_store_metadata: Optional[FeatureStoreMetadata] = None,
-        desc: str = "",
+        session: snowpark.Session,
+        database: str,
+        schema: str,
+        name: str,
+        selected_version: Optional[str] = None,
     ) -> None:
-        """Initialize dataset object.
+        """Initialize a lazily evaluated Dataset object"""
+        self._session = session
+        self._db = database
+        self._schema = schema
+        self._name = name
+        self._fully_qualified_name = identifier.get_schema_level_object_identifier(database, schema, name)
+
+        self._version = DatasetVersion(self, selected_version) if selected_version else None
+        self._reader: Optional[dataset_reader.DatasetReader] = None
+
+    @property
+    def fully_qualified_name(self) -> str:
+        return self._fully_qualified_name
+
+    @property
+    def selected_version(self) -> Optional[DatasetVersion]:
+        return self._version
+
+    @property
+    def read(self) -> dataset_reader.DatasetReader:
+        if not self.selected_version:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ATTRIBUTE,
+                original_exception=RuntimeError("No Dataset version selected."),
+            )
+        if self._reader is None:
+            v = self.selected_version
+            self._reader = dataset_reader.DatasetReader(
+                self._session,
+                [
+                    data_source.DataSource(
+                        fully_qualified_name=self._fully_qualified_name,
+                        version=v.name,
+                        url=v.url(),
+                        exclude_cols=v._get_exclude_cols(),
+                    )
+                ],
+            )
+        return self._reader
+
+    @staticmethod
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def load(session: snowpark.Session, name: str) -> "Dataset":
+        """
+        Load an existing Snowflake Dataset. DatasetVersions can be created from the Dataset object
+        using `Dataset.create_version()` and loaded with `Dataset.version()`.
+
+        Args:
+            session: Snowpark Session to interact with Snowflake backend.
+            name: Name of dataset to load. May optionally be a schema-level identifier.
+
+        Returns:
+            Dataset object representing loaded dataset
+
+        Raises:
+            ValueError: name is not a valid Snowflake identifier
+            DatasetNotExistError: Specified Dataset does not exist
+
+        # noqa: DAR402
+        """
+        db, schema, ds_name = _get_schema_level_identifier(session, name)
+        _validate_dataset_exists(session, db, schema, ds_name)
+        return Dataset(session, db, schema, ds_name)
+
+    @staticmethod
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def create(session: snowpark.Session, name: str, exist_ok: bool = False) -> "Dataset":
+        """
+        Create a new Snowflake Dataset. DatasetVersions can created from the Dataset object
+        using `Dataset.create_version()` and loaded with `Dataset.version()`.
 
         Args:
-            session: An active snowpark session.
-            df: A dataframe object representing the dataset generation.
-            generation_timestamp: The timestamp when this dataset is generated. It will use current time if
-                not provided.
-            materialized_table: The destination table name which data will writes into.
-            snapshot_table: A snapshot table name on the materialized table.
-            timestamp_col: Timestamp column which was used for point-in-time correct feature lookup.
-            label_cols: Name of column(s) in materialized_table that contains labels.
-            feature_store_metadata: A feature store metadata object.
-            desc: A description about this dataset.
-        """
-        self.df = df
-        self.generation_timestamp = generation_timestamp if generation_timestamp is not None else time.time()
-        self.materialized_table = materialized_table
-        self.snapshot_table = snapshot_table
-        self.timestamp_col = timestamp_col
-        self.label_cols = label_cols
-        self.feature_store_metadata = feature_store_metadata
-        self.desc = desc
-        self.owner = session.sql("SELECT CURRENT_USER()").collect()[0]["CURRENT_USER()"]
-        self.schema_version = DATASET_SCHEMA_VERSION
-
-        super().__init__(type=ArtifactType.DATASET, spec=self.to_json())
-
-    def load_features(self) -> Optional[List[str]]:
-        if self.feature_store_metadata is not None:
-            return self.feature_store_metadata.features
-        else:
-            return None
-
-    def features_df(self) -> DataFrame:
-        result = self.df
-        if self.timestamp_col is not None:
-            result = result.drop(self.timestamp_col)
-        if self.label_cols is not None:
-            result = result.drop(self.label_cols)
-        return result
-
-    def to_json(self) -> str:
-        if len(self.df.queries["queries"]) != 1:
-            raise ValueError(
-                f"""df dataframe must contain only 1 query.
-Got {len(self.df.queries['queries'])}: {self.df.queries['queries']}
-"""
-            )
-
-        state_dict = {
-            "df_query": _wrap_embedded_str(self.df.queries["queries"][0]),
-            "generation_timestamp": self.generation_timestamp,
-            "owner": self.owner,
-            "materialized_table": _wrap_embedded_str(_get_val_or_null(self.materialized_table)),
-            "snapshot_table": _wrap_embedded_str(_get_val_or_null(self.snapshot_table)),
-            "timestamp_col": _wrap_embedded_str(_get_val_or_null(self.timestamp_col)),
-            "label_cols": _get_val_or_null(self.label_cols),
-            "feature_store_metadata": _wrap_embedded_str(self.feature_store_metadata.to_json())
-            if self.feature_store_metadata is not None
-            else "null",
-            "schema_version": self.schema_version,
-            "desc": self.desc,
-        }
-        return json.dumps(state_dict)
-
-    @classmethod
-    def from_json(cls, json_str: str, session: Session) -> "Dataset":
-        json_dict = json.loads(json_str, strict=False)
-        json_dict["df"] = session.sql(json_dict.pop("df_query"))
-
-        fs_meta_json = json_dict["feature_store_metadata"]
-        json_dict["feature_store_metadata"] = (
-            FeatureStoreMetadata.from_json(fs_meta_json) if fs_meta_json != "null" else None
+            session: Snowpark Session to interact with Snowflake backend.
+            name: Name of dataset to create. May optionally be a schema-level identifier.
+            exist_ok: If False, raises an exception if specified Dataset already exists
+
+        Returns:
+            Dataset object representing created dataset
+
+        Raises:
+            ValueError: name is not a valid Snowflake identifier
+            DatasetExistError: Specified Dataset already exists
+            DatasetError: Dataset creation failed
+
+        # noqa: DAR401
+        # noqa: DAR402
+        """
+        db, schema, ds_name = _get_schema_level_identifier(session, name)
+        ds_fqn = identifier.get_schema_level_object_identifier(db, schema, ds_name)
+        query = f"CREATE DATASET{' IF NOT EXISTS' if exist_ok else ''} {ds_fqn}"
+        try:
+            session.sql(query).collect(statement_params=_TELEMETRY_STATEMENT_PARAMS)
+            return Dataset(session, db, schema, ds_name)
+        except snowpark_exceptions.SnowparkClientException as e:
+            # Snowpark wraps the Python Connector error code in the head of the error message.
+            if e.message.startswith(dataset_errors.ERRNO_OBJECT_ALREADY_EXISTS):
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.OBJECT_ALREADY_EXISTS,
+                    original_exception=dataset_errors.DatasetExistError(
+                        dataset_error_messages.DATASET_ALREADY_EXISTS.format(name)
+                    ),
+                ) from e
+            else:
+                raise
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def list_versions(self, detailed: bool = False) -> Union[List[str], List[snowpark.Row]]:
+        """Return list of versions"""
+        versions = self._list_versions()
+        versions.sort(key=lambda r: r[_DATASET_VERSION_NAME_COL])
+        if not detailed:
+            return [r[_DATASET_VERSION_NAME_COL] for r in versions]
+        return versions
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def select_version(self, version: str) -> "Dataset":
+        """Return a new Dataset instance with the specified version selected.
+
+        Args:
+            version: Dataset version name.
+
+        Returns:
+            Dataset object.
+        """
+        self._validate_version_exists(version)
+        return Dataset(self._session, self._db, self._schema, self._name, version)
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def create_version(
+        self,
+        version: str,
+        input_dataframe: snowpark.DataFrame,
+        shuffle: bool = False,
+        exclude_cols: Optional[List[str]] = None,
+        label_cols: Optional[List[str]] = None,
+        properties: Optional[dataset_metadata.DatasetPropertiesType] = None,
+        partition_by: Optional[str] = None,
+        comment: Optional[str] = None,
+    ) -> "Dataset":
+        """Create a new version of the current Dataset.
+
+        The result Dataset object captures the query result deterministically as stage files.
+
+        Args:
+            version: Dataset version name. Data contents are materialized to the Dataset entity.
+            input_dataframe: A Snowpark DataFrame which yields the Dataset contents.
+            shuffle: A boolean represents whether the data should be shuffled globally. Default to be false.
+            exclude_cols: Name of column(s) in dataset to be excluded during training/testing (e.g. timestamp).
+            label_cols: Name of column(s) in dataset that contains labels.
+            properties: Custom metadata properties, saved under `DatasetMetadata.properties`
+            partition_by: Optional partitioning scheme within the new Dataset version.
+            comment: A descriptive comment about this dataset.
+
+        Returns:
+            A Dataset object with the newly created version selected.
+
+        Raises:
+            SnowflakeMLException: The Dataset no longer exists.
+            SnowflakeMLException: The specified Dataset version already exists.
+            snowpark_exceptions.SnowparkClientException: An error occurred during Dataset creation.
+
+        Note: During the generation of stage files, data casting will occur. The casting rules are as follows::
+            - Data casting:
+                - DecimalType(NUMBER):
+                    - If its scale is zero, cast to BIGINT
+                    - If its scale is non-zero, cast to FLOAT
+                - DoubleType(DOUBLE): Cast to FLOAT.
+                - ByteType(TINYINT): Cast to SMALLINT.
+                - ShortType(SMALLINT):Cast to SMALLINT.
+                - IntegerType(INT): Cast to INT.
+                - LongType(BIGINT): Cast to BIGINT.
+            - No action:
+                - FloatType(FLOAT): No action.
+                - StringType(String): No action.
+                - BinaryType(BINARY): No action.
+                - BooleanType(BOOLEAN): No action.
+            - Not supported:
+                - ArrayType(ARRAY): Not supported. A warning will be logged.
+                - MapType(OBJECT): Not supported. A warning will be logged.
+                - TimestampType(TIMESTAMP): Not supported. A warning will be logged.
+                - TimeType(TIME): Not supported. A warning will be logged.
+                - DateType(DATE): Not supported. A warning will be logged.
+                - VariantType(VARIANT): Not supported. A warning will be logged.
+        """
+        casted_df = snowpark_dataframe_utils.cast_snowpark_dataframe(input_dataframe)
+
+        if shuffle:
+            casted_df = casted_df.order_by(functions.random())
+
+        source_query = json.dumps(input_dataframe.queries)
+        if len(source_query) > _METADATA_MAX_QUERY_LENGTH:
+            warnings.warn(
+                "Source query exceeded max query length, dropping from metadata (limit=%d, actual=%d)"
+                % (_METADATA_MAX_QUERY_LENGTH, len(source_query)),
+                stacklevel=2,
+            )
+            source_query = "<query too long>"
+
+        metadata = dataset_metadata.DatasetMetadata(
+            source_query=source_query,
+            owner=self._session.sql("SELECT CURRENT_USER()").collect(statement_params=_TELEMETRY_STATEMENT_PARAMS)[0][
+                "CURRENT_USER()"
+            ],
+            exclude_cols=exclude_cols,
+            label_cols=label_cols,
+            properties=properties,
+        )
+
+        post_actions = casted_df._plan.post_actions
+        try:
+            # Execute all but the last query, final query gets passed to ALTER DATASET ADD VERSION
+            query = casted_df._plan.queries[-1].sql.strip()
+            if len(casted_df._plan.queries) > 1:
+                casted_df._plan.queries = casted_df._plan.queries[:-1]
+                casted_df._plan.post_actions = []
+                casted_df.collect(statement_params=_TELEMETRY_STATEMENT_PARAMS)
+            sql_command = "ALTER DATASET {} ADD VERSION '{}' FROM ({})".format(
+                self.fully_qualified_name,
+                version,
+                query,
+            )
+            if partition_by:
+                sql_command += f" PARTITION BY {partition_by}"
+            if comment:
+                sql_command += f" COMMENT={formatting.format_value_for_select(comment)}"
+            sql_command += f" METADATA=$${metadata.to_json()}$$"
+            self._session.sql(sql_command).collect(statement_params=_TELEMETRY_STATEMENT_PARAMS)
+
+            return Dataset(self._session, self._db, self._schema, self._name, version)
+
+        except snowpark_exceptions.SnowparkClientException as e:
+            if e.message.startswith(dataset_errors.ERRNO_DATASET_NOT_EXIST):
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.NOT_FOUND,
+                    original_exception=dataset_errors.DatasetNotExistError(
+                        dataset_error_messages.DATASET_NOT_EXIST.format(self.fully_qualified_name)
+                    ),
+                ) from e
+            elif (
+                e.message.startswith(dataset_errors.ERRNO_DATASET_VERSION_ALREADY_EXISTS)
+                or e.message.startswith(dataset_errors.ERRNO_VERSION_ALREADY_EXISTS)
+                or e.message.startswith(dataset_errors.ERRNO_FILES_ALREADY_EXISTING)
+            ):
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.OBJECT_ALREADY_EXISTS,
+                    original_exception=dataset_errors.DatasetExistError(
+                        dataset_error_messages.DATASET_VERSION_ALREADY_EXISTS.format(self.fully_qualified_name, version)
+                    ),
+                ) from e
+            else:
+                raise
+        finally:
+            for action in post_actions:
+                self._session.sql(action.sql.strip()).collect(statement_params=_TELEMETRY_STATEMENT_PARAMS)
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def delete_version(self, version_name: str) -> None:
+        """Delete the Dataset version
+
+        Args:
+            version_name: Name of version to delete from Dataset
+
+        Raises:
+            SnowflakeMLException: An error occurred when the DatasetVersion cannot get deleted.
+        """
+        delete_sql = f"ALTER DATASET {self.fully_qualified_name} DROP VERSION '{version_name}'"
+        try:
+            self._session.sql(delete_sql).collect(
+                statement_params=_TELEMETRY_STATEMENT_PARAMS,
+            )
+        except snowpark_exceptions.SnowparkClientException as e:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_DELETE_FAILED,
+                original_exception=dataset_errors.DatasetCannotDeleteError(str(e)),
+            ) from e
+        return
+
+    @telemetry.send_api_usage_telemetry(project=_PROJECT)
+    def delete(self) -> None:
+        """Delete Dataset and all contained versions"""
+        # TODO: Check and warn if any versions exist
+        self._session.sql(f"DROP DATASET {self.fully_qualified_name}").collect(
+            statement_params=_TELEMETRY_STATEMENT_PARAMS
         )
 
-        schema_version = json_dict.pop("schema_version")
-        owner = json_dict.pop("owner")
+    def _list_versions(self, pattern: Optional[str] = None) -> List[snowpark.Row]:
+        """Return list of versions"""
+        try:
+            pattern_clause = f" LIKE '{pattern}'" if pattern else ""
+            return (
+                query_result_checker.SqlResultValidator(
+                    self._session,
+                    f"SHOW VERSIONS{pattern_clause} IN DATASET {self.fully_qualified_name}",
+                    statement_params=_TELEMETRY_STATEMENT_PARAMS,
+                )
+                .has_column(_DATASET_VERSION_NAME_COL, allow_empty=True)
+                .validate()
+            )
+        except snowpark_exceptions.SnowparkClientException as e:
+            # Snowpark wraps the Python Connector error code in the head of the error message.
+            if e.message.startswith(dataset_errors.ERRNO_OBJECT_NOT_EXIST):
+                raise snowml_exceptions.SnowflakeMLException(
+                    error_code=error_codes.NOT_FOUND,
+                    original_exception=dataset_errors.DatasetNotExistError(
+                        dataset_error_messages.DATASET_NOT_EXIST.format(self.fully_qualified_name)
+                    ),
+                ) from e
+            else:
+                raise
+
+    def _validate_version_exists(self, version: str) -> None:
+        """Verify that the requested version exists. Raises DatasetNotExist if version not found"""
+        matches = self._list_versions(version)
+        matches = [m for m in matches if m[_DATASET_VERSION_NAME_COL] == version]  # Case sensitive match
+        if len(matches) == 0:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.NOT_FOUND,
+                original_exception=dataset_errors.DatasetNotExistError(
+                    dataset_error_messages.DATASET_VERSION_NOT_EXIST.format(self.fully_qualified_name, version)
+                ),
+            )
+
 
-        result = cls(session, **json_dict)
-        result.schema_version = schema_version
-        result.owner = owner
+# Utility methods
 
-        return result
 
-    def __eq__(self, other: object) -> bool:
-        return isinstance(other, Dataset) and self.to_json() == other.to_json()
+def _get_schema_level_identifier(session: snowpark.Session, dataset_name: str) -> Tuple[str, str, str]:
+    """Resolve a dataset name into a validated schema-level location identifier"""
+    db, schema, object_name, others = identifier.parse_schema_level_object_identifier(dataset_name)
+    if others:
+        raise ValueError(f"Invalid identifier: unexpected '{others}'")
+    db = db or session.get_current_database()
+    schema = schema or session.get_current_schema()
+    return str(db), str(schema), str(object_name)
+
+
+def _validate_dataset_exists(session: snowpark.Session, db: str, schema: str, dataset_name: str) -> None:
+    # FIXME: Once we switch version to SQL Identifiers we can just use version check with version=''
+    dataset_name = identifier.resolve_identifier(dataset_name)
+    if len(dataset_name) > 0 and dataset_name[0] == '"' and dataset_name[-1] == '"':
+        dataset_name = identifier.get_unescaped_names(dataset_name)
+    # Case sensitive match
+    query = f"show datasets like '{dataset_name}' in schema {db}.{schema} starts with '{dataset_name}'"
+    ds_matches = session.sql(query).count()
+    if ds_matches == 0:
+        raise snowml_exceptions.SnowflakeMLException(
+            error_code=error_codes.NOT_FOUND,
+            original_exception=dataset_errors.DatasetNotExistError(
+                dataset_error_messages.DATASET_NOT_EXIST.format(dataset_name)
+            ),
+        )
```

## snowflake/ml/feature_store/feature_store.py

```diff
@@ -4,87 +4,135 @@
 import functools
 import json
 import logging
 import re
 import warnings
 from dataclasses import dataclass
 from enum import Enum
-from typing import Callable, Dict, List, Optional, Tuple, TypeVar, Union, cast
+from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, Union, cast
 
+import packaging.version as pkg_version
+import snowflake.ml.version as snowml_version
 from pytimeparse.timeparse import timeparse
 from typing_extensions import Concatenate, ParamSpec
 
+from snowflake.ml import dataset
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.exceptions import (
+    dataset_errors,
     error_codes,
     exceptions as snowml_exceptions,
 )
 from snowflake.ml._internal.utils import identifier
 from snowflake.ml._internal.utils.sql_identifier import (
     SqlIdentifier,
     to_sql_identifiers,
 )
-from snowflake.ml.dataset.dataset import Dataset, FeatureStoreMetadata
-from snowflake.ml.feature_store.entity import (
-    _ENTITY_NAME_LENGTH_LIMIT,
-    _FEATURE_VIEW_ENTITY_TAG_DELIMITER,
-    Entity,
-)
+from snowflake.ml.dataset.dataset_metadata import FeatureStoreMetadata
+from snowflake.ml.feature_store.entity import _ENTITY_NAME_LENGTH_LIMIT, Entity
 from snowflake.ml.feature_store.feature_view import (
     _FEATURE_OBJ_TYPE,
     _FEATURE_VIEW_NAME_DELIMITER,
     _TIMESTAMP_COL_PLACEHOLDER,
     FeatureView,
     FeatureViewSlice,
     FeatureViewStatus,
     FeatureViewVersion,
+    _FeatureViewMetadata,
 )
 from snowflake.snowpark import DataFrame, Row, Session, functions as F
-from snowflake.snowpark._internal import type_utils, utils as snowpark_utils
 from snowflake.snowpark.exceptions import SnowparkSQLException
-from snowflake.snowpark.types import StructField
+from snowflake.snowpark.types import (
+    ArrayType,
+    StringType,
+    StructField,
+    StructType,
+    TimestampType,
+)
 
 _Args = ParamSpec("_Args")
 _RT = TypeVar("_RT")
 
 logger = logging.getLogger(__name__)
 
 _ENTITY_TAG_PREFIX = "SNOWML_FEATURE_STORE_ENTITY_"
-_FEATURE_VIEW_ENTITY_TAG = "SNOWML_FEATURE_STORE_FV_ENTITIES"
-_FEATURE_VIEW_TS_COL_TAG = "SNOWML_FEATURE_STORE_FV_TS_COL"
 _FEATURE_STORE_OBJECT_TAG = "SNOWML_FEATURE_STORE_OBJECT"
+_FEATURE_VIEW_METADATA_TAG = "SNOWML_FEATURE_VIEW_METADATA"
+
+
+@dataclass(frozen=True)
+class _FeatureStoreObjInfo:
+    type: _FeatureStoreObjTypes
+    pkg_version: str
+
+    def to_json(self) -> str:
+        state_dict = self.__dict__.copy()
+        state_dict["type"] = state_dict["type"].value
+        return json.dumps(state_dict)
+
+    @classmethod
+    def from_json(cls, json_str: str) -> _FeatureStoreObjInfo:
+        json_dict = json.loads(json_str)
+        # since we may introduce new fields in the json blob in the future,
+        # in order to guarantee compatibility, we need to select ones that can be
+        # decoded in the current version
+        state_dict = {}
+        state_dict["type"] = _FeatureStoreObjTypes.parse(json_dict["type"])
+        state_dict["pkg_version"] = json_dict["pkg_version"]
+        return cls(**state_dict)  # type: ignore[arg-type]
 
 
 # TODO: remove "" after dataset is updated
 class _FeatureStoreObjTypes(Enum):
-    FEATURE_VIEW = "FEATURE_VIEW"
+    UNKNOWN = "UNKNOWN"  # for forward compatibility
+    MANAGED_FEATURE_VIEW = "MANAGED_FEATURE_VIEW"
+    EXTERNAL_FEATURE_VIEW = "EXTERNAL_FEATURE_VIEW"
     FEATURE_VIEW_REFRESH_TASK = "FEATURE_VIEW_REFRESH_TASK"
     TRAINING_DATA = ""
 
+    @classmethod
+    def parse(cls, val: str) -> _FeatureStoreObjTypes:
+        try:
+            return cls(val)
+        except ValueError:
+            return cls.UNKNOWN
+
 
 _PROJECT = "FeatureStore"
 _DT_OR_VIEW_QUERY_PATTERN = re.compile(
     r"""CREATE\ (OR\ REPLACE\ )?(?P<obj_type>(DYNAMIC\ TABLE|VIEW))\ .*
         COMMENT\ =\ '(?P<comment>.*)'\s*
-        TAG.*?{entity_tag}\ =\ '(?P<entities>.*?)',\n
-           .*?{ts_col_tag}\ =\ '(?P<ts_col>.*?)',?.*?
+        TAG.*?{fv_metadata_tag}\ =\ '(?P<fv_metadata>.*?)',?.*?
         AS\ (?P<query>.*)
     """.format(
-        entity_tag=_FEATURE_VIEW_ENTITY_TAG, ts_col_tag=_FEATURE_VIEW_TS_COL_TAG
+        fv_metadata_tag=_FEATURE_VIEW_METADATA_TAG,
     ),
     flags=re.DOTALL | re.IGNORECASE | re.X,
 )
 
+_LIST_FEATURE_VIEW_SCHEMA = StructType(
+    [
+        StructField("name", StringType()),
+        StructField("version", StringType()),
+        StructField("database_name", StringType()),
+        StructField("schema_name", StringType()),
+        StructField("created_on", TimestampType()),
+        StructField("owner", StringType()),
+        StructField("desc", StringType()),
+        StructField("entities", ArrayType(StringType())),
+    ]
+)
+
 
 class CreationMode(Enum):
     FAIL_IF_NOT_EXIST = 1
     CREATE_IF_NOT_EXIST = 2
 
 
-@dataclass
+@dataclass(frozen=True)
 class _FeatureStoreConfig:
     database: SqlIdentifier
     schema: SqlIdentifier
 
     @property
     def full_schema_path(self) -> str:
         return f"{self.database}.{self.schema}"
@@ -107,22 +155,22 @@
         finally:
             if warehouse_updated and original_warehouse is not None:
                 self._session.use_warehouse(original_warehouse)
 
     return wrapper
 
 
-def dispatch_decorator(
-    prpr_version: str,
-) -> Callable[[Callable[Concatenate[FeatureStore, _Args], _RT]], Callable[Concatenate[FeatureStore, _Args], _RT],]:
+def dispatch_decorator() -> Callable[
+    [Callable[Concatenate[FeatureStore, _Args], _RT]],
+    Callable[Concatenate[FeatureStore, _Args], _RT],
+]:
     def decorator(
         f: Callable[Concatenate[FeatureStore, _Args], _RT]
     ) -> Callable[Concatenate[FeatureStore, _Args], _RT]:
         @telemetry.send_api_usage_telemetry(project=_PROJECT)
-        @snowpark_utils.private_preview(version=prpr_version)
         @switch_warehouse
         @functools.wraps(f)
         def wrap(self: FeatureStore, /, *args: _Args.args, **kargs: _Args.kwargs) -> _RT:
             return f(self, *args, **kargs)
 
         return wrap
 
@@ -131,15 +179,14 @@
 
 class FeatureStore:
     """
     FeatureStore provides APIs to create, materialize, retrieve and manage feature pipelines.
     """
 
     @telemetry.send_api_usage_telemetry(project=_PROJECT)
-    @snowpark_utils.private_preview(version="1.0.8")
     def __init__(
         self,
         session: Session,
         database: str,
         name: str,
         default_warehouse: str,
         creation_mode: CreationMode = CreationMode.FAIL_IF_NOT_EXIST,
@@ -174,15 +221,15 @@
         )
         self._asof_join_enabled = None
 
         # A dict from object name to tuple of search space and object domain.
         # search space used in query "SHOW <object_TYPE> LIKE <object_name> IN <search_space>"
         # object domain used in query "TAG_REFERENCE(<object_name>, <object_domain>)"
         self._obj_search_spaces = {
-            "TABLES": (self._config.full_schema_path, "TABLE"),
+            "DATASETS": (self._config.full_schema_path, "DATASET"),
             "DYNAMIC TABLES": (self._config.full_schema_path, "TABLE"),
             "VIEWS": (self._config.full_schema_path, "TABLE"),
             "SCHEMAS": (f"DATABASE {self._config.database}", "SCHEMA"),
             "TAGS": (self._config.full_schema_path, None),
             "TASKS": (self._config.full_schema_path, "TASK"),
             "WAREHOUSES": (None, None),
         }
@@ -196,37 +243,37 @@
         else:
             try:
                 self._session.sql(f"CREATE SCHEMA IF NOT EXISTS {self._config.full_schema_path}").collect(
                     statement_params=self._telemetry_stmp
                 )
                 for tag in to_sql_identifiers(
                     [
-                        _FEATURE_VIEW_ENTITY_TAG,
-                        _FEATURE_VIEW_TS_COL_TAG,
+                        _FEATURE_VIEW_METADATA_TAG,
                     ]
                 ):
                     self._session.sql(f"CREATE TAG IF NOT EXISTS {self._get_fully_qualified_name(tag)}").collect(
                         statement_params=self._telemetry_stmp
                     )
 
                 self._session.sql(
-                    f"""CREATE TAG IF NOT EXISTS {self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}
-                    ALLOWED_VALUES {','.join([f"'{v.value}'" for v in _FeatureStoreObjTypes])}"""
+                    f"CREATE TAG IF NOT EXISTS {self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}"
                 ).collect(statement_params=self._telemetry_stmp)
             except Exception as e:
                 self.clear()
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                     original_exception=RuntimeError(f"Failed to create feature store {name}: {e}."),
                 )
 
+        # TODO: remove this after tag_ref_internal rollout
+        self._use_optimized_tag_ref = self._tag_ref_internal_enabled()
+        self._check_feature_store_object_versions()
         logger.info(f"Successfully connected to feature store: {self._config.full_schema_path}.")
 
     @telemetry.send_api_usage_telemetry(project=_PROJECT)
-    @snowpark_utils.private_preview(version="1.0.12")
     def update_default_warehouse(self, warehouse_name: str) -> None:
         """Update default warehouse for feature store.
 
         Args:
             warehouse_name: Name of warehouse.
 
         Raises:
@@ -238,15 +285,15 @@
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"Cannot find warehouse {warehouse}"),
             )
 
         self._default_warehouse = warehouse
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def register_entity(self, entity: Entity) -> Entity:
         """
         Register Entity in the FeatureStore.
 
         Args:
             entity: Entity object to register.
 
@@ -264,36 +311,36 @@
                 f"Entity {entity.name} already exists. Skip registration.",
                 stacklevel=2,
                 category=UserWarning,
             )
             return entity
 
         # allowed_values will add double-quotes around each value, thus use resolved str here.
-        join_keys = [f"'{key.resolved()}'" for key in entity.join_keys]
+        join_keys = [f"{key.resolved()}" for key in entity.join_keys]
         join_keys_str = ",".join(join_keys)
         full_tag_name = self._get_fully_qualified_name(tag_name)
         try:
             self._session.sql(
                 f"""CREATE TAG IF NOT EXISTS {full_tag_name}
-                    ALLOWED_VALUES {join_keys_str}
+                    ALLOWED_VALUES '{join_keys_str}'
                     COMMENT = '{entity.desc}'
                 """
             ).collect(statement_params=self._telemetry_stmp)
         except Exception as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                 original_exception=RuntimeError(f"Failed to register entity `{entity.name}`: {e}."),
             ) from e
 
         logger.info(f"Registered Entity {entity}.")
 
         return self.get_entity(entity.name)
 
     # TODO: add support to update column desc once SNOW-894249 is fixed
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def register_feature_view(
         self,
         feature_view: FeatureView,
         version: str,
         block: bool = True,
         overwrite: bool = False,
     ) -> FeatureView:
@@ -338,15 +385,14 @@
                     error_code=error_codes.NOT_FOUND,
                     original_exception=ValueError(
                         f"FeatureView {feature_view.name}/{feature_view.version} status is {feature_view.status}, "
                         + "but it doesn't exist."
                     ),
                 )
 
-        # TODO: ideally we should move this to FeatureView creation time
         for e in feature_view.entities:
             if not self._validate_entity_exists(e.name):
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.NOT_FOUND,
                     original_exception=ValueError(f"Entity {e.name} has not been registered."),
                 )
 
@@ -354,66 +400,73 @@
         if not overwrite:
             try:
                 return self._get_feature_view_if_exists(feature_view.name, str(version))
             except Exception:
                 pass
 
         fully_qualified_name = self._get_fully_qualified_name(feature_view_name)
-        entities = _FEATURE_VIEW_ENTITY_TAG_DELIMITER.join([e.name for e in feature_view.entities])
-        timestamp_col = (
-            feature_view.timestamp_col
-            if feature_view.timestamp_col is not None
-            else SqlIdentifier(_TIMESTAMP_COL_PLACEHOLDER)
-        )
+        refresh_freq = feature_view.refresh_freq
+
+        if refresh_freq is not None:
+            obj_info = _FeatureStoreObjInfo(_FeatureStoreObjTypes.MANAGED_FEATURE_VIEW, snowml_version.VERSION)
+        else:
+            obj_info = _FeatureStoreObjInfo(_FeatureStoreObjTypes.EXTERNAL_FEATURE_VIEW, snowml_version.VERSION)
+
+        tagging_clause = [
+            f"{self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)} = '{obj_info.to_json()}'",
+            f"{self._get_fully_qualified_name(_FEATURE_VIEW_METADATA_TAG)} = '{feature_view._metadata().to_json()}'",
+        ]
+        for e in feature_view.entities:
+            join_keys = [f"{key.resolved()}" for key in e.join_keys]
+            tagging_clause.append(
+                f"{self._get_fully_qualified_name(self._get_entity_name(e.name))} = '{','.join(join_keys)}'"
+            )
+        tagging_clause_str = ",\n".join(tagging_clause)
 
         def create_col_desc(col: StructField) -> str:
             desc = feature_view.feature_descs.get(SqlIdentifier(col.name), None)
             desc = "" if desc is None else f"COMMENT '{desc}'"
             return f"{col.name} {desc}"
 
         column_descs = ", ".join([f"{create_col_desc(col)}" for col in feature_view.output_schema.fields])
-        refresh_freq = feature_view.refresh_freq
 
         if refresh_freq is not None:
             schedule_task = refresh_freq != "DOWNSTREAM" and timeparse(refresh_freq) is None
             self._create_dynamic_table(
                 feature_view_name,
                 feature_view,
                 fully_qualified_name,
                 column_descs,
-                entities,
+                tagging_clause_str,
                 schedule_task,
                 self._default_warehouse,
-                timestamp_col,
                 block,
                 overwrite,
             )
         else:
             try:
                 overwrite_clause = " OR REPLACE" if overwrite else ""
                 query = f"""CREATE{overwrite_clause} VIEW {fully_qualified_name} ({column_descs})
                     COMMENT = '{feature_view.desc}'
                     TAG (
-                        {_FEATURE_VIEW_ENTITY_TAG} = '{entities}',
-                        {_FEATURE_VIEW_TS_COL_TAG} = '{timestamp_col}',
-                        {_FEATURE_STORE_OBJECT_TAG} = '{_FeatureStoreObjTypes.FEATURE_VIEW.value}'
+                        {tagging_clause_str}
                     )
                     AS {feature_view.query}
                 """
                 self._session.sql(query).collect(statement_params=self._telemetry_stmp)
             except Exception as e:
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                     original_exception=RuntimeError(f"Create view {fully_qualified_name} [\n{query}\n] failed: {e}"),
                 ) from e
 
-        logger.info(f"Registered FeatureView {feature_view.name}/{version}.")
+        logger.info(f"Registered FeatureView {feature_view.name}/{version} successfully.")
         return self.get_feature_view(feature_view.name, str(version))
 
-    @dispatch_decorator(prpr_version="1.1.0")
+    @dispatch_decorator()
     def update_feature_view(
         self, name: str, version: str, refresh_freq: Optional[str] = None, warehouse: Optional[str] = None
     ) -> FeatureView:
         """Update a registered feature view.
             Check feature_view.py for which fields are allowed to be updated after registration.
 
         Args:
@@ -452,15 +505,15 @@
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                 original_exception=RuntimeError(
                     f"Update feature view {feature_view.name}/{feature_view.version} failed: {e}"
                 ),
             ) from e
         return self.get_feature_view(name=name, version=version)
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def read_feature_view(self, feature_view: FeatureView) -> DataFrame:
         """
         Read FeatureView data.
 
         Args:
             feature_view: FeatureView to retrieve data from.
 
@@ -474,57 +527,48 @@
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"FeatureView {feature_view.name} has not been registered."),
             )
 
         return self._session.sql(f"SELECT * FROM {feature_view.fully_qualified_name()}")
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def list_feature_views(
         self,
         entity_name: Optional[str] = None,
         feature_view_name: Optional[str] = None,
-        as_dataframe: bool = True,
-    ) -> Union[Optional[DataFrame], List[FeatureView]]:
+    ) -> DataFrame:
         """
         List FeatureViews in the FeatureStore.
         If entity_name is specified, FeatureViews associated with that Entity will be listed.
         If feature_view_name is specified, further reducing the results to only match the specified name.
 
         Args:
             entity_name: Entity name.
             feature_view_name: FeatureView name.
-            as_dataframe: whether the return type should be a DataFrame.
 
         Returns:
-            List of FeatureViews or in a DataFrame representation.
+            FeatureViews information as a Snowpark DataFrame.
         """
-        if entity_name is not None:
-            entity_name = SqlIdentifier(entity_name)
         if feature_view_name is not None:
             feature_view_name = SqlIdentifier(feature_view_name)
 
         if entity_name is not None:
-            fvs = self._find_feature_views(entity_name, feature_view_name)
+            entity_name = SqlIdentifier(entity_name)
+            if self._use_optimized_tag_ref:
+                return self._optimized_find_feature_views(entity_name, feature_view_name)
+            else:
+                return self._find_feature_views(entity_name, feature_view_name)
         else:
-            fvs = []
-            entities = self.list_entities().collect()
+            output_values: List[List[Any]] = []
             for row in self._get_fv_backend_representations(feature_view_name, prefix_match=True):
-                fvs.append(self._compose_feature_view(row, entities))
+                self._extract_feature_view_info(row, output_values)
+            return self._session.create_dataframe(output_values, schema=_LIST_FEATURE_VIEW_SCHEMA)
 
-        if as_dataframe:
-            result = None
-            for fv in fvs:
-                fv_df = fv.to_df(self._session)
-                result = fv_df if result is None else result.union(fv_df)  # type: ignore[attr-defined]
-            return result
-        else:
-            return fvs
-
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def get_feature_view(self, name: str, version: str) -> FeatureView:
         """
         Retrieve previously registered FeatureView.
 
         Args:
             name: FeatureView name.
             version: FeatureView version.
@@ -545,51 +589,53 @@
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"Failed to find FeatureView {name}/{version}: {results}"),
             )
 
         return self._compose_feature_view(results[0], self.list_entities().collect())
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def resume_feature_view(self, feature_view: FeatureView) -> FeatureView:
         """
         Resume a previously suspended FeatureView.
 
         Args:
             feature_view: FeatureView to resume.
 
         Returns:
             A new feature view with updated status.
         """
         return self._update_feature_view_status(feature_view, "RESUME")
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def suspend_feature_view(self, feature_view: FeatureView) -> FeatureView:
         """
         Suspend an active FeatureView.
 
         Args:
             feature_view: FeatureView to suspend.
 
         Returns:
             A new feature view with updated status.
         """
         return self._update_feature_view_status(feature_view, "SUSPEND")
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def delete_feature_view(self, feature_view: FeatureView) -> None:
         """
         Delete a FeatureView.
 
         Args:
             feature_view: FeatureView to delete.
 
         Raises:
             SnowflakeMLException: [ValueError] FeatureView is not registered.
         """
+        # TODO: we should leverage lineage graph to check downstream deps, and block the deletion
+        # if there're other FVs depending on this
         if feature_view.status == FeatureViewStatus.DRAFT or feature_view.version is None:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"FeatureView {feature_view.name} has not been registered."),
             )
 
         fully_qualified_name = feature_view.fully_qualified_name()
@@ -604,15 +650,15 @@
             if feature_view.refresh_freq == "DOWNSTREAM":
                 self._session.sql(f"DROP TASK IF EXISTS {fully_qualified_name}").collect(
                     statement_params=self._telemetry_stmp
                 )
 
         logger.info(f"Deleted FeatureView {feature_view.name}/{feature_view.version}.")
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def list_entities(self) -> DataFrame:
         """
         List all Entities in the FeatureStore.
 
         Returns:
             Snowpark DataFrame containing the results.
         """
@@ -625,15 +671,15 @@
                 F.col('"name"').substr(prefix_len, _ENTITY_NAME_LENGTH_LIMIT).alias("NAME"),
                 F.col('"allowed_values"').alias("JOIN_KEYS"),
                 F.col('"comment"').alias("DESC"),
                 F.col('"owner"').alias("OWNER"),
             ),
         )
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def get_entity(self, name: str) -> Entity:
         """
         Retrieve previously registered Entity object.
 
         Args:
             name: Entity name.
 
@@ -655,25 +701,24 @@
             ) from e
         if len(result) == 0:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"Cannot find Entity with name: {name}."),
             )
 
-        raw_join_keys = result[0]["JOIN_KEYS"]
-        join_keys = raw_join_keys.strip("[]").split(",")
+        join_keys = self._recompose_join_keys(result[0]["JOIN_KEYS"])
 
         return Entity._construct_entity(
             name=SqlIdentifier(result[0]["NAME"], case_sensitive=True).identifier(),
             join_keys=join_keys,
             desc=result[0]["DESC"],
             owner=result[0]["OWNER"],
         )
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def delete_entity(self, name: str) -> None:
         """
         Delete a previously registered Entity.
 
         Args:
             name: Entity name.
 
@@ -686,34 +731,34 @@
 
         if not self._validate_entity_exists(name):
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"Entity {name} does not exist."),
             )
 
-        active_feature_views = cast(List[FeatureView], self.list_feature_views(entity_name=name, as_dataframe=False))
+        active_feature_views = self.list_feature_views(entity_name=name).collect(statement_params=self._telemetry_stmp)
+
         if len(active_feature_views) > 0:
+            active_fvs = [r["NAME"] for r in active_feature_views]
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.SNOWML_DELETE_FAILED,
-                original_exception=ValueError(
-                    f"Cannot delete Entity {name} due to active FeatureViews: {[f.name for f in active_feature_views]}."
-                ),
+                original_exception=ValueError(f"Cannot delete Entity {name} due to active FeatureViews: {active_fvs}."),
             )
 
         tag_name = self._get_fully_qualified_name(self._get_entity_name(name))
         try:
             self._session.sql(f"DROP TAG IF EXISTS {tag_name}").collect(statement_params=self._telemetry_stmp)
         except Exception as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                 original_exception=RuntimeError(f"Failed to alter schema or drop tag: {e}."),
             ) from e
         logger.info(f"Deleted Entity {name}.")
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def retrieve_feature_values(
         self,
         spine_df: DataFrame,
         features: Union[List[Union[FeatureView, FeatureViewSlice]], List[str]],
         spine_timestamp_col: Optional[str] = None,
         exclude_columns: Optional[List[str]] = None,
         include_feature_view_timestamp_col: bool = False,
@@ -753,167 +798,142 @@
         )
 
         if exclude_columns is not None:
             df = self._exclude_columns(df, exclude_columns)
 
         return df
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def generate_dataset(
         self,
+        name: str,
         spine_df: DataFrame,
         features: List[Union[FeatureView, FeatureViewSlice]],
-        materialized_table: Optional[str] = None,
+        version: Optional[str] = None,
         spine_timestamp_col: Optional[str] = None,
         spine_label_cols: Optional[List[str]] = None,
         exclude_columns: Optional[List[str]] = None,
-        save_mode: str = "errorifexists",
         include_feature_view_timestamp_col: bool = False,
         desc: str = "",
-    ) -> Dataset:
+    ) -> dataset.Dataset:
         """
         Generate dataset by given source table and feature views.
 
         Args:
+            name: The name of the Dataset to be generated. Datasets are uniquely identified within a schema
+                by their name and version.
             spine_df: The fact table contains the raw dataset.
             features: A list of FeatureView or FeatureViewSlice which contains features to be joined.
-            materialized_table: The destination table where produced result will be stored. If it's none, then result
-                won't be registered. If materialized_table is provided, then produced result will be written into
-                the provided table. Note result dataset will be a snowflake clone of registered table.
-                New data can append on same registered table and previously generated dataset won't be affected.
-                Default result table name will be a concatenation of materialized_table name and current timestamp.
+            version: The version of the Dataset to be generated. If none specified, the current timestamp
+                will be used instead.
             spine_timestamp_col: Name of timestamp column in spine_df that will be used to join
                 time-series features. If spine_timestamp_col is not none, the input features also must have
                 timestamp_col.
             spine_label_cols: Name of column(s) in spine_df that contains labels.
             exclude_columns: Column names to exclude from the result dataframe.
                 The underlying storage will still contain the columns.
-            save_mode: How new data is saved. currently support:
-                errorifexists: Raise error if registered table already exists.
-                merge: Merge new data if registered table already exists.
             include_feature_view_timestamp_col: Generated dataset will include timestamp column of feature view
                 (if feature view has timestamp column) if set true. Default to false.
             desc: A description about this dataset.
 
         Returns:
             A Dataset object.
 
         Raises:
-            SnowflakeMLException: [ValueError] save_mode is invalid.
             SnowflakeMLException: [ValueError] spine_df contains more than one query.
-            SnowflakeMLException: [ValueError] Materialized_table contains invalid char `.`.
-            SnowflakeMLException: [ValueError] Materialized_table already exists with save_mode `errorifexists`.
+            SnowflakeMLException: [ValueError] Dataset name/version already exists
             SnowflakeMLException: [ValueError] Snapshot creation failed.
             SnowflakeMLException: [RuntimeError] Failed to create clone from table.
             SnowflakeMLException: [RuntimeError] Failed to find resources.
         """
         if spine_timestamp_col is not None:
             spine_timestamp_col = SqlIdentifier(spine_timestamp_col)
         if spine_label_cols is not None:
             spine_label_cols = to_sql_identifiers(spine_label_cols)  # type: ignore[assignment]
 
-        allowed_save_mode = {"errorifexists", "merge"}
-        if save_mode.lower() not in allowed_save_mode:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INVALID_ARGUMENT,
-                original_exception=ValueError(
-                    f"'{save_mode}' is not supported. Current supported save modes: {','.join(allowed_save_mode)}"
-                ),
-            )
-
         if len(spine_df.queries["queries"]) != 1:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INVALID_ARGUMENT,
                 original_exception=ValueError(
                     f"spine_df must contain only one query. Got: {spine_df.queries['queries']}"
                 ),
             )
 
         result_df, join_keys = self._join_features(
             spine_df, features, spine_timestamp_col, include_feature_view_timestamp_col
         )
 
-        snapshot_table = None
-        if materialized_table is not None:
-            if "." in materialized_table:
-                raise snowml_exceptions.SnowflakeMLException(
-                    error_code=error_codes.INVALID_ARGUMENT,
-                    original_exception=ValueError(f"materialized_table {materialized_table} contains invalid char `.`"),
-                )
-
-            # TODO (wezhou) change materialized_table to SqlIdentifier
-            found_rows = self._find_object("TABLES", SqlIdentifier(materialized_table))
-            if save_mode.lower() == "errorifexists" and len(found_rows) > 0:
-                raise snowml_exceptions.SnowflakeMLException(
-                    error_code=error_codes.OBJECT_ALREADY_EXISTS,
-                    original_exception=ValueError(f"Dataset table {materialized_table} already exists."),
-                )
-
-            self._dump_dataset(result_df, materialized_table, join_keys, spine_timestamp_col)
-
-            snapshot_table = f"{materialized_table}_{datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')}"
-            snapshot_table = self._get_fully_qualified_name(snapshot_table)
-            materialized_table = self._get_fully_qualified_name(materialized_table)
-
-            try:
-                self._session.sql(f"CREATE TABLE {snapshot_table} CLONE {materialized_table}").collect(
-                    statement_params=self._telemetry_stmp
-                )
-            except Exception as e:
-                raise snowml_exceptions.SnowflakeMLException(
-                    error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
-                    original_exception=RuntimeError(
-                        f"Failed to create clone {materialized_table} from table {snapshot_table}: {e}."
-                    ),
-                ) from e
-
-            result_df = self._session.sql(f"SELECT * FROM {snapshot_table}")
+        # Convert name to fully qualified name if not already fully qualified
+        db_name, schema_name, object_name, _ = identifier.parse_schema_level_object_identifier(name)
+        name = "{}.{}.{}".format(
+            db_name or self._config.database,
+            schema_name or self._config.schema,
+            object_name,
+        )
+        version = version or datetime.datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
 
         if exclude_columns is not None:
             result_df = self._exclude_columns(result_df, exclude_columns)
 
         fs_meta = FeatureStoreMetadata(
             spine_query=spine_df.queries["queries"][0],
-            connection_params=vars(self._config),
-            features=[fv.to_json() for fv in features],
+            serialized_feature_views=[fv.to_json() for fv in features],
+            spine_timestamp_col=spine_timestamp_col,
         )
 
-        dataset = Dataset(
-            self._session,
-            df=result_df,
-            materialized_table=materialized_table,
-            snapshot_table=snapshot_table,
-            timestamp_col=spine_timestamp_col,
-            label_cols=spine_label_cols,
-            feature_store_metadata=fs_meta,
-            desc=desc,
-        )
-        return dataset
+        try:
+            ds: dataset.Dataset = dataset.create_from_dataframe(
+                self._session,
+                name,
+                version,
+                input_dataframe=result_df,
+                exclude_cols=[spine_timestamp_col],
+                label_cols=spine_label_cols,
+                properties=fs_meta,
+                comment=desc,
+            )
+            return ds
 
-    @dispatch_decorator(prpr_version="1.0.8")
-    def load_feature_views_from_dataset(self, dataset: Dataset) -> List[Union[FeatureView, FeatureViewSlice]]:
+        except dataset_errors.DatasetExistError as e:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.OBJECT_ALREADY_EXISTS,
+                original_exception=ValueError(str(e)),
+            ) from e
+        except SnowparkSQLException as e:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
+                original_exception=RuntimeError(f"An error occurred during Dataset generation: {e}."),
+            ) from e
+
+    @dispatch_decorator()
+    def load_feature_views_from_dataset(self, ds: dataset.Dataset) -> List[Union[FeatureView, FeatureViewSlice]]:
         """
         Retrieve FeatureViews used during Dataset construction.
 
         Args:
-            dataset: Dataset object created from feature store.
+            ds: Dataset object created from feature store.
 
         Returns:
             List of FeatureViews used during Dataset construction.
 
         Raises:
             ValueError: if dataset object is not generated from feature store.
         """
-        serialized_objs = dataset.load_features()
-        if serialized_objs is None:
-            raise ValueError(f"Dataset {dataset} does not contain valid feature view information.")
+        assert ds.selected_version is not None
+        source_meta = ds.selected_version._get_metadata()
+        if (
+            source_meta is None
+            or not isinstance(source_meta.properties, FeatureStoreMetadata)
+            or source_meta.properties.serialized_feature_views is None
+        ):
+            raise ValueError(f"Dataset {ds} does not contain valid feature view information.")
 
-        return self._load_serialized_feature_objects(serialized_objs)
+        return self._load_serialized_feature_objects(source_meta.properties.serialized_feature_views)
 
-    @dispatch_decorator(prpr_version="1.0.8")
+    @dispatch_decorator()
     def clear(self) -> None:
         """
         Clear all feature store internal objects including feature views, entities etc. Note feature store
         instance (snowflake schema) won't be deleted. Use snowflake to delete feature store instance.
 
         Raises:
             SnowflakeMLException: [RuntimeError] Failed to clear feature store.
@@ -925,27 +945,30 @@
                 FROM {self._config.database}.INFORMATION_SCHEMA.SCHEMATA
                 WHERE SCHEMA_NAME = '{self._config.schema.resolved()}'
             """
             ).collect()
             if len(result) == 0:
                 return
 
-            object_types = ["DYNAMIC TABLES", "TABLES", "VIEWS", "TASKS"]
+            fs_obj_tag = self._find_object("TAGS", SqlIdentifier(_FEATURE_STORE_OBJECT_TAG))
+            if len(fs_obj_tag) == 0:
+                return
+
+            object_types = ["DYNAMIC TABLES", "DATASETS", "VIEWS", "TASKS"]
             for obj_type in object_types:
                 all_object_rows = self._find_object(obj_type, None)
                 for row in all_object_rows:
                     obj_name = self._get_fully_qualified_name(SqlIdentifier(row["name"], case_sensitive=True))
                     self._session.sql(f"DROP {obj_type[:-1]} {obj_name}").collect()
                     logger.info(f"Deleted {obj_type[:-1]}: {obj_name}.")
 
             entity_tags = self._find_object("TAGS", SqlIdentifier(_ENTITY_TAG_PREFIX), prefix_match=True)
             all_tags = [
-                _FEATURE_VIEW_ENTITY_TAG,
-                _FEATURE_VIEW_TS_COL_TAG,
                 _FEATURE_STORE_OBJECT_TAG,
+                _FEATURE_VIEW_METADATA_TAG,
             ] + [SqlIdentifier(row["name"], case_sensitive=True) for row in entity_tags]
             for tag_name in all_tags:
                 obj_name = self._get_fully_qualified_name(tag_name)
                 self._session.sql(f"DROP TAG IF EXISTS {obj_name}").collect()
                 logger.info(f"Deleted TAG: {obj_name}.")
 
         except Exception as e:
@@ -961,58 +984,67 @@
             f"FeatureView {name}/{version} already exists. Skip registration."
             + " Set `overwrite` to True if you want to replace existing FeatureView.",
             stacklevel=2,
             category=UserWarning,
         )
         return existing_fv
 
+    def _recompose_join_keys(self, join_key: str) -> List[str]:
+        # ALLOWED_VALUES in TAG will follow format ["key_1,key2,..."]
+        # since keys are already resolved following the SQL identifier rule on the write path,
+        # we simply parse the keys back and wrap them with quotes to preserve cases
+        # Example join_key repr from TAG value: "[key1,key2,key3]"
+        join_keys = join_key[2:-2].split(",")
+        res = []
+        for k in join_keys:
+            res.append(f'"{k}"')
+        return res
+
     def _create_dynamic_table(
         self,
         feature_view_name: SqlIdentifier,
         feature_view: FeatureView,
         fully_qualified_name: str,
         column_descs: str,
-        entities: str,
+        tagging_clause: str,
         schedule_task: bool,
         warehouse: SqlIdentifier,
-        timestamp_col: SqlIdentifier,
         block: bool,
         override: bool,
     ) -> None:
         # TODO: cluster by join keys once DT supports that
-        override_clause = " OR REPLACE" if override else ""
-        query = f"""CREATE{override_clause} DYNAMIC TABLE {fully_qualified_name} ({column_descs})
-            TARGET_LAG = '{'DOWNSTREAM' if schedule_task else feature_view.refresh_freq}'
-            COMMENT = '{feature_view.desc}'
-            TAG (
-                {self._get_fully_qualified_name(_FEATURE_VIEW_ENTITY_TAG)} = '{entities}',
-                {self._get_fully_qualified_name(_FEATURE_VIEW_TS_COL_TAG)} = '{timestamp_col}',
-                {self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)} =
-                    '{_FeatureStoreObjTypes.FEATURE_VIEW.value}'
-            )
-            WAREHOUSE = {warehouse}
-            AS {feature_view.query}
-        """
         try:
+            override_clause = " OR REPLACE" if override else ""
+            query = f"""CREATE{override_clause} DYNAMIC TABLE {fully_qualified_name} ({column_descs})
+                TARGET_LAG = '{'DOWNSTREAM' if schedule_task else feature_view.refresh_freq}'
+                COMMENT = '{feature_view.desc}'
+                TAG (
+                    {tagging_clause}
+                )
+                WAREHOUSE = {warehouse}
+                AS {feature_view.query}
+            """
             self._session.sql(query).collect(block=block, statement_params=self._telemetry_stmp)
 
             if schedule_task:
+                task_obj_info = _FeatureStoreObjInfo(
+                    _FeatureStoreObjTypes.FEATURE_VIEW_REFRESH_TASK, snowml_version.VERSION
+                )
                 try:
                     self._session.sql(
                         f"""CREATE{override_clause} TASK {fully_qualified_name}
                             WAREHOUSE = {warehouse}
                             SCHEDULE = 'USING CRON {feature_view.refresh_freq}'
                             AS ALTER DYNAMIC TABLE {fully_qualified_name} REFRESH
                         """
                     ).collect(statement_params=self._telemetry_stmp)
                     self._session.sql(
                         f"""
                         ALTER TASK {fully_qualified_name}
-                        SET TAG {self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}
-                            ='{_FeatureStoreObjTypes.FEATURE_VIEW_REFRESH_TASK.value}'
+                        SET TAG {self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}='{task_obj_info.to_json()}'
                     """
                     ).collect(statement_params=self._telemetry_stmp)
                     self._session.sql(f"ALTER TASK {fully_qualified_name} RESUME").collect(
                         statement_params=self._telemetry_stmp
                     )
                 except Exception:
                     self._session.sql(f"DROP DYNAMIC TABLE IF EXISTS {fully_qualified_name}").collect(
@@ -1045,65 +1077,14 @@
                 "Your pipeline won't be incrementally refreshed due to: "
                 + f"\"{found_dts[0]['refresh_mode_reason']}\". "
                 + "It will likely incurr higher cost.",
                 stacklevel=2,
                 category=UserWarning,
             )
 
-    def _dump_dataset(
-        self,
-        df: DataFrame,
-        table_name: str,
-        join_keys: List[SqlIdentifier],
-        spine_timestamp_col: Optional[SqlIdentifier] = None,
-    ) -> None:
-        if len(df.queries["queries"]) != 1:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INVALID_ARGUMENT,
-                original_exception=ValueError(f"Dataset df must contain only one query. Got: {df.queries['queries']}"),
-            )
-        schema = ", ".join([f"{c.name} {type_utils.convert_sp_to_sf_type(c.datatype)}" for c in df.schema.fields])
-        fully_qualified_name = self._get_fully_qualified_name(table_name)
-
-        try:
-            self._session.sql(
-                f"""CREATE TABLE IF NOT EXISTS {fully_qualified_name} ({schema})
-                    CLUSTER BY ({', '.join(join_keys)})
-                    TAG ({self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)} = '')
-                """
-            ).collect(block=True, statement_params=self._telemetry_stmp)
-        except Exception as e:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
-                original_exception=RuntimeError(f"Failed to create table {fully_qualified_name}: {e}."),
-            ) from e
-
-        source_query = df.queries["queries"][0]
-
-        if spine_timestamp_col is not None:
-            join_keys.append(spine_timestamp_col)
-
-        _, _, dest_alias, _ = identifier.parse_schema_level_object_identifier(fully_qualified_name)
-        source_alias = f"{dest_alias}_source"
-        join_cond = " AND ".join([f"{dest_alias}.{k} = {source_alias}.{k}" for k in join_keys])
-        update_clause = ", ".join([f"{dest_alias}.{c} = {source_alias}.{c}" for c in df.columns])
-        insert_clause = ", ".join([f"{source_alias}.{c}" for c in df.columns])
-        query = f"""
-            MERGE INTO {fully_qualified_name} USING ({source_query}) {source_alias}  ON {join_cond}
-            WHEN MATCHED THEN UPDATE SET {update_clause}
-            WHEN NOT MATCHED THEN INSERT ({', '.join(df.columns)}) VALUES ({insert_clause})
-        """
-        try:
-            self._session.sql(query).collect(block=True, statement_params=self._telemetry_stmp)
-        except Exception as e:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
-                original_exception=RuntimeError(f"Failed to create dataset {fully_qualified_name} with merge: {e}."),
-            ) from e
-
     def _validate_entity_exists(self, name: SqlIdentifier) -> bool:
         full_entity_tag_name = self._get_entity_name(name)
         found_rows = self._find_object("TAGS", full_entity_tag_name)
         return len(found_rows) == 1
 
     def _join_features(
         self,
@@ -1146,15 +1127,15 @@
         for f in features:
             if isinstance(f, FeatureViewSlice):
                 cols = f.names
                 f = f.feature_view_ref
             else:
                 cols = f.feature_names
 
-            join_keys = [k for e in f.entities for k in e.join_keys]
+            join_keys = list({k for e in f.entities for k in e.join_keys})
             join_keys_str = ", ".join(join_keys)
             assert f.version is not None
             join_table_name = f.fully_qualified_name()
 
             if spine_timestamp_col is not None and f.timestamp_col is not None:
                 if self._asof_join_enabled:
                     if include_feature_view_timestamp_col:
@@ -1223,16 +1204,15 @@
                     f"Feature store schema {self._config.schema} does not exist. "
                     "Use CreationMode.CREATE_IF_NOT_EXIST mode instead if you want to create one."
                 ),
             )
         for tag_name in to_sql_identifiers(
             [
                 _FEATURE_STORE_OBJECT_TAG,
-                _FEATURE_VIEW_ENTITY_TAG,
-                _FEATURE_VIEW_TS_COL_TAG,
+                _FEATURE_VIEW_METADATA_TAG,
             ]
         ):
             tag_result = self._find_object("TAGS", tag_name)
             if len(tag_result) == 0:
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.NOT_FOUND,
                     original_exception=ValueError(
@@ -1336,15 +1316,16 @@
         window_cte = f"""
             windowed_{layer} AS (
                 {window_select}
             )"""
 
         # Part 4: join original spine table with window table
         prefix_f_only_cols = to_sql_identifiers(
-            [f"{temp_prefix}{name.resolved()}" for name in f_only_cols], case_sensitive=True
+            [f"{temp_prefix}{name.resolved()}" for name in f_only_cols],
+            case_sensitive=True,
         )
         last_select = f"""
             SELECT
                 {join_keys_str},
                 {s_ts_col},
                 {join_cols(s_only_cols, end_comma=True, rename=False)}
                 {join_cols(prefix_f_only_cols, end_comma=False, rename=True)}
@@ -1369,15 +1350,18 @@
         self, object_name: Optional[SqlIdentifier], prefix_match: bool = False
     ) -> List[Row]:
         dynamic_table_results = self._find_object("DYNAMIC TABLES", object_name, prefix_match)
         view_results = self._find_object("VIEWS", object_name, prefix_match)
         return dynamic_table_results + view_results
 
     def _update_feature_view_status(self, feature_view: FeatureView, operation: str) -> FeatureView:
-        assert operation in ["RESUME", "SUSPEND"], f"Operation: {operation} not supported"
+        assert operation in [
+            "RESUME",
+            "SUSPEND",
+        ], f"Operation: {operation} not supported"
         if feature_view.status == FeatureViewStatus.DRAFT or feature_view.version is None:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(f"FeatureView {feature_view.name} has not been registered."),
             )
 
         fully_qualified_name = feature_view.fully_qualified_name()
@@ -1393,25 +1377,84 @@
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                 original_exception=RuntimeError(f"Failed to update feature view {fully_qualified_name}'s status: {e}"),
             ) from e
 
         logger.info(f"Successfully {operation} FeatureView {feature_view.name}/{feature_view.version}.")
         return self.get_feature_view(feature_view.name, feature_view.version)
 
-    def _find_feature_views(
+    def _optimized_find_feature_views(
         self, entity_name: SqlIdentifier, feature_view_name: Optional[SqlIdentifier]
-    ) -> List[FeatureView]:
+    ) -> DataFrame:
         if not self._validate_entity_exists(entity_name):
-            return []
+            return self._session.create_dataframe([], schema=_LIST_FEATURE_VIEW_SCHEMA)
 
+        # TODO: this can be optimized further by directly getting all possible FVs and filter by tag
+        # it's easier to rewrite the code once we can remove the tag_reference path
         all_fvs = self._get_fv_backend_representations(object_name=None)
         fv_maps = {SqlIdentifier(r["name"], case_sensitive=True): r for r in all_fvs}
 
         if len(fv_maps.keys()) == 0:
-            return []
+            return self._session.create_dataframe([], schema=_LIST_FEATURE_VIEW_SCHEMA)
+
+        filter_clause = f"WHERE OBJECT_NAME LIKE '{feature_view_name.resolved()}%'" if feature_view_name else ""
+        try:
+            res = self._session.sql(
+                f"""
+                    SELECT
+                        OBJECT_NAME
+                    FROM TABLE(
+                        {self._config.database}.INFORMATION_SCHEMA.TAG_REFERENCES_INTERNAL(
+                            TAG_NAME => '{self._get_fully_qualified_name(self._get_entity_name(entity_name))}'
+                        )
+                    ) {filter_clause}"""
+            ).collect(statement_params=self._telemetry_stmp)
+        except Exception as e:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
+                original_exception=RuntimeError(f"Failed to find feature views' by entity {entity_name}: {e}"),
+            ) from e
+
+        output_values: List[List[Any]] = []
+        for r in res:
+            row = fv_maps[SqlIdentifier(r["OBJECT_NAME"], case_sensitive=True)]
+            self._extract_feature_view_info(row, output_values)
+
+        return self._session.create_dataframe(output_values, schema=_LIST_FEATURE_VIEW_SCHEMA)
+
+    def _extract_feature_view_info(self, row: Row, output_values: List[List[Any]]) -> None:
+        name, version = row["name"].split(_FEATURE_VIEW_NAME_DELIMITER)
+        m = re.match(_DT_OR_VIEW_QUERY_PATTERN, row["text"])
+        if m is None:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.INTERNAL_SNOWML_ERROR,
+                original_exception=RuntimeError(f"Failed to parse query text for FeatureView {name}/{version}: {row}."),
+            )
+
+        fv_metadata = _FeatureViewMetadata.from_json(m.group("fv_metadata"))
+
+        values: List[Any] = []
+        values.append(name)
+        values.append(version)
+        values.append(row["database_name"])
+        values.append(row["schema_name"])
+        values.append(row["created_on"])
+        values.append(row["owner"])
+        values.append(row["comment"])
+        values.append(fv_metadata.entities)
+        output_values.append(values)
+
+    def _find_feature_views(self, entity_name: SqlIdentifier, feature_view_name: Optional[SqlIdentifier]) -> DataFrame:
+        if not self._validate_entity_exists(entity_name):
+            return self._session.create_dataframe([], schema=_LIST_FEATURE_VIEW_SCHEMA)
+
+        all_fvs = self._get_fv_backend_representations(object_name=None)
+        fv_maps = {SqlIdentifier(r["name"], case_sensitive=True): r for r in all_fvs}
+
+        if len(fv_maps.keys()) == 0:
+            return self._session.create_dataframe([], schema=_LIST_FEATURE_VIEW_SCHEMA)
 
         # NOTE: querying INFORMATION_SCHEMA for Entity lineage can be expensive depending on how many active
         # FeatureViews there are. If this ever become an issue, consider exploring improvements.
         try:
             queries = [
                 f"""
                     SELECT
@@ -1420,50 +1463,51 @@
                     FROM TABLE(
                         {self._config.database}.INFORMATION_SCHEMA.TAG_REFERENCES(
                             '{self._get_fully_qualified_name(fv_name)}',
                             'table'
                         )
                     )
                     WHERE LEVEL = 'TABLE'
-                    AND TAG_NAME = '{_FEATURE_VIEW_ENTITY_TAG}'
+                    AND TAG_NAME = '{_FEATURE_VIEW_METADATA_TAG}'
                 """
                 for fv_name in fv_maps.keys()
             ]
 
             results = self._session.sql("\nUNION\n".join(queries)).collect(statement_params=self._telemetry_stmp)
         except Exception as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                 original_exception=RuntimeError(f"Failed to retrieve feature views' information: {e}"),
             ) from e
 
-        entities = self.list_entities().collect()
-        outputs = []
+        output_values: List[List[Any]] = []
         for r in results:
-            if entity_name == SqlIdentifier(r["TAG_VALUE"], case_sensitive=True):
-                fv_name, _ = r["OBJECT_NAME"].split(_FEATURE_VIEW_NAME_DELIMITER)
-                fv_name = SqlIdentifier(fv_name, case_sensitive=True)
-                obj_name = SqlIdentifier(r["OBJECT_NAME"], case_sensitive=True)
-                if feature_view_name is not None:
-                    if fv_name == feature_view_name:
-                        outputs.append(self._compose_feature_view(fv_maps[obj_name], entities))
+            fv_metadata = _FeatureViewMetadata.from_json(r["TAG_VALUE"])
+            for retrieved_entity in fv_metadata.entities:
+                if entity_name == SqlIdentifier(retrieved_entity, case_sensitive=True):
+                    fv_name, _ = r["OBJECT_NAME"].split(_FEATURE_VIEW_NAME_DELIMITER)
+                    fv_name = SqlIdentifier(fv_name, case_sensitive=True)
+                    obj_name = SqlIdentifier(r["OBJECT_NAME"], case_sensitive=True)
+                    if feature_view_name is not None:
+                        if fv_name == feature_view_name:
+                            self._extract_feature_view_info(fv_maps[obj_name], output_values)
+                        else:
+                            continue
                     else:
-                        continue
-                else:
-                    outputs.append(self._compose_feature_view(fv_maps[obj_name], entities))
-        return outputs
+                        self._extract_feature_view_info(fv_maps[obj_name], output_values)
+        return self._session.create_dataframe(output_values, schema=_LIST_FEATURE_VIEW_SCHEMA)
 
     def _compose_feature_view(self, row: Row, entity_list: List[Row]) -> FeatureView:
         def find_and_compose_entity(name: str) -> Entity:
             name = SqlIdentifier(name).resolved()
             for e in entity_list:
                 if e["NAME"] == name:
                     return Entity(
                         name=SqlIdentifier(e["NAME"], case_sensitive=True).identifier(),
-                        join_keys=e["JOIN_KEYS"].strip("[]").split(","),
+                        join_keys=self._recompose_join_keys(e["JOIN_KEYS"]),
                         desc=e["DESC"],
                     )
             raise RuntimeError(f"Cannot find entity {name} from retrieved entity list: {entity_list}")
 
         name, version = row["name"].split(_FEATURE_VIEW_NAME_DELIMITER)
         name = SqlIdentifier(name, case_sensitive=True)
         m = re.match(_DT_OR_VIEW_QUERY_PATTERN, row["text"])
@@ -1473,17 +1517,17 @@
                 original_exception=RuntimeError(f"Failed to parse query text for FeatureView {name}/{version}: {row}."),
             )
 
         if m.group("obj_type") == "DYNAMIC TABLE":
             query = m.group("query")
             df = self._session.sql(query)
             desc = m.group("comment")
-            entity_names = m.group("entities")
-            entities = [find_and_compose_entity(n) for n in entity_names.split(_FEATURE_VIEW_ENTITY_TAG_DELIMITER)]
-            ts_col = m.group("ts_col")
+            fv_metadata = _FeatureViewMetadata.from_json(m.group("fv_metadata"))
+            entities = [find_and_compose_entity(n) for n in fv_metadata.entities]
+            ts_col = fv_metadata.timestamp_col
             timestamp_col = ts_col if ts_col != _TIMESTAMP_COL_PLACEHOLDER else None
 
             fv = FeatureView._construct_feature_view(
                 name=name,
                 entities=entities,
                 feature_df=df,
                 timestamp_col=timestamp_col,
@@ -1502,17 +1546,17 @@
                 owner=row["owner"],
             )
             return fv
         else:
             query = m.group("query")
             df = self._session.sql(query)
             desc = m.group("comment")
-            entity_names = m.group("entities")
-            entities = [find_and_compose_entity(n) for n in entity_names.split(_FEATURE_VIEW_ENTITY_TAG_DELIMITER)]
-            ts_col = m.group("ts_col")
+            fv_metadata = _FeatureViewMetadata.from_json(m.group("fv_metadata"))
+            entities = [find_and_compose_entity(n) for n in fv_metadata.entities]
+            ts_col = fv_metadata.timestamp_col
             timestamp_col = ts_col if ts_col != _TIMESTAMP_COL_PLACEHOLDER else None
 
             fv = FeatureView._construct_feature_view(
                 name=name,
                 entities=entities,
                 feature_df=df,
                 timestamp_col=timestamp_col,
@@ -1538,15 +1582,18 @@
         descs = {}
         for r in res:
             if r["comment"] is not None:
                 descs[SqlIdentifier(r["name"], case_sensitive=True).identifier()] = r["comment"]
         return descs
 
     def _find_object(
-        self, object_type: str, object_name: Optional[SqlIdentifier], prefix_match: bool = False
+        self,
+        object_type: str,
+        object_name: Optional[SqlIdentifier],
+        prefix_match: bool = False,
     ) -> List[Row]:
         """Try to find an object by given type and name pattern.
 
         Args:
             object_type: Type of the object. Could be TABLES, TAGS etc.
             object_name: Name of object. It will match everything of object_type is object_name is None.
             prefix_match: Will search all objects with object_name as prefix if set True. Otherwise
@@ -1565,41 +1612,57 @@
             match_name = object_name.resolved() + "%"
         else:
             match_name = object_name.resolved()
 
         search_space, obj_domain = self._obj_search_spaces[object_type]
         all_rows = []
         fs_tag_objects = []
-        tag_free_object_types = ["TAGS", "SCHEMAS", "WAREHOUSES"]
+        tag_free_object_types = ["TAGS", "SCHEMAS", "WAREHOUSES", "DATASETS"]
         try:
             search_scope = f"IN {search_space}" if search_space is not None else ""
             all_rows = self._session.sql(f"SHOW {object_type} LIKE '{match_name}' {search_scope}").collect(
                 statement_params=self._telemetry_stmp
             )
             # There could be none-FS objects under FS schema, thus filter on objects with FS special tag.
             if object_type not in tag_free_object_types and len(all_rows) > 0:
-                # Note: <object_name> in TAG_REFERENCES(<object_name>) is case insensitive,
-                # use double quotes to make it case-sensitive.
-                queries = [
-                    f"""
-                        SELECT OBJECT_NAME
-                        FROM TABLE(
-                            {self._config.database}.INFORMATION_SCHEMA.TAG_REFERENCES(
-                                '{self._get_fully_qualified_name(SqlIdentifier(row['name'], case_sensitive=True))}',
-                                '{obj_domain}'
+                if self._use_optimized_tag_ref:
+                    fs_obj_rows = self._session.sql(
+                        f"""
+                            SELECT
+                                OBJECT_NAME
+                            FROM TABLE(
+                                {self._config.database}.INFORMATION_SCHEMA.TAG_REFERENCES_INTERNAL(
+                                    TAG_NAME => '{self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}'
+                                )
                             )
-                        )
-                        WHERE TAG_NAME = '{_FEATURE_STORE_OBJECT_TAG}'
-                        AND TAG_SCHEMA = '{self._config.schema.resolved()}'
-                    """
-                    for row in all_rows
-                ]
-                fs_obj_rows = self._session.sql("\nUNION\n".join(queries)).collect(
-                    statement_params=self._telemetry_stmp
-                )
+                            WHERE DOMAIN='{obj_domain}'
+                        """
+                    ).collect(statement_params=self._telemetry_stmp)
+                else:
+                    # TODO: remove this after tag_ref_internal rollout
+                    # Note: <object_name> in TAG_REFERENCES(<object_name>) is case insensitive,
+                    # use double quotes to make it case-sensitive.
+                    queries = [
+                        f"""
+                            SELECT OBJECT_NAME
+                            FROM TABLE(
+                                {self._config.database}.INFORMATION_SCHEMA.TAG_REFERENCES(
+                                    '{self._get_fully_qualified_name(SqlIdentifier(row['name'], case_sensitive=True))}',
+                                    '{obj_domain}'
+                                )
+                            )
+                            WHERE TAG_NAME = '{_FEATURE_STORE_OBJECT_TAG}'
+                            AND TAG_SCHEMA = '{self._config.schema.resolved()}'
+                        """
+                        for row in all_rows
+                    ]
+                    fs_obj_rows = self._session.sql("\nUNION\n".join(queries)).collect(
+                        statement_params=self._telemetry_stmp
+                    )
+
                 fs_tag_objects = [row["OBJECT_NAME"] for row in fs_obj_rows]
         except Exception as e:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INTERNAL_SNOWPARK_ERROR,
                 original_exception=RuntimeError(f"Failed to find object : {e}"),
             ) from e
 
@@ -1637,7 +1700,70 @@
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.INVALID_ARGUMENT,
                     original_exception=ValueError(
                         f"{col} in exclude_columns not exists in dataframe columns: {df_cols}"
                     ),
                 )
         return cast(DataFrame, df.drop(exclude_columns))
+
+    def _tag_ref_internal_enabled(self) -> bool:
+        try:
+            self._session.sql(
+                f"""
+                    SELECT * FROM TABLE(
+                        INFORMATION_SCHEMA.TAG_REFERENCES_INTERNAL(
+                            TAG_NAME => '{_FEATURE_STORE_OBJECT_TAG}'
+                        )
+                    ) LIMIT 1;
+                """
+            ).collect()
+            return True
+        except Exception:
+            return False
+
+    def _check_feature_store_object_versions(self) -> None:
+        versions = self._collapse_object_versions()
+        if len(versions) > 0 and pkg_version.parse(snowml_version.VERSION) < versions[0]:
+            warnings.warn(
+                "The current snowflake-ml-python version out of date, package upgrade recommended "
+                + f"(current={snowml_version.VERSION}, recommended>={str(versions[0])})",
+                stacklevel=2,
+                category=UserWarning,
+            )
+
+    def _collapse_object_versions(self) -> List[pkg_version.Version]:
+        if not self._use_optimized_tag_ref:
+            return []
+
+        query = f"""
+            SELECT
+                TAG_VALUE
+            FROM TABLE(
+                {self._config.database}.INFORMATION_SCHEMA.TAG_REFERENCES_INTERNAL(
+                    TAG_NAME => '{self._get_fully_qualified_name(_FEATURE_STORE_OBJECT_TAG)}'
+                )
+            )
+        """
+        try:
+            res = self._session.sql(query).collect(statement_params=self._telemetry_stmp)
+        except Exception:
+            # since this is a best effort user warning to upgrade pkg versions
+            # we are treating failures as benign error
+            return []
+        versions = set()
+        compatibility_breakage_detected = False
+        for r in res:
+            info = _FeatureStoreObjInfo.from_json(r["TAG_VALUE"])
+            if info.type == _FeatureStoreObjTypes.UNKNOWN:
+                compatibility_breakage_detected = True
+            versions.add(pkg_version.parse(info.pkg_version))
+
+        sorted_versions = sorted(versions, reverse=True)
+        if compatibility_breakage_detected:
+            raise snowml_exceptions.SnowflakeMLException(
+                error_code=error_codes.SNOWML_PACKAGE_OUTDATED,
+                original_exception=RuntimeError(
+                    f"The current snowflake-ml-python version {snowml_version.VERSION} is out of date, "
+                    + f"please upgrade to at least {sorted_versions[0]}."
+                ),
+            )
+        return sorted_versions
```

## snowflake/ml/feature_store/feature_view.py

```diff
@@ -1,12 +1,13 @@
 from __future__ import annotations
 
 import json
+import re
 from collections import OrderedDict
-from dataclasses import dataclass
+from dataclasses import asdict, dataclass
 from enum import Enum
 from typing import Dict, List, Optional
 
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions as snowml_exceptions,
 )
@@ -24,27 +25,50 @@
     TimeType,
     _NumericType,
 )
 
 _FEATURE_VIEW_NAME_DELIMITER = "$"
 _TIMESTAMP_COL_PLACEHOLDER = "FS_TIMESTAMP_COL_PLACEHOLDER_VAL"
 _FEATURE_OBJ_TYPE = "FEATURE_OBJ_TYPE"
+# Feature view version rule is aligned with dataset version rule in SQL.
+_FEATURE_VIEW_VERSION_RE = re.compile(r"^[a-zA-Z0-9][a-zA-Z0-9_.\-]*$")
+_FEATURE_VIEW_VERSION_MAX_LENGTH = 128
 
 
-class FeatureViewVersion(SqlIdentifier):
+@dataclass(frozen=True)
+class _FeatureViewMetadata:
+    """Represent metadata tracked on top of FV backend object"""
+
+    entities: List[str]
+    timestamp_col: str
+
+    def to_json(self) -> str:
+        return json.dumps(asdict(self))
+
+    @classmethod
+    def from_json(cls, json_str: str) -> _FeatureViewMetadata:
+        state_dict = json.loads(json_str)
+        return cls(**state_dict)
+
+
+class FeatureViewVersion(str):
     def __new__(cls, version: str) -> FeatureViewVersion:
-        if _FEATURE_VIEW_NAME_DELIMITER in version:
+        if not _FEATURE_VIEW_VERSION_RE.match(version) or len(version) > _FEATURE_VIEW_VERSION_MAX_LENGTH:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INVALID_ARGUMENT,
-                original_exception=ValueError(f"{_FEATURE_VIEW_NAME_DELIMITER} is not allowed in version: {version}."),
+                original_exception=ValueError(
+                    f"`{version}` is not a valid feature view version. "
+                    "It must start with letter or digit, and followed by letter, digit, '_', '-' or '.'. "
+                    f"The length limit is {_FEATURE_VIEW_VERSION_MAX_LENGTH}."
+                ),
             )
-        return super().__new__(cls, version)  # type: ignore[return-value]
+        return super().__new__(cls, version)
 
     def __init__(self, version: str) -> None:
-        super().__init__(version)
+        super().__init__()
 
 
 class FeatureViewStatus(Enum):
     DRAFT = "DRAFT"
     STATIC = "STATIC"
     RUNNING = "RUNNING"  # This can be deprecated after BCR 2024_02 gets fully deployed
     SUSPENDED = "SUSPENDED"
@@ -281,14 +305,19 @@
     def refresh_mode_reason(self) -> Optional[str]:
         return self._refresh_mode_reason
 
     @property
     def owner(self) -> Optional[str]:
         return self._owner
 
+    def _metadata(self) -> _FeatureViewMetadata:
+        entity_names = [e.name.identifier() for e in self.entities]
+        ts_col = self.timestamp_col.identifier() if self.timestamp_col is not None else _TIMESTAMP_COL_PLACEHOLDER
+        return _FeatureViewMetadata(entity_names, ts_col)
+
     def _get_query(self) -> str:
         if len(self._feature_df.queries["queries"]) != 1:
             raise ValueError(
                 f"""feature_df dataframe must contain only 1 query.
 Got {len(self._feature_df.queries['queries'])}: {self._feature_df.queries['queries']}
 """
             )
@@ -432,16 +461,16 @@
         feature_df: DataFrame,
         timestamp_col: Optional[str],
         desc: str,
         version: str,
         status: FeatureViewStatus,
         feature_descs: Dict[str, str],
         refresh_freq: Optional[str],
-        database: Optional[str],
-        schema: Optional[str],
+        database: str,
+        schema: str,
         warehouse: Optional[str],
         refresh_mode: Optional[str],
         refresh_mode_reason: Optional[str],
         owner: Optional[str],
     ) -> FeatureView:
         fv = FeatureView(
             name=name,
```

## snowflake/ml/fileset/sfcfs.py

```diff
@@ -181,15 +181,14 @@
         return self._stage_fs_set[stage_fs_key]
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         func_params_to_log=["detail"],
         conn_attr_name="_conn",
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def ls(self, path: str, detail: bool = False, **kwargs: Any) -> Union[List[str], List[Dict[str, Any]]]:
         """Override fsspec `ls` method. List single "directory" with or without details.
 
         Args:
             path : location at which to list files.
                 It should be in the format of "@{database}.{schema}.{stage}/{path}"
             detail : if True, each list item is a dict of file properties; otherwise, returns list of filenames.
@@ -212,15 +211,14 @@
         stage_path_list = cast(List[Dict[str, Any]], stage_path_list)
         return self._decorate_ls_res(stage_fs, stage_path_list, detail)
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         conn_attr_name="_conn",
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def optimize_read(self, files: Optional[List[str]] = None) -> None:
         """Prefetch and cache the presigned urls for all the given files to speed up the file opening.
 
         All the files introduced here will have their urls cached. Further open() on any of cached urls will lead to a
         batch refreshment of the cached urls in the same stage if that url is inactive.
 
         Args:
@@ -238,15 +236,14 @@
         for k, v in stage_file_paths.items():
             stage_fs_dict[k].optimize_read(v)
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         conn_attr_name="_conn",
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def _open(self, path: str, **kwargs: Any) -> fsspec.spec.AbstractBufferedFile:
         """Override fsspec `_open` method. Open a file for reading in 'rb' mode.
 
         The opened file will be readable for 4 hours. After that you will need to reopen it.
 
         Fsspec `open` API is built on `_open` method and supports non-binary mode.
         See more details in https://github.com/fsspec/filesystem_spec/blob/2022.10.0/fsspec/spec.py#L1019
@@ -264,15 +261,14 @@
         stage_fs = self._get_stage_fs(file_path)
         return stage_fs._open(file_path.filepath, **kwargs)
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         conn_attr_name="_conn",
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def info(self, path: str, **kwargs: Any) -> Dict[str, Any]:
         """Override fsspec `info` method. Give details of entry at path."""
         file_path = self._parse_file_path(path)
         stage_fs = self._get_stage_fs(file_path)
         res: Dict[str, Any] = stage_fs.info(file_path.filepath, **kwargs)
         if res:
             res["name"] = self._stage_path_to_absolute_path(stage_fs, res["name"])
```

## snowflake/ml/fileset/stage_fs.py

```diff
@@ -140,15 +140,14 @@
         """
         return f"@{self._db}.{self._schema}.{self._stage}"
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         func_params_to_log=["detail"],
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def ls(self, path: str, detail: bool = False) -> Union[List[str], List[Dict[str, Any]]]:
         """Override fsspec `ls` method. List single "directory" with or without details.
 
         Args:
             path: Relative file paths in the stage.
                 Example:
                     "": Empty string points to the stage root.
@@ -164,15 +163,15 @@
         Raises:
             SnowflakeMLException: An error occurred when the given path points to a stage that cannot be found.
             SnowflakeMLException: An error occurred when Snowflake cannot list files in the given stage path.
         """
         try:
             loc = self.stage_name
             path = path.lstrip("/")
-            objects = self._session.sql(f"LIST {loc}/{path}").collect()
+            objects = self._session.sql(f"LIST '{loc}/{path}'").collect()
         except snowpark_exceptions.SnowparkClientException as e:
             if e.message.startswith(fileset_errors.ERRNO_DOMAIN_NOT_EXIST):
                 raise snowml_exceptions.SnowflakeMLException(
                     error_code=error_codes.SNOWML_NOT_FOUND,
                     original_exception=fileset_errors.StageNotFoundError(
                         f"Stage {loc} does not exist or is not authorized."
                     ),
@@ -187,15 +186,14 @@
             return files
         else:
             return [f["name"] for f in files]
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def optimize_read(self, files: Optional[List[str]] = None) -> None:
         """Prefetch and cache the presigned urls for all the given files to speed up the read performance.
 
         All the files introduced here will have their urls cached. Further open() on any of cached urls will lead to a
         batch refreshment of all the cached urls if that url is inactive.
 
         Args:
@@ -214,15 +212,14 @@
             self._url_cache[file_path] = _PresignedUrl(url, expire_at)
             logging.debug(f"Retrieved presigned url for {file_path}.")
         logging.info(f"Finished batch fetching presigned urls for {self.stage_name}.")
 
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
     )
-    @snowpark._internal.utils.private_preview(version="0.2.0")
     def _open(self, path: str, mode: str = "rb", **kwargs: Any) -> fsspec.spec.AbstractBufferedFile:
         """Override fsspec `_open` method. Open a file for reading.
 
         The opened file will be readable for 4 hours. After that, you need to reopen the file.
 
         Args:
             path: Path of file in Snowflake stage.
```

## snowflake/ml/model/__init__.py

```diff
@@ -1,6 +1,6 @@
 from snowflake.ml.model._client.model.model_impl import Model
-from snowflake.ml.model._client.model.model_version_impl import ModelVersion
+from snowflake.ml.model._client.model.model_version_impl import ExportMode, ModelVersion
 from snowflake.ml.model.models.huggingface_pipeline import HuggingFacePipelineModel
 from snowflake.ml.model.models.llm import LLM, LLMOptions
 
-__all__ = ["Model", "ModelVersion", "HuggingFacePipelineModel", "LLM", "LLMOptions"]
+__all__ = ["Model", "ModelVersion", "ExportMode", "HuggingFacePipelineModel", "LLM", "LLMOptions"]
```

## snowflake/ml/model/_api.py

```diff
@@ -1,11 +1,12 @@
 from types import ModuleType
 from typing import Any, Dict, List, Literal, Optional, Union, cast, overload
 
 import pandas as pd
+from typing_extensions import deprecated
 
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions as snowml_exceptions,
 )
 from snowflake.ml.model import (
     deploy_platforms,
@@ -19,14 +20,15 @@
     infer_template,
 )
 from snowflake.ml.model._model_composer import model_composer
 from snowflake.ml.model._signatures import snowpark_handler
 from snowflake.snowpark import DataFrame as SnowparkDataFrame, Session, functions as F
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def save_model(
     *,
     name: str,
     model: model_types.SupportedNoSignatureRequirementsModelType,
     session: Session,
     stage_path: str,
@@ -57,14 +59,15 @@
         code_paths: Directory of code to import.
         ext_modules: External modules that user might want to get pickled with model object. Defaults to None.
         options: Model specific kwargs.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def save_model(
     *,
     name: str,
     model: model_types.SupportedRequireSignatureModelType,
     session: Session,
     stage_path: str,
@@ -97,14 +100,15 @@
         code_paths: Directory of code to import.
         ext_modules: External modules that user might want to get pickled with model object. Defaults to None.
         options: Model specific kwargs.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def save_model(
     *,
     name: str,
     model: model_types.SupportedRequireSignatureModelType,
     session: Session,
     stage_path: str,
@@ -138,14 +142,15 @@
         code_paths: Directory of code to import.
         ext_modules: External modules that user might want to get pickled with model object. Defaults to None.
         options: Model specific kwargs.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 def save_model(
     *,
     name: str,
     model: model_types.SupportedModelType,
     session: Session,
     stage_path: str,
     signatures: Optional[Dict[str, model_signature.ModelSignature]] = None,
@@ -204,49 +209,53 @@
         ext_modules=ext_modules,
         code_paths=code_paths,
         options=options,
     )
     return m
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def load_model(*, session: Session, stage_path: str) -> model_composer.ModelComposer:
     """Load the model into memory from a zip file in the stage.
 
     Args:
         session: Snowflake connection session.
         stage_path: Path to the stage where model will be loaded from.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def load_model(*, session: Session, stage_path: str, meta_only: Literal[False]) -> model_composer.ModelComposer:
     """Load the model into memory from a zip file in the stage.
 
     Args:
         session: Snowflake connection session.
         stage_path: Path to the stage where model will be loaded from.
         meta_only: Flag to indicate that if only load metadata.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def load_model(*, session: Session, stage_path: str, meta_only: Literal[True]) -> model_composer.ModelComposer:
     """Load the model into memory from a zip file in the stage with metadata only.
 
     Args:
         session: Snowflake connection session.
         stage_path: Path to the stage where model will be loaded from.
         meta_only: Flag to indicate that if only load metadata.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 def load_model(
     *,
     session: Session,
     stage_path: str,
     meta_only: bool = False,
 ) -> model_composer.ModelComposer:
     """Load the model into memory from directory or a zip file in the stage.
@@ -257,18 +266,19 @@
         stage_path: Path to the stage where model will be loaded from.
         meta_only: Flag to indicate that if only load metadata.
 
     Returns:
         Loaded model.
     """
     m = model_composer.ModelComposer(session=session, stage_path=stage_path)
-    m.load(meta_only=meta_only)
+    m.legacy_load(meta_only=meta_only)
     return m
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def deploy(
     session: Session,
     *,
     name: str,
     platform: deploy_platforms.TargetPlatform,
     target_method: Optional[str],
@@ -286,14 +296,15 @@
         stage_path: Path to the stage where model will be deployed.
         options: Additional options when deploying the model.
             Each target platform will have their own specifications of options.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def deploy(
     session: Session,
     *,
     model_id: str,
     name: str,
     platform: deploy_platforms.TargetPlatform,
@@ -315,14 +326,15 @@
         deployment_stage_path: Path to stage containing snowpark container service deployment artifacts.
         options: Additional options when deploying the model.
             Each target platform will have their own specifications of options.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 def deploy(
     session: Session,
     *,
     name: str,
     platform: deploy_platforms.TargetPlatform,
     stage_path: str,
     target_method: Optional[str] = None,
@@ -419,14 +431,15 @@
         )
     info = model_types.Deployment(
         name=name, platform=platform, target_method=target_method, signature=signature, options=options, details=details
     )
     return info
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def predict(
     session: Session,
     *,
     deployment: model_types.Deployment,
     X: model_types.SupportedLocalDataType,
     statement_params: Optional[Dict[str, Any]] = None,
@@ -439,14 +452,15 @@
         deployment: The deployment info to use for predict.
         X: The input data.
         statement_params: Statement Parameters for telemetry.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 @overload
 def predict(
     session: Session,
     *,
     deployment: model_types.Deployment,
     X: SnowparkDataFrame,
     statement_params: Optional[Dict[str, Any]] = None,
@@ -458,14 +472,15 @@
         deployment: The deployment info to use for predict.
         X: The input Snowpark dataframe.
         statement_params: Statement Parameters for telemetry.
     """
     ...
 
 
+@deprecated("Only used by PrPr model registry.")
 def predict(
     session: Session,
     *,
     deployment: model_types.Deployment,
     X: Union[model_types.SupportedDataType, SnowparkDataFrame],
     statement_params: Optional[Dict[str, Any]] = None,
 ) -> Union[pd.DataFrame, SnowparkDataFrame]:
```

## snowflake/ml/model/_client/model/model_impl.py

```diff
@@ -346,7 +346,34 @@
         self._model_ops.unset_tag(
             model_name=self._model_name,
             tag_database_name=tag_db_id,
             tag_schema_name=tag_schema_id,
             tag_name=tag_name_id,
             statement_params=statement_params,
         )
+
+    @telemetry.send_api_usage_telemetry(
+        project=_TELEMETRY_PROJECT,
+        subproject=_TELEMETRY_SUBPROJECT,
+    )
+    def rename(self, model_name: str) -> None:
+        """Rename a model. Can be used to move a model when a fully qualified name is provided.
+
+        Args:
+            model_name: The new model name.
+        """
+        statement_params = telemetry.get_statement_params(
+            project=_TELEMETRY_PROJECT,
+            subproject=_TELEMETRY_SUBPROJECT,
+        )
+        db, schema, model, _ = identifier.parse_schema_level_object_identifier(model_name)
+        new_model_db = sql_identifier.SqlIdentifier(db) if db else None
+        new_model_schema = sql_identifier.SqlIdentifier(schema) if schema else None
+        new_model_id = sql_identifier.SqlIdentifier(model)
+        self._model_ops.rename(
+            model_name=self._model_name,
+            new_model_db=new_model_db,
+            new_model_schema=new_model_schema,
+            new_model_name=new_model_id,
+            statement_params=statement_params,
+        )
+        self._model_name = new_model_id
```

## snowflake/ml/model/_client/model/model_version_impl.py

```diff
@@ -1,21 +1,33 @@
+import enum
+import pathlib
+import tempfile
+import warnings
 from typing import Any, Callable, Dict, List, Optional, Union
 
 import pandas as pd
 
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.utils import sql_identifier
+from snowflake.ml.model import type_hints as model_types
 from snowflake.ml.model._client.ops import metadata_ops, model_ops
+from snowflake.ml.model._model_composer import model_composer
 from snowflake.ml.model._model_composer.model_manifest import model_manifest_schema
+from snowflake.ml.model._packager.model_handlers import snowmlmodel
 from snowflake.snowpark import dataframe
 
 _TELEMETRY_PROJECT = "MLOps"
 _TELEMETRY_SUBPROJECT = "ModelManagement"
 
 
+class ExportMode(enum.Enum):
+    MODEL = "model"
+    FULL = "full"
+
+
 class ModelVersion:
     """Model Version Object representing a specific version of the model that could be run."""
 
     _model_ops: model_ops.ModelOperator
     _model_name: sql_identifier.SqlIdentifier
     _version_name: sql_identifier.SqlIdentifier
     _functions: List[model_manifest_schema.ModelFunctionInfo]
@@ -236,37 +248,44 @@
         subproject=_TELEMETRY_SUBPROJECT,
     )
     def run(
         self,
         X: Union[pd.DataFrame, dataframe.DataFrame],
         *,
         function_name: Optional[str] = None,
+        partition_column: Optional[str] = None,
         strict_input_validation: bool = False,
     ) -> Union[pd.DataFrame, dataframe.DataFrame]:
         """Invoke a method in a model version object.
 
         Args:
             X: The input data, which could be a pandas DataFrame or Snowpark DataFrame.
             function_name: The function name to run. It is the name used to call a function in SQL.
                 Defaults to None. It can only be None if there is only 1 method.
+            partition_column: The partition column name to partition by.
             strict_input_validation: Enable stricter validation for the input data. This will result value range based
                 type validation to make sure your input data won't overflow when providing to the model.
 
         Raises:
             ValueError: When no method with the corresponding name is available.
             ValueError: When there are more than 1 target methods available in the model but no function name specified.
+            ValueError: When the partition column is not a valid Snowflake identifier.
 
         Returns:
             The prediction data. It would be the same type dataframe as your input.
         """
         statement_params = telemetry.get_statement_params(
             project=_TELEMETRY_PROJECT,
             subproject=_TELEMETRY_SUBPROJECT,
         )
 
+        if partition_column is not None:
+            # Partition column must be a valid identifier
+            partition_column = sql_identifier.SqlIdentifier(partition_column)
+
         functions: List[model_manifest_schema.ModelFunctionInfo] = self._functions
         if function_name:
             req_method_name = sql_identifier.SqlIdentifier(function_name).identifier()
             find_method: Callable[[model_manifest_schema.ModelFunctionInfo], bool] = (
                 lambda method: method["name"] == req_method_name
             )
             target_function_info = next(
@@ -283,14 +302,130 @@
                 f"There are more than 1 target methods available in the model {self.fully_qualified_model_name}"
                 f" version {self.version_name}. Please specify a `method_name` when calling the `run` method."
             )
         else:
             target_function_info = functions[0]
         return self._model_ops.invoke_method(
             method_name=sql_identifier.SqlIdentifier(target_function_info["name"]),
+            method_function_type=target_function_info["target_method_function_type"],
             signature=target_function_info["signature"],
             X=X,
             model_name=self._model_name,
             version_name=self._version_name,
             strict_input_validation=strict_input_validation,
+            partition_column=partition_column,
+            statement_params=statement_params,
+        )
+
+    @telemetry.send_api_usage_telemetry(
+        project=_TELEMETRY_PROJECT, subproject=_TELEMETRY_SUBPROJECT, func_params_to_log=["export_mode"]
+    )
+    def export(self, target_path: str, *, export_mode: ExportMode = ExportMode.MODEL) -> None:
+        """Export model files to a local directory.
+
+        Args:
+            target_path: Path to a local directory to export files to. A directory will be created if does not exist.
+            export_mode: The mode to export the model. Defaults to ExportMode.MODEL.
+                ExportMode.MODEL: All model files including environment to load the model and model weights.
+                ExportMode.FULL: Additional files to run the model in Warehouse, besides all files in MODEL mode,
+
+        Raises:
+            ValueError: Raised when the target path is a file or an non-empty folder.
+        """
+        target_local_path = pathlib.Path(target_path)
+        if target_local_path.is_file() or any(target_local_path.iterdir()):
+            raise ValueError(f"Target path {target_local_path} is a file or an non-empty folder.")
+
+        target_local_path.mkdir(parents=False, exist_ok=True)
+        statement_params = telemetry.get_statement_params(
+            project=_TELEMETRY_PROJECT,
+            subproject=_TELEMETRY_SUBPROJECT,
+        )
+        self._model_ops.download_files(
+            model_name=self._model_name,
+            version_name=self._version_name,
+            target_path=target_local_path,
+            mode=export_mode.value,
+            statement_params=statement_params,
+        )
+
+    @telemetry.send_api_usage_telemetry(
+        project=_TELEMETRY_PROJECT, subproject=_TELEMETRY_SUBPROJECT, func_params_to_log=["force", "options"]
+    )
+    def load(
+        self,
+        *,
+        force: bool = False,
+        options: Optional[model_types.ModelLoadOption] = None,
+    ) -> model_types.SupportedModelType:
+        """Load the underlying original Python object back from a model.
+            This operation requires to have the exact the same environment as the one when logging the model, otherwise,
+            the model might be not functional or some other problems might occur.
+
+        Args:
+            force: Bypass the best-effort environment validation. Defaults to False.
+            options: Options to specify when loading the model, check `snowflake.ml.model.type_hints` for available
+                options. Defaults to None.
+
+        Raises:
+            ValueError: Raised when the best-effort environment validation fails.
+
+        Returns:
+            The original Python object loaded from the model object.
+        """
+        statement_params = telemetry.get_statement_params(
+            project=_TELEMETRY_PROJECT,
+            subproject=_TELEMETRY_SUBPROJECT,
+        )
+        if not force:
+            with tempfile.TemporaryDirectory() as tmp_workspace_for_validation:
+                ws_path_for_validation = pathlib.Path(tmp_workspace_for_validation)
+                self._model_ops.download_files(
+                    model_name=self._model_name,
+                    version_name=self._version_name,
+                    target_path=ws_path_for_validation,
+                    mode="minimal",
+                    statement_params=statement_params,
+                )
+                pk_for_validation = model_composer.ModelComposer.load(
+                    ws_path_for_validation, meta_only=True, options=options
+                )
+                assert pk_for_validation.meta, (
+                    "Unable to load model metadata for validation. "
+                    f"model_name={self._model_name}, version_name={self._version_name}"
+                )
+
+                validation_errors = pk_for_validation.meta.env.validate_with_local_env(
+                    check_snowpark_ml_version=(
+                        pk_for_validation.meta.model_type == snowmlmodel.SnowMLModelHandler.HANDLER_TYPE
+                    )
+                )
+                if validation_errors:
+                    raise ValueError(
+                        f"Unable to load this model due to following validation errors: {validation_errors}. "
+                        "Make sure your local environment is the same as that when you logged the model, "
+                        "or if you believe it should work, specify `force=True` to bypass this check."
+                    )
+
+        warnings.warn(
+            "Loading model requires to have the exact the same environment as the one when "
+            "logging the model, otherwise, the model might be not functional or "
+            "some other problems might occur.",
+            category=RuntimeWarning,
+            stacklevel=2,
+        )
+
+        # We need the folder to be existed.
+        workspace = pathlib.Path(tempfile.mkdtemp())
+        self._model_ops.download_files(
+            model_name=self._model_name,
+            version_name=self._version_name,
+            target_path=workspace,
+            mode="model",
             statement_params=statement_params,
         )
+        pk = model_composer.ModelComposer.load(workspace, meta_only=False, options=options)
+        assert pk.model, (
+            "Unable to load model. "
+            f"model_name={self._model_name}, version_name={self._version_name}, metadata={pk.meta}"
+        )
+        return pk.model
```

## snowflake/ml/model/_client/ops/model_ops.py

```diff
@@ -1,11 +1,11 @@
+import os
 import pathlib
 import tempfile
-from contextlib import contextmanager
-from typing import Any, Dict, Generator, List, Optional, Union, cast
+from typing import Any, Dict, List, Literal, Optional, Union, cast
 
 import yaml
 
 from snowflake.ml._internal.utils import identifier, sql_identifier
 from snowflake.ml.model import model_signature, type_hints
 from snowflake.ml.model._client.ops import metadata_ops
 from snowflake.ml.model._client.sql import (
@@ -15,15 +15,17 @@
     tag as tag_sql,
 )
 from snowflake.ml.model._model_composer import model_composer
 from snowflake.ml.model._model_composer.model_manifest import (
     model_manifest,
     model_manifest_schema,
 )
+from snowflake.ml.model._packager.model_env import model_env
 from snowflake.ml.model._packager.model_meta import model_meta
+from snowflake.ml.model._packager.model_runtime import model_runtime
 from snowflake.ml.model._signatures import snowpark_handler
 from snowflake.snowpark import dataframe, row, session
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 
 class ModelOperator:
     def __init__(
@@ -333,24 +335,14 @@
                 file_path=pathlib.PurePosixPath(model_manifest.ModelManifest.MANIFEST_FILE_REL_PATH),
                 target_path=pathlib.Path(tmpdir),
                 statement_params=statement_params,
             )
             mm = model_manifest.ModelManifest(pathlib.Path(tmpdir))
             return mm.load()
 
-    @contextmanager
-    def _enable_model_details(
-        self,
-        *,
-        statement_params: Optional[Dict[str, Any]] = None,
-    ) -> Generator[None, None, None]:
-        self._model_client.config_model_details(enable=True, statement_params=statement_params)
-        yield
-        self._model_client.config_model_details(enable=False, statement_params=statement_params)
-
     @staticmethod
     def _match_model_spec_with_sql_functions(
         sql_functions_names: List[sql_identifier.SqlIdentifier], target_methods: List[str]
     ) -> Dict[sql_identifier.SqlIdentifier, str]:
         res = {}
         for target_method in target_methods:
             # Here we need to find the SQL function corresponding to the Python function.
@@ -370,72 +362,71 @@
     def get_functions(
         self,
         *,
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> List[model_manifest_schema.ModelFunctionInfo]:
-        with self._enable_model_details(statement_params=statement_params):
-            raw_model_spec_res = self._model_client.show_versions(
-                model_name=model_name,
-                version_name=version_name,
-                check_model_details=True,
-                statement_params=statement_params,
-            )[0][self._model_client.MODEL_VERSION_MODEL_SPEC_COL_NAME]
-            model_spec_dict = yaml.safe_load(raw_model_spec_res)
-            model_spec = model_meta.ModelMetadata._validate_model_metadata(model_spec_dict)
-            show_functions_res = self._model_version_client.show_functions(
-                model_name=model_name,
-                version_name=version_name,
-                statement_params=statement_params,
+        raw_model_spec_res = self._model_client.show_versions(
+            model_name=model_name,
+            version_name=version_name,
+            check_model_details=True,
+            statement_params={**(statement_params or {}), "SHOW_MODEL_DETAILS_IN_SHOW_VERSIONS_IN_MODEL": True},
+        )[0][self._model_client.MODEL_VERSION_MODEL_SPEC_COL_NAME]
+        model_spec_dict = yaml.safe_load(raw_model_spec_res)
+        model_spec = model_meta.ModelMetadata._validate_model_metadata(model_spec_dict)
+        show_functions_res = self._model_version_client.show_functions(
+            model_name=model_name,
+            version_name=version_name,
+            statement_params=statement_params,
+        )
+        function_names_and_types = []
+        for r in show_functions_res:
+            function_name = sql_identifier.SqlIdentifier(
+                r[self._model_version_client.FUNCTION_NAME_COL_NAME], case_sensitive=True
             )
-            function_names_and_types = []
-            for r in show_functions_res:
-                function_name = sql_identifier.SqlIdentifier(
-                    r[self._model_version_client.FUNCTION_NAME_COL_NAME], case_sensitive=True
-                )
 
-                function_type = model_manifest_schema.ModelMethodFunctionTypes.FUNCTION.value
-                try:
-                    return_type = r[self._model_version_client.FUNCTION_RETURN_TYPE_COL_NAME]
-                except KeyError:
-                    pass
-                else:
-                    if "TABLE" in return_type:
-                        function_type = model_manifest_schema.ModelMethodFunctionTypes.TABLE_FUNCTION.value
-
-                function_names_and_types.append((function_name, function_type))
-
-            signatures = model_spec["signatures"]
-            function_names = [name for name, _ in function_names_and_types]
-            function_name_mapping = ModelOperator._match_model_spec_with_sql_functions(
-                function_names, list(signatures.keys())
-            )
-
-            return [
-                model_manifest_schema.ModelFunctionInfo(
-                    name=function_name.identifier(),
-                    target_method=function_name_mapping[function_name],
-                    target_method_function_type=function_type,
-                    signature=model_signature.ModelSignature.from_dict(
-                        signatures[function_name_mapping[function_name]]
-                    ),
-                )
-                for function_name, function_type in function_names_and_types
-            ]
+            function_type = model_manifest_schema.ModelMethodFunctionTypes.FUNCTION.value
+            try:
+                return_type = r[self._model_version_client.FUNCTION_RETURN_TYPE_COL_NAME]
+            except KeyError:
+                pass
+            else:
+                if "TABLE" in return_type:
+                    function_type = model_manifest_schema.ModelMethodFunctionTypes.TABLE_FUNCTION.value
+
+            function_names_and_types.append((function_name, function_type))
+
+        signatures = model_spec["signatures"]
+        function_names = [name for name, _ in function_names_and_types]
+        function_name_mapping = ModelOperator._match_model_spec_with_sql_functions(
+            function_names, list(signatures.keys())
+        )
+
+        return [
+            model_manifest_schema.ModelFunctionInfo(
+                name=function_name.identifier(),
+                target_method=function_name_mapping[function_name],
+                target_method_function_type=function_type,
+                signature=model_signature.ModelSignature.from_dict(signatures[function_name_mapping[function_name]]),
+            )
+            for function_name, function_type in function_names_and_types
+        ]
 
     def invoke_method(
         self,
         *,
         method_name: sql_identifier.SqlIdentifier,
+        method_function_type: str,
         signature: model_signature.ModelSignature,
         X: Union[type_hints.SupportedDataType, dataframe.DataFrame],
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         strict_input_validation: bool = False,
+        partition_column: Optional[sql_identifier.SqlIdentifier] = None,
         statement_params: Optional[Dict[str, str]] = None,
     ) -> Union[type_hints.SupportedDataType, dataframe.DataFrame]:
         identifier_rule = model_signature.SnowparkIdentifierRule.INFERRED
 
         # Validate and prepare input
         if not isinstance(X, dataframe.DataFrame):
             keep_order = True
@@ -465,32 +456,48 @@
         for output_feature in signature.outputs:
             output_name = identifier_rule.get_sql_identifier_from_feature(output_feature.name)
             returns.append((output_feature.name, output_feature.as_snowpark_type(), output_name))
             # Avoid removing output cols when output_with_input_features is False
             if output_name in original_cols:
                 original_cols.remove(output_name)
 
-        df_res = self._model_version_client.invoke_method(
-            method_name=method_name,
-            input_df=s_df,
-            input_args=input_args,
-            returns=returns,
-            model_name=model_name,
-            version_name=version_name,
-            statement_params=statement_params,
-        )
+        if method_function_type == model_manifest_schema.ModelMethodFunctionTypes.FUNCTION.value:
+            df_res = self._model_version_client.invoke_function_method(
+                method_name=method_name,
+                input_df=s_df,
+                input_args=input_args,
+                returns=returns,
+                model_name=model_name,
+                version_name=version_name,
+                statement_params=statement_params,
+            )
+        elif method_function_type == model_manifest_schema.ModelMethodFunctionTypes.TABLE_FUNCTION.value:
+            df_res = self._model_version_client.invoke_table_function_method(
+                method_name=method_name,
+                input_df=s_df,
+                input_args=input_args,
+                partition_column=partition_column,
+                returns=returns,
+                model_name=model_name,
+                version_name=version_name,
+                statement_params=statement_params,
+            )
 
         if keep_order:
             df_res = df_res.sort(
                 "_ID",
                 ascending=True,
             )
 
         if not output_with_input_features:
-            df_res = df_res.drop(*original_cols)
+            cols_to_drop = original_cols
+            if partition_column is not None:
+                # don't drop partition column
+                cols_to_drop.remove(partition_column.identifier())
+            df_res = df_res.drop(*cols_to_drop)
 
         # Get final result
         if not isinstance(X, dataframe.DataFrame):
             return snowpark_handler.SnowparkDataFrameHandler.convert_to_df(df_res, features=signature.outputs)
         else:
             return df_res
 
@@ -508,7 +515,70 @@
                 statement_params=statement_params,
             )
         else:
             self._model_client.drop_model(
                 model_name=model_name,
                 statement_params=statement_params,
             )
+
+    def rename(
+        self,
+        *,
+        model_name: sql_identifier.SqlIdentifier,
+        new_model_db: Optional[sql_identifier.SqlIdentifier],
+        new_model_schema: Optional[sql_identifier.SqlIdentifier],
+        new_model_name: sql_identifier.SqlIdentifier,
+        statement_params: Optional[Dict[str, Any]] = None,
+    ) -> None:
+        self._model_client.rename(
+            model_name=model_name,
+            new_model_db=new_model_db,
+            new_model_schema=new_model_schema,
+            new_model_name=new_model_name,
+            statement_params=statement_params,
+        )
+
+    # Map indicating in different modes, the path to list and download.
+    # The boolean value indicates if it is a directory,
+    MODEL_FILE_DOWNLOAD_PATTERN = {
+        "minimal": {
+            pathlib.PurePosixPath(model_composer.ModelComposer.MODEL_DIR_REL_PATH)
+            / model_meta.MODEL_METADATA_FILE: False,
+            pathlib.PurePosixPath(model_composer.ModelComposer.MODEL_DIR_REL_PATH) / model_env._DEFAULT_ENV_DIR: True,
+            pathlib.PurePosixPath(model_composer.ModelComposer.MODEL_DIR_REL_PATH)
+            / model_runtime.ModelRuntime.RUNTIME_DIR_REL_PATH: True,
+        },
+        "model": {pathlib.PurePosixPath(model_composer.ModelComposer.MODEL_DIR_REL_PATH): True},
+        "full": {pathlib.PurePosixPath(os.curdir): True},
+    }
+
+    def download_files(
+        self,
+        *,
+        model_name: sql_identifier.SqlIdentifier,
+        version_name: sql_identifier.SqlIdentifier,
+        target_path: pathlib.Path,
+        mode: Literal["full", "model", "minimal"] = "model",
+        statement_params: Optional[Dict[str, Any]] = None,
+    ) -> None:
+        for remote_rel_path, is_dir in self.MODEL_FILE_DOWNLOAD_PATTERN[mode].items():
+            list_file_res = self._model_version_client.list_file(
+                model_name=model_name,
+                version_name=version_name,
+                file_path=remote_rel_path,
+                is_dir=is_dir,
+                statement_params=statement_params,
+            )
+            file_list = [
+                pathlib.PurePosixPath(*pathlib.PurePosixPath(row.name).parts[2:])  # versions/<version_name>/...
+                for row in list_file_res
+            ]
+            for stage_file_path in file_list:
+                local_file_dir = target_path / stage_file_path.parent
+                local_file_dir.mkdir(parents=True, exist_ok=True)
+                self._model_version_client.get_file(
+                    model_name=model_name,
+                    version_name=version_name,
+                    file_path=stage_file_path,
+                    target_path=local_file_dir,
+                    statement_params=statement_params,
+                )
```

## snowflake/ml/model/_client/sql/model.py

```diff
@@ -117,25 +117,27 @@
     ) -> None:
         query_result_checker.SqlResultValidator(
             self._session,
             f"DROP MODEL {self.fully_qualified_model_name(model_name)}",
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
-    def config_model_details(
+    def rename(
         self,
         *,
-        enable: bool,
+        model_name: sql_identifier.SqlIdentifier,
+        new_model_db: Optional[sql_identifier.SqlIdentifier],
+        new_model_schema: Optional[sql_identifier.SqlIdentifier],
+        new_model_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
     ) -> None:
-        if enable:
-            query_result_checker.SqlResultValidator(
-                self._session,
-                "ALTER SESSION SET SHOW_MODEL_DETAILS_IN_SHOW_VERSIONS_IN_MODEL=true",
-                statement_params=statement_params,
-            ).has_dimensions(expected_rows=1, expected_cols=1).validate()
-        else:
-            query_result_checker.SqlResultValidator(
-                self._session,
-                "ALTER SESSION UNSET SHOW_MODEL_DETAILS_IN_SHOW_VERSIONS_IN_MODEL",
-                statement_params=statement_params,
-            ).has_dimensions(expected_rows=1, expected_cols=1).validate()
+        # Use registry's database and schema if a non fully qualified new model name is provided.
+        new_fully_qualified_name = identifier.get_schema_level_object_identifier(
+            new_model_db.identifier() if new_model_db else self._database_name.identifier(),
+            new_model_schema.identifier() if new_model_schema else self._schema_name.identifier(),
+            new_model_name.identifier(),
+        )
+        query_result_checker.SqlResultValidator(
+            self._session,
+            f"ALTER MODEL {self.fully_qualified_model_name(model_name)} RENAME TO {new_fully_qualified_name}",
+            statement_params=statement_params,
+        ).has_dimensions(expected_rows=1, expected_cols=1).validate()
```

## snowflake/ml/model/_client/sql/model_version.py

```diff
@@ -92,14 +92,46 @@
             (
                 f"ALTER MODEL {self.fully_qualified_model_name(model_name)} "
                 f"SET DEFAULT_VERSION = {version_name.identifier()}"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
+    def list_file(
+        self,
+        *,
+        model_name: sql_identifier.SqlIdentifier,
+        version_name: sql_identifier.SqlIdentifier,
+        file_path: pathlib.PurePosixPath,
+        is_dir: bool = False,
+        statement_params: Optional[Dict[str, Any]] = None,
+    ) -> List[row.Row]:
+        # Workaround for snowURL bug.
+        trailing_slash = "/" if is_dir else ""
+
+        stage_location = (
+            pathlib.PurePosixPath(
+                self.fully_qualified_model_name(model_name), "versions", version_name.resolved(), file_path
+            ).as_posix()
+            + trailing_slash
+        )
+        stage_location_url = ParseResult(
+            scheme="snow", netloc="model", path=stage_location, params="", query="", fragment=""
+        ).geturl()
+
+        return (
+            query_result_checker.SqlResultValidator(
+                self._session,
+                f"List {_normalize_url_for_sql(stage_location_url)}",
+                statement_params=statement_params,
+            )
+            .has_column("name")
+            .validate()
+        )
+
     def get_file(
         self,
         *,
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         file_path: pathlib.PurePosixPath,
         target_path: pathlib.Path,
@@ -158,15 +190,15 @@
             (
                 f"ALTER MODEL {self.fully_qualified_model_name(model_name)} "
                 f"MODIFY VERSION {version_name.identifier()} SET COMMENT=$${comment}$$"
             ),
             statement_params=statement_params,
         ).has_dimensions(expected_rows=1, expected_cols=1).validate()
 
-    def invoke_method(
+    def invoke_function_method(
         self,
         *,
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         method_name: sql_identifier.SqlIdentifier,
         input_df: dataframe.DataFrame,
         input_args: List[sql_identifier.SqlIdentifier],
@@ -228,14 +260,90 @@
         ).drop(INTERMEDIATE_OBJ_NAME)
 
         if statement_params:
             output_df._statement_params = statement_params  # type: ignore[assignment]
 
         return output_df
 
+    def invoke_table_function_method(
+        self,
+        *,
+        model_name: sql_identifier.SqlIdentifier,
+        version_name: sql_identifier.SqlIdentifier,
+        method_name: sql_identifier.SqlIdentifier,
+        input_df: dataframe.DataFrame,
+        input_args: List[sql_identifier.SqlIdentifier],
+        returns: List[Tuple[str, spt.DataType, sql_identifier.SqlIdentifier]],
+        partition_column: Optional[sql_identifier.SqlIdentifier],
+        statement_params: Optional[Dict[str, Any]] = None,
+    ) -> dataframe.DataFrame:
+        with_statements = []
+        if len(input_df.queries["queries"]) == 1 and len(input_df.queries["post_actions"]) == 0:
+            INTERMEDIATE_TABLE_NAME = "SNOWPARK_ML_MODEL_INFERENCE_INPUT"
+            with_statements.append(f"{INTERMEDIATE_TABLE_NAME} AS ({input_df.queries['queries'][0]})")
+        else:
+            tmp_table_name = snowpark_utils.random_name_for_temp_object(snowpark_utils.TempObjectType.TABLE)
+            INTERMEDIATE_TABLE_NAME = identifier.get_schema_level_object_identifier(
+                self._database_name.identifier(),
+                self._schema_name.identifier(),
+                tmp_table_name,
+            )
+            input_df.write.save_as_table(  # type: ignore[call-overload]
+                table_name=INTERMEDIATE_TABLE_NAME,
+                mode="errorifexists",
+                table_type="temporary",
+                statement_params=statement_params,
+            )
+
+        module_version_alias = "MODEL_VERSION_ALIAS"
+        with_statements.append(
+            f"{module_version_alias} AS "
+            f"MODEL {self.fully_qualified_model_name(model_name)} VERSION {version_name.identifier()}"
+        )
+
+        partition_by = partition_column.identifier() if partition_column is not None else "1"
+
+        args_sql_list = []
+        for input_arg_value in input_args:
+            args_sql_list.append(input_arg_value)
+
+        args_sql = ", ".join(args_sql_list)
+
+        sql = textwrap.dedent(
+            f"""WITH {','.join(with_statements)}
+                 SELECT *,
+                 FROM {INTERMEDIATE_TABLE_NAME},
+                     TABLE({module_version_alias}!{method_name.identifier()}({args_sql})
+                     OVER (PARTITION BY {partition_by}))"""
+        )
+
+        output_df = self._session.sql(sql)
+
+        # Prepare the output
+        output_cols = []
+        output_names = []
+
+        for output_name, output_type, output_col_name in returns:
+            output_cols.append(F.col(output_name).astype(output_type))
+            output_names.append(output_col_name)
+
+        if partition_column is not None:
+            output_cols.append(F.col(partition_column.identifier()))
+            output_names.append(partition_column)
+
+        output_df = output_df.with_columns(
+            col_names=output_names,
+            values=output_cols,
+        )
+
+        if statement_params:
+            output_df._statement_params = statement_params  # type: ignore[assignment]
+
+        return output_df
+
     def set_metadata(
         self,
         metadata_dict: Dict[str, Any],
         *,
         model_name: sql_identifier.SqlIdentifier,
         version_name: sql_identifier.SqlIdentifier,
         statement_params: Optional[Dict[str, Any]] = None,
```

## snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py

```diff
@@ -33,35 +33,38 @@
         *,
         context_dir: str,
         full_image_name: str,
         image_repo: str,
         session: snowpark.Session,
         artifact_stage_location: str,
         compute_pool: str,
+        job_name: str,
         external_access_integrations: List[str],
     ) -> None:
         """Initialization
 
         Args:
             context_dir: Local docker context dir.
             full_image_name: Full image name consists of image name and image tag.
             image_repo: Path to image repository.
             session: Snowpark session
             artifact_stage_location: Spec file and future deployment related artifacts will be stored under
                 {stage}/models/{model_id}
             compute_pool: The compute pool used to run docker image build workload.
+            job_name: job_name to use.
             external_access_integrations: EAIs for network connection.
         """
         self.context_dir = context_dir
         self.image_repo = image_repo
         self.full_image_name = full_image_name
         self.session = session
         self.artifact_stage_location = artifact_stage_location
         self.compute_pool = compute_pool
         self.external_access_integrations = external_access_integrations
+        self.job_name = job_name
         self.client = snowservice_client.SnowServiceClient(session)
 
         assert artifact_stage_location.startswith(
             "@"
         ), f"stage path should start with @, actual: {artifact_stage_location}"
 
     def build_and_upload_image(self) -> None:
@@ -199,13 +202,14 @@
             local_file_name=spec_file_path,
             stage_location=self.artifact_stage_location,
             auto_compress=False,
             overwrite=True,
         )
 
     def _launch_kaniko_job(self, spec_stage_location: str) -> None:
-        logger.debug("Submitting job for building docker image with kaniko")
+        logger.debug(f"Submitting job {self.job_name} for building docker image with kaniko")
         self.client.create_job(
+            job_name=self.job_name,
             compute_pool=self.compute_pool,
             spec_stage_location=spec_stage_location,
             external_access_integrations=self.external_access_integrations,
         )
```

## snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template

```diff
@@ -26,14 +26,15 @@
 RUN chmod +rx requirements.txt
 RUN chmod +x ./${entrypoint_script}
 
 USER mambauser
 
 # Set MAMBA_DOCKERFILE_ACTIVATE=1 to activate the conda environment during build time.
 ARG MAMBA_DOCKERFILE_ACTIVATE=1
+ARG MAMBA_NO_LOW_SPEED_LIMIT=1
 
 # Bitsandbytes uses this ENVVAR to determine CUDA library location
 ENV CONDA_PREFIX=/opt/conda
 
 # The micromamba image comes with an empty environment named base.
 # CONDA_OVERRIDE_CUDA ref https://conda.io/projects/conda/en/latest/user-guide/tasks/manage-virtual.html
 RUN --mount=type=cache,target=/opt/conda/pkgs CONDA_OVERRIDE_CUDA="${cuda_override_env}" \
```

## snowflake/ml/model/_deploy_client/snowservice/deploy.py

```diff
@@ -342,14 +342,15 @@
         self.service_func_name = service_func_name
         self.model_zip_stage_path = model_zip_stage_path
         self.options = options
         self.target_method = target_method
         (db, schema, _, _) = identifier.parse_schema_level_object_identifier(service_func_name)
 
         self._service_name = identifier.get_schema_level_object_identifier(db, schema, f"service_{model_id}")
+        self._job_name = identifier.get_schema_level_object_identifier(db, schema, f"build_{model_id}")
         # Spec file and future deployment related artifacts will be stored under {stage}/models/{model_id}
         self._model_artifact_stage_location = posixpath.join(deployment_stage_path, "models", self.id)
         self.debug_dir: Optional[str] = None
         if self.options.debug_mode:
             self.debug_dir = tempfile.mkdtemp()
             logger.warning(f"Debug model is enabled, deployment artifacts will be available in {self.debug_dir}")
 
@@ -464,14 +465,15 @@
             image_builder = server_image_builder.ServerImageBuilder(
                 context_dir=context_dir,
                 full_image_name=full_image_name,
                 image_repo=image_repo,
                 session=self.session,
                 artifact_stage_location=self._model_artifact_stage_location,
                 compute_pool=self.options.compute_pool,
+                job_name=self._job_name,
                 external_access_integrations=self.options.external_access_integrations,
             )
         else:
             image_builder = client_image_builder.ClientImageBuilder(
                 context_dir=context_dir, full_image_name=full_image_name, image_repo=image_repo, session=self.session
             )
         image_builder.build_and_upload_image()
```

## snowflake/ml/model/_deploy_client/utils/constants.py

```diff
@@ -13,19 +13,14 @@
     DELETING = "DELETING"  # resource set is being deleted
     FAILED = "FAILED"  # resource set has failed and cannot be used anymore
     DONE = "DONE"  # resource set has finished running
     NOT_FOUND = "NOT_FOUND"  # not found or deleted
     INTERNAL_ERROR = "INTERNAL_ERROR"  # there was an internal service error.
 
 
-RESOURCE_TO_STATUS_FUNCTION_MAPPING = {
-    ResourceType.SERVICE: "SYSTEM$GET_SERVICE_STATUS",
-    ResourceType.JOB: "SYSTEM$GET_JOB_STATUS",
-}
-
 PREDICT = "predict"
 STAGE = "stage"
 COMPUTE_POOL = "compute_pool"
 MIN_INSTANCES = "min_instances"
 MAX_INSTANCES = "max_instances"
 GPU_COUNT = "gpu"
 OVERRIDDEN_BASE_IMAGE = "image"
```

## snowflake/ml/model/_deploy_client/utils/snowservice_client.py

```diff
@@ -66,41 +66,43 @@
                 EXTERNAL_ACCESS_INTEGRATIONS = ({', '.join(external_access_integrations)})
             """
         )
         logger.info(f"Creating service {service_name}")
         logger.debug(f"Create service with SQL: \n {sql}")
         self.session.sql(sql).collect()
 
-    def create_job(self, compute_pool: str, spec_stage_location: str, external_access_integrations: List[str]) -> None:
+    def create_job(
+        self, job_name: str, compute_pool: str, spec_stage_location: str, external_access_integrations: List[str]
+    ) -> None:
         """Execute the job creation SQL command. Note that the job creation is synchronous, hence we execute it in a
         async way so that we can query the log in the meantime.
 
         Upon job failure, full job container log will be logged.
 
         Args:
+            job_name: name of the job
             compute_pool: name of the compute pool
             spec_stage_location: path to the stage location where the spec is located at.
             external_access_integrations: EAIs for network connection.
         """
         stage, path = uri.get_stage_and_path(spec_stage_location)
         sql = textwrap.dedent(
             f"""
-            EXECUTE SERVICE
+            EXECUTE JOB SERVICE
             IN COMPUTE POOL {compute_pool}
             FROM {stage}
-            SPEC = '{path}'
+            SPECIFICATION_FILE = '{path}'
+            NAME = {job_name}
             EXTERNAL_ACCESS_INTEGRATIONS = ({', '.join(external_access_integrations)})
             """
         )
         logger.debug(f"Create job with SQL: \n {sql}")
-        cur = self.session._conn._conn.cursor()
-        cur.execute_async(sql)
-        job_id = cur._sfqid
+        self.session.sql(sql).collect_nowait()
         self.block_until_resource_is_ready(
-            resource_name=str(job_id),
+            resource_name=job_name,
             resource_type=constants.ResourceType.JOB,
             container_name=constants.KANIKO_CONTAINER_NAME,
             max_retries=240,
             retry_interval_secs=15,
         )
 
     def _drop_service_if_exists(self, service_name: str) -> None:
@@ -178,18 +180,15 @@
             SnowflakeMLException: If the resource received the following status [failed, not_found, internal_error,
                 deleting]
             SnowflakeMLException: If the resource does not reach the ready/done state within the specified number
                 of retries.
         """
         assert resource_type == constants.ResourceType.SERVICE or resource_type == constants.ResourceType.JOB
         query_command = ""
-        if resource_type == constants.ResourceType.SERVICE:
-            query_command = f"CALL SYSTEM$GET_SERVICE_LOGS('{resource_name}', '0', '{container_name}')"
-        elif resource_type == constants.ResourceType.JOB:
-            query_command = f"CALL SYSTEM$GET_JOB_LOGS('{resource_name}', '{container_name}')"
+        query_command = f"CALL SYSTEM$GET_SERVICE_LOGS('{resource_name}', '0', '{container_name}')"
         logger.warning(
             f"Best-effort log streaming from SPCS will be enabled when python logging level is set to INFO."
             f"Alternatively, you can also query the logs by running the query '{query_command}'"
         )
         lsp = log_stream_processor.LogStreamProcessor()
 
         for attempt_idx in range(max_retries):
@@ -197,15 +196,15 @@
                 resource_log = self.get_resource_log(
                     resource_name=resource_name,
                     resource_type=resource_type,
                     container_name=container_name,
                 )
                 lsp.process_new_logs(resource_log, log_level=logging.INFO)
 
-            status = self.get_resource_status(resource_name=resource_name, resource_type=resource_type)
+            status = self.get_resource_status(resource_name=resource_name)
 
             if resource_type == constants.ResourceType.JOB and status == constants.ResourceStatus.DONE:
                 return
             elif resource_type == constants.ResourceType.SERVICE and status == constants.ResourceStatus.READY:
                 return
 
             if (
@@ -242,60 +241,32 @@
                     ),
                 )
             time.sleep(retry_interval_secs)
 
     def get_resource_log(
         self, resource_name: str, resource_type: constants.ResourceType, container_name: str
     ) -> Optional[str]:
-        if resource_type == constants.ResourceType.SERVICE:
-            try:
-                row = self.session.sql(
-                    f"CALL SYSTEM$GET_SERVICE_LOGS('{resource_name}', '0', '{container_name}')"
-                ).collect()
-                return str(row[0]["SYSTEM$GET_SERVICE_LOGS"])
-            except Exception:
-                return None
-        elif resource_type == constants.ResourceType.JOB:
-            try:
-                row = self.session.sql(f"CALL SYSTEM$GET_JOB_LOGS('{resource_name}', '{container_name}')").collect()
-                return str(row[0]["SYSTEM$GET_JOB_LOGS"])
-            except Exception:
-                return None
-        else:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.NOT_IMPLEMENTED,
-                original_exception=NotImplementedError(
-                    f"{resource_type.name} is not yet supported in get_resource_log function"
-                ),
-            )
-
-    def get_resource_status(
-        self, resource_name: str, resource_type: constants.ResourceType
-    ) -> Optional[constants.ResourceStatus]:
+        try:
+            row = self.session.sql(
+                f"CALL SYSTEM$GET_SERVICE_LOGS('{resource_name}', '0', '{container_name}')"
+            ).collect()
+            return str(row[0]["SYSTEM$GET_SERVICE_LOGS"])
+        except Exception:
+            return None
+
+    def get_resource_status(self, resource_name: str) -> Optional[constants.ResourceStatus]:
         """Get resource status.
 
         Args:
             resource_name: Name of the resource.
-            resource_type: Type of the resource.
-
-        Raises:
-            SnowflakeMLException: If resource type does not have a corresponding system function for querying status.
-            SnowflakeMLException: If corresponding status call failed.
 
         Returns:
             Optional[constants.ResourceStatus]: The status of the resource, or None if the resource status is empty.
         """
-        if resource_type not in constants.RESOURCE_TO_STATUS_FUNCTION_MAPPING:
-            raise snowml_exceptions.SnowflakeMLException(
-                error_code=error_codes.INVALID_ARGUMENT,
-                original_exception=ValueError(
-                    f"Status querying is not supported for resources of type '{resource_type}'."
-                ),
-            )
-        status_func = constants.RESOURCE_TO_STATUS_FUNCTION_MAPPING[resource_type]
+        status_func = "SYSTEM$GET_SERVICE_STATUS"
         try:
             row = self.session.sql(f"CALL {status_func}('{resource_name}');").collect()
         except Exception:
             # Silent fail as SPCS status call is not guaranteed to return in time. Will rely on caller to retry.
             return None
 
         resource_metadata = json.loads(row[0][status_func])[0]
```

## snowflake/ml/model/_model_composer/model_composer.py

```diff
@@ -4,16 +4,18 @@
 import uuid
 import zipfile
 from types import ModuleType
 from typing import Any, Dict, List, Optional
 
 from absl import logging
 from packaging import requirements
+from typing_extensions import deprecated
 
 from snowflake.ml._internal import env as snowml_env, env_utils, file_utils
+from snowflake.ml._internal.lineage import data_source
 from snowflake.ml.model import model_signature, type_hints as model_types
 from snowflake.ml.model._model_composer.model_manifest import model_manifest
 from snowflake.ml.model._packager import model_packager
 from snowflake.snowpark import Session
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 
@@ -130,24 +132,26 @@
         file_utils.make_archive(self.model_local_path, str(self._packager_workspace_path))
 
         self.manifest.save(
             session=self.session,
             model_meta=self.packager.meta,
             model_file_rel_path=pathlib.PurePosixPath(self.model_file_rel_path),
             options=options,
+            data_sources=self._get_data_sources(model),
         )
 
         file_utils.upload_directory_to_stage(
             self.session,
             local_path=self.workspace_path,
             stage_path=self.stage_path,
             statement_params=self._statement_params,
         )
 
-    def load(
+    @deprecated("Only used by PrPr model registry. Use static method version of load instead.")
+    def legacy_load(
         self,
         *,
         meta_only: bool = False,
         options: Optional[model_types.ModelLoadOption] = None,
     ) -> None:
         file_utils.download_directory_from_stage(
             self.session,
@@ -159,7 +163,24 @@
         # TODO (Server-side Model Rollout): Remove this section.
         model_zip_path = pathlib.Path(glob.glob(str(self.workspace_path / "*.zip"))[0])
         self.model_file_rel_path = str(model_zip_path.relative_to(self.workspace_path))
 
         with zipfile.ZipFile(self.model_local_path, mode="r", compression=zipfile.ZIP_DEFLATED) as zf:
             zf.extractall(path=self._packager_workspace_path)
         self.packager.load(meta_only=meta_only, options=options)
+
+    @staticmethod
+    def load(
+        workspace_path: pathlib.Path,
+        *,
+        meta_only: bool = False,
+        options: Optional[model_types.ModelLoadOption] = None,
+    ) -> model_packager.ModelPackager:
+        mp = model_packager.ModelPackager(str(workspace_path / ModelComposer.MODEL_DIR_REL_PATH))
+        mp.load(meta_only=meta_only, options=options)
+        return mp
+
+    def _get_data_sources(self, model: model_types.SupportedModelType) -> Optional[List[data_source.DataSource]]:
+        data_sources = getattr(model, "_data_sources", None)
+        if isinstance(data_sources, list) and all(isinstance(item, data_source.DataSource) for item in data_sources):
+            return data_sources
+        return None
```

## snowflake/ml/model/_model_composer/model_manifest/model_manifest.py

```diff
@@ -1,14 +1,15 @@
 import collections
 import copy
 import pathlib
 from typing import List, Optional, cast
 
 import yaml
 
+from snowflake.ml._internal.lineage import data_source
 from snowflake.ml.model import type_hints
 from snowflake.ml.model._model_composer.model_manifest import model_manifest_schema
 from snowflake.ml.model._model_composer.model_method import (
     function_generator,
     model_method,
 )
 from snowflake.ml.model._packager.model_meta import model_meta as model_meta_api
@@ -32,14 +33,15 @@
 
     def save(
         self,
         session: Session,
         model_meta: model_meta_api.ModelMetadata,
         model_file_rel_path: pathlib.PurePosixPath,
         options: Optional[type_hints.ModelSaveOption] = None,
+        data_sources: Optional[List[data_source.DataSource]] = None,
     ) -> None:
         if options is None:
             options = {}
 
         runtime_to_use = copy.deepcopy(model_meta.runtimes["cpu"])
         runtime_to_use.name = self._DEFAULT_RUNTIME_NAME
         runtime_to_use.imports.append(model_file_rel_path)
@@ -86,14 +88,18 @@
                         options, method.target_method
                     ),
                 )
                 for method in self.methods
             ],
         )
 
+        lineage_sources = self._extract_lineage_info(data_sources)
+        if lineage_sources:
+            manifest_dict["lineage_sources"] = lineage_sources
+
         with (self.workspace_path / ModelManifest.MANIFEST_FILE_REL_PATH).open("w", encoding="utf-8") as f:
             # Anchors are not supported in the server, avoid that.
             yaml.SafeDumper.ignore_aliases = lambda *args: True  # type: ignore[method-assign]
             yaml.safe_dump(manifest_dict, f)
 
     def load(self) -> model_manifest_schema.ModelManifestDict:
         with (self.workspace_path / ModelManifest.MANIFEST_FILE_REL_PATH).open("r", encoding="utf-8") as f:
@@ -104,7 +110,23 @@
         original_loaded_manifest_version = raw_input.get("manifest_version", None)
         if not original_loaded_manifest_version:
             raise ValueError("Unable to get the version of the MANIFEST file.")
 
         res = cast(model_manifest_schema.ModelManifestDict, raw_input)
 
         return res
+
+    def _extract_lineage_info(
+        self, data_sources: Optional[List[data_source.DataSource]]
+    ) -> List[model_manifest_schema.LineageSourceDict]:
+        result = []
+        if data_sources:
+            for source in data_sources:
+                result.append(
+                    model_manifest_schema.LineageSourceDict(
+                        # Currently, we only support lineage from Dataset.
+                        type=model_manifest_schema.LineageSourceTypes.DATASET.value,
+                        entity=source.fully_qualified_name,
+                        version=source.version,
+                    )
+                )
+        return result
```

## snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py

```diff
@@ -71,12 +71,23 @@
 
 
 class SnowparkMLDataDict(TypedDict):
     schema_version: Required[str]
     functions: Required[List[ModelFunctionInfoDict]]
 
 
+class LineageSourceTypes(enum.Enum):
+    DATASET = "DATASET"
+
+
+class LineageSourceDict(TypedDict):
+    type: Required[str]
+    entity: Required[str]
+    version: NotRequired[str]
+
+
 class ModelManifestDict(TypedDict):
     manifest_version: Required[str]
     runtimes: Required[Dict[str, ModelRuntimeDict]]
     methods: Required[List[ModelMethodDict]]
     user_data: NotRequired[Dict[str, Any]]
+    lineage_sources: NotRequired[List[LineageSourceDict]]
```

## snowflake/ml/model/_packager/model_packager.py

```diff
@@ -1,14 +1,13 @@
 import os
 from types import ModuleType
 from typing import Dict, List, Optional
 
 from absl import logging
 
-from snowflake.ml._internal import env_utils
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions as snowml_exceptions,
 )
 from snowflake.ml.model import custom_model, model_signature, type_hints as model_types
 from snowflake.ml.model._packager import model_handler
 from snowflake.ml.model._packager.model_meta import model_meta
@@ -125,16 +124,14 @@
 
         self.meta = model_meta.ModelMetadata.load(self.local_dir_path)
         if meta_only:
             return
 
         model_meta.load_code_path(self.local_dir_path)
 
-        env_utils.validate_py_runtime_version(self.meta.env.python_version)
-
         handler = model_handler.load_handler(self.meta.model_type)
         if handler is None:
             raise snowml_exceptions.SnowflakeMLException(
                 error_code=error_codes.INVALID_TYPE,
                 original_exception=TypeError(f"{self.meta.model_type} is not supported."),
             )
         model_blobs_path = os.path.join(self.local_dir_path, ModelPackager.MODEL_BLOBS_DIR)
```

## snowflake/ml/model/_packager/model_env/model_env.py

```diff
@@ -280,22 +280,24 @@
             if channel != env_utils.DEFAULT_CHANNEL_NAME:
                 warnings.warn(
                     (
                         "Found dependencies specified in the conda file from non-Snowflake channel."
                         " This may prevent model deploying to Snowflake Warehouse."
                     ),
                     category=UserWarning,
+                    stacklevel=2,
                 )
             if len(channel_dependencies) == 0 and channel not in self._conda_dependencies:
                 warnings.warn(
                     (
                         f"Found additional conda channel {channel} specified in the conda file."
                         " This may prevent model deploying to Snowflake Warehouse."
                     ),
                     category=UserWarning,
+                    stacklevel=2,
                 )
                 self._conda_dependencies[channel] = []
 
             for channel_dependency in channel_dependencies:
                 try:
                     env_utils.append_conda_dependency(self._conda_dependencies, (channel, channel_dependency))
                 except env_utils.DuplicateDependencyError:
@@ -303,23 +305,25 @@
                 except env_utils.DuplicateDependencyInMultipleChannelsError:
                     warnings.warn(
                         (
                             f"Dependency {channel_dependency.name} appeared in multiple channels as conda dependency."
                             " This may be unintentional."
                         ),
                         category=UserWarning,
+                        stacklevel=2,
                     )
 
         if pip_requirements_list:
             warnings.warn(
                 (
                     "Found dependencies specified as pip requirements."
                     " This may prevent model deploying to Snowflake Warehouse."
                 ),
                 category=UserWarning,
+                stacklevel=2,
             )
             for pip_dependency in pip_requirements_list:
                 if any(
                     channel_dependency.name == pip_dependency.name
                     for channel_dependency in itertools.chain(*self._conda_dependencies.values())
                 ):
                     continue
@@ -334,14 +338,15 @@
         if pip_requirements_list:
             warnings.warn(
                 (
                     "Found dependencies specified as pip requirements."
                     " This may prevent model deploying to Snowflake Warehouse."
                 ),
                 category=UserWarning,
+                stacklevel=2,
             )
             for pip_dependency in pip_requirements_list:
                 if any(
                     channel_dependency.name == pip_dependency.name
                     for channel_dependency in itertools.chain(*self._conda_dependencies.values())
                 ):
                     continue
@@ -368,7 +373,43 @@
         return {
             "conda": self.conda_env_rel_path.as_posix(),
             "pip": self.pip_requirements_rel_path.as_posix(),
             "python_version": self.python_version,
             "cuda_version": self.cuda_version,
             "snowpark_ml_version": self.snowpark_ml_version,
         }
+
+    def validate_with_local_env(
+        self, check_snowpark_ml_version: bool = False
+    ) -> List[env_utils.IncorrectLocalEnvironmentError]:
+        errors = []
+        try:
+            env_utils.validate_py_runtime_version(str(self._python_version))
+        except env_utils.IncorrectLocalEnvironmentError as e:
+            errors.append(e)
+
+        for conda_reqs in self._conda_dependencies.values():
+            for conda_req in conda_reqs:
+                try:
+                    env_utils.validate_local_installed_version_of_pip_package(
+                        env_utils.try_convert_conda_requirement_to_pip(conda_req)
+                    )
+                except env_utils.IncorrectLocalEnvironmentError as e:
+                    errors.append(e)
+
+        for pip_req in self._pip_requirements:
+            try:
+                env_utils.validate_local_installed_version_of_pip_package(pip_req)
+            except env_utils.IncorrectLocalEnvironmentError as e:
+                errors.append(e)
+
+        if check_snowpark_ml_version:
+            # For Modeling model
+            if self._snowpark_ml_version.base_version != snowml_env.VERSION:
+                errors.append(
+                    env_utils.IncorrectLocalEnvironmentError(
+                        f"The local installed version of Snowpark ML library is {snowml_env.VERSION} "
+                        f"which differs from required version {self.snowpark_ml_version}."
+                    )
+                )
+
+        return errors
```

## snowflake/ml/model/_packager/model_meta/model_meta.py

```diff
@@ -316,19 +316,15 @@
                 "version": model_meta_schema.MODEL_METADATA_VERSION,
                 "min_snowpark_ml_version": self.min_snowpark_ml_version,
             }
         )
 
         with open(model_yaml_path, "w", encoding="utf-8") as out:
             yaml.SafeDumper.ignore_aliases = lambda *args: True  # type: ignore[method-assign]
-            yaml.safe_dump(
-                model_dict,
-                stream=out,
-                default_flow_style=False,
-            )
+            yaml.safe_dump(model_dict, stream=out, default_flow_style=False)
 
     @staticmethod
     def _validate_model_metadata(loaded_meta: Any) -> model_meta_schema.ModelMetadataDict:
         if not isinstance(loaded_meta, dict):
             raise ValueError(f"Read ill-formatted model metadata, should be a dict, received {type(loaded_meta)}")
 
         original_loaded_meta_version = loaded_meta.get("version", None)
```

## snowflake/ml/modeling/_internal/model_trainer.py

```diff
@@ -18,7 +18,14 @@
 
     def train_fit_predict(
         self,
         expected_output_cols_list: List[str],
         drop_input_cols: Optional[bool] = False,
     ) -> Tuple[Union[DataFrame, pd.DataFrame], object]:
         raise NotImplementedError
+
+    def train_fit_transform(
+        self,
+        expected_output_cols_list: List[str],
+        drop_input_cols: Optional[bool] = False,
+    ) -> Tuple[Union[DataFrame, pd.DataFrame], object]:
+        raise NotImplementedError
```

## snowflake/ml/modeling/_internal/model_trainer_builder.py

```diff
@@ -134,29 +134,21 @@
             )
 
     @classmethod
     def build_fit_predict(
         cls,
         estimator: object,
         dataset: Union[DataFrame, pd.DataFrame],
-        input_cols: Optional[List[str]] = None,
+        input_cols: List[str],
         autogenerated: bool = False,
         subproject: str = "",
     ) -> ModelTrainer:
         """
         Builder method that creates an appropriate ModelTrainer instance based on the given params.
         """
-        if input_cols is None:
-            raise exceptions.SnowflakeMLException(
-                error_code=error_codes.NOT_FOUND,
-                original_exception=ValueError(
-                    "The input column names (input_cols) is None.\n"
-                    "Please put your input_cols when initializing the estimator\n"
-                ),
-            )
         if isinstance(dataset, pd.DataFrame):
             return PandasModelTrainer(
                 estimator=estimator,
                 dataset=dataset,
                 input_cols=input_cols,
                 label_cols=None,
                 sample_weight_col=None,
@@ -173,9 +165,50 @@
                 "autogenerated": autogenerated,
                 "subproject": subproject,
             }
             return trainer_klass(**init_args)  # type: ignore[arg-type]
         else:
             raise TypeError(
                 f"Unexpected dataset type: {type(dataset)}."
+                "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
+            )
+
+    @classmethod
+    def build_fit_transform(
+        cls,
+        estimator: object,
+        dataset: Union[DataFrame, pd.DataFrame],
+        input_cols: List[str],
+        label_cols: Optional[List[str]] = None,
+        sample_weight_col: Optional[str] = None,
+        autogenerated: bool = False,
+        subproject: str = "",
+    ) -> ModelTrainer:
+        """
+        Builder method that creates an appropriate ModelTrainer instance based on the given params.
+        """
+        if isinstance(dataset, pd.DataFrame):
+            return PandasModelTrainer(
+                estimator=estimator,
+                dataset=dataset,
+                input_cols=input_cols,
+                label_cols=label_cols,
+                sample_weight_col=sample_weight_col,
+            )
+        elif isinstance(dataset, DataFrame):
+            trainer_klass = SnowparkModelTrainer
+            init_args = {
+                "estimator": estimator,
+                "dataset": dataset,
+                "session": dataset._session,
+                "input_cols": input_cols,
+                "label_cols": label_cols,
+                "sample_weight_col": sample_weight_col,
+                "autogenerated": autogenerated,
+                "subproject": subproject,
+            }
+            return trainer_klass(**init_args)  # type: ignore[arg-type]
+        else:
+            raise TypeError(
+                f"Unexpected dataset type: {type(dataset)}."
                 "Supported dataset types: snowpark.DataFrame, pandas.DataFrame."
             )
```

## snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py

```diff
@@ -1,12 +1,14 @@
 import inspect
 from typing import List, Optional, Tuple
 
 import pandas as pd
 
+from snowflake.ml.modeling._internal.estimator_utils import handle_inference_result
+
 
 class PandasModelTrainer:
     """
     A class for training machine learning models using Pandas datasets.
     """
 
     def __init__(
@@ -68,15 +70,65 @@
             drop_input_cols (Optional[bool]): Boolean to determine whether to
                 drop the input columns from the output dataset.
 
         Returns:
             Tuple[pd.DataFrame, object]: [predicted dataset, estimator]
         """
         assert hasattr(self.estimator, "fit_predict")  # make type checker happy
-        args = {"X": self.dataset[self.input_cols]}
-        result = self.estimator.fit_predict(**args)
+        result = self.estimator.fit_predict(X=self.dataset[self.input_cols])
         result_df = pd.DataFrame(data=result, columns=expected_output_cols_list)
         if drop_input_cols:
             result_df = result_df
         else:
-            result_df = pd.concat([self.dataset, result_df], axis=1)
+            # in case the output column name overlap with the input column names,
+            # remove the ones in input column names
+            remove_dataset_col_name_exist_in_output_col = list(
+                set(self.dataset.columns) - set(expected_output_cols_list)
+            )
+            result_df = pd.concat([self.dataset[remove_dataset_col_name_exist_in_output_col], result_df], axis=1)
+        return (result_df, self.estimator)
+
+    def train_fit_transform(
+        self,
+        expected_output_cols_list: List[str],
+        drop_input_cols: Optional[bool] = False,
+    ) -> Tuple[pd.DataFrame, object]:
+        """Trains the model using specified features and target columns from the dataset.
+        This API is different from fit itself because it would also provide the transform
+        output.
+
+        Args:
+            expected_output_cols_list (List[str]): The output columns
+                name as a list. Defaults to None.
+            drop_input_cols (Optional[bool]): Boolean to determine whether to
+                drop the input columns from the output dataset.
+
+        Returns:
+            Tuple[pd.DataFrame, object]: [transformed dataset, estimator]
+        """
+        assert hasattr(self.estimator, "fit")  # make type checker happy
+        assert hasattr(self.estimator, "fit_transform")  # make type checker happy
+
+        argspec = inspect.getfullargspec(self.estimator.fit)
+        args = {"X": self.dataset[self.input_cols]}
+        if self.label_cols:
+            label_arg_name = "Y" if "Y" in argspec.args else "y"
+            args[label_arg_name] = self.dataset[self.label_cols].squeeze()
+
+        if self.sample_weight_col is not None and "sample_weight" in argspec.args:
+            args["sample_weight"] = self.dataset[self.sample_weight_col].squeeze()
+
+        inference_res = self.estimator.fit_transform(**args)
+
+        transformed_numpy_array, output_cols = handle_inference_result(
+            inference_res=inference_res, output_cols=expected_output_cols_list, inference_method="fit_transform"
+        )
+
+        result_df = pd.DataFrame(data=transformed_numpy_array, columns=output_cols)
+        if drop_input_cols:
+            result_df = result_df
+        else:
+            # in case the output column name overlap with the input column names,
+            # remove the ones in input column names
+            remove_dataset_col_name_exist_in_output_col = list(set(self.dataset.columns) - set(output_cols))
+            result_df = pd.concat([self.dataset[remove_dataset_col_name_exist_in_output_col], result_df], axis=1)
         return (result_df, self.estimator)
```

## snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py

```diff
@@ -68,32 +68,48 @@
             kwargs: additional keyword args.
 
         Returns:
             A new dataset of the same type as the input dataset.
 
         """
 
-        handler = SnowparkTransformHandlers(
-            dataset=self.dataset,
-            estimator=self.estimator,
-            class_name=self._class_name,
-            subproject=self._subproject,
-            autogenerated=self._autogenerated,
-        )
-        return handler.batch_inference(
-            inference_method,
-            input_cols,
-            expected_output_cols,
-            session,
-            dependencies,
-            drop_input_cols,
-            expected_output_cols_type,
-            *args,
-            **kwargs,
-        )
+        mlrs_inference_methods = ["predict", "predict_proba", "predict_log_proba"]
+
+        if inference_method in mlrs_inference_methods:
+            result_df = self.client.inference(
+                estimator=self.estimator,
+                dataset=self.dataset,
+                inference_method=inference_method,
+                input_cols=input_cols,
+                output_cols=expected_output_cols,
+                drop_input_cols=drop_input_cols,
+            )
+
+        else:
+            handler = SnowparkTransformHandlers(
+                dataset=self.dataset,
+                estimator=self.estimator,
+                class_name=self._class_name,
+                subproject=self._subproject,
+                autogenerated=self._autogenerated,
+            )
+            result_df = handler.batch_inference(
+                inference_method,
+                input_cols,
+                expected_output_cols,
+                session,
+                dependencies,
+                drop_input_cols,
+                expected_output_cols_type,
+                *args,
+                **kwargs,
+            )
+
+        assert isinstance(result_df, DataFrame)  # mypy - The MLRS return types are annotated as `object`.
+        return result_df
 
     def score(
         self,
         input_cols: List[str],
         label_cols: List[str],
         session: Session,
         dependencies: List[str],
```

## snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py

```diff
@@ -5,15 +5,19 @@
 from typing import Any, Dict, List, Optional
 from uuid import uuid4
 
 import cloudpickle as cp
 import pandas as pd
 
 from snowflake.ml._internal import telemetry
-from snowflake.ml._internal.utils import identifier, snowpark_dataframe_utils
+from snowflake.ml._internal.utils import (
+    identifier,
+    pkg_version_utils,
+    snowpark_dataframe_utils,
+)
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils.temp_file_utils import (
     cleanup_temp_files,
     get_temp_file_path,
 )
 from snowflake.ml.modeling._internal.estimator_utils import handle_inference_result
 from snowflake.snowpark import DataFrame, Session, functions as F, types as T
@@ -87,14 +91,15 @@
             args: additional positional arguments.
             kwargs: additional keyword args.
 
         Returns:
             A new dataset of the same type as the input dataset.
         """
 
+        dependencies = self._get_validated_snowpark_dependencies(session, dependencies)
         dataset = self.dataset
         estimator = self.estimator
         # Register vectorized UDF for batch inference
         batch_inference_udf_name = random_name_for_temp_object(TempObjectType.FUNCTION)
 
         dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)
         # Align the input_cols with snowpark dataframe's column name
@@ -206,15 +211,16 @@
             sample_weight_col: A column assigning relative weights to each row for scoring.
             args: additional positional arguments.
             kwargs: additional keyword args.
 
         Returns:
             An accuracy score for the model on the given test data.
         """
-
+        dependencies = self._get_validated_snowpark_dependencies(session, dependencies)
+        dependencies.append("snowflake-snowpark-python")
         dataset = self.dataset
         estimator = self.estimator
         dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)
 
         # Extract queries that generated the dataframe. We will need to pass it to score procedure.
         queries = dataset.queries["queries"]
 
@@ -331,7 +337,23 @@
             score_statement_params,
             **kwargs,
         )
 
         cleanup_temp_files([local_score_file_name])
 
         return score
+
+    def _get_validated_snowpark_dependencies(self, session: Session, dependencies: List[str]) -> List[str]:
+        """A helper function to validate dependencies and return the available packages that exists
+        in the snowflake anaconda channel
+
+        Args:
+            session: the active snowpark Session
+            dependencies: unvalidated dependencies
+
+        Returns:
+            A list of packages present in the snoflake conda channel.
+        """
+
+        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
+            pkg_versions=dependencies, session=session, subproject=self._subproject
+        )
```

## snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py

```diff
@@ -19,28 +19,34 @@
     snowpark_dataframe_utils,
 )
 from snowflake.ml._internal.utils.query_result_checker import SqlResultValidator
 from snowflake.ml._internal.utils.temp_file_utils import (
     cleanup_temp_files,
     get_temp_file_path,
 )
+from snowflake.ml.modeling._internal.estimator_utils import handle_inference_result
 from snowflake.ml.modeling._internal.model_specifications import (
     ModelSpecifications,
     ModelSpecificationsBuilder,
 )
-from snowflake.snowpark import DataFrame, Session, exceptions as snowpark_exceptions
+from snowflake.snowpark import (
+    DataFrame,
+    Session,
+    exceptions as snowpark_exceptions,
+    functions as F,
+)
 from snowflake.snowpark._internal.utils import (
     TempObjectType,
     random_name_for_temp_object,
 )
-from snowflake.snowpark.functions import sproc
 from snowflake.snowpark.stored_procedure import StoredProcedure
 
 cp.register_pickle_by_value(inspect.getmodule(get_temp_file_path))
 cp.register_pickle_by_value(inspect.getmodule(identifier.get_inferred_name))
+cp.register_pickle_by_value(inspect.getmodule(handle_inference_result))
 
 _PROJECT = "ModelDevelopment"
 
 
 class SnowparkModelTrainer:
     """
     A class for training models on Snowflake data using the Sproc.
@@ -118,15 +124,15 @@
         stage_transform_file_name = posixpath.join(stage_name, os.path.basename(local_transform_file_name))
         stage_result_file_name = posixpath.join(stage_name, os.path.basename(local_transform_file_name))
 
         statement_params = telemetry.get_function_usage_statement_params(
             project=_PROJECT,
             subproject=self._subproject,
             function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), self._class_name),
-            api_calls=[sproc],
+            api_calls=[F.sproc],
             custom_tags=dict([("autogen", True)]) if self._autogenerated else None,
         )
         # Put locally serialized transform on stage.
         self.session.file.put(
             local_transform_file_name,
             stage_transform_file_name,
             auto_compress=False,
@@ -288,15 +294,15 @@
                 like required imports, package dependencies, etc.
 
         Returns:
             A callable that can be registered as a stored procedure.
         """
         imports = model_spec.imports  # In order for the sproc to not resolve this reference in snowflake.ml
 
-        def fit_wrapper_function(
+        def fit_predict_wrapper_function(
             session: Session,
             sql_queries: List[str],
             stage_transform_file_name: str,
             stage_result_file_name: str,
             input_cols: List[str],
             statement_params: Dict[str, str],
             drop_input_cols: bool,
@@ -325,15 +331,15 @@
 
             local_transform_file_path = os.path.join(
                 local_transform_file_name, os.listdir(local_transform_file_name)[0]
             )
             with open(local_transform_file_path, mode="r+b") as local_transform_file_obj:
                 estimator = cp.load(local_transform_file_obj)
 
-            fit_predict_result = estimator.fit_predict(df[input_cols])
+            fit_predict_result = estimator.fit_predict(X=df[input_cols])
 
             local_result_file_name = get_temp_file_path()
 
             with open(local_result_file_name, mode="w+b") as local_result_file_obj:
                 cp.dump(estimator, local_result_file_obj)
 
             session.file.put(
@@ -345,37 +351,178 @@
             )
 
             # store the predict output
             if drop_input_cols:
                 fit_predict_result_pd = pd.DataFrame(data=fit_predict_result, columns=expected_output_cols_list)
             else:
                 df = df.copy()
-                fit_predict_result_pd = pd.DataFrame(data=fit_predict_result, columns=expected_output_cols_list)
-                fit_predict_result_pd = pd.concat([df, fit_predict_result_pd], axis=1)
+                # in case the output column name overlap with the input column names,
+                # remove the ones in input column names
+                remove_dataset_col_name_exist_in_output_col = list(set(df.columns) - set(expected_output_cols_list))
+                fit_predict_result_pd = pd.concat(
+                    [
+                        df[remove_dataset_col_name_exist_in_output_col],
+                        pd.DataFrame(data=fit_predict_result, columns=expected_output_cols_list),
+                    ],
+                    axis=1,
+                )
 
             # write into a temp table in sproc and load the table from outside
             session.write_pandas(
                 fit_predict_result_pd, fit_predict_result_name, auto_create_table=True, table_type="temp"
             )
 
             # Note: you can add something like  + "|" + str(df) to the return string
             # to pass debug information to the caller.
             return str(os.path.basename(local_result_file_name))
 
-        return fit_wrapper_function
+        return fit_predict_wrapper_function
+
+    def _build_fit_transform_wrapper_sproc(
+        self,
+        model_spec: ModelSpecifications,
+    ) -> Callable[
+        [
+            Session,
+            List[str],
+            str,
+            str,
+            List[str],
+            Optional[List[str]],
+            Optional[str],
+            Dict[str, str],
+            bool,
+            List[str],
+            str,
+        ],
+        str,
+    ]:
+        """
+        Constructs and returns a python stored procedure function to be used for training model.
+
+        Args:
+            model_spec: ModelSpecifications object that contains model specific information
+                like required imports, package dependencies, etc.
+
+        Returns:
+            A callable that can be registered as a stored procedure.
+        """
+        imports = model_spec.imports  # In order for the sproc to not resolve this reference in snowflake.ml
+
+        def fit_transform_wrapper_function(
+            session: Session,
+            sql_queries: List[str],
+            stage_transform_file_name: str,
+            stage_result_file_name: str,
+            input_cols: List[str],
+            label_cols: Optional[List[str]],
+            sample_weight_col: Optional[str],
+            statement_params: Dict[str, str],
+            drop_input_cols: bool,
+            expected_output_cols_list: List[str],
+            fit_transform_result_name: str,
+        ) -> str:
+            import os
+
+            import cloudpickle as cp
+            import pandas as pd
+
+            for import_name in imports:
+                importlib.import_module(import_name)
+
+            # Execute snowpark queries and obtain the results as pandas dataframe
+            # NB: this implies that the result data must fit into memory.
+            for query in sql_queries[:-1]:
+                _ = session.sql(query).collect(statement_params=statement_params)
+            sp_df = session.sql(sql_queries[-1])
+            df: pd.DataFrame = sp_df.to_pandas(statement_params=statement_params)
+            df.columns = sp_df.columns
+
+            local_transform_file_name = get_temp_file_path()
+
+            session.file.get(stage_transform_file_name, local_transform_file_name, statement_params=statement_params)
+
+            local_transform_file_path = os.path.join(
+                local_transform_file_name, os.listdir(local_transform_file_name)[0]
+            )
+            with open(local_transform_file_path, mode="r+b") as local_transform_file_obj:
+                estimator = cp.load(local_transform_file_obj)
+
+            argspec = inspect.getfullargspec(estimator.fit)
+            args = {"X": df[input_cols]}
+            if label_cols:
+                label_arg_name = "Y" if "Y" in argspec.args else "y"
+                args[label_arg_name] = df[label_cols].squeeze()
+
+            if sample_weight_col is not None and "sample_weight" in argspec.args:
+                args["sample_weight"] = df[sample_weight_col].squeeze()
+
+            fit_transform_result = estimator.fit_transform(**args)
+
+            local_result_file_name = get_temp_file_path()
+
+            with open(local_result_file_name, mode="w+b") as local_result_file_obj:
+                cp.dump(estimator, local_result_file_obj)
+
+            session.file.put(
+                local_result_file_name,
+                stage_result_file_name,
+                auto_compress=False,
+                overwrite=True,
+                statement_params=statement_params,
+            )
+
+            transformed_numpy_array, output_cols = handle_inference_result(
+                inference_res=fit_transform_result,
+                output_cols=expected_output_cols_list,
+                inference_method="fit_transform",
+                within_udf=True,
+            )
+
+            if len(transformed_numpy_array.shape) > 1:
+                if transformed_numpy_array.shape[1] != len(output_cols):
+                    series = pd.Series(transformed_numpy_array.tolist())
+                    transformed_pandas_df = pd.DataFrame(series, columns=output_cols)
+                else:
+                    transformed_pandas_df = pd.DataFrame(transformed_numpy_array.tolist(), columns=output_cols)
+            else:
+                transformed_pandas_df = pd.DataFrame(transformed_numpy_array, columns=output_cols)
+
+            # store the transform output
+            if not drop_input_cols:
+                df = df.copy()
+                # in case the output column name overlap with the input column names,
+                # remove the ones in input column names
+                remove_dataset_col_name_exist_in_output_col = list(set(df.columns) - set(output_cols))
+                transformed_pandas_df = pd.concat(
+                    [df[remove_dataset_col_name_exist_in_output_col], transformed_pandas_df], axis=1
+                )
+
+            # write into a temp table in sproc and load the table from outside
+            session.write_pandas(
+                transformed_pandas_df,
+                fit_transform_result_name,
+                auto_create_table=True,
+                table_type="temp",
+                quote_identifiers=False,
+            )
+
+            return str(os.path.basename(local_result_file_name))
+
+        return fit_transform_wrapper_function
 
     def _get_fit_predict_wrapper_sproc(self, statement_params: Dict[str, str]) -> StoredProcedure:
         # If the sproc already exists, don't register.
-        if not hasattr(self.session, "_FIT_PRE_WRAPPER_SPROCS"):
-            self.session._FIT_PRE_WRAPPER_SPROCS: Dict[str, StoredProcedure] = {}  # type: ignore[attr-defined, misc]
+        if not hasattr(self.session, "_FIT_WRAPPER_SPROCS"):
+            self.session._FIT_WRAPPER_SPROCS: Dict[str, StoredProcedure] = {}  # type: ignore[attr-defined, misc]
 
         model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
-        fit_predict_sproc_key = model_spec.__class__.__name__
-        if fit_predict_sproc_key in self.session._FIT_PRE_WRAPPER_SPROCS:  # type: ignore[attr-defined]
-            fit_sproc: StoredProcedure = self.session._FIT_PRE_WRAPPER_SPROCS[  # type: ignore[attr-defined]
+        fit_predict_sproc_key = model_spec.__class__.__name__ + "_fit_predict"
+        if fit_predict_sproc_key in self.session._FIT_WRAPPER_SPROCS:  # type: ignore[attr-defined]
+            fit_sproc: StoredProcedure = self.session._FIT_WRAPPER_SPROCS[  # type: ignore[attr-defined]
                 fit_predict_sproc_key
             ]
             return fit_sproc
 
         fit_predict_sproc_name = random_name_for_temp_object(TempObjectType.PROCEDURE)
 
         relaxed_dependencies = pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
@@ -388,20 +535,55 @@
             name=fit_predict_sproc_name,
             packages=["snowflake-snowpark-python"] + relaxed_dependencies,  # type: ignore[arg-type]
             replace=True,
             session=self.session,
             statement_params=statement_params,
         )
 
-        self.session._FIT_PRE_WRAPPER_SPROCS[  # type: ignore[attr-defined]
+        self.session._FIT_WRAPPER_SPROCS[  # type: ignore[attr-defined]
             fit_predict_sproc_key
         ] = fit_predict_wrapper_sproc
 
         return fit_predict_wrapper_sproc
 
+    def _get_fit_transform_wrapper_sproc(self, statement_params: Dict[str, str]) -> StoredProcedure:
+        # If the sproc already exists, don't register.
+        if not hasattr(self.session, "_FIT_WRAPPER_SPROCS"):
+            self.session._FIT_WRAPPER_SPROCS: Dict[str, StoredProcedure] = {}  # type: ignore[attr-defined, misc]
+
+        model_spec = ModelSpecificationsBuilder.build(model=self.estimator)
+        fit_transform_sproc_key = model_spec.__class__.__name__ + "_fit_transform"
+        if fit_transform_sproc_key in self.session._FIT_WRAPPER_SPROCS:  # type: ignore[attr-defined]
+            fit_sproc: StoredProcedure = self.session._FIT_WRAPPER_SPROCS[  # type: ignore[attr-defined]
+                fit_transform_sproc_key
+            ]
+            return fit_sproc
+
+        fit_transform_sproc_name = random_name_for_temp_object(TempObjectType.PROCEDURE)
+
+        relaxed_dependencies = pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
+            pkg_versions=model_spec.pkgDependencies, session=self.session
+        )
+
+        fit_transform_wrapper_sproc = self.session.sproc.register(
+            func=self._build_fit_transform_wrapper_sproc(model_spec=model_spec),
+            is_permanent=False,
+            name=fit_transform_sproc_name,
+            packages=["snowflake-snowpark-python"] + relaxed_dependencies,  # type: ignore[arg-type]
+            replace=True,
+            session=self.session,
+            statement_params=statement_params,
+        )
+
+        self.session._FIT_WRAPPER_SPROCS[  # type: ignore[attr-defined]
+            fit_transform_sproc_key
+        ] = fit_transform_wrapper_sproc
+
+        return fit_transform_wrapper_sproc
+
     def train(self) -> object:
         """
         Trains the model by pushing down the compute into Snowflake using stored procedures.
 
         Returns:
             Trained model
 
@@ -494,18 +676,18 @@
             project=_PROJECT,
             subproject=self._subproject,
             function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), self._class_name),
             api_calls=[Session.call],
             custom_tags=dict([("autogen", True)]) if self._autogenerated else None,
         )
 
-        fit_wrapper_sproc = self._get_fit_predict_wrapper_sproc(statement_params=statement_params)
+        fit_predict_wrapper_sproc = self._get_fit_predict_wrapper_sproc(statement_params=statement_params)
         fit_predict_result_name = random_name_for_temp_object(TempObjectType.TABLE)
 
-        sproc_export_file_name: str = fit_wrapper_sproc(
+        sproc_export_file_name: str = fit_predict_wrapper_sproc(
             self.session,
             queries,
             stage_transform_file_name,
             stage_result_file_name,
             self.input_cols,
             statement_params,
             drop_input_cols,
@@ -517,7 +699,70 @@
         fitted_estimator = self._fetch_model_from_stage(
             dir_path=stage_result_file_name,
             file_name=sproc_export_file_name,
             statement_params=statement_params,
         )
 
         return output_result_sp, fitted_estimator
+
+    def train_fit_transform(
+        self,
+        expected_output_cols_list: List[str],
+        drop_input_cols: Optional[bool] = False,
+    ) -> Tuple[Union[DataFrame, pd.DataFrame], object]:
+        """Trains the model by pushing down the compute into Snowflake using stored procedures.
+        This API is different from fit itself because it would also provide the transform
+        output.
+
+        Args:
+            expected_output_cols_list (List[str]): The output columns
+                name as a list. Defaults to None.
+            drop_input_cols (Optional[bool]): Boolean to determine whether to
+                drop the input columns from the output dataset.
+
+        Returns:
+            Tuple[Union[DataFrame, pd.DataFrame], object]: [transformed dataset, estimator]
+        """
+        dataset = snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(self.dataset)
+
+        # Extract query that generated the dataframe. We will need to pass it to the fit procedure.
+        queries = dataset.queries["queries"]
+
+        transform_stage_name = self._create_temp_stage()
+        (stage_transform_file_name, stage_result_file_name) = self._upload_model_to_stage(
+            stage_name=transform_stage_name
+        )
+
+        # Call fit sproc
+        statement_params = telemetry.get_function_usage_statement_params(
+            project=_PROJECT,
+            subproject=self._subproject,
+            function_name=telemetry.get_statement_params_full_func_name(inspect.currentframe(), self._class_name),
+            api_calls=[Session.call],
+            custom_tags=dict([("autogen", True)]) if self._autogenerated else None,
+        )
+
+        fit_transform_wrapper_sproc = self._get_fit_transform_wrapper_sproc(statement_params=statement_params)
+        fit_transform_result_name = random_name_for_temp_object(TempObjectType.TABLE)
+
+        sproc_export_file_name: str = fit_transform_wrapper_sproc(
+            self.session,
+            queries,
+            stage_transform_file_name,
+            stage_result_file_name,
+            self.input_cols,
+            self.label_cols,
+            self.sample_weight_col,
+            statement_params,
+            drop_input_cols,
+            expected_output_cols_list,
+            fit_transform_result_name,
+        )
+
+        output_result_sp = self.session.table(fit_transform_result_name)
+        fitted_estimator = self._fetch_model_from_stage(
+            dir_path=stage_result_file_name,
+            file_name=sproc_export_file_name,
+            statement_params=statement_params,
+        )
+
+        return output_result_sp, fitted_estimator
```

## snowflake/ml/modeling/calibration/calibrated_classifier_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.calibration".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class CalibratedClassifierCV(BaseTransformer):
     r"""Probability calibration with isotonic regression or logistic regression
     For more details on this class, see [sklearn.calibration.CalibratedClassifierCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html)
 
     Parameters
     ----------
@@ -324,28 +318,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -355,17 +346,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -405,15 +394,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -488,18 +478,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -558,24 +546,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -660,18 +672,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -730,18 +740,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -795,18 +803,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -864,18 +870,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -931,25 +935,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1006,19 +1008,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/affinity_propagation.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class AffinityPropagation(BaseTransformer):
     r"""Perform Affinity Propagation Clustering of data
     For more details on this class, see [sklearn.cluster.AffinityPropagation]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html)
 
     Parameters
     ----------
@@ -299,28 +293,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -330,17 +321,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -380,15 +369,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -463,18 +453,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -535,24 +523,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -635,18 +647,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -703,18 +713,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -768,18 +776,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -837,18 +843,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -902,25 +906,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -977,19 +979,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/agglomerative_clustering.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class AgglomerativeClustering(BaseTransformer):
     r"""Agglomerative Clustering
     For more details on this class, see [sklearn.cluster.AgglomerativeClustering]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.AgglomerativeClustering.html)
 
     Parameters
     ----------
@@ -332,28 +326,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -363,17 +354,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -411,15 +400,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -494,18 +484,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -566,24 +554,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -666,18 +678,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -734,18 +744,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -799,18 +807,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -868,18 +874,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -933,25 +937,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1008,19 +1010,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/birch.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Birch(BaseTransformer):
     r"""Implements the BIRCH clustering algorithm
     For more details on this class, see [sklearn.cluster.Birch]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html)
 
     Parameters
     ----------
@@ -290,28 +284,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -321,17 +312,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -371,15 +360,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -456,18 +446,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -528,24 +516,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.cluster.Birch.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.Birch.html#sklearn.cluster.Birch.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -628,18 +642,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -696,18 +708,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -761,18 +771,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -830,18 +838,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -895,25 +901,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -970,19 +974,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/bisecting_k_means.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BisectingKMeans(BaseTransformer):
     r"""Bisecting K-Means clustering
     For more details on this class, see [sklearn.cluster.BisectingKMeans]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html)
 
     Parameters
     ----------
@@ -339,28 +333,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -370,17 +361,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -420,15 +409,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -505,18 +495,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -577,24 +565,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Compute clustering and transform X to cluster-distance space
+        For more details on this function, see [sklearn.cluster.BisectingKMeans.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.BisectingKMeans.html#sklearn.cluster.BisectingKMeans.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -677,18 +691,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -745,18 +757,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -810,18 +820,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -879,18 +887,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -946,25 +952,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1021,19 +1025,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/dbscan.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class DBSCAN(BaseTransformer):
     r"""Perform DBSCAN clustering from vector array or distance matrix
     For more details on this class, see [sklearn.cluster.DBSCAN]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)
 
     Parameters
     ----------
@@ -307,28 +301,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -338,17 +329,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -386,15 +375,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -469,18 +459,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -541,24 +529,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -641,18 +653,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -709,18 +719,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -774,18 +782,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -843,18 +849,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -908,25 +912,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -983,19 +985,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/feature_agglomeration.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class FeatureAgglomeration(BaseTransformer):
     r"""Agglomerate features
     For more details on this class, see [sklearn.cluster.FeatureAgglomeration]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html)
 
     Parameters
     ----------
@@ -339,28 +333,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -370,17 +361,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -418,15 +407,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -503,18 +493,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -575,24 +563,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.cluster.FeatureAgglomeration.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.FeatureAgglomeration.html#sklearn.cluster.FeatureAgglomeration.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -675,18 +689,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -743,18 +755,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -808,18 +818,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -877,18 +885,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +948,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1021,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/k_means.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KMeans(BaseTransformer):
     r"""K-Means clustering
     For more details on this class, see [sklearn.cluster.KMeans]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)
 
     Parameters
     ----------
@@ -334,28 +328,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -365,17 +356,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -415,15 +404,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -500,18 +490,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -572,24 +560,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Compute clustering and transform X to cluster-distance space
+        For more details on this function, see [sklearn.cluster.KMeans.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -672,18 +686,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -740,18 +752,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -805,18 +815,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -874,18 +882,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -941,25 +947,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1016,19 +1020,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/mean_shift.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MeanShift(BaseTransformer):
     r"""Mean shift clustering using a flat kernel
     For more details on this class, see [sklearn.cluster.MeanShift]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html)
 
     Parameters
     ----------
@@ -310,28 +304,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -341,17 +332,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -391,15 +380,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -474,18 +464,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -546,24 +534,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -646,18 +658,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -714,18 +724,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -779,18 +787,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -848,18 +854,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -913,25 +917,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -988,19 +990,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/mini_batch_k_means.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MiniBatchKMeans(BaseTransformer):
     r"""Mini-Batch K-Means clustering
     For more details on this class, see [sklearn.cluster.MiniBatchKMeans]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html)
 
     Parameters
     ----------
@@ -360,28 +354,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -391,17 +382,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -441,15 +430,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -526,18 +516,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -598,24 +586,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Compute clustering and transform X to cluster-distance space
+        For more details on this function, see [sklearn.cluster.MiniBatchKMeans.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -698,18 +712,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -766,18 +778,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -831,18 +841,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -900,18 +908,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -967,25 +973,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1042,19 +1046,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/optics.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class OPTICS(BaseTransformer):
     r"""Estimate clustering structure from vector array
     For more details on this class, see [sklearn.cluster.OPTICS]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.OPTICS.html)
 
     Parameters
     ----------
@@ -380,28 +374,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -411,17 +402,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -459,15 +448,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -542,18 +532,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -614,24 +602,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -714,18 +726,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -782,18 +792,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -847,18 +855,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -916,18 +922,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -981,25 +985,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1056,19 +1058,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/spectral_biclustering.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SpectralBiclustering(BaseTransformer):
     r"""Spectral biclustering (Kluger, 2003)
     For more details on this class, see [sklearn.cluster.SpectralBiclustering]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralBiclustering.html)
 
     Parameters
     ----------
@@ -318,28 +312,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -349,17 +340,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -397,15 +386,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -480,18 +470,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -550,24 +538,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -650,18 +662,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -718,18 +728,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -783,18 +791,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -852,18 +858,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -917,25 +921,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -992,19 +994,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/spectral_clustering.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SpectralClustering(BaseTransformer):
     r"""Apply clustering to a projection of the normalized Laplacian
     For more details on this class, see [sklearn.cluster.SpectralClustering]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html)
 
     Parameters
     ----------
@@ -376,28 +370,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -407,17 +398,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -455,15 +444,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -538,18 +528,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -610,24 +598,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -710,18 +722,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -778,18 +788,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -843,18 +851,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -912,18 +918,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -977,25 +981,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1052,19 +1054,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/cluster/spectral_coclustering.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.cluster".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SpectralCoclustering(BaseTransformer):
     r"""Spectral Co-Clustering algorithm (Dhillon, 2001)
     For more details on this class, see [sklearn.cluster.SpectralCoclustering]
     (https://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralCoclustering.html)
 
     Parameters
     ----------
@@ -297,28 +291,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -328,17 +319,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -376,15 +365,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -459,18 +449,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -529,24 +517,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -629,18 +641,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -697,18 +707,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -762,18 +770,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -831,18 +837,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -896,25 +900,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -971,19 +973,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/compose/column_transformer.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.compose".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ColumnTransformer(BaseTransformer):
     r"""Applies transformers to columns of an array or pandas DataFrame
     For more details on this class, see [sklearn.compose.ColumnTransformer]
     (https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html)
 
     Parameters
     ----------
@@ -327,28 +321,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -358,17 +349,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -406,15 +395,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -491,18 +481,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -561,24 +549,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit all transformers, transform the data and concatenate results
+        For more details on this function, see [sklearn.compose.ColumnTransformer.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -661,18 +675,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -729,18 +741,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -794,18 +804,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -863,18 +871,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -928,25 +934,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1003,19 +1007,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/compose/transformed_target_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.compose".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class TransformedTargetRegressor(BaseTransformer):
     r"""Meta-estimator to regress on a transformed target
     For more details on this class, see [sklearn.compose.TransformedTargetRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.compose.TransformedTargetRegressor.html)
 
     Parameters
     ----------
@@ -288,28 +282,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -319,17 +310,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -369,15 +358,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -452,18 +442,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -522,24 +510,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -622,18 +634,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -690,18 +700,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -755,18 +763,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -824,18 +830,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -891,25 +895,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -966,19 +968,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/elliptic_envelope.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class EllipticEnvelope(BaseTransformer):
     r"""An object for detecting outliers in a Gaussian distributed dataset
     For more details on this class, see [sklearn.covariance.EllipticEnvelope]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EllipticEnvelope.html)
 
     Parameters
     ----------
@@ -283,28 +277,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -314,17 +305,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -364,15 +353,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -447,18 +437,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -519,24 +507,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -619,18 +631,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -687,18 +697,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -754,18 +762,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -825,18 +831,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -892,25 +896,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -967,19 +969,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/empirical_covariance.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class EmpiricalCovariance(BaseTransformer):
     r"""Maximum likelihood covariance estimator
     For more details on this class, see [sklearn.covariance.EmpiricalCovariance]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.EmpiricalCovariance.html)
 
     Parameters
     ----------
@@ -259,28 +253,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -290,17 +281,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -338,15 +327,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -421,18 +411,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -491,24 +479,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -591,18 +603,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -659,18 +669,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -724,18 +732,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +799,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -860,25 +864,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -935,19 +937,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/graphical_lasso.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GraphicalLasso(BaseTransformer):
     r"""Sparse inverse covariance estimation with an l1-penalized estimator
     For more details on this class, see [sklearn.covariance.GraphicalLasso]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLasso.html)
 
     Parameters
     ----------
@@ -307,28 +301,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -338,17 +329,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -386,15 +375,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -469,18 +459,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -539,24 +527,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -639,18 +651,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -707,18 +717,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -772,18 +780,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -841,18 +847,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -908,25 +912,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -983,19 +985,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/graphical_lasso_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GraphicalLassoCV(BaseTransformer):
     r"""Sparse inverse covariance w/ cross-validated choice of the l1 penalty
     For more details on this class, see [sklearn.covariance.GraphicalLassoCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.GraphicalLassoCV.html)
 
     Parameters
     ----------
@@ -333,28 +327,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -364,17 +355,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -412,15 +401,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -495,18 +485,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -565,24 +553,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -665,18 +677,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -733,18 +743,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -798,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -867,18 +873,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -934,25 +938,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1009,19 +1011,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/ledoit_wolf.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LedoitWolf(BaseTransformer):
     r"""LedoitWolf Estimator
     For more details on this class, see [sklearn.covariance.LedoitWolf]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.LedoitWolf.html)
 
     Parameters
     ----------
@@ -266,28 +260,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -297,17 +288,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -345,15 +334,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -428,18 +418,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -498,24 +486,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -598,18 +610,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -666,18 +676,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -731,18 +739,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -800,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -867,25 +871,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -942,19 +944,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/min_cov_det.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MinCovDet(BaseTransformer):
     r"""Minimum Covariance Determinant (MCD): robust estimator of covariance
     For more details on this class, see [sklearn.covariance.MinCovDet]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.MinCovDet.html)
 
     Parameters
     ----------
@@ -278,28 +272,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -309,17 +300,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -357,15 +346,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -440,18 +430,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -510,24 +498,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -610,18 +622,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -678,18 +688,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -743,18 +751,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -812,18 +818,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -879,25 +883,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -954,19 +956,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/oas.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class OAS(BaseTransformer):
     r"""Oracle Approximating Shrinkage Estimator as proposed in [1]_
     For more details on this class, see [sklearn.covariance.OAS]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.OAS.html)
 
     Parameters
     ----------
@@ -259,28 +253,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -290,17 +281,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -338,15 +327,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -421,18 +411,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -491,24 +479,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -591,18 +603,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -659,18 +669,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -724,18 +732,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +799,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -860,25 +864,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -935,19 +937,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/covariance/shrunk_covariance.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.covariance".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ShrunkCovariance(BaseTransformer):
     r"""Covariance estimator with shrinkage
     For more details on this class, see [sklearn.covariance.ShrunkCovariance]
     (https://scikit-learn.org/stable/modules/generated/sklearn.covariance.ShrunkCovariance.html)
 
     Parameters
     ----------
@@ -265,28 +259,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -296,17 +287,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -344,15 +333,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -427,18 +417,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -497,24 +485,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -597,18 +609,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -665,18 +675,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -730,18 +738,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -799,18 +805,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -866,25 +870,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -941,19 +943,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/dictionary_learning.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class DictionaryLearning(BaseTransformer):
     r"""Dictionary learning
     For more details on this class, see [sklearn.decomposition.DictionaryLearning]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html)
 
     Parameters
     ----------
@@ -371,28 +365,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -402,17 +393,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -450,15 +439,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -535,18 +525,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -605,24 +593,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the model from data in X and return the transformed data
+        For more details on this function, see [sklearn.decomposition.DictionaryLearning.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -705,18 +719,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -773,18 +785,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -838,18 +848,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -907,18 +915,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -972,25 +978,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1047,19 +1051,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/factor_analysis.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class FactorAnalysis(BaseTransformer):
     r"""Factor Analysis (FA)
     For more details on this class, see [sklearn.decomposition.FactorAnalysis]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html)
 
     Parameters
     ----------
@@ -308,28 +302,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -339,17 +330,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -387,15 +376,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -472,18 +462,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -542,24 +530,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.decomposition.FactorAnalysis.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -642,18 +656,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -710,18 +722,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -775,18 +785,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -846,18 +854,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -913,25 +919,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -988,19 +992,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/fast_ica.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class FastICA(BaseTransformer):
     r"""FastICA: a fast algorithm for Independent Component Analysis
     For more details on this class, see [sklearn.decomposition.FastICA]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html)
 
     Parameters
     ----------
@@ -326,28 +320,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -357,17 +348,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -405,15 +394,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -490,18 +480,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -560,24 +548,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the model and recover the sources from X
+        For more details on this function, see [sklearn.decomposition.FastICA.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -660,18 +674,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -728,18 +740,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +803,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -862,18 +870,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -927,25 +933,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1002,19 +1006,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/incremental_pca.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class IncrementalPCA(BaseTransformer):
     r"""Incremental principal components analysis (IPCA)
     For more details on this class, see [sklearn.decomposition.IncrementalPCA]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)
 
     Parameters
     ----------
@@ -278,28 +272,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -309,17 +300,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -357,15 +346,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -442,18 +432,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -512,24 +500,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.decomposition.IncrementalPCA.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -612,18 +626,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -680,18 +692,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -745,18 +755,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -814,18 +822,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -879,25 +885,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -954,19 +958,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/kernel_pca.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KernelPCA(BaseTransformer):
     r"""Kernel Principal component analysis (KPCA) [1]_
     For more details on this class, see [sklearn.decomposition.KernelPCA]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html)
 
     Parameters
     ----------
@@ -374,28 +368,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -405,17 +396,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -453,15 +442,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -538,18 +528,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -608,24 +596,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the model from data in X and transform X
+        For more details on this function, see [sklearn.decomposition.KernelPCA.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -708,18 +722,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -776,18 +788,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -841,18 +851,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -910,18 +918,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -975,25 +981,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1050,19 +1054,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MiniBatchDictionaryLearning(BaseTransformer):
     r"""Mini-batch dictionary learning
     For more details on this class, see [sklearn.decomposition.MiniBatchDictionaryLearning]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html)
 
     Parameters
     ----------
@@ -396,28 +390,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -427,17 +418,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -475,15 +464,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -560,18 +550,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -630,24 +618,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.decomposition.MiniBatchDictionaryLearning.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -730,18 +744,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -798,18 +810,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -863,18 +873,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -932,18 +940,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -997,25 +1003,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1072,19 +1076,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MiniBatchSparsePCA(BaseTransformer):
     r"""Mini-batch Sparse Principal Components Analysis
     For more details on this class, see [sklearn.decomposition.MiniBatchSparsePCA]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchSparsePCA.html)
 
     Parameters
     ----------
@@ -341,28 +335,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -372,17 +363,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -420,15 +409,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -505,18 +495,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -575,24 +563,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.decomposition.MiniBatchSparsePCA.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -675,18 +689,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -743,18 +755,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -808,18 +818,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -877,18 +885,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +948,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1021,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/pca.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class PCA(BaseTransformer):
     r"""Principal component analysis (PCA)
     For more details on this class, see [sklearn.decomposition.PCA]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)
 
     Parameters
     ----------
@@ -343,28 +337,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -374,17 +365,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -422,15 +411,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -507,18 +497,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -577,24 +565,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the model with X and apply the dimensionality reduction on X
+        For more details on this function, see [sklearn.decomposition.PCA.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -677,18 +691,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -745,18 +757,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -810,18 +820,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -881,18 +889,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -948,25 +954,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1023,19 +1027,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/sparse_pca.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SparsePCA(BaseTransformer):
     r"""Sparse Principal Components Analysis (SparsePCA)
     For more details on this class, see [sklearn.decomposition.SparsePCA]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html)
 
     Parameters
     ----------
@@ -316,28 +310,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -347,17 +338,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -395,15 +384,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -480,18 +470,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -550,24 +538,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.decomposition.SparsePCA.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -650,18 +664,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -718,18 +730,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -783,18 +793,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -852,18 +860,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -917,25 +923,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -992,19 +996,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/decomposition/truncated_svd.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.decomposition".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class TruncatedSVD(BaseTransformer):
     r"""Dimensionality reduction using truncated SVD (aka LSA)
     For more details on this class, see [sklearn.decomposition.TruncatedSVD]
     (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)
 
     Parameters
     ----------
@@ -297,28 +291,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -328,17 +319,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -376,15 +365,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -461,18 +451,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -531,24 +519,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit model to X and perform dimensionality reduction on X
+        For more details on this function, see [sklearn.decomposition.TruncatedSVD.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -631,18 +645,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -699,18 +711,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -764,18 +774,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -833,18 +841,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -898,25 +904,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -973,19 +977,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.discriminant_analysis".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LinearDiscriminantAnalysis(BaseTransformer):
     r"""Linear Discriminant Analysis
     For more details on this class, see [sklearn.discriminant_analysis.LinearDiscriminantAnalysis]
     (https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)
 
     Parameters
     ----------
@@ -314,28 +308,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -345,17 +336,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -395,15 +384,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -480,18 +470,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -550,24 +538,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -652,18 +666,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -722,18 +734,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -789,18 +799,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -858,18 +866,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -925,25 +931,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1000,19 +1004,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.discriminant_analysis".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class QuadraticDiscriminantAnalysis(BaseTransformer):
     r"""Quadratic Discriminant Analysis
     For more details on this class, see [sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis]
     (https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html)
 
     Parameters
     ----------
@@ -276,28 +270,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -307,17 +298,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -357,15 +346,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -440,18 +430,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -510,24 +498,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -612,18 +624,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -682,18 +692,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -749,18 +757,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -818,18 +824,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -885,25 +889,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -960,19 +962,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/ada_boost_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class AdaBoostClassifier(BaseTransformer):
     r"""An AdaBoost classifier
     For more details on this class, see [sklearn.ensemble.AdaBoostClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html)
 
     Parameters
     ----------
@@ -301,28 +295,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -332,17 +323,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -382,15 +371,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -465,18 +455,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -535,24 +523,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -637,18 +649,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -707,18 +717,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -774,18 +782,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -843,18 +849,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -910,25 +914,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -985,19 +987,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/ada_boost_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class AdaBoostRegressor(BaseTransformer):
     r"""An AdaBoost regressor
     For more details on this class, see [sklearn.ensemble.AdaBoostRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html)
 
     Parameters
     ----------
@@ -298,28 +292,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -329,17 +320,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -379,15 +368,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -462,18 +452,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -532,24 +520,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -632,18 +644,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -700,18 +710,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -765,18 +773,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -834,18 +840,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -901,25 +905,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -976,19 +978,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/bagging_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BaggingClassifier(BaseTransformer):
     r"""A Bagging classifier
     For more details on this class, see [sklearn.ensemble.BaggingClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html)
 
     Parameters
     ----------
@@ -333,28 +327,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -364,17 +355,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -414,15 +403,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -497,18 +487,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -567,24 +555,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -669,18 +681,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -739,18 +749,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -806,18 +814,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -875,18 +881,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +946,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1019,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/bagging_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BaggingRegressor(BaseTransformer):
     r"""A Bagging regressor
     For more details on this class, see [sklearn.ensemble.BaggingRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingRegressor.html)
 
     Parameters
     ----------
@@ -333,28 +327,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -364,17 +355,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -414,15 +403,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -497,18 +487,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -567,24 +555,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -667,18 +679,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -735,18 +745,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -800,18 +808,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -869,18 +875,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -936,25 +940,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1011,19 +1013,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/extra_trees_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ExtraTreesClassifier(BaseTransformer):
     r"""An extra-trees classifier
     For more details on this class, see [sklearn.ensemble.ExtraTreesClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html)
 
     Parameters
     ----------
@@ -436,28 +430,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -467,17 +458,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -517,15 +506,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -600,18 +590,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -670,24 +658,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -772,18 +784,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -842,18 +852,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -907,18 +915,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -976,18 +982,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1043,25 +1047,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1118,19 +1120,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/extra_trees_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ExtraTreesRegressor(BaseTransformer):
     r"""An extra-trees regressor
     For more details on this class, see [sklearn.ensemble.ExtraTreesRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html)
 
     Parameters
     ----------
@@ -415,28 +409,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -446,17 +437,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -496,15 +485,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -579,18 +569,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -649,24 +637,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -749,18 +761,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -817,18 +827,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -882,18 +890,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -951,18 +957,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1018,25 +1022,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1093,19 +1095,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GradientBoostingClassifier(BaseTransformer):
     r"""Gradient Boosting for classification
     For more details on this class, see [sklearn.ensemble.GradientBoostingClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html)
 
     Parameters
     ----------
@@ -448,28 +442,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -479,17 +470,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -529,15 +518,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -612,18 +602,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -682,24 +670,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -784,18 +796,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -854,18 +864,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -921,18 +929,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -990,18 +996,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1057,25 +1061,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1132,19 +1134,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GradientBoostingRegressor(BaseTransformer):
     r"""Gradient Boosting for regression
     For more details on this class, see [sklearn.ensemble.GradientBoostingRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html)
 
     Parameters
     ----------
@@ -457,28 +451,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -488,17 +479,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -538,15 +527,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -621,18 +611,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -691,24 +679,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -791,18 +803,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -859,18 +869,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -924,18 +932,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -993,18 +999,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1060,25 +1064,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1135,19 +1137,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class HistGradientBoostingClassifier(BaseTransformer):
     r"""Histogram-based Gradient Boosting Classification Tree
     For more details on this class, see [sklearn.ensemble.HistGradientBoostingClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html)
 
     Parameters
     ----------
@@ -429,28 +423,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -460,17 +451,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -510,15 +499,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -593,18 +583,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -663,24 +651,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -765,18 +777,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -835,18 +845,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -902,18 +910,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -971,18 +977,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1038,25 +1042,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1113,19 +1115,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class HistGradientBoostingRegressor(BaseTransformer):
     r"""Histogram-based Gradient Boosting Regression Tree
     For more details on this class, see [sklearn.ensemble.HistGradientBoostingRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html)
 
     Parameters
     ----------
@@ -420,28 +414,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -451,17 +442,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -501,15 +490,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -584,18 +574,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -654,24 +642,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -754,18 +766,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -822,18 +832,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -887,18 +895,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -956,18 +962,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1023,25 +1027,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1098,19 +1100,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/isolation_forest.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class IsolationForest(BaseTransformer):
     r"""Isolation Forest Algorithm
     For more details on this class, see [sklearn.ensemble.IsolationForest]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html)
 
     Parameters
     ----------
@@ -320,28 +314,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -351,17 +342,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -401,15 +390,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -484,18 +474,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -556,24 +544,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -656,18 +668,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -724,18 +734,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -791,18 +799,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -862,18 +868,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -927,25 +931,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1002,19 +1004,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/random_forest_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RandomForestClassifier(BaseTransformer):
     r"""A random forest classifier
     For more details on this class, see [sklearn.ensemble.RandomForestClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)
 
     Parameters
     ----------
@@ -432,28 +426,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -463,17 +454,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -513,15 +502,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -596,18 +586,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -666,24 +654,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -768,18 +780,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -838,18 +848,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -903,18 +911,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -972,18 +978,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1039,25 +1043,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1114,19 +1116,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/random_forest_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RandomForestRegressor(BaseTransformer):
     r"""A random forest regressor
     For more details on this class, see [sklearn.ensemble.RandomForestRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)
 
     Parameters
     ----------
@@ -411,28 +405,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -442,17 +433,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -492,15 +481,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -575,18 +565,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -645,24 +633,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -745,18 +757,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -813,18 +823,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -878,18 +886,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -947,18 +953,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1014,25 +1018,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1089,19 +1091,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/stacking_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class StackingRegressor(BaseTransformer):
     r"""Stack of estimators with a final regressor
     For more details on this class, see [sklearn.ensemble.StackingRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html)
 
     Parameters
     ----------
@@ -312,28 +306,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -343,17 +334,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -393,15 +382,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -478,18 +468,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -548,24 +536,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the estimators and return the predictions for X for each estimator
+        For more details on this function, see [sklearn.ensemble.StackingRegressor.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -648,18 +662,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -716,18 +728,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -781,18 +791,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -850,18 +858,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -917,25 +923,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -992,19 +996,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/voting_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class VotingClassifier(BaseTransformer):
     r"""Soft Voting/Majority Rule classifier for unfitted estimators
     For more details on this class, see [sklearn.ensemble.VotingClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html)
 
     Parameters
     ----------
@@ -294,28 +288,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -325,17 +316,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -375,15 +364,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -460,18 +450,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -530,24 +518,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Return class labels or probabilities for each estimator
+        For more details on this function, see [sklearn.ensemble.VotingClassifier.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -632,18 +646,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -702,18 +714,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -767,18 +777,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -836,18 +844,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -903,25 +909,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -978,19 +982,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/ensemble/voting_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.ensemble".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class VotingRegressor(BaseTransformer):
     r"""Prediction voting regressor for unfitted estimators
     For more details on this class, see [sklearn.ensemble.VotingRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html)
 
     Parameters
     ----------
@@ -276,28 +270,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -307,17 +298,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -357,15 +346,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -442,18 +432,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -512,24 +500,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Return class labels or probabilities for each estimator
+        For more details on this function, see [sklearn.ensemble.VotingRegressor.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingRegressor.html#sklearn.ensemble.VotingRegressor.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -612,18 +626,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -680,18 +692,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -745,18 +755,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -814,18 +822,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -881,25 +887,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -956,19 +960,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/generic_univariate_select.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GenericUnivariateSelect(BaseTransformer):
     r"""Univariate feature selector with configurable strategy
     For more details on this class, see [sklearn.feature_selection.GenericUnivariateSelect]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html)
 
     Parameters
     ----------
@@ -266,28 +260,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -297,17 +288,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -345,15 +334,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -430,18 +420,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -500,24 +488,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.GenericUnivariateSelect.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.GenericUnivariateSelect.html#sklearn.feature_selection.GenericUnivariateSelect.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -600,18 +614,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -668,18 +680,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -733,18 +743,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -802,18 +810,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -867,25 +873,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -942,19 +946,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/select_fdr.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SelectFdr(BaseTransformer):
     r"""Filter: Select the p-values for an estimated false discovery rate
     For more details on this class, see [sklearn.feature_selection.SelectFdr]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html)
 
     Parameters
     ----------
@@ -262,28 +256,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -293,17 +284,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -341,15 +330,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -426,18 +416,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -496,24 +484,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.SelectFdr.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFdr.html#sklearn.feature_selection.SelectFdr.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -596,18 +610,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -664,18 +676,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -729,18 +739,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -798,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -863,25 +869,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -938,19 +942,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/select_fpr.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SelectFpr(BaseTransformer):
     r"""Filter: Select the pvalues below alpha based on a FPR test
     For more details on this class, see [sklearn.feature_selection.SelectFpr]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html)
 
     Parameters
     ----------
@@ -262,28 +256,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -293,17 +284,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -341,15 +330,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -426,18 +416,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -496,24 +484,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.SelectFpr.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFpr.html#sklearn.feature_selection.SelectFpr.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -596,18 +610,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -664,18 +676,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -729,18 +739,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -798,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -863,25 +869,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -938,19 +942,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/select_fwe.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SelectFwe(BaseTransformer):
     r"""Filter: Select the p-values corresponding to Family-wise error rate
     For more details on this class, see [sklearn.feature_selection.SelectFwe]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html)
 
     Parameters
     ----------
@@ -262,28 +256,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -293,17 +284,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -341,15 +330,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -426,18 +416,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -496,24 +484,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.SelectFwe.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectFwe.html#sklearn.feature_selection.SelectFwe.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -596,18 +610,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -664,18 +676,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -729,18 +739,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -798,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -863,25 +869,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -938,19 +942,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/select_k_best.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SelectKBest(BaseTransformer):
     r"""Select features according to the k highest scores
     For more details on this class, see [sklearn.feature_selection.SelectKBest]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html)
 
     Parameters
     ----------
@@ -263,28 +257,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -294,17 +285,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -342,15 +331,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -427,18 +417,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -497,24 +485,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.SelectKBest.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -597,18 +611,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -665,18 +677,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -730,18 +740,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -799,18 +807,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -864,25 +870,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -939,19 +943,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/select_percentile.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SelectPercentile(BaseTransformer):
     r"""Select features according to a percentile of the highest scores
     For more details on this class, see [sklearn.feature_selection.SelectPercentile]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html)
 
     Parameters
     ----------
@@ -262,28 +256,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -293,17 +284,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -341,15 +330,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -426,18 +416,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -496,24 +484,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.SelectPercentile.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectPercentile.html#sklearn.feature_selection.SelectPercentile.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -596,18 +610,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -664,18 +676,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -729,18 +739,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -798,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -863,25 +869,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -938,19 +942,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/sequential_feature_selector.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SequentialFeatureSelector(BaseTransformer):
     r"""Transformer that performs Sequential Feature Selection
     For more details on this class, see [sklearn.feature_selection.SequentialFeatureSelector]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html)
 
     Parameters
     ----------
@@ -320,28 +314,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -351,17 +342,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -399,15 +388,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -484,18 +474,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -554,24 +542,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.SequentialFeatureSelector.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SequentialFeatureSelector.html#sklearn.feature_selection.SequentialFeatureSelector.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -654,18 +668,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -722,18 +734,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -787,18 +797,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -856,18 +864,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -921,25 +927,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -996,19 +1000,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/feature_selection/variance_threshold.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.feature_selection".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class VarianceThreshold(BaseTransformer):
     r"""Feature selector that removes all low-variance features
     For more details on this class, see [sklearn.feature_selection.VarianceThreshold]
     (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html)
 
     Parameters
     ----------
@@ -253,28 +247,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -284,17 +275,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -332,15 +321,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -417,18 +407,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -487,24 +475,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.feature_selection.VarianceThreshold.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.VarianceThreshold.html#sklearn.feature_selection.VarianceThreshold.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -587,18 +601,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -655,18 +667,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -720,18 +730,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -789,18 +797,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -854,25 +860,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -929,19 +933,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/framework/base.py

```diff
@@ -12,14 +12,15 @@
 from snowflake import snowpark
 from snowflake.ml._internal import telemetry
 from snowflake.ml._internal.exceptions import (
     error_codes,
     exceptions,
     modeling_error_messages,
 )
+from snowflake.ml._internal.lineage import data_source, dataset_dataframe
 from snowflake.ml._internal.utils import identifier, parallelize
 from snowflake.ml.modeling.framework import _utils
 from snowflake.snowpark import functions as F
 
 PROJECT = "ModelDevelopment"
 SUBPROJECT = "Preprocessing"
 
@@ -381,14 +382,15 @@
         """
         super().__init__()
 
         # transformer state
         self.file_names = file_names
         self.custom_states = custom_states
         self.sample_weight_col = sample_weight_col
+        self._data_sources: Optional[List[data_source.DataSource]] = None
 
         self.start_time = datetime.now().strftime(_utils.DATETIME_FORMAT)[:-3]
 
     def get_sample_weight_col(self) -> Optional[str]:
         """
         Sample weight column getter.
 
@@ -415,20 +417,25 @@
         Return the list of conda dependencies required to work with the object.
 
         Returns:
             List of dependencies.
         """
         return []
 
+    def _get_data_sources(self) -> Optional[List[data_source.DataSource]]:
+        return self._data_sources
+
     @telemetry.send_api_usage_telemetry(
         project=PROJECT,
         subproject=SUBPROJECT,
     )
     def fit(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> "BaseEstimator":
         """Runs universal logics for all fit implementations."""
+        if isinstance(dataset, dataset_dataframe.DatasetDataFrame):
+            self._data_sources = dataset._get_sources()
         return self._fit(dataset)
 
     @abstractmethod
     def _fit(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> "BaseEstimator":
         raise NotImplementedError()
 
     def _use_input_cols_only(self, dataset: pd.DataFrame) -> pd.DataFrame:
@@ -535,66 +542,86 @@
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Model {self.__class__.__name__} not fitted before calling predict/transform."
                 ),
             )
 
-    def _infer_input_output_cols(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> None:
+    def _infer_input_cols(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> List[str]:
         """
-        Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
+        Infer input_cols from the dataset. Input column are all columns in the input dataset that are not
+        designated as label, passthrough, or sample weight columns.
 
         Args:
             dataset: Input dataset.
 
+        Returns:
+            The list of input columns.
+        """
+        cols = [
+            c
+            for c in dataset.columns
+            if (c not in self.get_label_cols() and c not in self.get_passthrough_cols() and c != self.sample_weight_col)
+        ]
+        return cols
+
+    def _infer_output_cols(self) -> List[str]:
+        """Infer output column names from based on the estimator.
+
+        Returns:
+            The list of output columns.
+
         Raises:
             SnowflakeMLException: If unable to infer output columns
+
         """
-        if not self.input_cols:
-            cols = [
-                c
-                for c in dataset.columns
-                if (
-                    c not in self.get_label_cols()
-                    and c not in self.get_passthrough_cols()
-                    and c != self.sample_weight_col
-                )
-            ]
-            self.set_input_cols(input_cols=cols)
 
-        if not self.output_cols:
-            # keep mypy happy
-            assert self._sklearn_object is not None
+        # keep mypy happy
+        assert self._sklearn_object is not None
+        if hasattr(self._sklearn_object, "_estimator_type"):
+            # For supervised estimators, infer the output columns from the label columns
+            if self._sklearn_object._estimator_type in SKLEARN_SUPERVISED_ESTIMATORS:
+                cols = [identifier.concat_names(["OUTPUT_", c]) for c in self.label_cols]
+                return cols
+
+            # For density estimators, clusterers, and outlier detectors, there is always exactly one output column.
+            elif self._sklearn_object._estimator_type in SKLEARN_SINGLE_OUTPUT_ESTIMATORS:
+                return ["OUTPUT_0"]
 
-            if hasattr(self._sklearn_object, "_estimator_type"):
-                # For supervised estimators, infer the output columns from the label columns
-                if self._sklearn_object._estimator_type in SKLEARN_SUPERVISED_ESTIMATORS:
-                    cols = [identifier.concat_names(["OUTPUT_", c]) for c in self.label_cols]
-                    self.set_output_cols(output_cols=cols)
-
-                # For density estimators, clusterers, and outlier detectors, there is always exactly one output column.
-                elif self._sklearn_object._estimator_type in SKLEARN_SINGLE_OUTPUT_ESTIMATORS:
-                    self.set_output_cols(output_cols=["OUTPUT_0"])
-
-                else:
-                    raise exceptions.SnowflakeMLException(
-                        error_code=error_codes.INVALID_ARGUMENT,
-                        original_exception=ValueError(
-                            f"Unable to infer output columns for estimator type {self._sklearn_object._estimator_type}."
-                            f"Please include `output_cols` explicitly."
-                        ),
-                    )
             else:
                 raise exceptions.SnowflakeMLException(
                     error_code=error_codes.INVALID_ARGUMENT,
                     original_exception=ValueError(
-                        f"Unable to infer output columns for object {self._sklearn_object}."
+                        f"Unable to infer output columns for estimator type {self._sklearn_object._estimator_type}."
                         f"Please include `output_cols` explicitly."
                     ),
                 )
+        else:
+            raise exceptions.SnowflakeMLException(
+                error_code=error_codes.INVALID_ARGUMENT,
+                original_exception=ValueError(
+                    f"Unable to infer output columns for object {self._sklearn_object}."
+                    f"Please include `output_cols` explicitly."
+                ),
+            )
+
+    def _infer_input_output_cols(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> None:
+        """
+        Infer `self.input_cols` and `self.output_cols` if they are not explicitly set.
+
+        Args:
+            dataset: Input dataset.
+        """
+        if not self.input_cols:
+            cols = self._infer_input_cols(dataset=dataset)
+            self.set_input_cols(input_cols=cols)
+
+        if not self.output_cols:
+            cols = self._infer_output_cols()
+            self.set_output_cols(output_cols=cols)
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
 
         Args:
             output_cols_prefix: the prefix for output cols, such as its inference method.
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.gaussian_process".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GaussianProcessClassifier(BaseTransformer):
     r"""Gaussian process classification (GPC) based on Laplace approximation
     For more details on this class, see [sklearn.gaussian_process.GaussianProcessClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessClassifier.html)
 
     Parameters
     ----------
@@ -348,28 +342,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -379,17 +370,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -429,15 +418,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -512,18 +502,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -582,24 +570,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -684,18 +696,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -754,18 +764,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -819,18 +827,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -888,18 +894,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -955,25 +959,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1030,19 +1032,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.gaussian_process".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GaussianProcessRegressor(BaseTransformer):
     r"""Gaussian process regression (GPR)
     For more details on this class, see [sklearn.gaussian_process.GaussianProcessRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html)
 
     Parameters
     ----------
@@ -339,28 +333,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -370,17 +361,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -420,15 +409,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -503,18 +493,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -573,24 +561,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -673,18 +685,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -741,18 +751,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -806,18 +814,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -875,18 +881,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +946,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1019,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/impute/iterative_imputer.py

```diff
@@ -57,20 +57,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.impute".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class IterativeImputer(BaseTransformer):
     r"""Multivariate imputer that estimates each feature from all the others
     For more details on this class, see [sklearn.impute.IterativeImputer]
     (https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)
 
     Parameters
     ----------
@@ -381,28 +375,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -412,17 +403,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -460,15 +449,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -545,18 +535,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -615,24 +603,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the imputer on `X` and return the transformed `X`
+        For more details on this function, see [sklearn.impute.IterativeImputer.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html#sklearn.impute.IterativeImputer.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -715,18 +729,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -783,18 +795,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -848,18 +858,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -917,18 +925,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -982,25 +988,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1057,19 +1061,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/impute/knn_imputer.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.impute".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KNNImputer(BaseTransformer):
     r"""Imputation for completing missing values using k-Nearest Neighbors
     For more details on this class, see [sklearn.impute.KNNImputer]
     (https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html)
 
     Parameters
     ----------
@@ -307,28 +301,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -338,17 +329,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -386,15 +375,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -471,18 +461,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -541,24 +529,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.impute.KNNImputer.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -641,18 +655,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -709,18 +721,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -774,18 +784,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -843,18 +851,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -908,25 +914,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -983,19 +987,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/impute/missing_indicator.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.impute".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MissingIndicator(BaseTransformer):
     r"""Binary indicators for missing values
     For more details on this class, see [sklearn.impute.MissingIndicator]
     (https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html)
 
     Parameters
     ----------
@@ -281,28 +275,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -312,17 +303,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -360,15 +349,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -445,18 +435,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -515,24 +503,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Generate missing values indicator for `X`
+        For more details on this function, see [sklearn.impute.MissingIndicator.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.impute.MissingIndicator.html#sklearn.impute.MissingIndicator.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -615,18 +629,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -683,18 +695,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -748,18 +758,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -817,18 +825,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -882,25 +888,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -957,19 +961,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class AdditiveChi2Sampler(BaseTransformer):
     r"""Approximate feature map for additive chi2 kernel
     For more details on this class, see [sklearn.kernel_approximation.AdditiveChi2Sampler]
     (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html)
 
     Parameters
     ----------
@@ -256,28 +250,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -287,17 +278,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -335,15 +324,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -420,18 +410,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -490,24 +478,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.kernel_approximation.AdditiveChi2Sampler.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.AdditiveChi2Sampler.html#sklearn.kernel_approximation.AdditiveChi2Sampler.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -590,18 +604,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -658,18 +670,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -723,18 +733,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -792,18 +800,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -857,25 +863,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -932,19 +936,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/kernel_approximation/nystroem.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Nystroem(BaseTransformer):
     r"""Approximate a kernel map using a subset of the training data
     For more details on this class, see [sklearn.kernel_approximation.Nystroem]
     (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html)
 
     Parameters
     ----------
@@ -304,28 +298,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -335,17 +326,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -383,15 +372,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -468,18 +458,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -538,24 +526,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.kernel_approximation.Nystroem.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.Nystroem.html#sklearn.kernel_approximation.Nystroem.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -638,18 +652,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -706,18 +718,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -771,18 +781,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -840,18 +848,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -905,25 +911,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -980,19 +984,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class PolynomialCountSketch(BaseTransformer):
     r"""Polynomial kernel approximation via Tensor Sketch
     For more details on this class, see [sklearn.kernel_approximation.PolynomialCountSketch]
     (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.PolynomialCountSketch.html)
 
     Parameters
     ----------
@@ -280,28 +274,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -311,17 +302,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -359,15 +348,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -444,18 +434,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -514,24 +502,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.kernel_approximation.PolynomialCountSketch.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.PolynomialCountSketch.html#sklearn.kernel_approximation.PolynomialCountSketch.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -614,18 +628,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -682,18 +694,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -747,18 +757,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -816,18 +824,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -881,25 +887,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -956,19 +960,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/kernel_approximation/rbf_sampler.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RBFSampler(BaseTransformer):
     r"""Approximate a RBF kernel feature map using random Fourier features
     For more details on this class, see [sklearn.kernel_approximation.RBFSampler]
     (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html)
 
     Parameters
     ----------
@@ -267,28 +261,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -298,17 +289,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -346,15 +335,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -431,18 +421,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -501,24 +489,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.kernel_approximation.RBFSampler.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.RBFSampler.html#sklearn.kernel_approximation.RBFSampler.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -601,18 +615,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -669,18 +681,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -734,18 +744,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -803,18 +811,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -868,25 +874,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -943,19 +947,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_approximation".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SkewedChi2Sampler(BaseTransformer):
     r"""Approximate feature map for "skewed chi-squared" kernel
     For more details on this class, see [sklearn.kernel_approximation.SkewedChi2Sampler]
     (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.SkewedChi2Sampler.html)
 
     Parameters
     ----------
@@ -265,28 +259,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -296,17 +287,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -344,15 +333,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -429,18 +419,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -499,24 +487,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.kernel_approximation.SkewedChi2Sampler.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_approximation.SkewedChi2Sampler.html#sklearn.kernel_approximation.SkewedChi2Sampler.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -599,18 +613,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -667,18 +679,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -732,18 +742,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -801,18 +809,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -866,25 +872,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -941,19 +945,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/kernel_ridge/kernel_ridge.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.kernel_ridge".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KernelRidge(BaseTransformer):
     r"""Kernel ridge regression
     For more details on this class, see [sklearn.kernel_ridge.KernelRidge]
     (https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html)
 
     Parameters
     ----------
@@ -301,28 +295,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -332,17 +323,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -382,15 +371,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -465,18 +455,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -535,24 +523,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -635,18 +647,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -703,18 +713,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -768,18 +776,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -837,18 +843,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -904,25 +908,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -979,19 +981,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/lightgbm/lgbm_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "lightgbm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LGBMClassifier(BaseTransformer):
     r"""LightGBM classifier
     For more details on this class, see [lightgbm.LGBMClassifier]
     (https://lightgbm.readthedocs.io/en/v3.3.2/pythonapi/lightgbm.LGBMClassifier.html)
 
     Parameters
     ----------
@@ -290,28 +284,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -321,17 +312,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -371,15 +360,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -454,18 +444,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -524,24 +512,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -626,18 +638,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -696,18 +706,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -761,18 +769,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -830,18 +836,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -897,25 +901,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['lightgbm', 'sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -972,19 +974,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/lightgbm/lgbm_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "lightgbm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LGBMRegressor(BaseTransformer):
     r"""LightGBM regressor
     For more details on this class, see [lightgbm.LGBMRegressor]
     (https://lightgbm.readthedocs.io/en/v3.3.2/pythonapi/lightgbm.LGBMRegressor.html)
 
     Parameters
     ----------
@@ -290,28 +284,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -321,17 +312,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -371,15 +360,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -454,18 +444,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -524,24 +512,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -624,18 +636,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -692,18 +702,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -757,18 +765,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -826,18 +832,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -893,25 +897,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['lightgbm', 'sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -968,19 +970,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/ard_regression.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ARDRegression(BaseTransformer):
     r"""Bayesian ARD regression
     For more details on this class, see [sklearn.linear_model.ARDRegression]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ARDRegression.html)
 
     Parameters
     ----------
@@ -315,28 +309,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -346,17 +337,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -396,15 +385,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -479,18 +469,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -549,24 +537,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -649,18 +661,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -717,18 +727,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -782,18 +790,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -851,18 +857,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -918,25 +922,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -993,19 +995,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/bayesian_ridge.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BayesianRidge(BaseTransformer):
     r"""Bayesian ridge regression
     For more details on this class, see [sklearn.linear_model.BayesianRidge]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.BayesianRidge.html)
 
     Parameters
     ----------
@@ -326,28 +320,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -357,17 +348,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -407,15 +396,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -490,18 +480,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -560,24 +548,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -660,18 +672,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -728,18 +738,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +801,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -862,18 +868,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -929,25 +933,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1004,19 +1006,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/elastic_net.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ElasticNet(BaseTransformer):
     r"""Linear regression with combined L1 and L2 priors as regularizer
     For more details on this class, see [sklearn.linear_model.ElasticNet]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)
 
     Parameters
     ----------
@@ -325,28 +319,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -356,17 +347,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -406,15 +395,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -489,18 +479,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -559,24 +547,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -659,18 +671,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -727,18 +737,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -792,18 +800,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -861,18 +867,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -928,25 +932,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1003,19 +1005,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/elastic_net_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ElasticNetCV(BaseTransformer):
     r"""Elastic Net model with iterative fitting along a regularization path
     For more details on this class, see [sklearn.linear_model.ElasticNetCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNetCV.html)
 
     Parameters
     ----------
@@ -361,28 +355,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -392,17 +383,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -442,15 +431,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -525,18 +515,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -595,24 +583,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -695,18 +707,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -763,18 +773,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -828,18 +836,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -897,18 +903,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -964,25 +968,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1039,19 +1041,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/gamma_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GammaRegressor(BaseTransformer):
     r"""Generalized Linear Model with a Gamma distribution
     For more details on this class, see [sklearn.linear_model.GammaRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.GammaRegressor.html)
 
     Parameters
     ----------
@@ -306,28 +300,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -337,17 +328,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -387,15 +376,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -470,18 +460,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -540,24 +528,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -640,18 +652,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -708,18 +718,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -773,18 +781,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -842,18 +848,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -909,25 +913,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -984,19 +986,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/huber_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class HuberRegressor(BaseTransformer):
     r"""L2-regularized linear regression model that is robust to outliers
     For more details on this class, see [sklearn.linear_model.HuberRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html)
 
     Parameters
     ----------
@@ -289,28 +283,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -320,17 +311,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -370,15 +359,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -453,18 +443,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -523,24 +511,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -623,18 +635,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -691,18 +701,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -756,18 +764,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -825,18 +831,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -892,25 +896,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -967,19 +969,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lars.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Lars(BaseTransformer):
     r"""Least Angle Regression model a
     For more details on this class, see [sklearn.linear_model.Lars]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lars.html)
 
     Parameters
     ----------
@@ -318,28 +312,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -349,17 +340,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -399,15 +388,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -482,18 +472,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -552,24 +540,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -652,18 +664,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -720,18 +730,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -785,18 +793,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -854,18 +860,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -921,25 +925,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -996,19 +998,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lars_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LarsCV(BaseTransformer):
     r"""Cross-validated Least Angle Regression model
     For more details on this class, see [sklearn.linear_model.LarsCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LarsCV.html)
 
     Parameters
     ----------
@@ -326,28 +320,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -357,17 +348,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -407,15 +396,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -490,18 +480,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -560,24 +548,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -660,18 +672,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -728,18 +738,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +801,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -862,18 +868,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -929,25 +933,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1004,19 +1006,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lasso.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Lasso(BaseTransformer):
     r"""Linear Model trained with L1 prior as regularizer (aka the Lasso)
     For more details on this class, see [sklearn.linear_model.Lasso]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html)
 
     Parameters
     ----------
@@ -319,28 +313,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -350,17 +341,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -400,15 +389,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -483,18 +473,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -553,24 +541,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -653,18 +665,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -721,18 +731,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -786,18 +794,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -855,18 +861,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -922,25 +926,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -997,19 +999,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lasso_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LassoCV(BaseTransformer):
     r"""Lasso linear model with iterative fitting along a regularization path
     For more details on this class, see [sklearn.linear_model.LassoCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html)
 
     Parameters
     ----------
@@ -347,28 +341,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -378,17 +369,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -428,15 +417,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -511,18 +501,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -581,24 +569,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -681,18 +693,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -749,18 +759,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -814,18 +822,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -883,18 +889,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -950,25 +954,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1025,19 +1027,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lasso_lars.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LassoLars(BaseTransformer):
     r"""Lasso model fit with Least Angle Regression a
     For more details on this class, see [sklearn.linear_model.LassoLars]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLars.html)
 
     Parameters
     ----------
@@ -339,28 +333,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -370,17 +361,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -420,15 +409,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -503,18 +493,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -573,24 +561,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -673,18 +685,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -741,18 +751,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -806,18 +814,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -875,18 +881,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +946,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1019,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lasso_lars_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LassoLarsCV(BaseTransformer):
     r"""Cross-validated Lasso, using the LARS algorithm
     For more details on this class, see [sklearn.linear_model.LassoLarsCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsCV.html)
 
     Parameters
     ----------
@@ -340,28 +334,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -371,17 +362,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -421,15 +410,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -504,18 +494,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -574,24 +562,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -674,18 +686,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -742,18 +752,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -807,18 +815,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -876,18 +882,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -943,25 +947,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1018,19 +1020,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/lasso_lars_ic.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LassoLarsIC(BaseTransformer):
     r"""Lasso model fit with Lars using BIC or AIC for model selection
     For more details on this class, see [sklearn.linear_model.LassoLarsIC]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoLarsIC.html)
 
     Parameters
     ----------
@@ -323,28 +317,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -354,17 +345,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -404,15 +393,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -487,18 +477,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -557,24 +545,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -657,18 +669,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -725,18 +735,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -790,18 +798,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -859,18 +865,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -926,25 +930,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1001,19 +1003,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/linear_regression.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LinearRegression(BaseTransformer):
     r"""Ordinary least squares Linear Regression
     For more details on this class, see [sklearn.linear_model.LinearRegression]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)
 
     Parameters
     ----------
@@ -276,28 +270,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -307,17 +298,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -357,15 +346,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -440,18 +430,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -510,24 +498,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -610,18 +622,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -678,18 +688,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -743,18 +751,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -812,18 +818,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -879,25 +883,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -954,19 +956,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/logistic_regression.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LogisticRegression(BaseTransformer):
     r"""Logistic Regression (aka logit, MaxEnt) classifier
     For more details on this class, see [sklearn.linear_model.LogisticRegression]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)
 
     Parameters
     ----------
@@ -390,28 +384,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -421,17 +412,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -471,15 +460,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -554,18 +544,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -624,24 +612,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -726,18 +738,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -796,18 +806,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -863,18 +871,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -932,18 +938,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -999,25 +1003,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1074,19 +1076,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/logistic_regression_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LogisticRegressionCV(BaseTransformer):
     r"""Logistic Regression CV (aka logit, MaxEnt) classifier
     For more details on this class, see [sklearn.linear_model.LogisticRegressionCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegressionCV.html)
 
     Parameters
     ----------
@@ -411,28 +405,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -442,17 +433,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -492,15 +481,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -575,18 +565,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -645,24 +633,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -747,18 +759,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -817,18 +827,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -884,18 +892,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -953,18 +959,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1020,25 +1024,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1095,19 +1097,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MultiTaskElasticNet(BaseTransformer):
     r"""Multi-task ElasticNet model trained with L1/L2 mixed-norm as regularizer
     For more details on this class, see [sklearn.linear_model.MultiTaskElasticNet]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNet.html)
 
     Parameters
     ----------
@@ -309,28 +303,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -340,17 +331,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -390,15 +379,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -473,18 +463,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -543,24 +531,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -643,18 +655,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -711,18 +721,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -776,18 +784,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -845,18 +851,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -912,25 +916,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -987,19 +989,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MultiTaskElasticNetCV(BaseTransformer):
     r"""Multi-task L1/L2 ElasticNet with built-in cross-validation
     For more details on this class, see [sklearn.linear_model.MultiTaskElasticNetCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskElasticNetCV.html)
 
     Parameters
     ----------
@@ -350,28 +344,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -381,17 +372,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -431,15 +420,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -514,18 +504,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -584,24 +572,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -684,18 +696,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -752,18 +762,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -817,18 +825,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -886,18 +892,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -953,25 +957,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1028,19 +1030,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/multi_task_lasso.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MultiTaskLasso(BaseTransformer):
     r"""Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer
     For more details on this class, see [sklearn.linear_model.MultiTaskLasso]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLasso.html)
 
     Parameters
     ----------
@@ -301,28 +295,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -332,17 +323,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -382,15 +371,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -465,18 +455,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -535,24 +523,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -635,18 +647,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -703,18 +713,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -768,18 +776,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -837,18 +843,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -904,25 +908,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -979,19 +981,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MultiTaskLassoCV(BaseTransformer):
     r"""Multi-task Lasso model trained with L1/L2 mixed-norm as regularizer
     For more details on this class, see [sklearn.linear_model.MultiTaskLassoCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.MultiTaskLassoCV.html)
 
     Parameters
     ----------
@@ -336,28 +330,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -367,17 +358,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -417,15 +406,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -500,18 +490,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -570,24 +558,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -670,18 +682,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -738,18 +748,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -803,18 +811,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -872,18 +878,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -939,25 +943,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1014,19 +1016,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class OrthogonalMatchingPursuit(BaseTransformer):
     r"""Orthogonal Matching Pursuit model (OMP)
     For more details on this class, see [sklearn.linear_model.OrthogonalMatchingPursuit]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.OrthogonalMatchingPursuit.html)
 
     Parameters
     ----------
@@ -284,28 +278,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -315,17 +306,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -365,15 +354,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -448,18 +438,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -518,24 +506,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -618,18 +630,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -686,18 +696,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -751,18 +759,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -820,18 +826,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -887,25 +891,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -962,19 +964,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class PassiveAggressiveClassifier(BaseTransformer):
     r"""Passive Aggressive Classifier
     For more details on this class, see [sklearn.linear_model.PassiveAggressiveClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveClassifier.html)
 
     Parameters
     ----------
@@ -358,28 +352,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -389,17 +380,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -439,15 +428,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -522,18 +512,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -592,24 +580,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -692,18 +704,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -760,18 +770,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -827,18 +835,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -896,18 +902,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -963,25 +967,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1038,19 +1040,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class PassiveAggressiveRegressor(BaseTransformer):
     r"""Passive Aggressive Regressor
     For more details on this class, see [sklearn.linear_model.PassiveAggressiveRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PassiveAggressiveRegressor.html)
 
     Parameters
     ----------
@@ -344,28 +338,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -375,17 +366,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -425,15 +414,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -508,18 +498,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -578,24 +566,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -678,18 +690,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -746,18 +756,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -811,18 +819,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -880,18 +886,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -947,25 +951,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1022,19 +1024,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/perceptron.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Perceptron(BaseTransformer):
     r"""Linear perceptron classifier
     For more details on this class, see [sklearn.linear_model.Perceptron]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.html)
 
     Parameters
     ----------
@@ -357,28 +351,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -388,17 +379,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -438,15 +427,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -521,18 +511,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -591,24 +579,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -691,18 +703,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -759,18 +769,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -826,18 +834,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -895,18 +901,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -962,25 +966,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1037,19 +1039,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/poisson_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class PoissonRegressor(BaseTransformer):
     r"""Generalized Linear Model with a Poisson distribution
     For more details on this class, see [sklearn.linear_model.PoissonRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.PoissonRegressor.html)
 
     Parameters
     ----------
@@ -306,28 +300,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -337,17 +328,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -387,15 +376,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -470,18 +460,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -540,24 +528,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -640,18 +652,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -708,18 +718,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -773,18 +781,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -842,18 +848,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -909,25 +913,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -984,19 +986,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/ransac_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RANSACRegressor(BaseTransformer):
     r"""RANSAC (RANdom SAmple Consensus) algorithm
     For more details on this class, see [sklearn.linear_model.RANSACRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html)
 
     Parameters
     ----------
@@ -362,28 +356,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -393,17 +384,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -443,15 +432,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -526,18 +516,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -596,24 +584,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -696,18 +708,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -764,18 +774,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -829,18 +837,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -898,18 +904,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -965,25 +969,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1040,19 +1042,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/ridge.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Ridge(BaseTransformer):
     r"""Linear least squares with l2 regularization
     For more details on this class, see [sklearn.linear_model.Ridge]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html)
 
     Parameters
     ----------
@@ -354,28 +348,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -385,17 +376,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -435,15 +424,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -518,18 +508,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -588,24 +576,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -688,18 +700,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -756,18 +766,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -821,18 +829,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -890,18 +896,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -957,25 +961,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1032,19 +1034,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/ridge_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RidgeClassifier(BaseTransformer):
     r"""Classifier using Ridge regression
     For more details on this class, see [sklearn.linear_model.RidgeClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifier.html)
 
     Parameters
     ----------
@@ -354,28 +348,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -385,17 +376,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -435,15 +424,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -518,18 +508,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -588,24 +576,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -688,18 +700,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -756,18 +766,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -823,18 +831,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -892,18 +898,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -959,25 +963,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1034,19 +1036,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/ridge_classifier_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RidgeClassifierCV(BaseTransformer):
     r"""Ridge classifier with built-in cross-validation
     For more details on this class, see [sklearn.linear_model.RidgeClassifierCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeClassifierCV.html)
 
     Parameters
     ----------
@@ -305,28 +299,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -336,17 +327,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -386,15 +375,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -469,18 +459,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -539,24 +527,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -639,18 +651,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -707,18 +717,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -774,18 +782,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -843,18 +849,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -910,25 +914,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -985,19 +987,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/ridge_cv.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RidgeCV(BaseTransformer):
     r"""Ridge regression with built-in cross-validation
     For more details on this class, see [sklearn.linear_model.RidgeCV]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RidgeCV.html)
 
     Parameters
     ----------
@@ -326,28 +320,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -357,17 +348,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -407,15 +396,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -490,18 +480,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -560,24 +548,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -660,18 +672,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -728,18 +738,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +801,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -862,18 +868,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -929,25 +933,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1004,19 +1006,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/sgd_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SGDClassifier(BaseTransformer):
     r"""Linear classifiers (SVM, logistic regression, etc
     For more details on this class, see [sklearn.linear_model.SGDClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html)
 
     Parameters
     ----------
@@ -445,28 +439,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -476,17 +467,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -526,15 +515,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -609,18 +599,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -679,24 +667,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -781,18 +793,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -851,18 +861,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -918,18 +926,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -987,18 +993,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1054,25 +1058,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1129,19 +1131,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/sgd_one_class_svm.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SGDOneClassSVM(BaseTransformer):
     r"""Solves linear One-Class SVM using Stochastic Gradient Descent
     For more details on this class, see [sklearn.linear_model.SGDOneClassSVM]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDOneClassSVM.html)
 
     Parameters
     ----------
@@ -343,28 +337,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -374,17 +365,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -424,15 +413,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -507,18 +497,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -579,24 +567,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -679,18 +691,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -747,18 +757,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -814,18 +822,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -885,18 +891,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -950,25 +954,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1025,19 +1027,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/sgd_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SGDRegressor(BaseTransformer):
     r"""Linear model fitted by minimizing a regularized empirical loss with SGD
     For more details on this class, see [sklearn.linear_model.SGDRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html)
 
     Parameters
     ----------
@@ -411,28 +405,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -442,17 +433,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -492,15 +481,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -575,18 +565,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -645,24 +633,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -745,18 +757,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -813,18 +823,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -878,18 +886,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -947,18 +953,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1014,25 +1018,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1089,19 +1091,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/theil_sen_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class TheilSenRegressor(BaseTransformer):
     r"""Theil-Sen Estimator: robust multivariate regression model
     For more details on this class, see [sklearn.linear_model.TheilSenRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html)
 
     Parameters
     ----------
@@ -313,28 +307,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -344,17 +335,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -394,15 +383,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -477,18 +467,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -547,24 +535,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -647,18 +659,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -715,18 +725,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -780,18 +788,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -849,18 +855,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -916,25 +920,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -991,19 +993,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/linear_model/tweedie_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.linear_model".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class TweedieRegressor(BaseTransformer):
     r"""Generalized Linear Model with a Tweedie distribution
     For more details on this class, see [sklearn.linear_model.TweedieRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TweedieRegressor.html)
 
     Parameters
     ----------
@@ -339,28 +333,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -370,17 +361,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -420,15 +409,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -503,18 +493,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -573,24 +561,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -673,18 +685,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -741,18 +751,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -806,18 +814,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -875,18 +881,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +946,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1019,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/manifold/isomap.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return True and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class Isomap(BaseTransformer):
     r"""Isomap Embedding
     For more details on this class, see [sklearn.manifold.Isomap]
     (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html)
 
     Parameters
     ----------
@@ -335,28 +329,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -366,17 +357,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -414,15 +403,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -499,18 +489,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -569,24 +557,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the model from data in X and transform X
+        For more details on this function, see [sklearn.manifold.Isomap.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -669,18 +683,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -737,18 +749,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -802,18 +812,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -871,18 +879,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -936,25 +942,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1011,19 +1015,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/manifold/mds.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return True and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MDS(BaseTransformer):
     r"""Multidimensional scaling
     For more details on this class, see [sklearn.manifold.MDS]
     (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html)
 
     Parameters
     ----------
@@ -318,28 +312,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -349,17 +340,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -397,15 +386,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -480,18 +470,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -550,24 +538,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the data from `X`, and returns the embedded coordinates
+        For more details on this function, see [sklearn.manifold.MDS.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.MDS.html#sklearn.manifold.MDS.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -650,18 +664,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -718,18 +730,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -783,18 +793,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -852,18 +860,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -917,25 +923,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -992,19 +996,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/manifold/spectral_embedding.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return True and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SpectralEmbedding(BaseTransformer):
     r"""Spectral embedding for non-linear dimensionality reduction
     For more details on this class, see [sklearn.manifold.SpectralEmbedding]
     (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html)
 
     Parameters
     ----------
@@ -320,28 +314,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -351,17 +342,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -399,15 +388,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -482,18 +472,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -552,24 +540,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit the model from data in X and transform X
+        For more details on this function, see [sklearn.manifold.SpectralEmbedding.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html#sklearn.manifold.SpectralEmbedding.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -652,18 +666,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -720,18 +732,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -785,18 +795,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -854,18 +862,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -919,25 +925,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -994,19 +998,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/manifold/tsne.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.manifold".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return True and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class TSNE(BaseTransformer):
     r"""T-distributed Stochastic Neighbor Embedding
     For more details on this class, see [sklearn.manifold.TSNE]
     (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)
 
     Parameters
     ----------
@@ -379,28 +373,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -410,17 +401,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -458,15 +447,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -541,18 +531,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -611,24 +599,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit X into an embedded space and return that transformed output
+        For more details on this function, see [sklearn.manifold.TSNE.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -711,18 +725,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -779,18 +791,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -844,18 +854,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -913,18 +921,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -978,25 +984,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1053,19 +1057,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.mixture".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BayesianGaussianMixture(BaseTransformer):
     r"""Variational Bayesian estimation of a Gaussian mixture
     For more details on this class, see [sklearn.mixture.BayesianGaussianMixture]
     (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.BayesianGaussianMixture.html)
 
     Parameters
     ----------
@@ -382,28 +376,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -413,17 +404,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -463,15 +452,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -546,18 +536,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -618,24 +606,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -720,18 +732,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -790,18 +800,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -855,18 +863,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -926,18 +932,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -993,25 +997,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1068,19 +1070,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/mixture/gaussian_mixture.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.mixture".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GaussianMixture(BaseTransformer):
     r"""Gaussian Mixture
     For more details on this class, see [sklearn.mixture.GaussianMixture]
     (https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html)
 
     Parameters
     ----------
@@ -355,28 +349,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -386,17 +377,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -436,15 +425,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -519,18 +509,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -591,24 +579,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -693,18 +705,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -763,18 +773,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -828,18 +836,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -899,18 +905,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -966,25 +970,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1041,19 +1043,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/model_selection/grid_search_cv.py

```diff
@@ -330,28 +330,29 @@
             subproject=_SUBPROJECT,
         )
         self._sklearn_object = model_trainer.train()
         self._is_fitted = True
         self._generate_model_signatures(dataset)
         return self
 
-    def _batch_inference_validate_snowpark(self, dataset: DataFrame, inference_method: str) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    def _batch_inference_validate_snowpark(
+        self,
+        dataset: DataFrame,
+        inference_method: str,
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
 
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -359,18 +360,14 @@
 
         session = dataset._session
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError("Session must not specified for snowpark dataset."),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT
-        )
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
     def predict(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[DataFrame, pd.DataFrame]:
@@ -411,18 +408,16 @@
                     )
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
 
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
@@ -472,15 +467,16 @@
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
         inference_method = "transform"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -531,15 +527,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         inference_method = "predict_proba"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -591,15 +588,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         inference_method = "predict_log_proba"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 drop_input_cols=self._drop_input_cols,
                 dependencies=self._deps,
@@ -651,15 +649,16 @@
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
         inference_method = "decision_function"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -712,15 +711,16 @@
         inference_method = "score_samples"
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -763,25 +763,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: ScoreKwargsTypedDict = dict()
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session)  # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=["sklearn"],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
```

## snowflake/ml/modeling/model_selection/randomized_search_cv.py

```diff
@@ -343,34 +343,44 @@
             subproject=_SUBPROJECT,
         )
         self._sklearn_object = model_trainer.train()
         self._is_fitted = True
         self._generate_model_signatures(dataset)
         return self
 
-    def _batch_inference_validate_snowpark(self, dataset: DataFrame, inference_method: str) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe."""
+    def _batch_inference_validate_snowpark(
+        self,
+        dataset: DataFrame,
+        inference_method: str,
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
+
+        Args:
+            dataset: snowpark dataframe
+            inference_method: the inference method such as predict, score...
+
+        Raises:
+            SnowflakeMLException: If the estimator is not fitted, raise error
+            SnowflakeMLException: If the session is None, raise error
+
+        """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
             )
 
         session = dataset._session
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError("Session must not specified for snowpark dataset."),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT
-        )
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
     def predict(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[DataFrame, pd.DataFrame]:
@@ -410,18 +420,17 @@
                     )
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
+
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -469,15 +478,17 @@
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
         inference_method = "transform"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
+
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -527,15 +538,17 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         inference_method = "predict_proba"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
+
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -587,15 +600,17 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         inference_method = "predict_log_proba"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
+
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -646,15 +661,17 @@
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
         inference_method = "decision_function"
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
+
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -707,15 +724,17 @@
         inference_method = "score_samples"
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
+
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -757,26 +776,25 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used.
         transform_kwargs: ScoreKwargsTypedDict = dict()
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
+
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
 
             assert isinstance(dataset._session, Session)  # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=["sklearn"],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
```

## snowflake/ml/modeling/multiclass/one_vs_one_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.multiclass".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class OneVsOneClassifier(BaseTransformer):
     r"""One-vs-one multiclass strategy
     For more details on this class, see [sklearn.multiclass.OneVsOneClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html)
 
     Parameters
     ----------
@@ -267,28 +261,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -298,17 +289,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -348,15 +337,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -431,18 +421,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -501,24 +489,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -601,18 +613,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -669,18 +679,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -736,18 +744,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -805,18 +811,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -872,25 +876,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -947,19 +949,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.multiclass".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class OneVsRestClassifier(BaseTransformer):
     r"""One-vs-the-rest (OvR) multiclass strategy
     For more details on this class, see [sklearn.multiclass.OneVsRestClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html)
 
     Parameters
     ----------
@@ -276,28 +270,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -307,17 +298,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -357,15 +346,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -440,18 +430,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -510,24 +498,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -612,18 +624,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -682,18 +692,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -749,18 +757,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -818,18 +824,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -885,25 +889,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -960,19 +962,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/multiclass/output_code_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.multiclass".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class OutputCodeClassifier(BaseTransformer):
     r"""(Error-Correcting) Output-Code multiclass strategy
     For more details on this class, see [sklearn.multiclass.OutputCodeClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OutputCodeClassifier.html)
 
     Parameters
     ----------
@@ -279,28 +273,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -310,17 +301,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -360,15 +349,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -443,18 +433,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -513,24 +501,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -613,18 +625,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -681,18 +691,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -746,18 +754,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -815,18 +821,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -882,25 +886,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -957,19 +959,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/naive_bayes/bernoulli_nb.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BernoulliNB(BaseTransformer):
     r"""Naive Bayes classifier for multivariate Bernoulli models
     For more details on this class, see [sklearn.naive_bayes.BernoulliNB]
     (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)
 
     Parameters
     ----------
@@ -279,28 +273,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -310,17 +301,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -360,15 +349,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -443,18 +433,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -513,24 +501,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -615,18 +627,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -685,18 +695,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -750,18 +758,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -819,18 +825,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -886,25 +890,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -961,19 +963,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/naive_bayes/categorical_nb.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class CategoricalNB(BaseTransformer):
     r"""Naive Bayes classifier for categorical features
     For more details on this class, see [sklearn.naive_bayes.CategoricalNB]
     (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html)
 
     Parameters
     ----------
@@ -285,28 +279,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -316,17 +307,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -366,15 +355,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -449,18 +439,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -519,24 +507,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -621,18 +633,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -691,18 +701,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -756,18 +764,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -825,18 +831,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -892,25 +896,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -967,19 +969,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/naive_bayes/complement_nb.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ComplementNB(BaseTransformer):
     r"""The Complement Naive Bayes classifier described in Rennie et al
     For more details on this class, see [sklearn.naive_bayes.ComplementNB]
     (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.ComplementNB.html)
 
     Parameters
     ----------
@@ -279,28 +273,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -310,17 +301,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -360,15 +349,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -443,18 +433,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -513,24 +501,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -615,18 +627,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -685,18 +695,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -750,18 +758,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -819,18 +825,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -886,25 +890,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -961,19 +963,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/naive_bayes/gaussian_nb.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class GaussianNB(BaseTransformer):
     r"""Gaussian Naive Bayes (GaussianNB)
     For more details on this class, see [sklearn.naive_bayes.GaussianNB]
     (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)
 
     Parameters
     ----------
@@ -260,28 +254,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -291,17 +282,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -341,15 +330,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -424,18 +414,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -494,24 +482,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -596,18 +608,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -666,18 +676,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -731,18 +739,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -800,18 +806,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -867,25 +871,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -942,19 +944,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/naive_bayes/multinomial_nb.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.naive_bayes".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MultinomialNB(BaseTransformer):
     r"""Naive Bayes classifier for multinomial models
     For more details on this class, see [sklearn.naive_bayes.MultinomialNB]
     (https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)
 
     Parameters
     ----------
@@ -273,28 +267,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -304,17 +295,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -354,15 +343,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -437,18 +427,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -507,24 +495,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -609,18 +621,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -679,18 +689,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -744,18 +752,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -813,18 +819,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -880,25 +884,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -955,19 +957,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/k_neighbors_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KNeighborsClassifier(BaseTransformer):
     r"""Classifier implementing the k-nearest neighbors vote
     For more details on this class, see [sklearn.neighbors.KNeighborsClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)
 
     Parameters
     ----------
@@ -330,28 +324,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -361,17 +352,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -411,15 +400,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -494,18 +484,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -564,24 +552,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -666,18 +678,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -736,18 +746,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -801,18 +809,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -870,18 +876,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -937,25 +941,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1014,19 +1016,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/k_neighbors_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KNeighborsRegressor(BaseTransformer):
     r"""Regression based on k-nearest neighbors
     For more details on this class, see [sklearn.neighbors.KNeighborsRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html)
 
     Parameters
     ----------
@@ -332,28 +326,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -363,17 +354,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -413,15 +402,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -496,18 +486,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -566,24 +554,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -666,18 +678,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -734,18 +744,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -799,18 +807,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -868,18 +874,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -935,25 +939,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1012,19 +1014,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/kernel_density.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class KernelDensity(BaseTransformer):
     r"""Kernel Density Estimation
     For more details on this class, see [sklearn.neighbors.KernelDensity]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KernelDensity.html)
 
     Parameters
     ----------
@@ -309,28 +303,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -340,17 +331,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -388,15 +377,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -471,18 +461,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -541,24 +529,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -641,18 +653,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -709,18 +719,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -774,18 +782,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -845,18 +851,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -912,25 +916,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -987,19 +989,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/local_outlier_factor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LocalOutlierFactor(BaseTransformer):
     r"""Unsupervised Outlier Detection using the Local Outlier Factor (LOF)
     For more details on this class, see [sklearn.neighbors.LocalOutlierFactor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html)
 
     Parameters
     ----------
@@ -337,28 +331,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -368,17 +359,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -418,15 +407,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -501,18 +491,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -573,24 +561,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -673,18 +685,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -741,18 +751,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -808,18 +816,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -879,18 +885,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -944,25 +948,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1021,19 +1023,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/nearest_centroid.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class NearestCentroid(BaseTransformer):
     r"""Nearest centroid classifier
     For more details on this class, see [sklearn.neighbors.NearestCentroid]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestCentroid.html)
 
     Parameters
     ----------
@@ -270,28 +264,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -301,17 +292,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -351,15 +340,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -434,18 +424,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -504,24 +492,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -604,18 +616,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -672,18 +682,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -737,18 +745,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -806,18 +812,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -873,25 +877,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -948,19 +950,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/nearest_neighbors.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class NearestNeighbors(BaseTransformer):
     r"""Unsupervised learner for implementing neighbor searches
     For more details on this class, see [sklearn.neighbors.NearestNeighbors]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NearestNeighbors.html)
 
     Parameters
     ----------
@@ -320,28 +314,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -351,17 +342,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -399,15 +388,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -482,18 +472,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -552,24 +540,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -652,18 +664,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -720,18 +730,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -785,18 +793,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -854,18 +860,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -919,25 +923,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -996,19 +998,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class NeighborhoodComponentsAnalysis(BaseTransformer):
     r"""Neighborhood Components Analysis
     For more details on this class, see [sklearn.neighbors.NeighborhoodComponentsAnalysis]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html)
 
     Parameters
     ----------
@@ -341,28 +335,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -372,17 +363,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -420,15 +409,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -505,18 +495,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -575,24 +563,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.neighbors.NeighborhoodComponentsAnalysis.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -675,18 +689,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -743,18 +755,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -808,18 +818,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -877,18 +885,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -942,25 +948,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1017,19 +1021,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RadiusNeighborsClassifier(BaseTransformer):
     r"""Classifier implementing a vote among neighbors within a given radius
     For more details on this class, see [sklearn.neighbors.RadiusNeighborsClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsClassifier.html)
 
     Parameters
     ----------
@@ -342,28 +336,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -373,17 +364,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -423,15 +412,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -506,18 +496,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -576,24 +564,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -678,18 +690,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -748,18 +758,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -813,18 +821,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -882,18 +888,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -949,25 +953,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1024,19 +1026,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neighbors".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class RadiusNeighborsRegressor(BaseTransformer):
     r"""Regression based on neighbors within a fixed radius
     For more details on this class, see [sklearn.neighbors.RadiusNeighborsRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.RadiusNeighborsRegressor.html)
 
     Parameters
     ----------
@@ -332,28 +326,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -363,17 +354,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -413,15 +402,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -496,18 +486,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -566,24 +554,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -666,18 +678,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -734,18 +744,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -799,18 +807,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -868,18 +874,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -935,25 +939,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1010,19 +1012,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neural_network/bernoulli_rbm.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neural_network".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class BernoulliRBM(BaseTransformer):
     r"""Bernoulli Restricted Boltzmann Machine (RBM)
     For more details on this class, see [sklearn.neural_network.BernoulliRBM]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html)
 
     Parameters
     ----------
@@ -289,28 +283,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -320,17 +311,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -368,15 +357,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -453,18 +443,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -523,24 +511,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.neural_network.BernoulliRBM.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.BernoulliRBM.html#sklearn.neural_network.BernoulliRBM.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -623,18 +637,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -691,18 +703,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -756,18 +766,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -827,18 +835,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -892,25 +898,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -967,19 +971,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neural_network/mlp_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neural_network".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MLPClassifier(BaseTransformer):
     r"""Multi-layer Perceptron classifier
     For more details on this class, see [sklearn.neural_network.MLPClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html)
 
     Parameters
     ----------
@@ -444,28 +438,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -475,17 +466,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -525,15 +514,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -608,18 +598,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -678,24 +666,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -780,18 +792,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -850,18 +860,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -915,18 +923,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -984,18 +990,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1051,25 +1055,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1126,19 +1128,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/neural_network/mlp_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.neural_network".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class MLPRegressor(BaseTransformer):
     r"""Multi-layer Perceptron regressor
     For more details on this class, see [sklearn.neural_network.MLPRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html)
 
     Parameters
     ----------
@@ -440,28 +434,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -471,17 +462,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -521,15 +510,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -604,18 +594,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -674,24 +662,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -774,18 +786,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -842,18 +852,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -907,18 +915,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -976,18 +982,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1043,25 +1047,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1118,19 +1120,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/pipeline/pipeline.py

```diff
@@ -1,27 +1,38 @@
 #!/usr/bin/env python3
+import inspect
+import os
+import posixpath
+import tempfile
 from itertools import chain
 from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union
 
+import cloudpickle as cp
 import numpy as np
 import pandas as pd
 from sklearn import __version__ as skversion, pipeline
 from sklearn.compose import ColumnTransformer
 from sklearn.preprocessing import FunctionTransformer
 from sklearn.utils import metaestimators
 
 from snowflake import snowpark
-from snowflake.ml._internal import telemetry
+from snowflake.ml._internal import file_utils, telemetry
 from snowflake.ml._internal.exceptions import error_codes, exceptions
-from snowflake.ml._internal.utils import snowpark_dataframe_utils
+from snowflake.ml._internal.utils import snowpark_dataframe_utils, temp_file_utils
 from snowflake.ml.model.model_signature import ModelSignature, _infer_signature
+from snowflake.ml.modeling._internal.model_transformer_builder import (
+    ModelTransformerBuilder,
+)
 from snowflake.ml.modeling.framework import _utils, base
+from snowflake.snowpark import Session, functions as F
+from snowflake.snowpark._internal import utils as snowpark_utils
 
 _PROJECT = "ModelDevelopment"
 _SUBPROJECT = "Framework"
+IN_ML_RUNTIME_ENV_VAR = "IN_SPCS_ML_RUNTIME"
 
 
 def _final_step_has(attr: str) -> Callable[..., bool]:
     """Check that final_estimator has `attr`. Used together with `available_if` in `Pipeline`."""
 
     def check(self: "Pipeline") -> bool:
         # Raises original `AttributeError` if `attr` does not exist.
@@ -109,14 +120,16 @@
         self._model_signature_dict: Optional[Dict[str, ModelSignature]] = None
 
         deps: Set[str] = {f"pandas=={pd.__version__}", f"scikit-learn=={skversion}"}
         for _, obj in steps:
             if isinstance(obj, base.BaseTransformer):
                 deps = deps | set(obj._get_dependencies())
         self._deps = list(deps)
+        self._sklearn_object = None
+        self.label_cols = self._get_label_cols()
 
     @staticmethod
     def _is_estimator(obj: object) -> bool:
         # TODO(SNOW-723770): Figure out a better way to identify estimator objects.
         return has_callable_attr(obj, "fit") and has_callable_attr(obj, "predict")
 
     @staticmethod
@@ -143,14 +156,41 @@
 
     def _reset(self) -> None:
         super()._reset()
         self._feature_names_in = []
         self._n_features_in = []
         self._transformers_to_input_indices = {}
 
+    def _is_convertible_to_sklearn_object(self) -> bool:
+        """Checks if the pipeline can be converted to a native sklearn pipeline.
+        - We can not create an sklearn pipeline if its label or sample weight column are
+          modified in the pipeline.
+        - We can not create an sklearn pipeline if any of its steps cannot be converted to an sklearn pipeline
+        - We can not create an sklearn pipeline if input columns are specified in any step other than
+          the first step
+
+        Returns:
+            True if the pipeline can be converted to a native sklearn pipeline, else false.
+        """
+        if self._is_pipeline_modifying_label_or_sample_weight():
+            return False
+
+        # check that nested pipelines can be converted to sklearn
+        for _, base_estimator in self.steps:
+            if hasattr(base_estimator, "_is_convertible_to_sklearn_object"):
+                if not base_estimator._is_convertible_to_sklearn_object():
+                    return False
+
+        # check that no column after the first column has 'input columns' set.
+        for _, base_estimator in self.steps[1:]:
+            if base_estimator.get_input_cols():
+                # We only want Falsy values - None and []
+                return False
+        return True
+
     def _is_pipeline_modifying_label_or_sample_weight(self) -> bool:
         """
         Checks if pipeline is modifying label or sample_weight columns.
 
         Returns:
             True if pipeline is processing label or sample_weight columns, False otherwise.
         """
@@ -210,56 +250,210 @@
         self._reset()
         self._is_convertible_to_sklearn = not self._is_pipeline_modifying_label_or_sample_weight()
         transformed_dataset = dataset
         for name, trans in self._get_transformers():
             self._append_step_feature_consumption_info(
                 step_name=name, all_cols=transformed_dataset.columns[:], input_cols=trans.get_input_cols()
             )
-            if has_callable_attr(trans, "fit_transform"):
-                transformed_dataset = trans.fit_transform(transformed_dataset)
-            else:
-                trans.fit(transformed_dataset)
-                transformed_dataset = trans.transform(transformed_dataset)
+            trans.fit(transformed_dataset)
+            transformed_dataset = trans.transform(transformed_dataset)
 
         return transformed_dataset
 
+    def _upload_model_to_stage(self, stage_name: str, estimator: object, session: Session) -> Tuple[str, str]:
+        """
+        Util method to pickle and upload the model to a temp Snowflake stage.
+
+        Args:
+            stage_name: Stage name to save model.
+            estimator: the pipeline estimator itself
+            session: Session object
+
+        Returns:
+            a tuple containing stage file paths for pickled input model for training and location to store trained
+            models(response from training sproc).
+        """
+        # Create a temp file and dump the transform to that file.
+        local_transform_file_name = temp_file_utils.get_temp_file_path()
+        with open(local_transform_file_name, mode="w+b") as local_transform_file:
+            cp.dump(estimator, local_transform_file)
+
+        # Use posixpath to construct stage paths
+        stage_transform_file_name = posixpath.join(stage_name, os.path.basename(local_transform_file_name))
+        stage_result_file_name = posixpath.join(stage_name, os.path.basename(local_transform_file_name))
+
+        # Put locally serialized transform on stage.
+        session.file.put(
+            local_transform_file_name,
+            stage_transform_file_name,
+            auto_compress=False,
+            overwrite=True,
+        )
+
+        temp_file_utils.cleanup_temp_files([local_transform_file_name])
+        return (stage_transform_file_name, stage_result_file_name)
+
+    def _fit_snowpark_dataframe_within_one_sproc(self, session: Session, dataset: snowpark.DataFrame) -> None:
+        # Extract queries that generated the dataframe. We will need to pass it to score procedure.
+        sql_queries = dataset.queries["queries"]
+
+        # Zip the current snowml package
+        with tempfile.TemporaryDirectory() as tmpdir:
+            snowml_zip_module_filename = os.path.join(tmpdir, "snowflake-ml-python.zip")
+            file_utils.zip_python_package(snowml_zip_module_filename, "snowflake.ml")
+            imports = [snowml_zip_module_filename]
+
+            sproc_name = snowpark_utils.random_name_for_temp_object(snowpark_utils.TempObjectType.PROCEDURE)
+            required_deps = self._deps
+            sproc_statement_params = telemetry.get_function_usage_statement_params(
+                project=_PROJECT,
+                subproject="PIPELINE",
+                function_name=telemetry.get_statement_params_full_func_name(
+                    inspect.currentframe(), self.__class__.__name__
+                ),
+                api_calls=[F.sproc],
+            )
+            transform_stage_name = snowpark_utils.random_name_for_temp_object(snowpark_utils.TempObjectType.STAGE)
+            stage_creation_query = f"CREATE OR REPLACE TEMPORARY STAGE {transform_stage_name};"
+            session.sql(stage_creation_query).collect()
+            (stage_estimator_file_name, stage_result_file_name) = self._upload_model_to_stage(
+                transform_stage_name, self, session
+            )
+
+            def pipeline_within_one_sproc(
+                session: Session,
+                sql_queries: List[str],
+                stage_estimator_file_name: str,
+                stage_result_file_name: str,
+                sproc_statement_params: Dict[str, str],
+            ) -> str:
+                import os
+
+                import cloudpickle as cp
+                import pandas as pd
+
+                for query in sql_queries[:-1]:
+                    _ = session.sql(query).collect(statement_params=sproc_statement_params)
+                sp_df = session.sql(sql_queries[-1])
+                df: pd.DataFrame = sp_df.to_pandas(statement_params=sproc_statement_params)
+                df.columns = sp_df.columns
+
+                local_estimator_file_name = temp_file_utils.get_temp_file_path()
+
+                session.file.get(stage_estimator_file_name, local_estimator_file_name)
+
+                local_estimator_file_path = os.path.join(
+                    local_estimator_file_name, os.listdir(local_estimator_file_name)[0]
+                )
+                with open(local_estimator_file_path, mode="r+b") as local_estimator_file_obj:
+                    estimator = cp.load(local_estimator_file_obj)
+
+                estimator.fit(df)
+
+                local_result_file_name = temp_file_utils.get_temp_file_path()
+
+                with open(local_result_file_name, mode="w+b") as local_result_file_obj:
+                    cp.dump(estimator, local_result_file_obj)
+
+                session.file.put(
+                    local_result_file_name,
+                    stage_result_file_name,
+                    auto_compress=False,
+                    overwrite=True,
+                    statement_params=sproc_statement_params,
+                )
+
+                return str(os.path.basename(local_result_file_name))
+
+            session.sproc.register(
+                func=pipeline_within_one_sproc,
+                is_permanent=False,
+                name=sproc_name,
+                packages=required_deps,  # type: ignore[arg-type]
+                replace=True,
+                session=session,
+                anonymous=True,
+                imports=imports,  # type: ignore[arg-type]
+                statement_params=sproc_statement_params,
+            )
+
+            sproc_export_file_name: str = pipeline_within_one_sproc(
+                session,
+                sql_queries,
+                stage_estimator_file_name,
+                stage_result_file_name,
+                sproc_statement_params,
+            )
+
+            local_result_file_name = temp_file_utils.get_temp_file_path()
+            session.file.get(
+                posixpath.join(stage_estimator_file_name, sproc_export_file_name),
+                local_result_file_name,
+                statement_params=sproc_statement_params,
+            )
+
+            with open(os.path.join(local_result_file_name, sproc_export_file_name), mode="r+b") as result_file_obj:
+                fit_estimator = cp.load(result_file_obj)
+
+            temp_file_utils.cleanup_temp_files([local_result_file_name])
+            for key, val in vars(fit_estimator).items():
+                setattr(self, key, val)
+
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
-    def fit(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> "Pipeline":
+    def fit(self, dataset: Union[snowpark.DataFrame, pd.DataFrame], squash: Optional[bool] = False) -> "Pipeline":
         """
         Fit the entire pipeline using the dataset.
 
         Args:
             dataset: Input dataset.
+            squash: Run the whole pipeline within a stored procedure
 
         Returns:
             Fitted pipeline.
+
+        Raises:
+            ValueError: A pipeline incompatible with sklearn is used on MLRS
         """
 
         self._validate_steps()
         dataset = (
             snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)
             if isinstance(dataset, snowpark.DataFrame)
             else dataset
         )
-        transformed_dataset = self._fit_transform_dataset(dataset)
 
-        estimator = self._get_estimator()
-        if estimator:
-            all_cols = transformed_dataset.columns[:]
-            estimator[1].fit(transformed_dataset)
+        if self._can_be_trained_in_ml_runtime(dataset):
+            if not self._is_convertible_to_sklearn_object():
+                raise ValueError("This pipeline cannot be converted to an sklearn pipeline.")
+            self._fit_ml_runtime(dataset)
+
+        elif squash and isinstance(dataset, snowpark.DataFrame):
+            session = dataset._session
+            assert session is not None
+            self._fit_snowpark_dataframe_within_one_sproc(session=session, dataset=dataset)
+
+        else:
+            transformed_dataset = self._fit_transform_dataset(dataset)
+
+            estimator = self._get_estimator()
+            if estimator:
+                all_cols = transformed_dataset.columns[:]
+                estimator[1].fit(transformed_dataset)
 
-            self._append_step_feature_consumption_info(
-                step_name=estimator[0], all_cols=all_cols, input_cols=estimator[1].get_input_cols()
-            )
+                self._append_step_feature_consumption_info(
+                    step_name=estimator[0], all_cols=all_cols, input_cols=estimator[1].get_input_cols()
+                )
+
+            self._generate_model_signatures(dataset=dataset)
 
-        self._generate_model_signatures(dataset=dataset)
         self._is_fitted = True
+
         return self
 
     @metaestimators.available_if(_final_step_has("transform"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
@@ -276,14 +470,30 @@
         self._enforce_fit()
         dataset = (
             snowpark_dataframe_utils.cast_snowpark_dataframe_column_types(dataset)
             if isinstance(dataset, snowpark.DataFrame)
             else dataset
         )
 
+        if self._sklearn_object is not None:
+            handler = ModelTransformerBuilder.build(
+                dataset=dataset,
+                estimator=self._sklearn_object,
+                class_name="Pipeline",
+                subproject="",
+                autogenerated=False,
+            )
+            return handler.batch_inference(
+                inference_method="transform",
+                input_cols=self.input_cols if self.input_cols else self._infer_input_cols(dataset),
+                expected_output_cols=self._infer_output_cols(),
+                session=dataset._session,
+                dependencies=self._deps,
+            )
+
         transformed_dataset = self._transform_dataset(dataset=dataset)
         estimator = self._get_estimator()
         if estimator:
             return estimator[1].transform(transformed_dataset)
         return transformed_dataset
 
     def _final_step_can_fit_transform(self) -> bool:
@@ -385,16 +595,40 @@
         Transform the dataset by applying all the transformers in order and predict using the estimator.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
+
+        Raises:
+            ValueError: An sklearn object has not been fit and stored before calling this function.
         """
-        return self._invoke_estimator_func("predict", dataset)
+        if os.environ.get(IN_ML_RUNTIME_ENV_VAR):
+            if self._sklearn_object is None:
+                raise ValueError("Model must be fit before inference.")
+
+            expected_output_cols = self._infer_output_cols()
+            handler = ModelTransformerBuilder.build(
+                dataset=dataset,
+                estimator=self._sklearn_object,
+                class_name="Pipeline",
+                subproject="",
+                autogenerated=False,
+            )
+            return handler.batch_inference(
+                inference_method="predict",
+                input_cols=self.input_cols if self.input_cols else self._infer_input_cols(dataset),
+                expected_output_cols=expected_output_cols,
+                session=dataset._session,
+                dependencies=self._deps,
+            )
+
+        else:
+            return self._invoke_estimator_func("predict", dataset)
 
     @metaestimators.available_if(_final_step_has("score_samples"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
     def score_samples(
@@ -404,16 +638,40 @@
         Transform the dataset by applying all the transformers in order and predict using the estimator.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
+
+        Raises:
+            ValueError: An sklearn object has not been fit before calling this function
         """
-        return self._invoke_estimator_func("score_samples", dataset)
+
+        if os.environ.get(IN_ML_RUNTIME_ENV_VAR):
+            if self._sklearn_object is None:
+                raise ValueError("Model must be fit before inference.")
+
+            expected_output_cols = self._get_output_column_names("score_samples")
+            handler = ModelTransformerBuilder.build(
+                dataset=dataset,
+                estimator=self._sklearn_object,
+                class_name="Pipeline",
+                subproject="",
+                autogenerated=False,
+            )
+            return handler.batch_inference(
+                inference_method="score_samples",
+                input_cols=self.input_cols if self.input_cols else self._infer_input_cols(dataset),
+                expected_output_cols=expected_output_cols,
+                session=dataset._session,
+                dependencies=self._deps,
+            )
+        else:
+            return self._invoke_estimator_func("score_samples", dataset)
 
     @metaestimators.available_if(_final_step_has("predict_proba"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
     def predict_proba(
@@ -423,16 +681,40 @@
         Transform the dataset by applying all the transformers in order and apply `predict_proba` using the estimator.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
+
+        Raises:
+            ValueError: An sklearn object has not been fit before calling this function
         """
-        return self._invoke_estimator_func("predict_proba", dataset)
+
+        if os.environ.get(IN_ML_RUNTIME_ENV_VAR):
+            if self._sklearn_object is None:
+                raise ValueError("Model must be fit before inference.")
+            expected_output_cols = self._get_output_column_names("predict_proba")
+
+            handler = ModelTransformerBuilder.build(
+                dataset=dataset,
+                estimator=self._sklearn_object,
+                class_name="Pipeline",
+                subproject="",
+                autogenerated=False,
+            )
+            return handler.batch_inference(
+                inference_method="predict_proba",
+                input_cols=self.input_cols if self.input_cols else self._infer_input_cols(dataset),
+                expected_output_cols=expected_output_cols,
+                session=dataset._session,
+                dependencies=self._deps,
+            )
+        else:
+            return self._invoke_estimator_func("predict_proba", dataset)
 
     @metaestimators.available_if(_final_step_has("predict_log_proba"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
     def predict_log_proba(
@@ -443,16 +725,39 @@
         estimator.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
+
+        Raises:
+            ValueError: An sklearn object has not been fit before calling this function
         """
-        return self._invoke_estimator_func("predict_log_proba", dataset)
+        if os.environ.get(IN_ML_RUNTIME_ENV_VAR):
+            if self._sklearn_object is None:
+                raise ValueError("Model must be fit before inference.")
+
+            expected_output_cols = self._get_output_column_names("predict_log_proba")
+            handler = ModelTransformerBuilder.build(
+                dataset=dataset,
+                estimator=self._sklearn_object,
+                class_name="Pipeline",
+                subproject="",
+                autogenerated=False,
+            )
+            return handler.batch_inference(
+                inference_method="predict_log_proba",
+                input_cols=self.input_cols if self.input_cols else self._infer_input_cols(dataset),
+                expected_output_cols=expected_output_cols,
+                session=dataset._session,
+                dependencies=self._deps,
+            )
+        else:
+            return self._invoke_estimator_func("predict_log_proba", dataset)
 
     @metaestimators.available_if(_final_step_has("score"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
     )
     def score(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> Union[snowpark.DataFrame, pd.DataFrame]:
@@ -460,16 +765,38 @@
         Transform the dataset by applying all the transformers in order and apply `score` using the estimator.
 
         Args:
             dataset: Input dataset.
 
         Returns:
             Output dataset.
+
+        Raises:
+            ValueError: An sklearn object has not been fit before calling this function
         """
-        return self._invoke_estimator_func("score", dataset)
+
+        if os.environ.get(IN_ML_RUNTIME_ENV_VAR):
+            if self._sklearn_object is None:
+                raise ValueError("Model must be fit before scoreing.")
+            handler = ModelTransformerBuilder.build(
+                dataset=dataset,
+                estimator=self._sklearn_object,
+                class_name="Pipeline",
+                subproject="",
+                autogenerated=False,
+            )
+            return handler.score(
+                input_cols=self._infer_input_cols(),
+                label_cols=self._get_label_cols(),
+                session=dataset._session,
+                dependencies=self._deps,
+                score_sproc_imports=[],
+            )
+        else:
+            return self._invoke_estimator_func("score", dataset)
 
     def _invoke_estimator_func(
         self, func_name: str, dataset: Union[snowpark.DataFrame, pd.DataFrame]
     ) -> Union[snowpark.DataFrame, pd.DataFrame]:
         """
         Transform the dataset by applying all the transformers in order and apply specified estimator function.
 
@@ -491,23 +818,14 @@
 
         transformed_dataset = self._transform_dataset(dataset=dataset)
         estimator = self._get_estimator()
         assert estimator is not None
         res: snowpark.DataFrame = getattr(estimator[1], func_name)(transformed_dataset)
         return res
 
-    def _create_unfitted_sklearn_object(self) -> pipeline.Pipeline:
-        sksteps = []
-        for step in self.steps:
-            if isinstance(step[1], base.BaseTransformer):
-                sksteps.append(tuple([step[0], _utils.to_native_format(step[1])]))
-            else:
-                sksteps.append(tuple([step[0], step[1]]))
-        return pipeline.Pipeline(steps=sksteps)
-
     def _construct_fitted_column_transformer_object(
         self,
         step_name_in_pipeline: str,
         step_index_in_pipeline: int,
         step_name_in_ct: str,
         step_transformer_obj: Any,
         remainder_action: str,
@@ -558,23 +876,142 @@
             ft.n_features_in_ = len(remaining)
             ct._name_to_fitted_passthrough = {"remainder": ft}
         elif step_transformer_obj == "passthrough":
             ft.n_features_in_ = self._n_features_in[step_index_in_pipeline]
             ct._name_to_fitted_passthrough = {step_name_in_ct: ft}
         return ct
 
+    def _fit_ml_runtime(self, dataset: snowpark.DataFrame) -> None:
+        """Train the pipeline in the ML Runtime.
+
+        Args:
+            dataset: The training Snowpark dataframe
+
+        Raises:
+            ModuleNotFoundError: The ML Runtime Client is not installed.
+        """
+        try:
+            from snowflake.ml.runtime import MLRuntimeClient
+        except ModuleNotFoundError as e:
+            # The snowflake.ml.runtime module should always be present when
+            # the env var IN_SPCS_ML_RUNTIME is present.
+            raise ModuleNotFoundError("ML Runtime Python Client is not installed.") from e
+
+        client = MLRuntimeClient()
+        ml_runtime_compatible_pipeline = self._create_unfitted_sklearn_object()
+
+        label_cols = self._get_label_cols()
+        all_df_cols = dataset.columns
+        input_cols = [col for col in all_df_cols if col not in label_cols]
+
+        trained_pipeline = client.train(
+            estimator=ml_runtime_compatible_pipeline,
+            dataset=dataset,
+            input_cols=input_cols,
+            label_cols=label_cols,
+            sample_weight_col=self.sample_weight_col,
+        )
+
+        self._sklearn_object = trained_pipeline
+
+    def _get_label_cols(self) -> List[str]:
+        """Util function to get the label columns from the pipeline.
+        The label column is only present in the estimator
+
+        Returns:
+            List of label columns, or empty list if no label cols.
+        """
+        label_cols = []
+        estimator = self._get_estimator()
+        if estimator is not None:
+            label_cols = estimator[1].get_label_cols()
+
+        return label_cols
+
+    def _can_be_trained_in_ml_runtime(self, dataset: Union[snowpark.DataFrame, pd.DataFrame]) -> bool:
+        """A utility function to determine if the pipeline cam be pushed down to the ML Runtime for training.
+        Currently, this is true if:
+        - The training dataset is a snowpark dataframe,
+        - The IN_SPCS_ML_RUNTIME environment is present and
+        - The pipeline can be converted to an sklearn pipeline.
+
+        Args:
+            dataset: The training dataset
+
+        Returns:
+            True if the dataset can be fit in the ml runtime, else false.
+
+        """
+        if not isinstance(dataset, snowpark.DataFrame):
+            return False
+
+        if not os.environ.get(IN_ML_RUNTIME_ENV_VAR):
+            return False
+
+        return self._is_convertible_to_sklearn_object()
+
+    @staticmethod
+    def _wrap_transformer_in_column_transformer(
+        transformer_name: str, transformer: base.BaseTransformer
+    ) -> ColumnTransformer:
+        """A helper function to convert a transformer object to an sklearn object and wrap in an sklearn
+            ColumnTransformer.
+
+        Args:
+            transformer_name: Name of the transformer to be wrapped.
+            transformer: The transformer object to be wrapped.
+
+        Returns:
+            A column transformer sklearn object that uses the input columns from the initial snowpark ml transformer.
+        """
+        column_transformer = ColumnTransformer(
+            transformers=[(transformer_name, Pipeline._get_native_object(transformer), transformer.get_input_cols())],
+            remainder="passthrough",
+        )
+        return column_transformer
+
+    def _create_unfitted_sklearn_object(self) -> pipeline.Pipeline:
+        """Create a sklearn pipeline from the current snowml pipeline.
+        ColumnTransformers are used to wrap transformers as their input columns can be specified
+        as a subset of the pipeline's input columns.
+
+        Returns:
+            An unfit pipeline that can be fit using the ML runtime client.
+        """
+
+        sklearn_pipeline_steps = []
+
+        first_step_name, first_step_object = self.steps[0]
+
+        # Only the first step can have the input_cols field not None/empty.
+        if first_step_object.get_input_cols():
+            first_step_column_transformer = Pipeline._wrap_transformer_in_column_transformer(
+                first_step_name, first_step_object
+            )
+            first_step_skl = (first_step_name, first_step_column_transformer)
+        else:
+            first_step_skl = (first_step_name, Pipeline._get_native_object(first_step_object))
+
+        sklearn_pipeline_steps.append(first_step_skl)
+
+        for step_name, step_object in self.steps[1:]:
+            skl_step = (step_name, Pipeline._get_native_object(step_object))
+            sklearn_pipeline_steps.append(skl_step)
+
+        return pipeline.Pipeline(sklearn_pipeline_steps)
+
     def _create_sklearn_object(self) -> pipeline.Pipeline:
         if not self._is_fitted:
             return self._create_unfitted_sklearn_object()
 
         if not self._is_convertible_to_sklearn:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=ValueError(
-                    "The pipeline can't be converted to SKLearn equivalent because it processing label or "
+                    "The pipeline can't be converted to SKLearn equivalent because it modifies processing label or "
                     "sample_weight columns as part of pipeline preprocessing steps which is not allowed in SKLearn."
                 ),
             )
 
         # Create a fitted sklearn pipeline object by translating each non-estimator step in pipeline with with
         # a fitted column transformer.
         sksteps = []
@@ -627,7 +1064,52 @@
     def model_signatures(self) -> Dict[str, ModelSignature]:
         if self._model_signature_dict is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.INVALID_ATTRIBUTE,
                 original_exception=RuntimeError("Estimator not fitted before accessing property model_signatures!"),
             )
         return self._model_signature_dict
+
+    @staticmethod
+    def _get_native_object(estimator: base.BaseEstimator) -> object:
+        """A helper function to get the native(sklearn, xgboost, or lightgbm)
+        object from a snowpark ml estimator.
+        TODO - better type hinting - is there a common base class for all xgb/lgbm estimators?
+
+        Args:
+            estimator: the estimator from which to derive the native object.
+
+        Returns:
+            a native estimator object
+
+        Raises:
+            ValueError: The estimator is not an sklearn, xgboost, or lightgbm estimator.
+        """
+        methods = ["to_sklearn", "to_xgboost", "to_lightgbm"]
+        for method_name in methods:
+            if hasattr(estimator, method_name):
+                try:
+                    result = getattr(estimator, method_name)()
+                    return result
+                except exceptions.SnowflakeMLException:
+                    pass  # Do nothing and continue to the next method
+        raise ValueError("The estimator must be an sklearn, xgboost, or lightgbm estimator.")
+
+    def to_sklearn(self) -> pipeline.Pipeline:
+        """Returns an sklearn Pipeline representing the object, if possible.
+
+        Returns:
+            previously fit sklearn Pipeline if present, else an unfit pipeline
+
+        Raises:
+            ValueError: The pipeline cannot be represented as an sklearn pipeline.
+        """
+        if self._is_fitted:
+            if self._sklearn_object is not None:
+                return self._sklearn_object
+            else:
+                return self._create_sklearn_object()
+        else:
+            if self._is_convertible_to_sklearn_object():
+                return self._create_unfitted_sklearn_object()
+            else:
+                raise ValueError("This pipeline can not be converted to an sklearn pipeline.")
```

## snowflake/ml/modeling/preprocessing/one_hot_encoder.py

```diff
@@ -828,14 +828,26 @@
             split_pandas[_CATEGORY] = input_col_state_pandas[_CATEGORY].values
 
             # merge split encoding columns to the state pandas
             state_pandas = state_pandas.merge(split_pandas, on=[_COLUMN_NAME, _CATEGORY], how="left")
 
         # columns: COLUMN_NAME, CATEGORY, COUNT, FITTED_CATEGORY, ENCODING, N_FEATURES_OUT, ENCODED_VALUE, OUTPUT_CATs
         assert dataset._session is not None
+
+        def convert_to_string_excluding_nan(item: Any) -> Union[None, str]:
+            if pd.isna(item):
+                return None  # or np.nan if you prefer to keep as NaN
+            else:
+                return str(item)
+
+        # In case of fitting with pandas dataframe and transforming with snowpark dataframe
+        # state_pandas cannot recognize the datatype of _CATEGORY and _FITTED_CATEGORY column
+        # Therefore, apply the convert_to_string_excluding_nan function to _CATEGORY and _FITTED_CATEGORY
+        state_pandas[[_CATEGORY]] = state_pandas[[_CATEGORY]].applymap(convert_to_string_excluding_nan)
+        state_pandas[[_FITTED_CATEGORY]] = state_pandas[[_FITTED_CATEGORY]].applymap(convert_to_string_excluding_nan)
         state_df = dataset._session.create_dataframe(state_pandas)
 
         transformed_dataset = dataset
         original_dataset_columns = transformed_dataset.columns[:]
         all_output_cols = []
         for input_col in self.input_cols:
             output_cols = [
```

## snowflake/ml/modeling/preprocessing/polynomial_features.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.preprocessing".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class PolynomialFeatures(BaseTransformer):
     r"""Generate polynomial and interaction features
     For more details on this class, see [sklearn.preprocessing.PolynomialFeatures]
     (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)
 
     Parameters
     ----------
@@ -279,28 +273,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -310,17 +301,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -358,15 +347,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -443,18 +433,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -513,24 +501,50 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Fit to data, then transform it
+        For more details on this function, see [sklearn.preprocessing.PolynomialFeatures.fit_transform]
+        (https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures.fit_transform)
+
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -613,18 +627,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -681,18 +693,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -746,18 +756,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -815,18 +823,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -880,25 +886,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -955,19 +959,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/semi_supervised/label_propagation.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.semi_supervised".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LabelPropagation(BaseTransformer):
     r"""Label Propagation classifier
     For more details on this class, see [sklearn.semi_supervised.LabelPropagation]
     (https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelPropagation.html)
 
     Parameters
     ----------
@@ -285,28 +279,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -316,17 +307,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -366,15 +355,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -449,18 +439,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -519,24 +507,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -621,18 +633,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -691,18 +701,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -756,18 +764,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -825,18 +831,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -892,25 +896,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -967,19 +969,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/semi_supervised/label_spreading.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.semi_supervised".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LabelSpreading(BaseTransformer):
     r"""LabelSpreading model for semi-supervised learning
     For more details on this class, see [sklearn.semi_supervised.LabelSpreading]
     (https://scikit-learn.org/stable/modules/generated/sklearn.semi_supervised.LabelSpreading.html)
 
     Parameters
     ----------
@@ -294,28 +288,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -325,17 +316,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -375,15 +364,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -458,18 +448,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -528,24 +516,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -630,18 +642,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -700,18 +710,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -765,18 +773,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -834,18 +840,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -901,25 +905,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -976,19 +978,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/svm/linear_svc.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LinearSVC(BaseTransformer):
     r"""Linear Support Vector Classification
     For more details on this class, see [sklearn.svm.LinearSVC]
     (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)
 
     Parameters
     ----------
@@ -350,28 +344,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -381,17 +372,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -431,15 +420,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -514,18 +504,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -584,24 +572,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -684,18 +696,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -752,18 +762,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -819,18 +827,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -888,18 +894,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -955,25 +959,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1030,19 +1032,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/svm/linear_svr.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class LinearSVR(BaseTransformer):
     r"""Linear Support Vector Regression
     For more details on this class, see [sklearn.svm.LinearSVR]
     (https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVR.html)
 
     Parameters
     ----------
@@ -322,28 +316,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -353,17 +344,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -403,15 +392,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -486,18 +476,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -556,24 +544,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -656,18 +668,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -724,18 +734,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -789,18 +797,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -858,18 +864,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -925,25 +929,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1000,19 +1002,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/svm/nu_svc.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class NuSVC(BaseTransformer):
     r"""Nu-Support Vector Classification
     For more details on this class, see [sklearn.svm.NuSVC]
     (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html)
 
     Parameters
     ----------
@@ -356,28 +350,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -387,17 +378,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -437,15 +426,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -520,18 +510,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -590,24 +578,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -692,18 +704,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -762,18 +772,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -829,18 +837,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -898,18 +904,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -965,25 +969,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1040,19 +1042,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/svm/nu_svr.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class NuSVR(BaseTransformer):
     r"""Nu Support Vector Regression
     For more details on this class, see [sklearn.svm.NuSVR]
     (https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVR.html)
 
     Parameters
     ----------
@@ -317,28 +311,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -348,17 +339,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -398,15 +387,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -481,18 +471,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -551,24 +539,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -651,18 +663,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -719,18 +729,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -784,18 +792,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -853,18 +859,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -920,25 +924,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -995,19 +997,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/svm/svc.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SVC(BaseTransformer):
     r"""C-Support Vector Classification
     For more details on this class, see [sklearn.svm.SVC]
     (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)
 
     Parameters
     ----------
@@ -359,28 +353,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -390,17 +381,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -440,15 +429,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -523,18 +513,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -593,24 +581,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -695,18 +707,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -765,18 +775,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -832,18 +840,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -901,18 +907,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -968,25 +972,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1043,19 +1045,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/svm/svr.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.svm".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class SVR(BaseTransformer):
     r"""Epsilon-Support Vector Regression
     For more details on this class, see [sklearn.svm.SVR]
     (https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html)
 
     Parameters
     ----------
@@ -320,28 +314,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -351,17 +342,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -401,15 +390,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -484,18 +474,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -554,24 +542,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -654,18 +666,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -722,18 +732,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -787,18 +795,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -856,18 +862,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -923,25 +927,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -998,19 +1000,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/tree/decision_tree_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class DecisionTreeClassifier(BaseTransformer):
     r"""A decision tree classifier
     For more details on this class, see [sklearn.tree.DecisionTreeClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)
 
     Parameters
     ----------
@@ -387,28 +381,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -418,17 +409,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -468,15 +457,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -551,18 +541,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -621,24 +609,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -723,18 +735,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -793,18 +803,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -858,18 +866,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -927,18 +933,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -994,25 +998,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1069,19 +1071,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/tree/decision_tree_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class DecisionTreeRegressor(BaseTransformer):
     r"""A decision tree regressor
     For more details on this class, see [sklearn.tree.DecisionTreeRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)
 
     Parameters
     ----------
@@ -369,28 +363,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -400,17 +391,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -450,15 +439,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -533,18 +523,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -603,24 +591,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -703,18 +715,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -771,18 +781,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -836,18 +844,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -905,18 +911,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -972,25 +976,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1047,19 +1049,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/tree/extra_tree_classifier.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ExtraTreeClassifier(BaseTransformer):
     r"""An extremely randomized tree classifier
     For more details on this class, see [sklearn.tree.ExtraTreeClassifier]
     (https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeClassifier.html)
 
     Parameters
     ----------
@@ -379,28 +373,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -410,17 +401,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -460,15 +449,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -543,18 +533,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -613,24 +601,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -715,18 +727,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -785,18 +795,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -850,18 +858,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -919,18 +925,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -986,25 +990,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1061,19 +1063,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/tree/extra_tree_regressor.py

```diff
@@ -56,20 +56,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "sklearn.tree".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class ExtraTreeRegressor(BaseTransformer):
     r"""An extremely randomized tree regressor
     For more details on this class, see [sklearn.tree.ExtraTreeRegressor]
     (https://scikit-learn.org/stable/modules/generated/sklearn.tree.ExtraTreeRegressor.html)
 
     Parameters
     ----------
@@ -361,28 +355,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -392,17 +383,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -442,15 +431,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -525,18 +515,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -595,24 +583,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -695,18 +707,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -763,18 +773,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -828,18 +836,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -897,18 +903,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -964,25 +968,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['sklearn'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1039,19 +1041,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/xgboost/xgb_classifier.py

```diff
@@ -55,20 +55,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class XGBClassifier(BaseTransformer):
     r"""Implementation of the scikit-learn API for XGBoost classification
     For more details on this class, see [xgboost.XGBClassifier]
     (https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBClassifier)
 
     Parameters
     ----------
@@ -479,28 +473,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -510,17 +501,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -560,15 +549,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -643,18 +633,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -713,24 +701,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -815,18 +827,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -885,18 +895,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -950,18 +958,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -1019,18 +1025,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1086,25 +1090,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['xgboost'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1161,19 +1163,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/xgboost/xgb_regressor.py

```diff
@@ -55,20 +55,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class XGBRegressor(BaseTransformer):
     r"""Implementation of the scikit-learn API for XGBoost regression
     For more details on this class, see [xgboost.XGBRegressor]
     (https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)
 
     Parameters
     ----------
@@ -478,28 +472,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -509,17 +500,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -559,15 +548,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -642,18 +632,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -712,24 +700,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -812,18 +824,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -880,18 +890,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -945,18 +953,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -1014,18 +1020,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1081,25 +1085,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['xgboost'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1156,19 +1158,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/xgboost/xgbrf_classifier.py

```diff
@@ -55,20 +55,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class XGBRFClassifier(BaseTransformer):
     r"""scikit-learn API for XGBoost random forest classification
     For more details on this class, see [xgboost.XGBRFClassifier]
     (https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRFClassifier)
 
     Parameters
     ----------
@@ -483,28 +477,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -514,17 +505,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -564,15 +553,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -647,18 +637,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -717,24 +705,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -819,18 +831,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -889,18 +899,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -954,18 +962,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -1023,18 +1029,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1090,25 +1094,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['xgboost'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1165,19 +1167,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/modeling/xgboost/xgbrf_regressor.py

```diff
@@ -55,20 +55,14 @@
 # Derive subproject from module name by removing "sklearn"
 # and converting module name from underscore to CamelCase
 # e.g. sklearn.linear_model -> LinearModel.
 _SUBPROJECT = "".join([s.capitalize() for s in "xgboost".replace("sklearn.", "").split("_")])
 
 DATAFRAME_TYPE = Union[DataFrame, pd.DataFrame]
 
-def _is_fit_transform_method_enabled() -> Callable[[Any], bool]:	
-    def check(self: BaseTransformer) -> TypeGuard[Callable[..., object]]:	
-        return False and callable(getattr(self._sklearn_object, "fit_transform", None))	
-    return check	
-
-
 class XGBRFRegressor(BaseTransformer):
     r"""scikit-learn API for XGBoost random forest regression
     For more details on this class, see [xgboost.XGBRFRegressor]
     (https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRFRegressor)
 
     Parameters
     ----------
@@ -483,28 +477,25 @@
         self._generate_model_signatures(dataset)
         return self
 
     def _batch_inference_validate_snowpark(
         self,
         dataset: DataFrame,
         inference_method: str,
-    ) -> List[str]:
-        """Util method to run validate that batch inference can be run on a snowpark dataframe and
-        return the available package that exists in the snowflake anaconda channel
+    ) -> None:
+        """Util method to run validate that batch inference can be run on a snowpark dataframe.
 
         Args:
             dataset: snowpark dataframe
             inference_method: the inference method such as predict, score...
-
+            
         Raises:
             SnowflakeMLException: If the estimator is not fitted, raise error
             SnowflakeMLException: If the session is None, raise error
 
-        Returns:
-            A list of available package that exists in the snowflake anaconda channel
         """
         if not self._is_fitted:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.METHOD_NOT_ALLOWED,
                 original_exception=RuntimeError(
                     f"Estimator {self.__class__.__name__} not fitted before calling {inference_method} method."
                 ),
@@ -514,17 +505,15 @@
         if session is None:
             raise exceptions.SnowflakeMLException(
                 error_code=error_codes.NOT_FOUND,
                 original_exception=ValueError(
                     "Session must not specified for snowpark dataset."
                 ),
             )
-        # Validate that key package version in user workspace are supported in snowflake conda channel
-        return pkg_version_utils.get_valid_pkg_versions_supported_in_snowflake_conda_channel(
-            pkg_versions=self._get_dependencies(), session=session, subproject=_SUBPROJECT)
+
 
     @available_if(original_estimator_has_callable("predict"))  # type: ignore[misc]
     @telemetry.send_api_usage_telemetry(
         project=_PROJECT,
         subproject=_SUBPROJECT,
         custom_tags=dict([("autogen", True)]),
     )
@@ -564,15 +553,16 @@
                     raise exceptions.SnowflakeMLException(
                         error_code=error_codes.INVALID_ATTRIBUTE,
                         original_exception=ValueError(error_str),
                     )
 
                 expected_type_inferred = convert_sp_to_sf_type(label_cols_signatures[0].as_snowpark_type())
             
-            self._deps = self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
@@ -647,18 +637,16 @@
                     output_types = [signature.as_snowpark_type() for signature in _infer_signature(dataset[self.input_cols], "output", use_snowflake_identifiers=True)]
                     # We can only infer the output types from the input types if the following two statemetns are true:
                     # 1) All of the output types are the same. Otherwise, we still have to fall back to variant because `_sklearn_inference` only accepts one type.
                     # 2) The length of the input columns equals the length of the output columns. Otherwise the transform will likely result in an `ARRAY`.
                     if all(x == output_types[0] for x in output_types) and len(output_types) == len(self.output_cols):
                         expected_dtype = convert_sp_to_sf_type(output_types[0])
             
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
 
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
                 expected_output_cols_type=expected_dtype,
@@ -717,24 +705,48 @@
                 self.output_cols if self.output_cols else self._get_output_column_names(output_cols_prefix)
             ),
         )
         self._sklearn_object = fitted_estimator
         self._is_fitted = True
         return output_result
 
+            
+    @available_if(original_estimator_has_callable("fit_transform"))  # type: ignore[misc]
+    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame], output_cols_prefix: str = "fit_transform_",) -> Union[DataFrame, pd.DataFrame]:
+        """ Method not supported for this class.
 
-    @available_if(_is_fit_transform_method_enabled())  # type: ignore[misc]
-    def fit_transform(self, dataset: Union[DataFrame, pd.DataFrame]) -> Union[Any, npt.NDArray[Any]]:
-        """ 
+
+        Raises:
+            TypeError: Supported dataset types: snowpark.DataFrame, pandas.DataFrame.
+
+        Args:
+            dataset: Union[snowflake.snowpark.DataFrame, pandas.DataFrame]
+                Snowpark or Pandas DataFrame.
+        output_cols_prefix: Prefix for the response columns
         Returns:
             Transformed dataset.
         """
-        self.fit(dataset)
-        assert self._sklearn_object is not None
-        return self._sklearn_object.embedding_
+        self._infer_input_output_cols(dataset)
+        super()._check_dataset_type(dataset)
+        model_trainer = ModelTrainerBuilder.build_fit_transform(
+            estimator=self._sklearn_object,
+            dataset=dataset,
+            input_cols=self.input_cols,
+            label_cols=self.label_cols,
+            sample_weight_col=self.sample_weight_col,
+            autogenerated=self._autogenerated,
+            subproject=_SUBPROJECT,
+        )
+        output_result, fitted_estimator = model_trainer.train_fit_transform(
+            drop_input_cols=self._drop_input_cols,
+            expected_output_cols_list=self.output_cols,
+        )
+        self._sklearn_object = fitted_estimator
+        self._is_fitted = True
+        return output_result
 
 
     def _get_output_column_names(self, output_cols_prefix: str, output_cols: Optional[List[str]] = None) -> List[str]:
         """ Returns the list of output columns for predict_proba(), decision_function(), etc.. functions.
         Returns a list with output_cols_prefix as the only element if the estimator is not a classifier.
         """
         output_cols_prefix = identifier.resolve_identifier(output_cols_prefix)
@@ -817,18 +829,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -885,18 +895,16 @@
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -950,18 +958,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()  
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(
                 dataset._session, Session
             )  # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols=self._drop_input_cols,
@@ -1019,18 +1025,16 @@
         # This dictionary contains optional kwargs for batch inference. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: BatchInferenceKwargsTypedDict = dict()
 
         expected_output_cols = self._get_output_column_names(output_cols_prefix)
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session=dataset._session,
                 dependencies=self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="float",
             )
@@ -1086,25 +1090,23 @@
         super()._check_dataset_type(dataset)
 
         # This dictionary contains optional kwargs for scoring. These kwargs
         # are specific to the type of dataset used. 
         transform_kwargs: ScoreKwargsTypedDict = dict()  
 
         if isinstance(dataset, DataFrame):
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method="score",
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method="score")
+            self._deps = self._get_dependencies()
             selected_cols = self._get_active_columns()
             if len(selected_cols) > 0:
                 dataset = dataset.select(selected_cols)
             assert isinstance(dataset._session, Session) # keep mypy happy
             transform_kwargs = dict(
                 session=dataset._session,
-                dependencies=["snowflake-snowpark-python"] + self._deps,
+                dependencies=self._deps,
                 score_sproc_imports=['xgboost'],
             )
         elif isinstance(dataset, pd.DataFrame):
             # pandas_handler.score() does not require any extra kwargs.
             transform_kwargs = dict()
 
         transform_handlers = ModelTransformerBuilder.build(
@@ -1161,19 +1163,16 @@
         transform_kwargs: BatchInferenceKwargsTypedDict = dict() 
         output_cols = ["neigh_ind"]
         if return_distance:
             output_cols.insert(0, "neigh_dist")
 
         if isinstance(dataset, DataFrame):
 
-            self._deps = self._batch_inference_validate_snowpark(
-                dataset=dataset,
-                inference_method=inference_method,
-
-            )
+            self._batch_inference_validate_snowpark(dataset=dataset, inference_method=inference_method)
+            self._deps = self._get_dependencies()
             assert isinstance(dataset._session, Session) # mypy does not recognize the check in _batch_inference_validate_snowpark()
             transform_kwargs = dict(
                 session = dataset._session,
                 dependencies = self._deps,
                 drop_input_cols = self._drop_input_cols,
                 expected_output_cols_type="array",
                 n_neighbors = n_neighbors,
```

## snowflake/ml/registry/model_registry.py

```diff
@@ -25,27 +25,21 @@
     formatting,
     identifier,
     query_result_checker,
     spcs_attribution_utils,
     table_manager,
     uri,
 )
-from snowflake.ml.dataset import dataset
 from snowflake.ml.model import (
     _api as model_api,
     deploy_platforms,
     model_signature,
     type_hints as model_types,
 )
-from snowflake.ml.registry import (
-    _artifact_manager,
-    _initial_schema,
-    _schema_version_manager,
-    artifact,
-)
+from snowflake.ml.registry import _initial_schema, _schema_version_manager
 from snowflake.snowpark._internal import utils as snowpark_utils
 
 if TYPE_CHECKING:
     import pandas as pd
 
 _DEFAULT_REGISTRY_NAME: str = "_SYSTEM_MODEL_REGISTRY"
 _DEFAULT_SCHEMA_NAME: str = "_SYSTEM_MODEL_REGISTRY_SCHEMA"
@@ -138,27 +132,25 @@
 def _create_registry_views(
     session: snowpark.Session,
     database_name: str,
     schema_name: str,
     registry_table_name: str,
     metadata_table_name: str,
     deployment_table_name: str,
-    artifact_table_name: str,
     statement_params: Dict[str, Any],
 ) -> None:
     """Create views on underlying ModelRegistry tables.
 
     Args:
         session: Session object to communicate with Snowflake.
         database_name: Desired name of the model registry database.
         schema_name: Desired name of the schema used by this model registry inside the database.
         registry_table_name: Name for the main model registry table.
         metadata_table_name: Name for the metadata table used by the model registry.
         deployment_table_name: Name for the deployment event table.
-        artifact_table_name: Name for the artifact table.
         statement_params: Function usage statement parameters used in sql query executions.
     """
     fully_qualified_schema_name = table_manager.get_fully_qualified_schema_name(database_name, schema_name)
 
     # From the documentation: Each DDL statement executes as a separate transaction. Races should not be an issue.
     # https://docs.snowflake.com/en/sql-reference/transactions.html#ddl
 
@@ -231,31 +223,14 @@
     metadata_select_fields_formatted = ",".join(metadata_select_fields)
     session.sql(
         f"""CREATE OR REPLACE TEMPORARY VIEW {fully_qualified_schema_name}.{registry_view_name} COPY GRANTS AS
                 SELECT {registry_table_name}.*, {metadata_select_fields_formatted}
                 FROM {registry_table_name} {metadata_views_join}"""
     ).collect(statement_params=statement_params)
 
-    # Create artifact view. it joins artifact tables with registry table on model id.
-    artifact_view_name = identifier.concat_names([artifact_table_name, "_VIEW"])
-    session.sql(
-        f"""CREATE OR REPLACE TEMPORARY VIEW {fully_qualified_schema_name}.{artifact_view_name} COPY GRANTS AS
-                SELECT
-                    {registry_table_name}.NAME AS MODEL_NAME,
-                    {registry_table_name}.VERSION AS MODEL_VERSION,
-                    {artifact_table_name}.*
-                FROM {registry_table_name}
-                LEFT JOIN {artifact_table_name}
-                ON (ARRAY_CONTAINS(
-                    {artifact_table_name}.ID::VARIANT,
-                    {registry_table_name}.ARTIFACT_IDS)
-                )
-        """
-    ).collect(statement_params=statement_params)
-
 
 def _create_active_permanent_deployment_view(
     session: snowpark.Session,
     fully_qualified_schema_name: str,
     registry_table_name: str,
     deployment_table_name: str,
     statement_params: Dict[str, Any],
@@ -333,19 +308,16 @@
         self._schema = identifier.get_inferred_name(schema_name)
         self._registry_table = identifier.get_inferred_name(_MODELS_TABLE_NAME)
         self._registry_table_view = identifier.concat_names([self._registry_table, "_VIEW"])
         self._metadata_table = identifier.get_inferred_name(_METADATA_TABLE_NAME)
         self._deployment_table = identifier.get_inferred_name(_DEPLOYMENT_TABLE_NAME)
         self._permanent_deployment_view = identifier.concat_names([self._deployment_table, "_VIEW"])
         self._permanent_deployment_stage = identifier.concat_names([self._deployment_table, "_STAGE"])
-        self._artifact_table = identifier.get_inferred_name(_initial_schema._ARTIFACT_TABLE_NAME)
-        self._artifact_view = identifier.concat_names([self._artifact_table, "_VIEW"])
         self._session = session
         self._svm = _schema_version_manager.SchemaVersionManager(self._session, self._name, self._schema)
-        self._artifact_manager = _artifact_manager.ArtifactManager(self._session, self._name, self._schema)
 
         # A in-memory deployment info cache to store information of temporary deployments
         # TODO(zhe): Use a temporary table to replace the in-memory cache.
         self._temporary_deployments: Dict[str, model_types.Deployment] = {}
 
         _initial_schema.check_access(self._session, self._name, self._schema)
 
@@ -355,15 +327,14 @@
         _create_registry_views(
             session,
             self._name,
             self._schema,
             self._registry_table,
             self._metadata_table,
             self._deployment_table,
-            self._artifact_table,
             statement_params,
         )
 
     # Private methods
 
     def _get_statement_params(self, frame: Optional[types.FrameType]) -> Dict[str, Any]:
         return telemetry.get_function_usage_statement_params(
@@ -395,17 +366,14 @@
         """Get the fully qualified name to the current deployment table."""
         return table_manager.get_fully_qualified_table_name(self._name, self._schema, self._deployment_table)
 
     def _fully_qualified_permanent_deployment_view_name(self) -> str:
         """Get the fully qualified name to the permanent deployment view."""
         return table_manager.get_fully_qualified_table_name(self._name, self._schema, self._permanent_deployment_view)
 
-    def _fully_qualified_artifact_view_name(self) -> str:
-        return table_manager.get_fully_qualified_table_name(self._name, self._schema, self._artifact_view)
-
     def _fully_qualified_schema_name(self) -> str:
         """Get the fully qualified name to the current registry schema."""
         return table_manager.get_fully_qualified_schema_name(self._name, self._schema)
 
     def _fully_qualified_deployment_name(self, deployment_name: str) -> str:
         """Get the fully qualified name to the given deployment."""
         return table_manager.get_fully_qualified_table_name(self._name, self._schema, deployment_name)
@@ -854,15 +822,14 @@
         *,
         type: str,
         uri: str,
         input_spec: Optional[Dict[str, str]] = None,
         output_spec: Optional[Dict[str, str]] = None,
         description: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
-        artifacts: Optional[List[artifact.Artifact]] = None,
     ) -> None:
         """Helper function to register model metadata.
 
         Args:
             model_name: Name to be set for the model. The model name can NOT be changed after registration. The
                 combination of name and version is expected to be unique inside the registry.
             model_version: Version string to be set for the model. The model version string can NOT be changed after
@@ -874,18 +841,16 @@
             input_spec: The expected input schema of the model. Dictionary where the keys are
                 expected column names and the values are the value types.
             output_spec: The expected output schema of the model. Dictionary where the keys
                 are expected column names and the values are the value types.
             description: A description for the model. The description can be changed later.
             tags: Key-value pairs of tags to be set for this model. Tags can be modified
                 after model registration.
-            artifacts: A list of artifact references.
 
         Raises:
-            ValueError: Artifact ids not found in model registry.
             DataError: The given model already exists.
             DatabaseError: Unable to register the model properties into table.
         """
         new_model: Dict[Any, Any] = {}
         new_model["ID"] = model_id
         new_model["NAME"] = model_name
         new_model["VERSION"] = model_version
@@ -893,20 +858,14 @@
         new_model["URI"] = uri
         new_model["INPUT_SPEC"] = input_spec
         new_model["OUTPUT_SPEC"] = output_spec
         new_model["CREATION_TIME"] = formatting.SqlStr("CURRENT_TIMESTAMP()")
         new_model["CREATION_ROLE"] = self._session.get_current_role()
         new_model["CREATION_ENVIRONMENT_SPEC"] = {"python": ".".join(map(str, sys.version_info[:3]))}
 
-        if artifacts is not None:
-            for atf in artifacts:
-                if not self._artifact_manager.exists(atf.name if atf.name is not None else "", atf.version):
-                    raise ValueError(f"Artifact {atf.name}/{atf.version} not found in model registry.")
-            new_model["ARTIFACT_IDS"] = [art._id for art in artifacts]
-
         existing_model_nums = self._list_selected_models(model_name=model_name, model_version=model_version).count()
         if existing_model_nums:
             raise connector.DataError(
                 f"Model {model_name}/{model_version} already exists. Unable to register the model."
             )
 
         if self._insert_registry_entry(id=model_id, name=model_name, version=model_version, properties=new_model):
@@ -1352,50 +1311,14 @@
 
         if result:
             ret: Dict[str, object] = json.loads(result)
             return ret
         else:
             return dict()
 
-    @telemetry.send_api_usage_telemetry(
-        project=_TELEMETRY_PROJECT,
-        subproject=_TELEMETRY_SUBPROJECT,
-    )
-    @snowpark._internal.utils.private_preview(version="1.0.10")
-    def log_artifact(
-        self,
-        artifact: artifact.Artifact,
-        name: str,
-        version: Optional[str] = None,
-    ) -> artifact.Artifact:
-        """Upload and register an artifact to the Model Registry.
-
-        Args:
-            artifact: artifact object.
-            name: name of artifact.
-            version: version of artifact.
-
-        Raises:
-            DataError: Artifact with same name and version already exists.
-
-        Returns:
-            Return a reference to the artifact.
-        """
-
-        if self._artifact_manager.exists(name, version):
-            raise connector.DataError(f"Artifact {name}/{version} already exists.")
-
-        artifact_id = self._get_new_unique_identifier()
-        return self._artifact_manager.add(
-            artifact=artifact,
-            artifact_id=artifact_id,
-            artifact_name=name,
-            artifact_version=version,
-        )
-
     # Combined Registry and Repository operations.
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     @snowpark._internal.utils.private_preview(version="0.2.0")
     def log_model(
@@ -1406,15 +1329,14 @@
         model: Any,
         description: Optional[str] = None,
         tags: Optional[Dict[str, str]] = None,
         conda_dependencies: Optional[List[str]] = None,
         pip_requirements: Optional[List[str]] = None,
         signatures: Optional[Dict[str, model_signature.ModelSignature]] = None,
         sample_input_data: Optional[Any] = None,
-        artifacts: Optional[List[artifact.Artifact]] = None,
         code_paths: Optional[List[str]] = None,
         options: Optional[model_types.BaseModelSaveOption] = None,
     ) -> Optional["ModelReference"]:
         """Uploads and register a model to the Model Registry.
 
         Args:
             model_name: The given name for the model. The combination (name + version) must be unique for each model.
@@ -1427,53 +1349,37 @@
                 specify a dependency. It is a recommended way to specify your dependencies using conda. When channel is
                 not specified, defaults channel will be used. When deploying to Snowflake Warehouse, defaults channel
                 would be replaced with the Snowflake Anaconda channel.
             pip_requirements: List of PIP package specs. Model will not be able to deploy to the warehouse if there is
                 pip requirements.
             signatures: Signatures of the model, which is a mapping from target method name to signatures of input and
                 output, which could be inferred by calling `infer_signature` method with sample input data.
-            sample_input_data: Sample of the input data for the model. If artifacts contains a feature store
-                generated dataset, then sample_input_data is not needed. If both sample_input_data and dataset provided
-                , then sample_input_data will be used to infer model signature.
-            artifacts: A list of artifact ids, which are generated from log_artifact().
+            sample_input_data: Sample of the input data for the model.
             code_paths: Directory of code to import when loading and deploying the model.
             options: Additional options when saving the model.
 
         Raises:
             DataError: Raised when:
                 1) the given model already exists;
-                2) given artifacts does not exists in this registry.
             ValueError: Raised when:  # noqa: DAR402
-                1) Signatures, sample_input_data and artifact(dataset) are both not provided and model is not a
+                1) Signatures and sample_input_data are both not provided and model is not a
                     snowflake estimator.
             Exception: Raised when there is any error raised when saving the model.
 
         Returns:
             Model Reference . None if failed.
         """
         # Ideally, the whole operation should be a single transaction. Currently, transactions do not support stage
         # operations.
 
         statement_params = self._get_statement_params(inspect.currentframe())
         self._svm.validate_schema_version(statement_params)
 
         self._model_identifier_is_nonempty_or_raise(model_name, model_version)
 
-        if artifacts is not None:
-            for atf in artifacts:
-                if not self._artifact_manager.exists(atf.name if atf.name is not None else "", atf.version):
-                    raise connector.DataError(f"Artifact {atf.name}/{atf.version} does not exists.")
-
-        if sample_input_data is None and artifacts is not None:
-            for atf in artifacts:
-                if atf.type == artifact.ArtifactType.DATASET:
-                    ds = self.get_artifact(atf.name if atf.name is not None else "", atf.version)
-                    sample_input_data = ds.features_df()
-                    break
-
         existing_model_nums = self._list_selected_models(model_name=model_name, model_version=model_version).count()
         if existing_model_nums:
             raise connector.DataError(f"Model {model_name}/{model_version} already exists. Unable to log the model.")
         model_id, fully_qualified_model_stage_name = self._log_model_path(
             model_name=model_name,
             model_version=model_version,
         )
@@ -1504,15 +1410,14 @@
             model_name=model_name,
             model_version=model_version,
             model_id=model_id,
             type=model_composer.packager.meta.model_type,
             uri=uri.get_uri_from_snowflake_stage_path(stage_path),
             description=description,
             tags=tags,
-            artifacts=artifacts,
         )
 
         return ModelReference(registry=self, model_name=model_name, model_version=model_version)
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
@@ -1729,33 +1634,14 @@
             deployments_df["SIGNATURE"],
             deployments_df["OPTIONS"],
             deployments_df["STAGE_PATH"],
             deployments_df["ROLE"],
         )
         return cast(snowpark.DataFrame, res)
 
-    @snowpark._internal.utils.private_preview(version="1.0.1")
-    def list_artifacts(self, model_name: str, model_version: Optional[str] = None) -> snowpark.DataFrame:
-        """List all artifacts that associated with given model name and version.
-
-        Args:
-            model_name: Name of model.
-            model_version: Version of model. If version is none then only filter on name.
-                Defaults to none.
-
-        Returns:
-            A snowpark dataframe that contains all artifacts that associated with the given model.
-        """
-        artifacts = self._session.sql(f"SELECT * FROM {self._fully_qualified_artifact_view_name()}").filter(
-            snowpark.Column("MODEL_NAME") == model_name
-        )
-        if model_version is not None:
-            artifacts = artifacts.filter(snowpark.Column("MODEL_VERSION") == model_version)
-        return cast(snowpark.DataFrame, artifacts)
-
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
     @snowpark._internal.utils.private_preview(version="1.0.1")
     def get_deployment(self, model_name: str, model_version: str, *, deployment_name: str) -> snowpark.DataFrame:
         """Get the permanent deployment with target name of the given model.
@@ -1782,46 +1668,14 @@
             )
         return cast(snowpark.DataFrame, deployment)
 
     @telemetry.send_api_usage_telemetry(
         project=_TELEMETRY_PROJECT,
         subproject=_TELEMETRY_SUBPROJECT,
     )
-    @snowpark._internal.utils.private_preview(version="1.0.11")
-    def get_artifact(self, name: str, version: Optional[str] = None) -> Optional[artifact.Artifact]:
-        """Get artifact with the given (name, version).
-
-        Args:
-            name: Name of artifact.
-            version: Version of artifact.
-
-        Returns:
-            A reference to artifact if found, otherwise none.
-        """
-        artifacts = self._artifact_manager.get(
-            name,
-            version,
-        ).collect()
-
-        if len(artifacts) == 0:
-            return None
-
-        atf = artifacts[0]
-        if atf["TYPE"] == artifact.ArtifactType.DATASET.value:
-            ds = dataset.Dataset.from_json(atf["ARTIFACT_SPEC"], self._session)
-            ds._log(name=atf["NAME"], version=atf["VERSION"], id=atf["ID"])
-            return ds
-
-        assert f"Unrecognized artifact type: {atf['TYPE']}"
-        return None
-
-    @telemetry.send_api_usage_telemetry(
-        project=_TELEMETRY_PROJECT,
-        subproject=_TELEMETRY_SUBPROJECT,
-    )
     @snowpark._internal.utils.private_preview(version="1.0.1")
     def delete_deployment(self, model_name: str, model_version: str, *, deployment_name: str) -> None:
         """Delete the target permanent deployment of the given model.
 
         Deleting temporary deployment are currently not supported.
         Temporary deployment will get cleaned automatically when the current session closed.
```

## Comparing `snowflake_ml_python-1.4.1.dist-info/LICENSE.txt` & `snowflake_ml_python-1.5.0.dist-info/LICENSE.txt`

 * *Files identical despite different names*

## Comparing `snowflake_ml_python-1.4.1.dist-info/METADATA` & `snowflake_ml_python-1.5.0.dist-info/METADATA`

 * *Files 6% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: snowflake-ml-python
-Version: 1.4.1
+Version: 1.5.0
 Summary: The machine learning client library that is used for interacting with Snowflake to build machine learning solutions.
 Author-email: "Snowflake, Inc" <support@snowflake.com>
 License: 
                                          Apache License
                                    Version 2.0, January 2004
                                 http://www.apache.org/licenses/
         
@@ -367,15 +367,76 @@
 ```
 
 Note that until a `snowflake-ml-python` package version is available in the official Snowflake conda channel, there may
 be compatibility issues. Server-side functionality that `snowflake-ml-python` depends on may not yet be released.
 
 # Release History
 
-## 1.4.1
+## 1.5.0
+
+### Bug Fixes
+
+- Registry: Fix invalid parameter 'SHOW_MODEL_DETAILS_IN_SHOW_VERSIONS_IN_MODEL' error.
+
+### Behavior Changes
+
+- Model Development: The behavior of `fit_transform` for all estimators is changed.
+  Firstly, it will cover all the estimator that contains this function,
+  secondly, the output would be the union of pandas DataFrame and snowpark DataFrame.
+
+#### Model Registry (PrPr)
+
+`snowflake.ml.registry.artifact` and related `snowflake.ml.model_registry.ModelRegistry` APIs have been removed.
+
+- Removed `snowflake.ml.registry.artifact` module.
+- Removed `ModelRegistry.log_artifact()`, `ModelRegistry.list_artifacts()`, `ModelRegistry.get_artifact()`
+- Removed `artifacts` argument from `ModelRegistry.log_model()`
+
+#### Dataset (PrPr)
+
+`snowflake.ml.dataset.Dataset` has been redesigned to be backed by Snowflake Dataset entities.
+
+- New `Dataset`s can be created with `Dataset.create()` and existing `Dataset`s may be loaded
+  with `Dataset.load()`.
+- `Dataset`s now maintain an immutable `selected_version` state. The `Dataset.create_version()` and
+  `Dataset.load_version()` APIs return new `Dataset` objects with the requested `selected_version` state.
+- Added `dataset.create_from_dataframe()` and `dataset.load_dataset()` convenience APIs as a shortcut
+  to creating and loading `Dataset`s with a pre-selected version.
+- `Dataset.materialized_table` and `Dataset.snapshot_table` no longer exist with `Dataset.fully_qualified_name`
+  as the closest equivalent.
+- `Dataset.df` no longer exists. Instead, use `DatasetReader.read.to_snowpark_dataframe()`.
+- `Dataset.owner` has been moved to `Dataset.selected_version.owner`
+- `Dataset.desc` has been moved to `DatasetVersion.selected_version.comment`
+- `Dataset.timestamp_col`, `Dataset.label_cols`, `Dataset.feature_store_metadata`, and
+  `Dataset.schema_version` have been removed.
+
+#### Feature Store (PrPr)
+
+`FeatureStore.generate_dataset` argument list has been changed to match the new
+`snowflake.ml.dataset.Dataset` definition
+
+- `materialized_table` has been removed and replaced with `name` and `version`.
+- `name` moved to first positional argument
+- `save_mode` has been removed as `merge` behavior is no longer supported. The new behavior is always `errorifexists`.
+
+### New Features
+
+- Registry: Add `export` method to `ModelVersion` instance to export model files.
+- Registry: Add `load` method to `ModelVersion` instance to load the underlying object from the model.
+- Registry: Add `Model.rename` method to `Model` instance to rename or move a model.
+
+#### Dataset (PrPr)
+
+- Added Snowpark DataFrame integration using `Dataset.read.to_snowpark_dataframe()`
+- Added Pandas DataFrame integration using `Dataset.read.to_pandas()`
+- Added PyTorch and TensorFlow integrations using `Dataset.read.to_torch_datapipe()`
+    and `Dataset.read.to_tf_dataset()` respectively.
+- Added `fsspec` style file integration using `Dataset.read.files()` and `Dataset.read.filesystem()`
+
+## 1.4.1 (2024-04-18)
 
 ### New Features
 
 - Registry: Add support for `catboost` model (`catboost.CatBoostClassifier`, `catboost.CatBoostRegressor`).
 - Registry: Add support for `lightgbm` model (`lightgbm.Booster`, `lightgbm.LightGBMClassifier`, `lightgbm.LightGBMRegressor`).
 
 ### Bug Fixes
```

## Comparing `snowflake_ml_python-1.4.1.dist-info/RECORD` & `snowflake_ml_python-1.5.0.dist-info/RECORD`

 * *Files 4% similar despite different names*

```diff
@@ -1,36 +1,40 @@
 snowflake/cortex/__init__.py,sha256=CAUk94eXmNBXXaiLg-yNodyM2FPHvacErKtdVQYqtRM,360
 snowflake/cortex/_complete.py,sha256=C2wLk5RMtg-d2bkdbQKou6U8nvR8g3vykpCkH9-gF9g,1226
 snowflake/cortex/_extract_answer.py,sha256=4tiz4pUisw035ZLmCQDcGuwoT-jFpuo5dzrQYhvYHCA,1358
 snowflake/cortex/_sentiment.py,sha256=7X_a8qJNFFgn-Y1tjwMDkyNJHz5yYl0PvnezVCc4TsM,1149
 snowflake/cortex/_summarize.py,sha256=DJRxUrPrTVmtQNgus0ZPF1z8nPmn4Rs5oL3U25CfXxQ,1075
 snowflake/cortex/_translate.py,sha256=JPMIXxHTgJPfJqT5Hw_WtYM6FZ8NuQufZ4XR-M8wnyo,1420
 snowflake/cortex/_util.py,sha256=0xDaDSctenhuj59atZenZp5q9zuhji0WQ77KPjqqNoc,1557
-snowflake/ml/version.py,sha256=PhS7XK3XWIDp2SwZaZV976hFEG5kMWzFvOxTAICredA,16
+snowflake/ml/version.py,sha256=3R9EThHATDpTgUWWErtoGeHWTDLDz1F9kgtQ7lMuM80,16
 snowflake/ml/_internal/env.py,sha256=kCrJTRnqQ97VGUVI1cWUPD8HuBWeL5vOOtwUR0NB9Mg,161
-snowflake/ml/_internal/env_utils.py,sha256=Kntfp8gqF4BvaaWQuLpwMtRuPXjlx_EuJY6SZAO0rEw,26212
+snowflake/ml/_internal/env_utils.py,sha256=aqaF-bXPUvXxONfMRxIuH-JKyu3oWkRqyC_1jDEmK4Y,27629
 snowflake/ml/_internal/file_utils.py,sha256=OyXHv-UcItiip1YgLnab6etonUQkYuyDtmplZA0CaoU,13622
 snowflake/ml/_internal/init_utils.py,sha256=U-oPOtyVf22hCwDH_CH2uDr9yuN6Mr3kwQ_yRAs1mcM,2696
 snowflake/ml/_internal/migrator_utils.py,sha256=k3erO8x3YJcX6nkKeyJAUNGg1qjE3RFmD-W6dtLzIH0,161
 snowflake/ml/_internal/telemetry.py,sha256=oM7dDcs1GKgxKP2UM7va1j1YfQGISFiGYyiT9zM7Yxc,22763
 snowflake/ml/_internal/type_utils.py,sha256=0AjimiQoAPHGnpLV_zCR6vlMR5lJ8CkZkKFwiUHYDCo,2168
 snowflake/ml/_internal/container_services/image_registry/credential.py,sha256=nShNgIb2yNu9w6vceOY3aSgjpuOoi0spWWmvgEafPSk,3291
 snowflake/ml/_internal/container_services/image_registry/http_client.py,sha256=_zqPPp76Vk0jQ8eVK0OJ4mJgcWsdY4suUd1P7Orqmm8,5214
 snowflake/ml/_internal/container_services/image_registry/imagelib.py,sha256=Vh684uUZfwGGnxO-BZ4tRGa50l2uGM-4WfTg6QftlMY,14537
 snowflake/ml/_internal/container_services/image_registry/registry_client.py,sha256=Zic4bF67DMqEZbQMHffyeNoa83-FhswpZx02iBMjyrc,9115
-snowflake/ml/_internal/exceptions/error_codes.py,sha256=bgoTb5DRSIt_JfI0VcaslgiexSul7-LMrWsXjBns8BE,5317
+snowflake/ml/_internal/exceptions/dataset_error_messages.py,sha256=h7uGJbxBM6se-TW_64LKGGGdBCbwflzbBnmijWKX3Gc,285
+snowflake/ml/_internal/exceptions/dataset_errors.py,sha256=wZTPKZRJSYsfeTs0vDL8r4bFFSP_9ob8XinMgPi63RM,762
+snowflake/ml/_internal/exceptions/error_codes.py,sha256=eMgsEfIYFQesK_pqLIsyxRZojz8Ke9DTlA5ni60RLv4,5453
 snowflake/ml/_internal/exceptions/error_messages.py,sha256=vF9XOWJoBuKvFxBkGcDelhXK1dipzTt-AdK4NkCbwTo,47
 snowflake/ml/_internal/exceptions/exceptions.py,sha256=ub0fthrNTVoKhpj1pXnKRfO1Gqnmbe7wY51vaoEOp5M,1653
 snowflake/ml/_internal/exceptions/fileset_error_messages.py,sha256=dqPpRu0cKyQA_0gahvbizgQBTwNhnwveN286JrJLvi8,419
 snowflake/ml/_internal/exceptions/fileset_errors.py,sha256=ZJfkpeDgRIw3qA876fk9FIzxIrm-yZ8I9RXUbzaeM84,1040
 snowflake/ml/_internal/exceptions/modeling_error_messages.py,sha256=q1Nh7KvnUebdKCwwAPmotdAVS578CgAXcfDOfKoweVw,665
 snowflake/ml/_internal/human_readable_id/adjectives.txt,sha256=5o4MbVeHoELAqyLpyuKleOKR47jPjC_nKoziOIZMwT0,804
 snowflake/ml/_internal/human_readable_id/animals.txt,sha256=GDLzMwzxiL07PhIMxw4t89bhYqqg0bQfPiuQT8VNeME,837
 snowflake/ml/_internal/human_readable_id/hrid_generator.py,sha256=LYWB86qZgsVBvnc6Q5VjfDOmnGSQU3cTRKfId_nJSPY,1341
 snowflake/ml/_internal/human_readable_id/hrid_generator_base.py,sha256=D1yoVG1vmAFUhWQ5xCRRU6HCCBPbXHpOXagFd0jK0O8,4519
+snowflake/ml/_internal/lineage/data_source.py,sha256=D24FdR6Wq_PdUuCsBDvSMCr5CfHqpMamrc8-F5iZVJ0,214
+snowflake/ml/_internal/lineage/dataset_dataframe.py,sha256=qbEXgkHxzx6zZzJCGpIhFV7-OdAuA_qrO9AixxyxHSk,1712
 snowflake/ml/_internal/utils/formatting.py,sha256=PswZ6Xas7sx3Ok1MBLoH2o7nfXOxaJqpUPg_UqXrQb8,3676
 snowflake/ml/_internal/utils/identifier.py,sha256=eokEDF7JIML2gm_3FfknPdPR9aBT3woweA5S4z_46-E,10925
 snowflake/ml/_internal/utils/import_utils.py,sha256=eexwIe7auT17s4aVxAns7se0_K15rcq3O17MkIvDpPI,2068
 snowflake/ml/_internal/utils/log_stream_processor.py,sha256=pBf8ycEamhHjEzUT55Rx_tFqSkYRpD5Dt71Mx9ZdaS8,1001
 snowflake/ml/_internal/utils/parallelize.py,sha256=Q6_-P2t4DoYNO8DyC1kOl7H3qNL-bUK6EgtlQ_b5ThY,4534
 snowflake/ml/_internal/utils/pkg_version_utils.py,sha256=tpu6B0HKpbT-svvU2Pbz7zNqzg-jgoSmwYvtTzXYyzw,5857
 snowflake/ml/_internal/utils/query_result_checker.py,sha256=h1nbUImdB9lSNCON3uIA0xCm8_JrS-TE-jQXJJs9WfU,10668
@@ -40,67 +44,73 @@
 snowflake/ml/_internal/utils/snowflake_env.py,sha256=Mrov0v95pzVUeAe7r1e1PtlIco9ytj5SGAuUWORQaKs,2927
 snowflake/ml/_internal/utils/snowpark_dataframe_utils.py,sha256=HPyWxj-SwgvWUrYR38BkBtx813eMqz5wmQosgc1sce0,5403
 snowflake/ml/_internal/utils/spcs_attribution_utils.py,sha256=9XPKe1BDkWhnGuHDXBHE4FP-m3U22lTZnrQLsHGFhWU,4292
 snowflake/ml/_internal/utils/sql_identifier.py,sha256=CHTxr3qtc1ygNkA5oOQQa-XEoosw5sjfHe7J4WZlkDQ,3270
 snowflake/ml/_internal/utils/table_manager.py,sha256=jHGfl0YSqhFLL7DOOQkjUMzTmLkqFDIM7Gs0LBQw8BM,4384
 snowflake/ml/_internal/utils/temp_file_utils.py,sha256=7JNib0DvjxW7Eu3bimwAHibGosf0u8W49HEc49BghF8,1402
 snowflake/ml/_internal/utils/uri.py,sha256=pvskcWoeS0M66DaU2XlJzK9wce55z4J5dn5kTy_-Tqs,2828
-snowflake/ml/dataset/dataset.py,sha256=OG_RonPgj86mRKRgN-xhJV0uZfa78ohVBpxsoYYnceY,6078
+snowflake/ml/dataset/__init__.py,sha256=jyoLosJL9mTWTULTqJB3WaDS77-zsPcdoWDTNxyN-rY,236
+snowflake/ml/dataset/dataset.py,sha256=V6QeEN8WatRnpoXRss950ISKiE0hKrJg7PCFkHKmfYo,20863
+snowflake/ml/dataset/dataset_factory.py,sha256=qdS6jX8uiCpW5TIKnZ-_2HRfWN3c_N1bZ6lBC1bLy5g,1712
+snowflake/ml/dataset/dataset_metadata.py,sha256=lvaYd1sNOgWcXD1q_-J7fQZ0ndOC8guR9IgKpChBcFA,3992
+snowflake/ml/dataset/dataset_reader.py,sha256=0Vgr_xV4YIBewPeymGNpaSCA7AVZcCgyq8pSuSwc2Ys,8205
 snowflake/ml/feature_store/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/feature_store/entity.py,sha256=dCpzLC3jrt5wDHqFYJXbAYkMiZ0zEmiVDMGkks6MXkA,3378
-snowflake/ml/feature_store/feature_store.py,sha256=TyLzlsVkPb0Vh-jyCA0prX5vMi6Q8DtxbU2uFElRnVM,71962
-snowflake/ml/feature_store/feature_view.py,sha256=2WtcqcHcvh51ojbf8K7a16GSiY5WbMDDwTMQmj_AQT8,17544
+snowflake/ml/feature_store/feature_store.py,sha256=SrJkB_9Y6clbw6jCWmKvjGoZ78PU2hzUSnHTVnTfDKo,76062
+snowflake/ml/feature_store/feature_view.py,sha256=Epqmm5QiK6jdgrgE-x54Lq1B6VBocFLAVk5bt13eyYg,18624
+snowflake/ml/fileset/embedded_stage_fs.py,sha256=gzMX6RbU_K9jCy1zEfF0YZ7nZSVHW4b3UTVAuAn-etY,5740
 snowflake/ml/fileset/fileset.py,sha256=QRhxLeKf1QBqvXO4RyyRd1c8TixhYpHuBEII8Qi3C_M,26201
 snowflake/ml/fileset/parquet_parser.py,sha256=sjyRB59cGBzSzvbcYLvu_ApMPtrR-zwZsQkxekMR4FA,6884
-snowflake/ml/fileset/sfcfs.py,sha256=aRhGMLnFLRQcvhN3epScTLUoOFNM9UQJwVpF8reZ-Yo,15596
-snowflake/ml/fileset/stage_fs.py,sha256=Lzt5qglRE6p27MYBlb2CO2KdqvTlzuOGXoVmJ1Xfnec,18595
+snowflake/ml/fileset/sfcfs.py,sha256=a77UJFz5Ve9s_26QpcOOoFNOBIKN91KmhYVTQkafn0c,15344
+snowflake/ml/fileset/snowfs.py,sha256=qwBbmMh5mGneO3scriERbdWJ8e717NfkUMXAz2Y4Dhg,6914
+snowflake/ml/fileset/stage_fs.py,sha256=yKMbPlc4chLG2svHKYy7tlefq6hy7kvnefHGunE7DPA,18408
 snowflake/ml/fileset/tf_dataset.py,sha256=K8jafWBsyRaIYEmxaYAYNDj3dLApK82cg0Mlx52jX8I,3849
 snowflake/ml/fileset/torch_datapipe.py,sha256=O2irHckqLzPDnXemEbAEjc3ZCVnLufPdPbt9WKYiBp0,2386
-snowflake/ml/model/__init__.py,sha256=fk8OMvOyrSIkAhX0EcrgBBvdz1VGRsdMmfYFV2GCf14,367
-snowflake/ml/model/_api.py,sha256=Y3r-Rm1-TJ0rnuydcWs6ENGdNp86T57PbmCWJlB0o0U,21595
+snowflake/ml/model/__init__.py,sha256=KgZmgLHXmkmEU5Q7pzYQlpfvIll4SRTSiT9s4RjeleI,393
+snowflake/ml/model/_api.py,sha256=u2VUcZ0OK4b8DtlqB_IMaT8EWt_onRVaw3VaWAm4OA4,22329
 snowflake/ml/model/custom_model.py,sha256=xvu7WZ1YmOdvuPePyAj6qMwKq-HNeVV9bNfkOT09CRI,8267
 snowflake/ml/model/deploy_platforms.py,sha256=r6cS3gTNWG9i4P00fHehY6Q8eBiNva6501OTyp_E5m0,144
 snowflake/ml/model/model_signature.py,sha256=UQSGieGJcnmC02V4feCYMdhMXnGoOUa9KBuDrbeivBM,29342
 snowflake/ml/model/type_hints.py,sha256=aUg_1xNtzdH2_kH48v918jbpEnHPNIn6MmfrwdvYvdg,12705
-snowflake/ml/model/_client/model/model_impl.py,sha256=QmTJr1JLdqBHWrFFpR2xARfbx0INYPzbfKWJn--3yX4,12525
-snowflake/ml/model/_client/model/model_version_impl.py,sha256=OQJab7XkFGBenuKw5_xqUibXhTU6ZUWTAjCghBooLTY,11160
+snowflake/ml/model/_client/model/model_impl.py,sha256=vyjPVoiEUFLW_XGz2tMXHBOk0_yDI5DgnDq3RTmQuW0,13623
+snowflake/ml/model/_client/model/model_version_impl.py,sha256=92uLtim0rZAngg1D0GSFNhPH1QDwZaT6jxOg_rRKcL4,17320
 snowflake/ml/model/_client/ops/metadata_ops.py,sha256=XFNolmueu0nC3nAjb2Lj3v1NffDAhAq0JWMek9JVO38,4094
-snowflake/ml/model/_client/ops/model_ops.py,sha256=Rc0jRRkTbJpymlKLOc1iK7xDoKwnP80sB6NjK1XhhLQ,20264
-snowflake/ml/model/_client/sql/model.py,sha256=bIw606G3GP0OQRwYKDywWEpZOIisQP3JjEoWVdTUvpo,5386
-snowflake/ml/model/_client/sql/model_version.py,sha256=YNngtSVrr9-RHlDMpF1RdxjHRNZPfQX14-KywPER2hU,10172
+snowflake/ml/model/_client/ops/model_ops.py,sha256=J96RRYHOxtO3ScQZYS176Mr4B1BDuWh7UwH-2cSIgng,23490
+snowflake/ml/model/_client/sql/model.py,sha256=dem_jSDQb16bW0U0PvtNbR48XEiceo-sKn86dNFoXBs,5687
+snowflake/ml/model/_client/sql/model_version.py,sha256=HKAiIWvqWtcADUSMXUtAISGj-z8AJFbxGSBackuo3N0,14290
 snowflake/ml/model/_client/sql/stage.py,sha256=4zP8aO6cv0IDrZEqhkheNWwy4qBuv1qyGLwMFSW-7EI,1497
 snowflake/ml/model/_client/sql/tag.py,sha256=RYvblBfQmK4xmLF0pz0BNUd9wddqlfHtEK1JRRpJGPE,4646
 snowflake/ml/model/_deploy_client/image_builds/base_image_builder.py,sha256=clCaoO0DZam4X79UtQV1ZuMQtTezAJkhLu9ViAX18Xk,302
 snowflake/ml/model/_deploy_client/image_builds/client_image_builder.py,sha256=G74D9lV2B3d544YzFN-YrjPkaST7tbQeh-rM17dtoJc,10681
 snowflake/ml/model/_deploy_client/image_builds/docker_context.py,sha256=7uhAJsHsk7LbiZv_w3xOCE2O88rTUVnS3_B6OAz-JG4,6129
 snowflake/ml/model/_deploy_client/image_builds/gunicorn_run.sh,sha256=1pntXgqFthW4gdomqlyWx9CJF-Wqv8VMoLkgSiTHEJ0,1578
-snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py,sha256=HnTaj0v27R9PCRuXpcP1nWv5tGBsXGSq6Xwep1m0bb0,9947
+snowflake/ml/model/_deploy_client/image_builds/server_image_builder.py,sha256=SNXqUBkI_tPAgdnLrQW10smG_7O_DGwAuK3dLFE-wJA,10095
 snowflake/ml/model/_deploy_client/image_builds/inference_server/main.py,sha256=Ltk7KrYsp-nrghMhbMWKqi3snU8inbqmKLHFFyBCeBY,11148
-snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template,sha256=WAqYQaaY5AFywg9yNLKRw350c2fpM4vxgdjYJ50VJJA,1752
+snowflake/ml/model/_deploy_client/image_builds/templates/dockerfile_template,sha256=8jYNmQfGw7bJgHCEd3iK9Tj68ne_x5U0hWhgKqPxEXw,1783
 snowflake/ml/model/_deploy_client/image_builds/templates/image_build_job_spec_template,sha256=g8mEvpJmwQ9OnAkZomeErPQ6h4OJ5NdtRCoylyIp7f4,1225
 snowflake/ml/model/_deploy_client/image_builds/templates/kaniko_shell_script_template,sha256=nEK7fqo_XHJEVKLNe--EkES4oiDm7M5E9CacxGItFU0,3633
-snowflake/ml/model/_deploy_client/snowservice/deploy.py,sha256=U0axqxy9YdJTsGz0bXSRSM2f7nziRnB83mvK6Rz9tlI,29141
+snowflake/ml/model/_deploy_client/snowservice/deploy.py,sha256=3Jn--iC5_dP_lJLWVShPvivH7EccT9AUWBX2tOavjYU,29286
 snowflake/ml/model/_deploy_client/snowservice/deploy_options.py,sha256=X4ncWgcgS9DKaNDiauOR9aVC6D27yb3DNouXDEHEjMQ,5989
 snowflake/ml/model/_deploy_client/snowservice/instance_types.py,sha256=YHI5D7UXNlEbV_Bzk0Nq6nrzfv2VUJfxwchLe7hY-lA,232
 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template,sha256=hZX8XYPAlEU2R6JhZLj46js91g7XSfe2pysflCYH4HM,734
 snowflake/ml/model/_deploy_client/snowservice/templates/service_spec_template_with_model,sha256=2SUfeKVOSuZJgY6HZLi0m80ZrOzofjABbnusUl_JT1U,540
-snowflake/ml/model/_deploy_client/utils/constants.py,sha256=ysEBrEs0sBCGHnk9uBX-IPZ_JA3ReRyyrDTFO_FNDPw,1841
-snowflake/ml/model/_deploy_client/utils/snowservice_client.py,sha256=R_ilt8SGwQR6qh_roaUvst0YrnjbJbAyxYIPn4efo4E,13284
+snowflake/ml/model/_deploy_client/utils/constants.py,sha256=Ip_2GgsCYRXj_mD4MUdktQRlYGkqOXoznE49oignd7Y,1696
+snowflake/ml/model/_deploy_client/utils/snowservice_client.py,sha256=k0SulzWdttRvJkyuXM59aluEVgQg8Qd7XZUUpEBKuO4,11671
 snowflake/ml/model/_deploy_client/warehouse/deploy.py,sha256=yZR9M76oh6JbPQJHb6t3wGO3wuD04w0zLEXiEyZW_tg,8358
 snowflake/ml/model/_deploy_client/warehouse/infer_template.py,sha256=1THMd6JX1nW-OozECyxXbn9HJXDgNBUIdhfC9ODPDWY,3011
-snowflake/ml/model/_model_composer/model_composer.py,sha256=ShoSp74xImfdXuIMTVJKt09sIBS8uxz-0rCbYBxLX9o,6337
-snowflake/ml/model/_model_composer/model_manifest/model_manifest.py,sha256=kvNMuL8L2Yvtbgf9wr_nly6DmL8wAkwT976rgdqRQPE,4722
-snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py,sha256=SCXyYZ2-UN_wcLZRM6wf2N4zy6ObpLsUwOxJBxhHXYI,2291
+snowflake/ml/model/_model_composer/model_composer.py,sha256=QSliEFs-wr9LguSTkQWIA9Nbw_9mOsHpUnanVsgV3Qs,7325
+snowflake/ml/model/_model_composer/model_manifest/model_manifest.py,sha256=5tMz0d7t9f0oJAEAOXC4BDDpMNAV4atKoK9C66ZHgvU,5667
+snowflake/ml/model/_model_composer/model_manifest/model_manifest_schema.py,sha256=PsRVrOt15Zr-t2K64_GK5aHjTWN4yLgixRqaYchY2rA,2530
 snowflake/ml/model/_model_composer/model_method/function_generator.py,sha256=2B-fykyanYlGWA4Ie2nOwXx2N5D2qZEvTbbPuSSreeI,1837
 snowflake/ml/model/_model_composer/model_method/infer_function.py_template,sha256=QpQXAIKDs9cotLOL0JdI6xLet1QJU7KtaF7O10nDQcs,2291
 snowflake/ml/model/_model_composer/model_method/infer_table_function.py_template,sha256=gex5if17PZ6t6fPcr2i_LO_3IRY03Ykcv_XAyKJt8pg,2170
 snowflake/ml/model/_model_composer/model_method/model_method.py,sha256=cr5soVDesBm19tjDG6lHLN6xrxj_uwPv1lKt8FgpM-c,6682
 snowflake/ml/model/_packager/model_handler.py,sha256=wMPGOegXx5GgiSA81gbKpfODosdj2mvD1bFbeN4OmNc,2642
-snowflake/ml/model/_packager/model_packager.py,sha256=WwF54Qu5Q-p6qGRcY7BzXNBFCRJRjUWFLpXiYnK7Uf0,5958
-snowflake/ml/model/_packager/model_env/model_env.py,sha256=MHajuZ7LnMadPImXESeEQDocgKh2E3QiKqC-fqmDKio,16640
+snowflake/ml/model/_packager/model_packager.py,sha256=6YQkmE5LCYIni7bKLMc9yDyS_ozdWuvExh5Wt7Ez2uY,5836
+snowflake/ml/model/_packager/model_env/model_env.py,sha256=3FTftb2OMqCjushFLBISbF6E4z2CQ8G_rNewf-ahVGQ,18312
 snowflake/ml/model/_packager/model_handlers/_base.py,sha256=-FfoDfULcfFRizya5ZHOjx48_w04Zy4eLEqOOrQIDHM,6033
 snowflake/ml/model/_packager/model_handlers/_utils.py,sha256=KKwS93yZnrUr2JERuRGWpzxCWwD6LOCCvR3ZfjZTnyQ,2622
 snowflake/ml/model/_packager/model_handlers/catboost.py,sha256=FC0Yw2QDknaR9jdzncTI4QckozT-y87hWSHsqQYHLTs,8142
 snowflake/ml/model/_packager/model_handlers/custom.py,sha256=y5CHdEeKWAO08uor2OtEob4-67zv1CVfRf1CLvBHN40,7325
 snowflake/ml/model/_packager/model_handlers/huggingface_pipeline.py,sha256=Z7vZ5zhZByLVPfNdSkhgzBot1Y8UBOM3ITj3Qfway3A,19985
 snowflake/ml/model/_packager/model_handlers/lightgbm.py,sha256=PWPdpOdden2vfloXZA5sA20b2dCBiGO1-NfJ8atH-Uc,8445
 snowflake/ml/model/_packager/model_handlers/llm.py,sha256=SgCgy9Ys5KivNymjF35ufCpPOtMtSby2Zu4Tllir8Mg,10772
@@ -112,15 +122,15 @@
 snowflake/ml/model/_packager/model_handlers/tensorflow.py,sha256=ujBcbJ1-Ymv7ZeLfuxuDBe7QZ7KNU7x1p2k6OM_yi-0,8179
 snowflake/ml/model/_packager/model_handlers/torchscript.py,sha256=8s8sMWQ9ydJpK1Nk2uPQ-FVeB-xclfX5qzRDr9G1bdk,8104
 snowflake/ml/model/_packager/model_handlers/xgboost.py,sha256=x5bXz5DRzb3O7DMDOF535LBPGnydCa78JHP_7-vsnjY,8874
 snowflake/ml/model/_packager/model_handlers_migrator/base_migrator.py,sha256=BZo14UrywGZM1kTqzN4VFQcYjl7dggDp1U90ZBCMuOg,1409
 snowflake/ml/model/_packager/model_meta/_core_requirements.py,sha256=zObSLyhu56hMnIfdv7PMkzHJrTP3-FAroNZ6-Rji7J4,274
 snowflake/ml/model/_packager/model_meta/_packaging_requirements.py,sha256=TfJNtrfyZoNiJZYFfmTbmiWMlXKM-QxkOBIJVFvPit0,44
 snowflake/ml/model/_packager/model_meta/model_blob_meta.py,sha256=qwgBneEA9xu34FBKDDhxM1igRiviUsuQSGUfKatu_Ro,1818
-snowflake/ml/model/_packager/model_meta/model_meta.py,sha256=UnmTBsWDbvW0iJCkXNYJG2J7qEehrvS3Ds_3G-P7VRM,17266
+snowflake/ml/model/_packager/model_meta/model_meta.py,sha256=0bzons03s0cF2RxbtxS7rPGeZG_Z8BouehqJPd3pfH8,17203
 snowflake/ml/model/_packager/model_meta/model_meta_schema.py,sha256=8eutgCBiL8IFjFIya0NyHLekPhtAsuMhyMA8MCA9VOQ,2380
 snowflake/ml/model/_packager/model_meta_migrator/base_migrator.py,sha256=SORlqpPbOeBg6dvJ3DidHeLVi0w9YF0Zv4tC0Kbc20g,1311
 snowflake/ml/model/_packager/model_meta_migrator/migrator_plans.py,sha256=nf6PWDH_gvX_OiS4A-G6BzyCLFEG4dASU0t5JTsijM4,1041
 snowflake/ml/model/_packager/model_meta_migrator/migrator_v1.py,sha256=qEPzdCw_FzExMbPuyFHupeWlYD88yejLdcmkPwjJzDk,2070
 snowflake/ml/model/_packager/model_runtime/_snowml_inference_alternative_requirements.py,sha256=urdG-zCiGWnVBYrvPzeEeaISjBDQwBCft6QJXBmVHWY,248
 snowflake/ml/model/_packager/model_runtime/model_runtime.py,sha256=Hnu0ND3fEmuI29-ommNJdJRzII3tekHrU4z8mUEUqTk,5872
 snowflake/ml/model/_signatures/base_handler.py,sha256=WwBfe-83Y0m-HcDx1YSYCGwanIe0fb2MWhTeXc1IeJI,1304
@@ -133,240 +143,238 @@
 snowflake/ml/model/_signatures/tensorflow_handler.py,sha256=VZcws6svwupulhDodRYTn6GmlWZRqY9fW_gLkT8slxA,6082
 snowflake/ml/model/_signatures/utils.py,sha256=aP5lkxiT4lY5gtN6vnupAJhXwRXFSlWFumIYNVH7AtU,12687
 snowflake/ml/model/models/huggingface_pipeline.py,sha256=62GpPZxBheqCnFNxNOggiDE1y9Dhst-v6D4IkGLuDeQ,10221
 snowflake/ml/model/models/llm.py,sha256=ofrdHH4LQEQmnxYAGwmHV2sWLPenf0WcgBLg9MPwSmY,3616
 snowflake/ml/modeling/_internal/constants.py,sha256=xI4ofa3ATQ2UszRPpkfUAxghV_gXmvxleqOew4UI1PM,45
 snowflake/ml/modeling/_internal/estimator_utils.py,sha256=ajRlCHvb4a-rGzMAVvtKhEE5ijObzW7YA_Ox5u2Orr4,9215
 snowflake/ml/modeling/_internal/model_specifications.py,sha256=nAqgw7i1LcYMKRQq9mg2I50Kl0tsayh2_do5UMDXdT0,4801
-snowflake/ml/modeling/_internal/model_trainer.py,sha256=AlnTRnIowaF39Qjy2Zv4U3JsMydzCxfcBB2pgLIzNpk,694
-snowflake/ml/modeling/_internal/model_trainer_builder.py,sha256=0zazMgVNmBly7jKLGEwwjirb6VUsmA5bnplCzWxfTP8,7269
+snowflake/ml/modeling/_internal/model_trainer.py,sha256=wLAfgWjwWXj3dqhyzZLCJVYSSgujq6zrYBa4q0pw_II,923
+snowflake/ml/modeling/_internal/model_trainer_builder.py,sha256=11cpEaxU1D7R7m79nVLcCA9dryUPsElS7YdlKZh850U,8422
 snowflake/ml/modeling/_internal/model_transformer_builder.py,sha256=Y6Y8XSr7X7xAy1FvjPuHTb9Opy7tnGoCuOUBc5WEBJ4,3364
 snowflake/ml/modeling/_internal/transformer_protocols.py,sha256=adbJH9BcD52Z1VbqoCE_9IexjIxERTXE8932Hz-gw3E,6482
 snowflake/ml/modeling/_internal/local_implementations/pandas_handlers.py,sha256=xrayZRLP8_qrnfLJE4uPZ1uz0z3xy4Y5HrJqM3c7MA4,7831
-snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py,sha256=QuXUeeFzktfxStkXFlFSzB7QAuaTG2mPQJVBlRkb0WI,3169
-snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py,sha256=y9PZ3xgPGDHPBcNHY0f2Fk0nMZMRsPcLWy2cIDTALT4,4850
+snowflake/ml/modeling/_internal/local_implementations/pandas_trainer.py,sha256=MyTRkBV3zbDeO7HJaWrKYT3KkVqW51Q0AX2BbUtN4og,5737
+snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_handlers.py,sha256=fgm1DpBBO0qUo2fXFwuN2uFAyTFhcIhT5_bC326VTVw,5544
 snowflake/ml/modeling/_internal/ml_runtime_implementations/ml_runtime_trainer.py,sha256=lM1vYwpJ1jgTh8vnuyMp4tnFibM6UFf50W1IpPWwUWE,2535
 snowflake/ml/modeling/_internal/snowpark_implementations/distributed_hpo_trainer.py,sha256=baETLCVGDcaaGXwiOx6QyhaMX_zQ1Kt7xGjotd_MSKo,54368
-snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py,sha256=8rVe366RSYYYAwGSqhKyxZYhW3nAqC7MiTucnFLvNQM,13616
-snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py,sha256=w3zCrv-TwDB1o9eFppMaiXWmMeEPz_EAn_vl_2_6GL8,21699
+snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_handlers.py,sha256=AgNupOLqXJytkbdQ1p5Nj1L5QShwi8PSUSYj506SxhM,14539
+snowflake/ml/modeling/_internal/snowpark_implementations/snowpark_trainer.py,sha256=MYLvpNscLKI0AOuJIdKprtw3VinO0MtSVPj376k-ILo,31819
 snowflake/ml/modeling/_internal/snowpark_implementations/xgboost_external_memory_trainer.py,sha256=VBYWGTy6ajQ-u2aiEvVU6NnKobEqJyz65oaHJS-ZjBs,17208
 snowflake/ml/modeling/calibration/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=jUlsmletPjnV8KYLJIoRkd8JrDO33F7VlEf-xpxXZ2s,51017
+snowflake/ml/modeling/calibration/calibrated_classifier_cv.py,sha256=AGQub8A5L_xTB1gEJsbzTSZdsISnhdsAp3OmbEwRutw,51278
 snowflake/ml/modeling/cluster/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=afp4K6VIXjph1oDxEDHbG_fHBlGo0qwM2ZXjl9Pe_90,48846
-snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=rmExcHQw4rkNruTRclVNfiGrcqqURsiYk3x9TO-IAqA,50883
-snowflake/ml/modeling/cluster/birch.py,sha256=DCK5o2TSPwmBxxxZYgQsGuwIOErYu9jUAFnhHdTKJqY,48571
-snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=731dOd6cvOVDL2IFmY3yoPvUe-GvU7qdUdolCiZDhg0,51270
-snowflake/ml/modeling/cluster/dbscan.py,sha256=A9C_8_BB-KPvlvNtdVcHuMhga6pPG_IW-r8VwKdu3lE,48933
-snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=Q5gMSa-W-j9WMhYLcQDQVnI6ErcKYla_yavs9I9GTxs,51401
-snowflake/ml/modeling/cluster/k_means.py,sha256=g2H5DwxAcmBfzs3dyHgNmkLLMqYNuerR-o0QWFTcQyc,50826
-snowflake/ml/modeling/cluster/mean_shift.py,sha256=ycVAbdL14g_lR3gUlgaj0v1Lt8_eoJcrsXLySav1Vq4,49143
-snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=OOhW2gMlljzJWtI_61psJ2LOx95YSms2IDKnsVIHS0A,52188
-snowflake/ml/modeling/cluster/optics.py,sha256=EngSMXX1eTLKJCmDZ8KjtBZElpnln_rnGyLnyHyaZ6k,52247
-snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=bk2PMHOMnJG6h0zSLyJplUKwGxmBjxdGNSHQBIXTpVU,49142
-snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=VZXmEOcb-E1o894iBhnFqqjMkShBEql5U2lb1OFkhTs,52334
-snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=rKhNgIitjXGupWfFqYjimk49nCXYS_j3I57m-OHpcso,48275
+snowflake/ml/modeling/cluster/affinity_propagation.py,sha256=YeMtuQvE5f2InBc7CyAnlFf0hGy0Okz5X09AQ9C64bI,49107
+snowflake/ml/modeling/cluster/agglomerative_clustering.py,sha256=g_-psPSkpQt05ryMQFeS_GndA9hB8Dkg12ao2s7VNoQ,51144
+snowflake/ml/modeling/cluster/birch.py,sha256=0zAT_k-ZgnaGFfcZu5XGll7kAH8BZ8lFCTajWFYmV2o,49034
+snowflake/ml/modeling/cluster/bisecting_k_means.py,sha256=l9nxeYVGqf11msu1i6iG12i8YiWD2uZiVtH4WXDDzjI,51793
+snowflake/ml/modeling/cluster/dbscan.py,sha256=qAtH_LxcmVg7B8NerncUsfBoKiUJwb2N1FYiBM8olxk,49194
+snowflake/ml/modeling/cluster/feature_agglomeration.py,sha256=uPo3I-ROixH6SsWknj7f6AaEYLybpxHs1M4LKTkouRU,51909
+snowflake/ml/modeling/cluster/k_means.py,sha256=fiFQSvRRwPbQp8b9UhOeZ97hRTn-HZN4aRGXlehnGyU,51322
+snowflake/ml/modeling/cluster/mean_shift.py,sha256=4H7iQtz-tqhUZVjecoWIk7Z5dtYuWJVD3sy1R_xL2DI,49404
+snowflake/ml/modeling/cluster/mini_batch_k_means.py,sha256=DUM05k_RNIeBenVXtssGnqEerp0OAI0z11k-GO9n7P0,52711
+snowflake/ml/modeling/cluster/optics.py,sha256=-j_q6rcEEzkO9m6owloMRBWUnrCCZK1QUgBUo7mlYYo,52508
+snowflake/ml/modeling/cluster/spectral_biclustering.py,sha256=kWW9zsBhQH6KpLFuw2htgEYolhy8fpQZXdggWx33hAg,49403
+snowflake/ml/modeling/cluster/spectral_clustering.py,sha256=CCZrM9Z_5BJEl3pywVOvCO8vOGGdfPLc7qtP4sdVkoQ,52595
+snowflake/ml/modeling/cluster/spectral_coclustering.py,sha256=nUrqHjI3pDC21OhHsaFoJ9jrCa5nY_a-EdHDW_XWb8U,48536
 snowflake/ml/modeling/compose/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/compose/column_transformer.py,sha256=JM95YHKS1l7SPWIGjtkCfQefMo31X5idL7-RhFAiD8A,50841
-snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=9dwmnkydmJogcBcxn5ChMvYMQ9iaEySthbbwACg0sns,48829
+snowflake/ml/modeling/compose/column_transformer.py,sha256=Js74mW_UUGrT0W7ZmBtiF-4a3dLvppgSmMxB7AeitgI,51374
+snowflake/ml/modeling/compose/transformed_target_regressor.py,sha256=eCLZdS6zYgyPUQy_9wnA13s1rPE_9U4oX3qNY8VF6tM,49090
 snowflake/ml/modeling/covariance/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=4FesJyaTg4p7QEDA146Yah6qa6pITAydu3bjrWXpR9s,49167
-snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=MnAbsL383FamDGP-w75j0XNyKyWGZJVeiFox1IrnN18,46975
-snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=Wsx7XXEbk2mLihAGLD0-8JlDUt1PzU5ScNibIaPBYo4,48839
-snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=tpaXIOw2rcbn7PhTRLXPjYNZ8w0_YHOkZ44AYLApp-E,50004
-snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=1DjNUC6vyafLkS-FBByGJX2daB8GwV-nx_gp2MPoSTc,47113
-snowflake/ml/modeling/covariance/min_cov_det.py,sha256=4LRTnoNaQ8QMvLZ7rfV00oujVKKY9CqmfRBaX88Rw_g,47868
-snowflake/ml/modeling/covariance/oas.py,sha256=b5aPAfCQZIW4VwRLIj06OcVPOABFx7wHwYxX89-bVdo,46754
-snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=t2faZLN_pr6tqN48gQvkaVBNuxq1PwIVroeE07CG-ws,47130
+snowflake/ml/modeling/covariance/elliptic_envelope.py,sha256=awX5rRrXDb-o4CPl2JFXO7BWt4LBLlYEwW3Wo9TLeFQ,49428
+snowflake/ml/modeling/covariance/empirical_covariance.py,sha256=hIRqpixvv2XPwkoDohmjKnGzXaEnFaEx6M-08CJJlYQ,47236
+snowflake/ml/modeling/covariance/graphical_lasso.py,sha256=-GssWNHzC2DMZuMChEGUzHP8cvOTQr1RYXr4r0rBFPo,49100
+snowflake/ml/modeling/covariance/graphical_lasso_cv.py,sha256=7zobpOYlEn45h4JymakbzRgxOeqUpeZdFqKy3bdZmJQ,50265
+snowflake/ml/modeling/covariance/ledoit_wolf.py,sha256=Ns4vqx2OD2Y9pgs1tithBwej1E0E_6wVlXIRS85mTqw,47374
+snowflake/ml/modeling/covariance/min_cov_det.py,sha256=6hK8ePqINpDEEqUi5bM6FPOLWGhk-9ePVJaG82huQIA,48129
+snowflake/ml/modeling/covariance/oas.py,sha256=P64GRhbVHGhZ1Od-n238do_tZQ5cO3SXRqVoqclLcl4,47015
+snowflake/ml/modeling/covariance/shrunk_covariance.py,sha256=yPwbsZ7_Tlj-Sj6CiayG9ic3J6G_yuGZtu9C2V0x1aw,47391
 snowflake/ml/modeling/decomposition/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=LY-6TJJ9_m822mCAzkH-FqyI2LtGpOKg-pwfz_UViUQ,51838
-snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=J-c6ya1twEGf9oZnGWOruCZwMrDVG23sFesbdK2Thpo,49543
-snowflake/ml/modeling/decomposition/fast_ica.py,sha256=x6wJ-wKbLFAfwHgiacDbbyJZj82zz3LVfFDRUaYVAt0,49484
-snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=Isda0dKytZ4sWkj276Sfk7u4obj1ViRWmINfMDu1-8Y,47837
-snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=de5vTOkvpQebwJBGFzKWN3e4eDANiHcvYtNwUAsjxWc,51836
-snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=rceFeLfNzMkZJ7ZNM_D8rI6Z3Af6-4aBXYAZpY5-8Qg,52882
-snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=9uvLahexV7VCH6KD3i6DchRoulOOtK9IBpebB-NEZVE,50174
-snowflake/ml/modeling/decomposition/pca.py,sha256=xOWvnQt7EFWjxB5HLc_61-k8t_d33fFTaD64NhCXais,51103
-snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=U52ueZYP3dsrGY8EgAiCWwXyHb7ZDGlx_YEBx6Sg9yI,49006
-snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=UOsph316mxYS6SsiVsM1hMkWF_X_jz2q4rYfkHQ6Wi4,48594
+snowflake/ml/modeling/decomposition/dictionary_learning.py,sha256=Glb9UBBcuweG4bzgYK8r0kb4VQqk795L58C4J8JnoQM,52388
+snowflake/ml/modeling/decomposition/factor_analysis.py,sha256=-ip83Gh3nKzkFGceq602ACdK0Fbo2989_Qr2UEGsSDQ,50051
+snowflake/ml/modeling/decomposition/fast_ica.py,sha256=RxhD4aZFNN-Y4_wNnSYS-Mxle--_XWvTxMMAZV5SLJI,49985
+snowflake/ml/modeling/decomposition/incremental_pca.py,sha256=-sbUNsBgn6NoLgmBME-cfoiwfaNx6unDN6Tr_tQi5sw,48345
+snowflake/ml/modeling/decomposition/kernel_pca.py,sha256=KWSFgSYQJS6hZcUksKuSuxE_L31NAPbHvEmQNCajcHY,52343
+snowflake/ml/modeling/decomposition/mini_batch_dictionary_learning.py,sha256=xHYcxU8q5q7JirOlR9duL7cxQ7j0P41LhJTnJs6X6Fc,53429
+snowflake/ml/modeling/decomposition/mini_batch_sparse_pca.py,sha256=Zf0ZWiUDu6UsmWWaNrvi1X87QIPpJddGOvuNDV-FtMo,50694
+snowflake/ml/modeling/decomposition/pca.py,sha256=-m6Fh_6Sldf-70_B_d8J7FAlXG-FLvEnIgsk63sTu6E,51612
+snowflake/ml/modeling/decomposition/sparse_pca.py,sha256=FTaRx6CbM5WYYGQbCvccR514uzIWsBZYIX5Ql_ofNkM,49499
+snowflake/ml/modeling/decomposition/truncated_svd.py,sha256=RiOmwnQpNx2MU0Z6wZ0pIUzsKfjDzJeWrY9zid8n6Bk,49122
 snowflake/ml/modeling/discriminant_analysis/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=2j10IY2v_XN-Couke3gXHuKYYxBfaGan9Td3dPv9cfQ,51309
-snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=0p1j-2tHGF63iNRH9Cu7Bpvx3F_Wnu7xZvSbGgixz3w,49398
+snowflake/ml/modeling/discriminant_analysis/linear_discriminant_analysis.py,sha256=p68gHC7ur53Qp7MMnPyc2FSgWBjDmo10guNtbo0MARc,51877
+snowflake/ml/modeling/discriminant_analysis/quadratic_discriminant_analysis.py,sha256=C8T-Ol8s20nx-IeZMs7KXH-BjGRyNtG3exXQn-cbNMc,49659
 snowflake/ml/modeling/ensemble/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=VgxneH2u795BnWhSibz0WsFx6CrS9HNa5G3vDXLb1HA,50216
-snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=3ZD9rt4_BIb5UY5627iw2sAQ9nclCrr0gAERdJ-FUUY,49107
-snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=n-w6SXU9GAyiRzvVQzf41JTFuhMZTiNA2gfMAQdIwY8,51127
-snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=ba0Qk0sq6LkjyIMMYj4imuX3OGvoQVOZQ4HbSNq1JoE,50363
-snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=ao2055ldja5mNCArNlQiI7y5-EUrWNXpZs3oJ6DA5Lw,56047
-snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=wJ87KI6Zrcg1i8AULKYPJVwrrejrdZ5nSJp9G0-rnzk,54651
-snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=zwoNi8NL6kmAd5HuYmbKUC6cTr2q_5YX-eVf1xLyReg,57502
-snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=GNHEy6qKoj_LvbBXQHi9XWmroyvA037leP88ZWgYaKk,57095
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=fWpMe1KGXkgTyTtGZHCsy_CdIxUXYt5qcfqM-LrRHb0,57336
-snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=CevblPb_gRICCWI_-9-WX0bUU8xfHfzcOaNMgLcP2dE,55821
-snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=VxAodKDVI2zlGlDguAjcttkUIsCjbFMuv3buUkIIYTw,50315
-snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=cQPdeI3Z3pxFnEjy9qkOG3JTp--B1jogDZ22tlmDKgk,56030
-snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=gmDcsGH4Nfo1y-b5ISwBT9yfS1Qr6nImy4OYhlflkuo,54622
-snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=uvy-Lvv-JMKJB5-FBNSabuAKD5B--QFknm2GhLt0e8M,50060
-snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=hLDRXzIg3Ub-xWU5gTK2Qgyx2ojDS3sBGLy8mtORCd0,49631
-snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=0FFUoxzg0hcGjDlrU4wxFJZ3RWcoPulziiwT0WI43xQ,48160
+snowflake/ml/modeling/ensemble/ada_boost_classifier.py,sha256=2pfHIBCvqZSw3p1b_KqqQPkZ9s-JfndDDS-s3l2lnww,50477
+snowflake/ml/modeling/ensemble/ada_boost_regressor.py,sha256=wu_4FpEKU-bVZDx5OPjDcQDR190dcS_farXa3kMWGTE,49368
+snowflake/ml/modeling/ensemble/bagging_classifier.py,sha256=L7bVu-soVZJIOV9qm6IPbh3sYLNj-R_bYmPCEiz_qAE,51388
+snowflake/ml/modeling/ensemble/bagging_regressor.py,sha256=uQ2ibCiPCdQUS-h51H-TPM5KP9UzMsrUWfDrGQ4pdlA,50624
+snowflake/ml/modeling/ensemble/extra_trees_classifier.py,sha256=-fZBhcMXhcld8ZckIlee_N4GVd84ojcOze3hIPHXdlU,56308
+snowflake/ml/modeling/ensemble/extra_trees_regressor.py,sha256=KVGJbZVHdNFvXy0-H3-_hqLj8BVZM0kUSJ8Ssrm7Cr8,54912
+snowflake/ml/modeling/ensemble/gradient_boosting_classifier.py,sha256=Ra4pfsXH2qniulcS4oOC1HGI8yu043vaEwawisuAn5o,57763
+snowflake/ml/modeling/ensemble/gradient_boosting_regressor.py,sha256=SXGVyiFEpOHcAMzyxlOqUcsmGFQ-iqMi_Vj7m-m_r1g,57356
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_classifier.py,sha256=77kepZu2p40ZEJe-Hlwwr1YWl9vJccFoz07CzkWZY_Q,57597
+snowflake/ml/modeling/ensemble/hist_gradient_boosting_regressor.py,sha256=ze6-uVYgAMJueZXMADb6n9AYRXYxUU5Bealob8IZ2oA,56082
+snowflake/ml/modeling/ensemble/isolation_forest.py,sha256=GTQF6U-Z84a5ojKdwviEv_ewtCK2wbL6-YXZwxfrLPc,50576
+snowflake/ml/modeling/ensemble/random_forest_classifier.py,sha256=y5zD_vjuZ4Yw_fDoBtEMVTN8m6lSLiOSlfWOI1x816k,56291
+snowflake/ml/modeling/ensemble/random_forest_regressor.py,sha256=0uH82Fs1n75hfU3uX6ggCZsDFx94_DtURqbsm3z-8lw,54883
+snowflake/ml/modeling/ensemble/stacking_regressor.py,sha256=dP6WDh0_YYZeGZVBvUQWN-hjKMqDSwVVc98xE8pg88U,50602
+snowflake/ml/modeling/ensemble/voting_classifier.py,sha256=I6t3h-qId9TykuvTgwfTNR2Q3T_NhZN_4KakmjadUG4,50155
+snowflake/ml/modeling/ensemble/voting_regressor.py,sha256=PDoRDW8vBXPDX73WjUtW-xXNnn8BG8omJhrEr9t1PHY,48681
 snowflake/ml/modeling/feature_selection/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=iEI1NAJRAYWbvV7W0JxCzHHxfTS4wHra7K4bas7lMYg,47471
-snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=s12RZaRVjthJPtF4gKTADmrAdUxmlDsoyy-SOhEDosY,47114
-snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=2yugXuswa1eryGuj5cEFTnLh78l4igRrX3QqqFRJ52c,47108
-snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=mBxNLyPeu6EC2YxELsAllAHAoDMw7g9phh-Jdo_qWC8,47116
-snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=ArGC7MVPil3_l-WEcFofOespzYuzqzAIjKpSJ6BOamE,47201
-snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=sodp-xs8NoBuveCiUlpOt6AW0cwFSQlA0SCqnIQqOwo,47241
-snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=3a9I2j7_EHTXnL4IuIYpIqzgeUtaHp__rfO5chONW_8,49834
-snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=H_S5UhsRwYRedSSx1c0fJnKcxeRmKLT5EU6kWJ8rDm0,46794
+snowflake/ml/modeling/feature_selection/generic_univariate_select.py,sha256=VtYpm0z7tvr-ksYHgk6LZWDjN7UFqHLqtEsnG51O_W8,48018
+snowflake/ml/modeling/feature_selection/select_fdr.py,sha256=MxADr7gOnVj5V3YRQytXmieXpOzybZK6r37QdRpxOoA,47619
+snowflake/ml/modeling/feature_selection/select_fpr.py,sha256=QSF8zO9qzVv7auA4yBz1jkTqEA1mL_C3tSx93xUufMU,47613
+snowflake/ml/modeling/feature_selection/select_fwe.py,sha256=9gpQbytZwQjPBREld8r-p_bG7sZm3h7nQz4bgH7UL7k,47621
+snowflake/ml/modeling/feature_selection/select_k_best.py,sha256=wZyd-94tGP-FzK85b4dFPzncx0BpfbLME9Mjr5vw2ns,47712
+snowflake/ml/modeling/feature_selection/select_percentile.py,sha256=hnTQ5DtNot9OJah20V-tpkXAzLSbRfJjWhv2gxH11gc,47767
+snowflake/ml/modeling/feature_selection/sequential_feature_selector.py,sha256=-0ZEZboABCnbfguy1YmopbjVZ0-eut24E9XaPol97Sk,50387
+snowflake/ml/modeling/feature_selection/variance_threshold.py,sha256=KwTTDc1WLWz6SiXWz3Bks7u-zP53kdqPRLB3RHtvodw,47323
 snowflake/ml/modeling/framework/_utils.py,sha256=7k9iU5zAWa4ZpMZlg8KfSMi4vH3o69w5aAh5RTRNdZ4,10203
-snowflake/ml/modeling/framework/base.py,sha256=6ebxZIkUfDsLcEufokyN7XVKyvfvjhys3pFsUyQtfQ4,30470
+snowflake/ml/modeling/framework/base.py,sha256=K6qW43lGX99a5v9qIhOTJptPrkMSzzZDa5sVgqii0dM,31359
 snowflake/ml/modeling/gaussian_process/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=xoaQud9cg8-qNG65Zf1KkJdbcuBp8lR9uYxU7M5yhrE,52768
-snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=t_UPbZ9uR9mIYgoNlcBR9JEpcCBY_h3EzpmWgQV_nSg,51833
+snowflake/ml/modeling/gaussian_process/gaussian_process_classifier.py,sha256=1nDZ_nda4WBHqi4tW4avY2b0fNwV5ZF-DI1-BwQjtNg,53029
+snowflake/ml/modeling/gaussian_process/gaussian_process_regressor.py,sha256=zkHptpQKCEGJspDnZtNTBswzn-0wVDrOJRJ34ach8dM,52094
 snowflake/ml/modeling/impute/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/impute/iterative_imputer.py,sha256=ShzrWrfRHN7U5OsWWpRnB_-yRe6i5pKCZWIOdCIVwJs,53331
-snowflake/ml/modeling/impute/knn_imputer.py,sha256=oTwdqdnsDfdLcx9nb6Jqe9H71B-VutU0vaZrj8UrxaQ,49088
-snowflake/ml/modeling/impute/missing_indicator.py,sha256=v0fSUO9hoQ9zk7121ZZhr8i1bo483IlHPcyZdMwkATo,47916
+snowflake/ml/modeling/impute/iterative_imputer.py,sha256=Y3yPVO6EFqK-ZBxe_55Gp7To2jIt0N01HTyAyBkpVkI,53847
+snowflake/ml/modeling/impute/knn_imputer.py,sha256=0-YnL5I2aX9BlLDe0BcyUnYdZbbmNX4_Kxj-rcJ61DA,49563
+snowflake/ml/modeling/impute/missing_indicator.py,sha256=gWz2h46qVp7wVqpKa2pyQtO7Mw8bKzEy4M4gWOubiXc,48420
 snowflake/ml/modeling/impute/simple_imputer.py,sha256=awM33HugS5jGs3JXud1U8eEMm2VLdIAf7z_eVXAzKD0,18499
 snowflake/ml/modeling/kernel_approximation/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=SqtQEX4ez9km91JDYN2NcIHEAKT8lxKkEWlZX8HM9M4,46907
-snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=SZRx5NieE2Skj_6FvqiBXNQ3neaB3F3U874Avk7P3Zs,48711
-snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=eO2fS91GkcHfWTyxFNRQVdtPhNd21AuV1myUpks_Uiw,47920
-snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=c87jMzeIguq_CPsXN05hX5pKhU0WDLxEgOk3dsEJcR0,47307
-snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=BpOYKneZk8jwklR2hopITgnA3WVKBk5fgunzO1aonEw,47334
+snowflake/ml/modeling/kernel_approximation/additive_chi2_sampler.py,sha256=pFXnaevash3T9li0RrHqvOceWZZ0TdgyZ20ftfwe6Vc,47451
+snowflake/ml/modeling/kernel_approximation/nystroem.py,sha256=JyMxxr-2uyOnimXM8UzNnT9lV3rGgTiOczp-7cy8uP8,49222
+snowflake/ml/modeling/kernel_approximation/polynomial_count_sketch.py,sha256=H1nXaQ-jmyh5UID2MrZMiJugkGZu4l33qgrYk8PtXJU,48470
+snowflake/ml/modeling/kernel_approximation/rbf_sampler.py,sha256=daDnxgESQqUWC1ZfzguTK9x715n24hWfC4DtwP3ddQw,47824
+snowflake/ml/modeling/kernel_approximation/skewed_chi2_sampler.py,sha256=9_3V5uZ7hR1wgsg4LXPnhofTr7urOoMIOYznEcwziXs,47872
 snowflake/ml/modeling/kernel_ridge/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=bvaF-kDn4wWUhijpH-J2IYhvdVjKWqJ3rSt4SQtO-tc,49147
+snowflake/ml/modeling/kernel_ridge/kernel_ridge.py,sha256=j6ZRziTOvZK2IlHKC9a8HAdnGJnLUtg4WEbpoZzW6Dk,49408
 snowflake/ml/modeling/lightgbm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=DTK5Ja5jNtZueOCt_9_-tPWJCH7K9LS8qIG6Olpq8I0,48715
-snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=LgVfIPNhWLkvpwYcNmcSoh438EtfxMyc2rqf5nxQiK0,48218
+snowflake/ml/modeling/lightgbm/lgbm_classifier.py,sha256=O05uhJd9w8VJzmJdSVesTxVjvnqXO0FPxid7HtWgJPo,48976
+snowflake/ml/modeling/lightgbm/lgbm_regressor.py,sha256=BgRHeYkUHa3eLDfyTjVAV7aG-munFAzU1t2IVMnVu18,48479
 snowflake/ml/modeling/linear_model/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/linear_model/ard_regression.py,sha256=MBbPZEbrVKJFgVa4X_JWK7gOY5oyqJUJkA40uRHTA68,49092
-snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=-bMiotMYqoLRAd9qVkFFDPJKjJqKHoH5vDX1-rrpAMY,49508
-snowflake/ml/modeling/linear_model/elastic_net.py,sha256=gqv2mrTmcWrcxF2XDDoHEq3WQv2P5FwDLXz82hwseRE,50080
-snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=5ZtqlSMQTLP_DyUMPdrvPzgR40RvGW5iLnC2WP4qlyM,51348
-snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=p4oOBdWGqtoqrHxzhi793ST-MTs95IjW1k7gvwTWOX4,49160
-snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=pnkGNWrHDphyan52lFo7qfqDLqktq5QM7i7kd8Lz0SA,48357
-snowflake/ml/modeling/linear_model/lars.py,sha256=Dg43PU2uhy-Z0TneRgc1EpaKGOKMc7BVHdJGEaYvVNQ,49580
-snowflake/ml/modeling/linear_model/lars_cv.py,sha256=Vt_ll0a0Tgjf3ZZAHzssb-YN8iTti0CeX4JEmtjGAm8,49801
-snowflake/ml/modeling/linear_model/lasso.py,sha256=9789pUfQLG66ox4EaM4-W3-ay801yz4w8HxduGyRKi8,49686
-snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=TlzF3JRIdBqO1SZbmLvEe5GE_f2KaIiQBbw1ypar70E,50471
-snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=te8RfcaK3Fd493Vw4P7rh0Cq62Ow6GqePgEcePE7I58,50716
-snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=nVCYsE3kFLkeDov521rmusmdbqJtADFczI3DgciazIE,50677
-snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=ZAX7IbjPYwYNg8TO7Vwf7_rc-JZxqtAHOCNwqvXbFxg,50023
-snowflake/ml/modeling/linear_model/linear_regression.py,sha256=rjiPOnQrpzbBpH4v5z5lPDA9Q_M_zI9bGaX30iNwD30,47899
-snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=MiJcM0p2SZEv73NgCIym71ljk5F5rckyNSKXrdyKZQ0,54154
-snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=CNnWwT6pKuVKLVkiuYTMKDeVpYlz4sg4JOJv6mlEEng,55194
-snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=fg7lgy8vwQCaubDnxFB97Vgj0i_Jt9T2XG7q9eWRdwY,49371
-snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=qBdIWIkpZ7IKjimbx05Uj72Inh3Gwu_Wbv8vnctNGKk,51009
-snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=lIBHh_XaHP9DuJEOslATxa2VseaE47WI--Eo_5XBf0M,48919
-snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=Ki_CLjIKUzSHavZDwpmkoeRmIUmRHvp_Rz1fqYpFZk4,50181
-snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=jBRIMM96qihVhRkz4WqYSXQIhKvDGblwSN0pZSMh3XM,48526
-snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=TmKUYInBA5K8hURL7fjGKwGZ45LhiKUllyQzqlth1ps,51882
-snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=_H9FFj9TS8JrOfgF2g6TH_YBwxUirQMv5MW9LayfKgc,50949
-snowflake/ml/modeling/linear_model/perceptron.py,sha256=r_iWf0r1H0uUWwJuVL1ln3-1odN8LLBnzrd4LEhQNTs,51266
-snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=CKXD2gLEsz8V5Xi7sKyAftiS352LpLYXyFu08Oinw08,49205
-snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=mYRyQgBTp45Wb4BeNSl6rTK_SzYvQVvzx8hpgOh57_M,52324
-snowflake/ml/modeling/linear_model/ridge.py,sha256=Mmhb3rQQgs2Nc5GYU88_lE_DGBUlKEQqr2z4TAa7cgE,51230
-snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=5wCXK6g-BlMUTc_PVuH44PMH6tzYC-KeUCnK7XS9pNE,51618
-snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=8RAc3tcTNjQ86kI9AOuwpheRqr0q76HA-o5DoRiQZOM,49613
-snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=PfIfSR3zKgdxGlkeTGFgbYAITtYkk0s6Q8Nz3QopVW0,50314
-snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=8TGJqBTXs0b_BVn7p2qRPAfMxenMQhGW914P_h7ThPs,56691
-snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=9seurHoZp5Ycd2qjNFmyyX5ijuY536h907WM3FL2iQ4,51489
-snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=BqnU5tr04mi2Kr0prb1bs0dArI8-2ZfQAEfAGkcEX-k,54162
-snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=oD3OXFHyiHqnVfzYgKRN9S9t1BmonmcZkSdvs8_yd0Y,49640
-snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=bhBMLULBc_sEdaKX7tAsdoyn_1FuK11X2oiXucb1_V8,50596
+snowflake/ml/modeling/linear_model/ard_regression.py,sha256=HaPbHZuQI173f0M2hgFsoJDUVwJAKtgRC5z7hTgzIc0,49353
+snowflake/ml/modeling/linear_model/bayesian_ridge.py,sha256=WczXSOPGws4hmCKO93TRv7a16Gxls9IW6D4Ifpy4md8,49769
+snowflake/ml/modeling/linear_model/elastic_net.py,sha256=MjqqWIBdaK2BDYc1RAza1RW2PUSAKNF6_SNhh3EJJWI,50341
+snowflake/ml/modeling/linear_model/elastic_net_cv.py,sha256=vK51acP5WZcaf1jEVvrrsJt-Brr69wnm7OvWz4Ummsk,51609
+snowflake/ml/modeling/linear_model/gamma_regressor.py,sha256=eYsPDEsnpJvaB3Jz3irw0uhTTawNK6T_ujtesphqt6Q,49421
+snowflake/ml/modeling/linear_model/huber_regressor.py,sha256=wh8U5NwrwDG21h8FlX148Y31RWNS53RgjH2d5neNdJc,48618
+snowflake/ml/modeling/linear_model/lars.py,sha256=i9EhEs0SB5n41chcDB7uavlUiwsPAvqK8PRG1l-A2vc,49841
+snowflake/ml/modeling/linear_model/lars_cv.py,sha256=Rz6jD0PCc45fTqzv3gqSehWIMKyPJO3ByMWrOpbOYoA,50062
+snowflake/ml/modeling/linear_model/lasso.py,sha256=MyNEp2q5IxmCTqoL-j_o67kAkY8X5QYBTIgh7XMwWO8,49947
+snowflake/ml/modeling/linear_model/lasso_cv.py,sha256=Aqi9VlF0Az_wIB_m6LUnoB2x8uO9GamPeBAphAMbVoI,50732
+snowflake/ml/modeling/linear_model/lasso_lars.py,sha256=aoyGqoQLslpeta9PdDHr0ZEnJLhR19qMh4c7nOv27BI,50977
+snowflake/ml/modeling/linear_model/lasso_lars_cv.py,sha256=D10l4jKKNAfjio8sZCtDt7fKqa1NCWRyCaBHe55Lgys,50938
+snowflake/ml/modeling/linear_model/lasso_lars_ic.py,sha256=9Mkmzw5mRGJX146stdHRSxxHvzHTaG0odEvnFhqpeFM,50284
+snowflake/ml/modeling/linear_model/linear_regression.py,sha256=uZyGyOrdy6lu6Esp6V_kLgha8sBPPommm5AKyF6G0J0,48160
+snowflake/ml/modeling/linear_model/logistic_regression.py,sha256=cbJz4vOHmQGQblfeQ_m0Szh0ScoFTQHm1uEoQGznSCQ,54415
+snowflake/ml/modeling/linear_model/logistic_regression_cv.py,sha256=8t5ndapP8JF2hOMHnUWAP_CDSrZniKlXHo57cujjnFs,55455
+snowflake/ml/modeling/linear_model/multi_task_elastic_net.py,sha256=kTPTxp12BmVhGuKYY009l6jr7cULbpRm9Dp7qW8-Hqs,49632
+snowflake/ml/modeling/linear_model/multi_task_elastic_net_cv.py,sha256=LMh9nyld0bndsDmIIlahhxq9TIbDEFSArlfoMildflw,51270
+snowflake/ml/modeling/linear_model/multi_task_lasso.py,sha256=eC7AD7AlgXBXnGwP2WNUSJsiYIl69XkpXGArtdj0s5M,49180
+snowflake/ml/modeling/linear_model/multi_task_lasso_cv.py,sha256=7HYneCH5xNlMLcqDSBLiAjbztkLZcVfZlxkJIaOZP9Y,50442
+snowflake/ml/modeling/linear_model/orthogonal_matching_pursuit.py,sha256=YhfJfMXvRBDOQl5EOmIDJ7xJAvbhWzhmIh2LyA46L6I,48787
+snowflake/ml/modeling/linear_model/passive_aggressive_classifier.py,sha256=FRu7zp4m4EvTDxVYZeV_-Vknrz2hrVFNSkd2YFXepjY,52143
+snowflake/ml/modeling/linear_model/passive_aggressive_regressor.py,sha256=inz27nO6hjKG4DEcLXc-XfYdUc7xy3x62R9Y6SVrv3A,51210
+snowflake/ml/modeling/linear_model/perceptron.py,sha256=Q2yzFRQ-s_RJGUCsCbBT50SX35MnJGzPLGRiLzc07ys,51527
+snowflake/ml/modeling/linear_model/poisson_regressor.py,sha256=w1BXkGQxmGQj-Lysn7tlEQF_Obg00G7v-310JYUjeiE,49466
+snowflake/ml/modeling/linear_model/ransac_regressor.py,sha256=e7UGWQ743I-g1bYSKKnGefGvq-M0AjJ3jDZ7B2kdxL4,52585
+snowflake/ml/modeling/linear_model/ridge.py,sha256=P6AzZbnl60I_425tdOQepJCXN9Edg00GPE0iiLaGBBk,51491
+snowflake/ml/modeling/linear_model/ridge_classifier.py,sha256=bN0EPKU4pOPoQJF9ZQe0j0dPayWl_Up32NbIh2eoRLY,51879
+snowflake/ml/modeling/linear_model/ridge_classifier_cv.py,sha256=ryDxqt-DH0jOZBap8YGBJPWbv-UGFcNRY4iQIe2-xRw,49874
+snowflake/ml/modeling/linear_model/ridge_cv.py,sha256=ywQjvUjnymlVZwpzAN38rgLxl1pFkSyl5yZyW6DxMk8,50575
+snowflake/ml/modeling/linear_model/sgd_classifier.py,sha256=pG5dEakeR1v8W1Acd2yRlHgQHxQAmA0aHqsjRhWOiJ8,56952
+snowflake/ml/modeling/linear_model/sgd_one_class_svm.py,sha256=xFkcwEL9cHCWDQrt8MMzc9JoAGli_mS7pujbrWL6IYE,51750
+snowflake/ml/modeling/linear_model/sgd_regressor.py,sha256=Ke7nGiusplQDF9F1OjUI0Q9Sz-HKjDasAU7hFQVSXEc,54423
+snowflake/ml/modeling/linear_model/theil_sen_regressor.py,sha256=MpNQb9Q_EA1IleQZiP92U1_FXuwA9-zJmVmPZnHP3lA,49901
+snowflake/ml/modeling/linear_model/tweedie_regressor.py,sha256=OYT3gshbB2vxNseTpmVKtDVfeKhbK48Fe6utSlK47Ro,50857
 snowflake/ml/modeling/manifold/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/manifold/isomap.py,sha256=60ASOONodAkjyhXRdFOy2APjUvCuMmrtT7KuMWXJXcY,49600
-snowflake/ml/modeling/manifold/mds.py,sha256=VxHFQJc9JW-cUA35pxPmPXJwK0936HI95fp2Rb_gHNI,48814
-snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=L8OYTKQwqEWybCKyLnwWq7zPES7AtlWQIufDM076wZE,49638
-snowflake/ml/modeling/manifold/tsne.py,sha256=wipO5HnzUQhfBC-6reP_JalRmrP9mNwdfhv0apss1nU,52597
+snowflake/ml/modeling/manifold/isomap.py,sha256=ztBEcsRoyM0v9ckQJoYEUTsj5HwRGGXQOjMY3e-_UWQ,50084
+snowflake/ml/modeling/manifold/mds.py,sha256=NlDzOMhCzHAU9cueoJzMT6qKyC_UV_PS_YnlIam58CI,49304
+snowflake/ml/modeling/manifold/spectral_embedding.py,sha256=ojQQ9Pt_bUvRzPSBuLgd2BhjWPdPjAz80XtDJi4vlT8,50155
+snowflake/ml/modeling/manifold/tsne.py,sha256=CpP4qv_V_ibyucBzTIsGq6N7BOPDtzFQ46C5HpJJSCE,53094
 snowflake/ml/modeling/metrics/__init__.py,sha256=pyZnmdcefErGbbhQPIo-_nGps7B09veZtjKZn4lI8Tg,524
 snowflake/ml/modeling/metrics/classification.py,sha256=5XbbpxYu9HXB7FUbBJfT7wVNMKBfzxwcaVzlMSyHAWg,66499
 snowflake/ml/modeling/metrics/correlation.py,sha256=Roi17Sx5F81VlJaLQTeBAe5qZ7sZYc31UkIuC6z4qkQ,4803
 snowflake/ml/modeling/metrics/covariance.py,sha256=HxJK1mwyt6lMSg8yonHFQ8IxAEa62MHeb1M3eHEtqlk,4672
 snowflake/ml/modeling/metrics/metrics_utils.py,sha256=NETSOkhP9m_GZSywiDydCQXKuXOp3qltNgbabROtJAw,13114
 snowflake/ml/modeling/metrics/ranking.py,sha256=gA1R1x1jUXA9bRrYn8IfJPM5BDY4DK1JCtoPQVsz5z4,17569
 snowflake/ml/modeling/metrics/regression.py,sha256=OEawjdiMZHYlycQFXM_h2czIZmmGe5GKciEQD9MSWx4,25845
 snowflake/ml/modeling/mixture/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=2TogPi_LioMVhi5Daldlvze_o-hD1WQpNnkTf5AkFxE,54396
-snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=Eh1N2aYsRnuFBxV2QTaVwrtlNRE1Cap3G0Aa-p2kYdU,52297
+snowflake/ml/modeling/mixture/bayesian_gaussian_mixture.py,sha256=UV7nEG1H493qFZecZ0uI9JUh-ooeesZLcCPlH_UdDdQ,54657
+snowflake/ml/modeling/mixture/gaussian_mixture.py,sha256=PPCRLAnDcLdMP190UjV6VGhU28rIHg7wkliaV8EARBI,52558
 snowflake/ml/modeling/model_selection/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=JyE8MHM0tiSdRFFzcTswhLk--n5yt-4yj6znx5EyoaQ,38453
-snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=3BmQQe23wvnHWN3-BfG7zzKiG-6X-FfVu0_2A9yhqdU,38692
+snowflake/ml/modeling/model_selection/grid_search_cv.py,sha256=B18rb0gh4TK9z2G5XVCx5nav_a9jWDH7q7XdLzAkRwI,38125
+snowflake/ml/modeling/model_selection/randomized_search_cv.py,sha256=ipnRe8z3G09wTy1I2s33CzRsit8pIBfGaZGy4IZfjdM,38867
 snowflake/ml/modeling/multiclass/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=YV9-vVxIkQzcjAVauatxfoGx4mP7OvWUTD-pzh-oXt8,47899
-snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=Cc6v9n1bdIyFzqGSPnqFiHWZbofqDxAEiPCsW2zJ4hk,48833
-snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=6QtdMlV8VNGXgHzd-Lo3eszCRhFCz96PWurlbukifeU,48169
+snowflake/ml/modeling/multiclass/one_vs_one_classifier.py,sha256=h9PDQMMXD9w1GF9nYrAzhOzlo9Tu9gqgm2euvBcmwyA,48160
+snowflake/ml/modeling/multiclass/one_vs_rest_classifier.py,sha256=RX7kdJmEdb_JUUfsie7Q44CFWaRate7N23kKyreT49k,49094
+snowflake/ml/modeling/multiclass/output_code_classifier.py,sha256=dGQLuw7i7IXXv3VQnKxDSQaU8yUpphsVcwPxbpb3uf8,48430
 snowflake/ml/modeling/naive_bayes/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=xus3Of6nKtPCJYsnWZf3gL42pdvfGnkEkq2yPj49mE0,48438
-snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=3JoeKRyn0CoAjA8Gk_2Uhk1wtj2FvbtXk2wNyaLnKlM,48773
-snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=BiegAGRFG92DLesOBTvbPZZZGovTC6_mHtksSraSxB8,48453
-snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=ebY5aK-nHoM9NKVlEO207JijRO2aI33cjqwO8qgOox4,47582
-snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=q_Q20vfh4OdUQ9U1XW9PNQt8XyB-mzYAAELhZTek28s,48218
+snowflake/ml/modeling/naive_bayes/bernoulli_nb.py,sha256=lrWiVXGFQpqbeF0g8va1cMhPyKxxqvgHddaBJLe_J5Y,48699
+snowflake/ml/modeling/naive_bayes/categorical_nb.py,sha256=agWXHZNNV5AzhCH8g0HJFdCpg7qOF0CjRSejcknYwbs,49034
+snowflake/ml/modeling/naive_bayes/complement_nb.py,sha256=dJ9hB8CjM9wOlax4i0yH82LnOeb6dUQRXvPhQX2zYzw,48714
+snowflake/ml/modeling/naive_bayes/gaussian_nb.py,sha256=d-QpjpNF2QUVQQBt2sg-SF_WsidfZ8rQZmiXs-OE2T8,47843
+snowflake/ml/modeling/naive_bayes/multinomial_nb.py,sha256=O36O12tu80hJLag6s34dmZpmf7VojUeuaVe6ceCtgts,48479
 snowflake/ml/modeling/neighbors/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=EgOxWl3hJlhBGUh_ERzLgUpvbfSaEa9fmG9NwCvl-WU,51287
-snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=AnFJpXgQQyJqtJdHImbCsVBO9U6IEw4zsexvCYYayh8,50758
-snowflake/ml/modeling/neighbors/kernel_density.py,sha256=QSgpGXXEyrgjubKeophBMEppyfSEHjj0CFn-iHI6VHY,49116
-snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=PUR1R-jEc65iKofDnoPdwy3-RRNGPKJDY2akwKJHRuY,51694
-snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=DbntjDRekGwuGYq77BON-TX_1_oEFwPgcVpo00jrymk,47776
-snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=ncPlfTMY426L82uU7RRGivV-IBVc1BL61VDh91TY5E0,49585
-snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=VuIWZD8dy0C2SzSgTzA_lx-F_hQdWvAyXrkLSl1E0wU,50965
-snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=oHWTW1aGLHPqZMK7OGou5DeZEaIIXsW3nh-ep0nBquY,51700
-snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=z7aYidP3kmDJG61Jmv_x9QVHM5fNavqsEBy8KBsDJEo,50581
+snowflake/ml/modeling/neighbors/k_neighbors_classifier.py,sha256=5WC1on082prjxYQczW9wz1pK9EUmwAoNmGl1eWjZ_4w,51548
+snowflake/ml/modeling/neighbors/k_neighbors_regressor.py,sha256=aYMLol69mzwGZLsT3rrVMSY8qauNPuNCBkwi5kFyKac,51019
+snowflake/ml/modeling/neighbors/kernel_density.py,sha256=SqV1AsKmPWxSbob7EGiBu7c_xfk9DCIMU9e4_rto1dU,49377
+snowflake/ml/modeling/neighbors/local_outlier_factor.py,sha256=7s3UKO1UCCVJ_14Kd8nH0kKpkK8BUJwaMT8f8ONSXoI,51955
+snowflake/ml/modeling/neighbors/nearest_centroid.py,sha256=4CP51Ze9kXA7Ipd5z1ZMN-3l77hMjdpLFoll5jLoeYc,48037
+snowflake/ml/modeling/neighbors/nearest_neighbors.py,sha256=QQMbsZGRge2dj0i-zXu7GwX1SYqxSuC90Alt4QppiW0,49846
+snowflake/ml/modeling/neighbors/neighborhood_components_analysis.py,sha256=-R2ifpMaDhOCcxL_ugnlC1Sp6iHxBqxJAmpVwPPpHU0,51509
+snowflake/ml/modeling/neighbors/radius_neighbors_classifier.py,sha256=oyejLj_noQhKnXnTLrThpAdSVfe7F6WnfrL8enAEKKs,51961
+snowflake/ml/modeling/neighbors/radius_neighbors_regressor.py,sha256=WS5xaHj91Ne6_jBDX8i79lRd4Xr0t_2YW7-c3PMd6xc,50842
 snowflake/ml/modeling/neural_network/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=JDjwNsPZJcNjHgUkbNGM3LnQgFAQdbE5v_TJIcOGSdA,48080
-snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=etjW75x8b4iCFIM8lnuaZPLC5UakLzVDE8_LDr-EN3Q,55660
-snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=JheiwTgmArE_VX6OcXNvPFt0V2H_R-5rKcsHcZXuaHs,54929
+snowflake/ml/modeling/neural_network/bernoulli_rbm.py,sha256=sM6PA51sYbvJABIw8UiN7QrkcWIU9ajx7biKeswbIJU,48585
+snowflake/ml/modeling/neural_network/mlp_classifier.py,sha256=XYFnxnnEmMa3jlUfKbjDU6U5rTjYMRjfL3-BHmr5Bbs,55921
+snowflake/ml/modeling/neural_network/mlp_regressor.py,sha256=JYb6MziLoTHJvUvnInszDG9RlK17Nxw0SglJqydaP-k,55190
 snowflake/ml/modeling/parameters/disable_distributed_hpo.py,sha256=jyjlLPrtnDSQxlTTM0ayMjWKVL_IP3snd--yeXK5htY,221
 snowflake/ml/modeling/pipeline/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
-snowflake/ml/modeling/pipeline/pipeline.py,sha256=R5F0jVmKyVZWXHE64UQnBa2dVjSFZFQCYXlA_s1x5qg,25456
+snowflake/ml/modeling/pipeline/pipeline.py,sha256=KyA9a7CTTWYaONuB-BAn7dJ1PfXFKReNBKeM398qvi4,45360
 snowflake/ml/modeling/preprocessing/__init__.py,sha256=dYtqk_GD_hAAZjGfH1maWlZQ30h4hu_KGaf-_y9_AD8,298
 snowflake/ml/modeling/preprocessing/binarizer.py,sha256=noHrlTqpI7RRzYbCSuCjKHxhL8NUCDKNw-kDNTwyY_U,6999
 snowflake/ml/modeling/preprocessing/k_bins_discretizer.py,sha256=g7kY0LHjnCaBzkslCkjdPV06eL2KRYwZuYKRmDef3ew,20970
 snowflake/ml/modeling/preprocessing/label_encoder.py,sha256=C35I9biWxefltNmXzqaJoqVgOP8eOnTNP7NIsnfR2xE,7405
 snowflake/ml/modeling/preprocessing/max_abs_scaler.py,sha256=xpuybHsjrL68u0qNe9DTrQOJsqzb8GOvHT0-_tIBzvM,8768
 snowflake/ml/modeling/preprocessing/min_max_scaler.py,sha256=agZt9B37PsVhmS8AkH8ix0bZFsf-EGapeTp6-OD1pwI,12200
 snowflake/ml/modeling/preprocessing/normalizer.py,sha256=iv3MgJZ4B9-X1fAlC0pWsrYuQvRz1iJrM0_f4XfZKc0,6584
-snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=Ro8Rjg4cqdGZgkyIbb4X75qEExVVztIzuIM6ndslZnQ,71579
+snowflake/ml/modeling/preprocessing/one_hot_encoder.py,sha256=5kj3V48bYmXnorf0xnp5AqRbAiJtgswepgUicyNdFHM,72322
 snowflake/ml/modeling/preprocessing/ordinal_encoder.py,sha256=3c6XnwnMpbHbAITzo5YoJoI86YI-Q_BBFajoEa-7q80,33276
-snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=nq4e1QC8R9-5m3UFNr4PBlo-HF7R7dbjIqIWe-RC2ro,47991
+snowflake/ml/modeling/preprocessing/polynomial_features.py,sha256=VTapvnHDxiUyNw48F0OwGY4xsPFWjst0t70Rm560WN4,48511
 snowflake/ml/modeling/preprocessing/robust_scaler.py,sha256=iBwCP10CljdGmjEo-JEZMsHsk_3tccSXYbxN4xVq5Do,12398
 snowflake/ml/modeling/preprocessing/standard_scaler.py,sha256=LxvcZ4a5xuHJLtIvkLafNjv0HsZd7mqzp_cdI378kkM,11395
 snowflake/ml/modeling/semi_supervised/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=-PiLSdnYJUXbMuOGTZCxfI23MUtKZIrNCI8CkXefUqU,48675
-snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=RqwGkGNabLUdOZ_xT_o-JeoOpzCWD49MacGYf42sb7o,49024
+snowflake/ml/modeling/semi_supervised/label_propagation.py,sha256=ZAwWHQa2A24z9uLIXSucVFj1C7S0LhYT8eQwdLk3g9s,48936
+snowflake/ml/modeling/semi_supervised/label_spreading.py,sha256=yMRrAJG80sM4l48DMqzpIYwQacvuRvDrxc-AflWAt-E,49285
 snowflake/ml/modeling/svm/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/svm/linear_svc.py,sha256=cg8XdIXUgCb2hqH6lUV_8SznsSYM_4Rv-Us6y_e_Uw4,51485
-snowflake/ml/modeling/svm/linear_svr.py,sha256=jsOCsb2YXGHjp5IvnHBLdXKL7LizM2YH6eTQydTDn_w,49838
-snowflake/ml/modeling/svm/nu_svc.py,sha256=edjqwhcv8H2NiK5QQds85D0fFXw54i53L6aF7fTLJKE,51797
-snowflake/ml/modeling/svm/nu_svr.py,sha256=A2DDWc8GO-M-2kKfbZbCM5g11UJUPoW_MVFHh7dFhak,48876
-snowflake/ml/modeling/svm/svc.py,sha256=jJ7DwlXwmN0M0Jr3MN-ERrUsJzvX9IcTtSjGr_7z8wg,51946
-snowflake/ml/modeling/svm/svr.py,sha256=Nhs16EL09Fpmciqj6h_U1CeVf4WV5fU5cJ_3PT8LFIM,49065
+snowflake/ml/modeling/svm/linear_svc.py,sha256=zcfbzUtusA-a0b0dRTSbGfj8X5rrZ3W2LfoHj7IFqNQ,51746
+snowflake/ml/modeling/svm/linear_svr.py,sha256=P8151xjFlLYj3YhQEbJKiOJEQHKAfrPntv2ufTJl5y4,50099
+snowflake/ml/modeling/svm/nu_svc.py,sha256=me_YR4WL1sW8dRrIrO929p7vc8Ow7_jSftWf9YUHqhY,52058
+snowflake/ml/modeling/svm/nu_svr.py,sha256=Nqyvvt_OsoVe_py602HtsaXnTmykhdQy0fb8YdHFB9s,49137
+snowflake/ml/modeling/svm/svc.py,sha256=6efrAzY2u-5qPKMQb9__L5XNPf33rRFzzMHi4QJnYVA,52207
+snowflake/ml/modeling/svm/svr.py,sha256=WSrWS4uPAVGfPAPiqB-FphSPuNVF43kYk9kyW1ACfx0,49326
 snowflake/ml/modeling/tree/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=bWkIi4j8PzPTP2djteY4rllrjiFdvIdpcyNdk9DULxM,54222
-snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=5NYUDCTEvQEg99b2CSBZjieh2Nqn5EyUwVH_Ybs5q74,52921
-snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=PzKYyjJod6_712C3cYg83kYXlXupnhq37mQasn_dgC0,53564
-snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=oh1D5gSzt9G4HwLX6KXniWh9Ur25Gc-XpagE8NJcg6k,52272
+snowflake/ml/modeling/tree/decision_tree_classifier.py,sha256=4D2m5Xg1DNd12WF2-aPxCIdeIAkdO90psJOa5fy1Cc0,54483
+snowflake/ml/modeling/tree/decision_tree_regressor.py,sha256=t5DOqtWJaDV1pSkjXHC66zSQcBFnXIlhrKSYriS38ec,53182
+snowflake/ml/modeling/tree/extra_tree_classifier.py,sha256=Vvg7e0tBbJzjd7mxrNZoX_0xdlZcPVLSPb7-XTrVLnM,53825
+snowflake/ml/modeling/tree/extra_tree_regressor.py,sha256=hJP9mnMF_B63Gj4XI87YgAXEH8KRGxzSTlgvA9gsBvk,52533
 snowflake/ml/modeling/xgboost/__init__.py,sha256=rY5qSOkHj59bHiTV6LhBiEhUA0StoCb0ACNR2vkV4v0,297
-snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=GH12HzpDhIlyUf9goywmywndTczaPUyYIpsMveyGUC8,59220
-snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=ddmYxpexJsodWT6bTI8LG6wxGWpry1YdvfFUj76t_fA,58719
-snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=4gMA2jD7yi11YrR160FBfGDz4x2s-SSdvHaXXlTLE6E,59396
-snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=nXnnK26ORSy2exd0pTB4DmJJCeED5_vNQND0loagXI4,58922
+snowflake/ml/modeling/xgboost/xgb_classifier.py,sha256=ntJc0lzSLuJHCTv9LhWrwLsj7_aKNK3uwaQNu5dedbA,59481
+snowflake/ml/modeling/xgboost/xgb_regressor.py,sha256=BjwTmEJ0rhVWVnBoUw_i3A4cAKmXzzIxnsy80MHYzp4,58980
+snowflake/ml/modeling/xgboost/xgbrf_classifier.py,sha256=47owolJMSu3veYvPIYwGyzrZpBXboJAJTMnidHFqYJo,59657
+snowflake/ml/modeling/xgboost/xgbrf_regressor.py,sha256=oeZM8KLqILuM_uzPH8HGR3KtMpcHGTPz2VjVyAndn-c,59183
 snowflake/ml/monitoring/monitor.py,sha256=M9IRk6bnVwKNEvCexEJ5Rf95zEFap4O5qjbwfwdXGS0,7135
 snowflake/ml/monitoring/shap.py,sha256=Dp9nYquPEZjxMTW62YYA9g9qUdmCEFxcSk7ejvOP7PE,3597
 snowflake/ml/registry/__init__.py,sha256=XdPQK9ejYkSJVrSQ7HD3jKQO0hKq2mC4bPCB6qrtH3U,76
-snowflake/ml/registry/_artifact_manager.py,sha256=Q-6cRfU-pQBNVroh1_YIhd8hQtk8lC0y9vRBCDVizGQ,5544
 snowflake/ml/registry/_initial_schema.py,sha256=KusBbu0vpgCh-dPHgC90xRSfP6Z79qC-eXTqT8GXpFI,5316
 snowflake/ml/registry/_schema.py,sha256=GOA427_mVKkq9RWRENHuqDimRS0SmmP4EWThNCu1Kz4,3166
 snowflake/ml/registry/_schema_upgrade_plans.py,sha256=LxZNXYGjGG-NmB7w7_SxgaJpZuXUO66XVMuh04oL6SI,4209
 snowflake/ml/registry/_schema_version_manager.py,sha256=-9wGH-7ELSZxp7-fW7hXTMqkJSIebXdSpwwgzdvnoYs,6922
-snowflake/ml/registry/artifact.py,sha256=9JDcr4aaR0d4cp4YSRnGMFRIdu-k0tABbs6jDH4VDGQ,1263
-snowflake/ml/registry/model_registry.py,sha256=MgI4Dj9kvxfNd3kQ3tWY6ygmxUd6kzb430-GKkn4BA0,91007
+snowflake/ml/registry/model_registry.py,sha256=x42wR2lEyW99NnG8auNPOowg34bF87ksXQqrjMFd7Pw,84795
 snowflake/ml/registry/registry.py,sha256=RxEM0xLWdF3kIPf5upJffaPPP9liNMMZOnVeSyYNIb8,10949
 snowflake/ml/registry/_manager/model_manager.py,sha256=LYX_nS_egwum7F_LCbz_a3hibIHOTDK8LO1DPOWxPrE,5809
 snowflake/ml/utils/connection_params.py,sha256=JRpQppuWRk6bhdLzVDhMfz3Y6yInobFNLHmIBaXD7po,8005
 snowflake/ml/utils/sparse.py,sha256=XqDQkw39Ml6YIknswdkvFIwUwBk_GBXAbP8IACfPENg,3817
-snowflake_ml_python-1.4.1.dist-info/LICENSE.txt,sha256=PdEp56Av5m3_kl21iFkVTX_EbHJKFGEdmYeIO1pL_Yk,11365
-snowflake_ml_python-1.4.1.dist-info/METADATA,sha256=dz4Jp2I7bs8n4X7l5EzuXLqsZq8F9fzZ8IyImn2SFII,47072
-snowflake_ml_python-1.4.1.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
-snowflake_ml_python-1.4.1.dist-info/top_level.txt,sha256=TY0gFSHKDdZy3THb0FGomyikWQasEGldIR1O0HGOHVw,10
-snowflake_ml_python-1.4.1.dist-info/RECORD,,
+snowflake_ml_python-1.5.0.dist-info/LICENSE.txt,sha256=PdEp56Av5m3_kl21iFkVTX_EbHJKFGEdmYeIO1pL_Yk,11365
+snowflake_ml_python-1.5.0.dist-info/METADATA,sha256=4_GHqJoiNYXF-WBWL6qwV6wA13ZlB6ySiGQZBLCPwRY,50050
+snowflake_ml_python-1.5.0.dist-info/WHEEL,sha256=GJ7t_kWBFywbagK5eo9IoUwLW6oyOeTKmQ-9iHFVNxQ,92
+snowflake_ml_python-1.5.0.dist-info/top_level.txt,sha256=TY0gFSHKDdZy3THb0FGomyikWQasEGldIR1O0HGOHVw,10
+snowflake_ml_python-1.5.0.dist-info/RECORD,,
```

