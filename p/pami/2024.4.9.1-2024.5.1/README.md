# Comparing `tmp/pami-2024.4.9.1.tar.gz` & `tmp/pami-2024.5.1.tar.gz`

## filetype from file(1)

```diff
@@ -1 +1 @@
-gzip compressed data, was "pami-2024.4.9.1.tar", last modified: Tue Apr  9 02:13:28 2024, max compression
+gzip compressed data, was "pami-2024.5.1.tar", last modified: Wed May  1 06:53:05 2024, max compression
```

## Comparing `pami-2024.4.9.1.tar` & `pami-2024.5.1.tar`

### file list

```diff
@@ -1,505 +1,511 @@
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.602975 pami-2024.4.9.1/
--rw-r--r--   0 vanithak   (502) staff       (20)    35149 2024-03-12 04:33:29.000000 pami-2024.4.9.1/LICENSE
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.793712 pami-2024.4.9.1/PAMI/
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.795296 pami-2024.4.9.1/PAMI/AssociationRules/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.809616 pami-2024.4.9.1/PAMI/AssociationRules/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    14314 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithConfidence.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14661 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLeverage.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14622 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLift.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20378 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/RuleMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6594 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/AssociationRules/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)      139 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.811156 pami-2024.4.9.1/PAMI/correlatedPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.817103 pami-2024.4.9.1/PAMI/correlatedPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    27142 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)    29081 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMinePlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6208 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/correlatedPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.818346 pami-2024.4.9.1/PAMI/coveragePattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.826282 pami-2024.4.9.1/PAMI/coveragePattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    15616 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/CMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)    18923 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/CPPG.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7155 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/coveragePattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.838845 pami-2024.4.9.1/PAMI/extras/
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.853335 pami-2024.4.9.1/PAMI/extras/DF2DB/
--rw-r--r--   0 vanithak   (502) staff       (20)     4360 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DB.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4287 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DBPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    10331 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/DenseFormatDF.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5413 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/SparseFormatDF.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3103 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/createTDB.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6948 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    11940 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5336 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.858573 pami-2024.4.9.1/PAMI/extras/calculateMISValues/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6468 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingBeta.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6499 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingSD.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6964 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/convertMultiTSIntoFuzzy.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.876165 pami-2024.4.9.1/PAMI/extras/dbStats/
--rw-r--r--   0 vanithak   (502) staff       (20)    14951 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/FuzzyDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    13796 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16034 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/dbStats/SequentialDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16883 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/TemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12839 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/TransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15120 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    11953 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12679 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/dbStats/UtilityDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/dbStats/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.880811 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5238 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8594 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8792 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8313 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.889684 pami-2024.4.9.1/PAMI/extras/generateDatabase/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5685 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     9558 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5971 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5156 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/generateLatexGraphFile.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.901708 pami-2024.4.9.1/PAMI/extras/graph/
--rw-r--r--   0 vanithak   (502) staff       (20)     3223 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/DF2Fig.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3577 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/DF2Tex.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/graph/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2750 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3599 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4465 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4240 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/graph/visualizePatterns.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.904228 pami-2024.4.9.1/PAMI/extras/image2Database/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/image2Database/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.906815 pami-2024.4.9.1/PAMI/extras/imageProcessing/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/imageProcessing/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6488 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/imageProcessing/imagery2Databases.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.911056 pami-2024.4.9.1/PAMI/extras/messaging/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/messaging/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)      533 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/messaging/discord.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1575 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/messaging/gmail.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.917378 pami-2024.4.9.1/PAMI/extras/neighbours/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/neighbours/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4789 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4415 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4310 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5011 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/plotPointOnMap.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5182 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/plotPointOnMap_dump.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.922472 pami-2024.4.9.1/PAMI/extras/sampleDatasets/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/sampleDatasets/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4023 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/scatterPlotSpatialPoints.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.932869 pami-2024.4.9.1/PAMI/extras/stats/
--rw-r--r--   0 vanithak   (502) staff       (20)    12724 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/TransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/stats/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4144 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/stats/graphDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15998 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/sequentialDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16926 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/temporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)    12692 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/stats/utilityDatabase.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.964837 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/
--rw-r--r--   0 vanithak   (502) staff       (20)     8471 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5543 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2325 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2254 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2539 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1880 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1843 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2117 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2066 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2262 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1121 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1111 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1625 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1610 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3613 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3603 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4324 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3283 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4842 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
--rw-r--r--   0 vanithak   (502) staff       (20)     3238 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/topKPatterns.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2321 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/uncertaindb_convert.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.969666 pami-2024.4.9.1/PAMI/extras/visualize/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/extras/visualize/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1897 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/extras/visualize/graphs.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.971091 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.976128 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    15253 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    24431 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6856 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.977409 pami-2024.4.9.1/PAMI/frequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.988154 pami-2024.4.9.1/PAMI/frequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    15220 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/Apriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14362 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14885 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATDiffset.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15362 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATbitset.py
--rw-r--r--   0 vanithak   (502) staff       (20)    22703 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/FPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7867 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:27.991640 pami-2024.4.9.1/PAMI/frequentPattern/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)    22294 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/CHARM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6580 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/closed/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.052468 pami-2024.4.9.1/PAMI/frequentPattern/cuda/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5980 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15188 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuApriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16228 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14636 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclat.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16604 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclatBit.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16419 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    21681 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
--rw-r--r--   0 vanithak   (502) staff       (20)    15055 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.056713 pami-2024.4.9.1/PAMI/frequentPattern/maximal/
--rw-r--r--   0 vanithak   (502) staff       (20)    27531 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6561 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/maximal/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.064878 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5573 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    16543 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelApriori.py
--rw-r--r--   0 vanithak   (502) staff       (20)    14856 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19026 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.073549 pami-2024.4.9.1/PAMI/frequentPattern/topk/
--rw-r--r--   0 vanithak   (502) staff       (20)    16687 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/FAE.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4575 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/frequentPattern/topk/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.075074 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.080427 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    32988 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6645 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.085119 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.092414 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    25855 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    32901 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6428 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.094061 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.101426 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29496 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    32795 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6724 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.103125 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.110674 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    33628 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    39030 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.114640 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.121669 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    25016 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6463 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.122749 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.128995 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29275 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    32773 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6678 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.130182 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.138617 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    23633 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6782 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.139762 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.145864 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    23719 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    22366 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6689 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.148990 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6690 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.150420 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.155070 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    23257 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6178 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.156998 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.160629 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    41228 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6179 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.162526 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.169244 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    46718 2024-04-09 02:02:27.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6307 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.170240 pami-2024.4.9.1/PAMI/highUtilityPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.178668 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    37128 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/EFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)    30770 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/HMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    32709 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/UPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20285 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/basic/efimParallel.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.185414 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    18207 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/efimparallel.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.191526 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/
--rw-r--r--   0 vanithak   (502) staff       (20)    32873 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
--rw-r--r--   0 vanithak   (502) staff       (20)    35784 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5193 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.194856 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6716 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.201955 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    35564 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)    40140 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5934 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.205999 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/
--rw-r--r--   0 vanithak   (502) staff       (20)    40254 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.207268 pami-2024.4.9.1/PAMI/localPeriodicPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.214488 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    34480 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23965 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    22949 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     8385 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.217320 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.222491 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    25470 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23153 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5921 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.224344 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.234442 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    28279 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    22083 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5398 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.235758 pami-2024.4.9.1/PAMI/partialPeriodicPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.251077 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    26417 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4329 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26878 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20736 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5520 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.257160 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)    24133 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5605 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.274878 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/
--rw-r--r--   0 vanithak   (502) staff       (20)    29141 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4278 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.286087 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5765 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    30910 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.293743 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6441 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20928 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.303465 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
--rw-r--r--   0 vanithak   (502) staff       (20)    28158 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5556 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.309577 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.319198 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    27514 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6640 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.320539 pami-2024.4.9.1/PAMI/periodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.340954 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    17905 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28583 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26383 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)    18106 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
--rw-r--r--   0 vanithak   (502) staff       (20)    36300 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      726 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6545 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26643 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.347343 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)    24312 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6539 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.356329 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6568 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    23867 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)    18982 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.363446 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/
--rw-r--r--   0 vanithak   (502) staff       (20)    31832 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7869 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.373650 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     5219 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26749 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.376040 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.383726 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
--rw-r--r--   0 vanithak   (502) staff       (20)    19951 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6862 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.387608 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4589 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    17514 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.389862 pami-2024.4.9.1/PAMI/recurringPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.394287 pami-2024.4.9.1/PAMI/recurringPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    29135 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/RPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6637 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/recurringPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.396535 pami-2024.4.9.1/PAMI/relativeFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.403016 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    30349 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4261 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.404650 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.407969 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    35406 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6052 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.410534 pami-2024.4.9.1/PAMI/sequence/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequence/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.411261 pami-2024.4.9.1/PAMI/sequentialPatternMining/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.421832 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    42265 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPADE.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19986 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPAM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6569 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    24786 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.426003 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6285 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/bide.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.427496 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.436338 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    19114 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26504 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    21391 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7271 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.440665 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/
--rw-r--r--   0 vanithak   (502) staff       (20)    27859 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7173 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.441985 pami-2024.4.9.1/PAMI/subgraphMining/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.456287 pami-2024.4.9.1/PAMI/subgraphMining/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1241 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2396 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/dfsCode.py
--rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/edge.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2616 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/extendedEdge.py
--rw-r--r--   0 vanithak   (502) staff       (20)      670 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/frequentSubgraph.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4943 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/graph.py
--rw-r--r--   0 vanithak   (502) staff       (20)    28244 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/gspan.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1748 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
--rw-r--r--   0 vanithak   (502) staff       (20)      826 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/basic/vertex.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.478210 pami-2024.4.9.1/PAMI/subgraphMining/topK/
--rw-r--r--   0 vanithak   (502) staff       (20)     1949 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSCode.py
--rw-r--r--   0 vanithak   (502) staff       (20)      593 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSThread.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1316 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/edge.py
--rw-r--r--   0 vanithak   (502) staff       (20)     2613 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/extendedEdge.py
--rw-r--r--   0 vanithak   (502) staff       (20)      674 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/frequentSubgraph.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4295 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/graph.py
--rw-r--r--   0 vanithak   (502) staff       (20)     1486 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
--rw-r--r--   0 vanithak   (502) staff       (20)    20979 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/tkg.py
--rw-r--r--   0 vanithak   (502) staff       (20)      818 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/subgraphMining/topK/vertex.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.482255 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)    17358 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6756 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.483938 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.506622 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    28610 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
--rw-r--r--   0 vanithak   (502) staff       (20)    26391 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19572 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19454 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
--rw-r--r--   0 vanithak   (502) staff       (20)    27790 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
--rw-r--r--   0 vanithak   (502) staff       (20)    25664 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    19522 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4945 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.508127 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.520288 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    30868 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4986 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.524799 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.537346 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    33395 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)    33912 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6536 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.538277 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.546173 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    30821 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6603 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.549586 pami-2024.4.9.1/PAMI/weightedFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.558446 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    27246 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/WFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     6659 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.563418 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.574205 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    30320 2024-04-09 02:02:26.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     7495 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.575583 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/__init__.py
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.579690 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/
--rw-r--r--   0 vanithak   (502) staff       (20)    31958 2024-03-29 21:11:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
--rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
--rw-r--r--   0 vanithak   (502) staff       (20)     4771 2024-03-12 04:33:29.000000 pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
--rw-r--r--   0 vanithak   (502) staff       (20)    67204 2024-04-09 02:13:28.599349 pami-2024.4.9.1/PKG-INFO
--rw-r--r--   0 vanithak   (502) staff       (20)    65760 2024-03-29 21:11:29.000000 pami-2024.4.9.1/README.md
-drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-04-09 02:13:28.589956 pami-2024.4.9.1/pami.egg-info/
--rw-r--r--   0 vanithak   (502) staff       (20)    67204 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/PKG-INFO
--rw-r--r--   0 vanithak   (502) staff       (20)    18058 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/SOURCES.txt
--rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/dependency_links.txt
--rw-r--r--   0 vanithak   (502) staff       (20)      237 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/requires.txt
--rw-r--r--   0 vanithak   (502) staff       (20)        5 2024-04-09 02:13:27.000000 pami-2024.4.9.1/pami.egg-info/top_level.txt
--rw-r--r--   0 vanithak   (502) staff       (20)       38 2024-04-09 02:13:28.603191 pami-2024.4.9.1/setup.cfg
--rw-r--r--   0 vanithak   (502) staff       (20)     1494 2024-04-09 02:13:23.000000 pami-2024.4.9.1/setup.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.857908 pami-2024.5.1/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35149 2024-03-12 04:33:29.000000 pami-2024.5.1/LICENSE
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.315498 pami-2024.5.1/PAMI/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.316523 pami-2024.5.1/PAMI/AssociationRules/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/AssociationRules/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.325226 pami-2024.5.1/PAMI/AssociationRules/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    13852 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/AssociationRules/basic/ARWithConfidence.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14350 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/AssociationRules/basic/ARWithLeverage.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14263 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/AssociationRules/basic/ARWithLift.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19416 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/AssociationRules/basic/RuleMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/AssociationRules/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6594 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/AssociationRules/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      139 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.326241 pami-2024.5.1/PAMI/correlatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/correlatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.330275 pami-2024.5.1/PAMI/correlatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25689 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/correlatedPattern/basic/CoMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    27462 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/correlatedPattern/basic/CoMinePlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/correlatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6208 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/correlatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.331115 pami-2024.5.1/PAMI/coveragePattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/coveragePattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.335347 pami-2024.5.1/PAMI/coveragePattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14643 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/coveragePattern/basic/CMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17200 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/coveragePattern/basic/CPPG.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/coveragePattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7155 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/coveragePattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.344231 pami-2024.5.1/PAMI/extras/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.353515 pami-2024.5.1/PAMI/extras/DF2DB/
+-rw-r--r--   0 vanithak   (502) staff       (20)     4360 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/DF2DB.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4287 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/DF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    10331 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/DenseFormatDF.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5413 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/SparseFormatDF.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/DF2DB/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3103 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/createTDB.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6948 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/denseDF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    11940 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/denseDF2DB_dump.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5336 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.356881 pami-2024.5.1/PAMI/extras/calculateMISValues/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/calculateMISValues/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6468 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/calculateMISValues/usingBeta.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6499 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/calculateMISValues/usingSD.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7345 2024-04-17 06:00:20.000000 pami-2024.5.1/PAMI/extras/convertMultiTSIntoFuzzy.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.370072 pami-2024.5.1/PAMI/extras/dbStats/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14951 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/dbStats/FuzzyDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13796 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16034 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/dbStats/SequentialDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16883 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/dbStats/TemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12839 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/dbStats/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15120 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    11953 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12679 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/dbStats/UtilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/dbStats/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.374748 pami-2024.5.1/PAMI/extras/fuzzyTransformation/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/fuzzyTransformation/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5238 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/fuzzyTransformation/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8594 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8792 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8313 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.380232 pami-2024.5.1/PAMI/extras/generateDatabase/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/generateDatabase/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5685 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     9558 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5971 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5157 2024-04-10 08:52:08.000000 pami-2024.5.1/PAMI/extras/generateLatexGraphFile.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.387990 pami-2024.5.1/PAMI/extras/graph/
+-rw-r--r--   0 vanithak   (502) staff       (20)     3223 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/graph/DF2Fig.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3577 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/graph/DF2Tex.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/graph/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2750 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/graph/plotLineGraphFromDictionary.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3599 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4465 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/graph/visualizeFuzzyPatterns.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4240 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/graph/visualizePatterns.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.388961 pami-2024.5.1/PAMI/extras/image2Database/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/image2Database/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.390625 pami-2024.5.1/PAMI/extras/imageProcessing/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/imageProcessing/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6488 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/imageProcessing/imagery2Databases.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.395348 pami-2024.5.1/PAMI/extras/messaging/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/messaging/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      533 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/messaging/discord.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1575 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/messaging/gmail.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.399343 pami-2024.5.1/PAMI/extras/neighbours/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/neighbours/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4784 2024-04-17 06:00:20.000000 pami-2024.5.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4410 2024-04-17 06:00:20.000000 pami-2024.5.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4305 2024-04-17 06:00:20.000000 pami-2024.5.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5013 2024-04-10 08:52:08.000000 pami-2024.5.1/PAMI/extras/plotPointOnMap.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5183 2024-04-10 08:52:08.000000 pami-2024.5.1/PAMI/extras/plotPointOnMap_dump.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.400956 pami-2024.5.1/PAMI/extras/sampleDatasets/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/sampleDatasets/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4024 2024-04-10 08:52:08.000000 pami-2024.5.1/PAMI/extras/scatterPlotSpatialPoints.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.410030 pami-2024.5.1/PAMI/extras/stats/
+-rw-r--r--   0 vanithak   (502) staff       (20)    12724 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/stats/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/stats/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4144 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/stats/graphDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15998 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/stats/sequentialDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    16926 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/stats/temporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12692 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/extras/stats/utilityDatabase.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.439339 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/
+-rw-r--r--   0 vanithak   (502) staff       (20)     7276 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6548 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/TransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2325 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2254 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2539 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1880 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1843 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2117 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2066 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2262 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/fuzzyDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1121 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1111 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1625 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1610 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3613 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3603 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/georeferencedTemporalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/georeferencedTransactionalDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4324 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3283 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4842 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     3240 2024-04-10 08:52:08.000000 pami-2024.5.1/PAMI/extras/topKPatterns.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2322 2024-04-10 08:52:08.000000 pami-2024.5.1/PAMI/extras/uncertaindb_convert.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.441309 pami-2024.5.1/PAMI/extras/visualize/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/extras/visualize/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1897 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/extras/visualize/graphs.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.442670 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.452722 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    14510 2024-05-01 06:38:00.000000 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23166 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6856 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.454470 pami-2024.5.1/PAMI/frequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.472104 pami-2024.5.1/PAMI/frequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    13840 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/Apriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5467 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/Apriori2.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13544 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/Aprioribitset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13342 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/ECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13827 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/ECLATDiffset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13937 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/ECLATbitset.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19498 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/FPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15219 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/_Apriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22703 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/_FPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6818 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.475124 pami-2024.5.1/PAMI/frequentPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    20178 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/closed/CHARM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6580 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.483358 pami-2024.5.1/PAMI/frequentPattern/cuda/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5980 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13664 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cuApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14418 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cuAprioriBit.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    13015 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cuEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14583 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cuEclatBit.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14499 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17118 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    14178 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.486680 pami-2024.5.1/PAMI/frequentPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    26092 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6561 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.493933 pami-2024.5.1/PAMI/frequentPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5573 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    15452 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/pyspark/parallelApriori.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    12947 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/pyspark/parallelECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17438 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.497491 pami-2024.5.1/PAMI/frequentPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)    15426 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/frequentPattern/topk/FAE.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4575 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/frequentPattern/topk/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.498463 pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.502344 pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28229 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6652 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.503409 pami-2024.5.1/PAMI/fuzzyFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.510424 pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22973 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28621 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6442 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.511597 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.516470 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    26162 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28607 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6730 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.517593 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.522525 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28521 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    33585 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6623 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.523710 pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.528564 pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22562 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6469 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.529908 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.534445 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25786 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    27472 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6683 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.535681 pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.538485 pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22107 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6791 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.539582 pami-2024.5.1/PAMI/georeferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/georeferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.546563 pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22965 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21152 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6697 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.547700 pami-2024.5.1/PAMI/georeferencedFrequentSequencePattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/georeferencedFrequentSequencePattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6696 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/georeferencedFrequentSequencePattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.548714 pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.552062 pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    21718 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6178 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.553287 pami-2024.5.1/PAMI/highUtilityFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.556044 pami-2024.5.1/PAMI/highUtilityFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    38524 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6179 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.559387 pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.563106 pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    42772 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6307 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.564260 pami-2024.5.1/PAMI/highUtilityPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.572558 pami-2024.5.1/PAMI/highUtilityPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35224 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPattern/basic/EFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26511 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPattern/basic/HMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28921 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPattern/basic/UPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19909 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPattern/basic/efimParallel.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.578593 pami-2024.5.1/PAMI/highUtilityPattern/parallel/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPattern/parallel/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5166 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPattern/parallel/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17831 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPattern/parallel/efimparallel.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.584069 pami-2024.5.1/PAMI/highUtilityPatternsInStreams/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30022 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPatternsInStreams/HUPMS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    32848 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPatternsInStreams/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5193 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilityPatternsInStreams/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.586403 pami-2024.5.1/PAMI/highUtilitySpatialPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6716 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.594624 pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29850 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    37379 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5934 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.599643 pami-2024.5.1/PAMI/highUtilitySpatialPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)    37519 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6618 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/highUtilitySpatialPattern/topk/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.601218 pami-2024.5.1/PAMI/localPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/localPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.607770 pami-2024.5.1/PAMI/localPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35370 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    24681 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23709 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/localPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     8385 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/localPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.611097 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.616047 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    24308 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    22008 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5921 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.617479 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.622661 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28270 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21954 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5398 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.626027 pami-2024.5.1/PAMI/partialPeriodicPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.634531 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    26420 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4329 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/Gabstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    25525 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19577 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5520 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.638550 pami-2024.5.1/PAMI/partialPeriodicPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    22070 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/closed/PPPClose.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5605 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.644525 pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29144 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4278 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.648144 pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5765 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28554 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.652976 pami-2024.5.1/PAMI/partialPeriodicPattern/topk/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/topk/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6441 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/topk/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19588 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.659750 pami-2024.5.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28160 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6350 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.660855 pami-2024.5.1/PAMI/periodicCorrelatedPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicCorrelatedPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.664112 pami-2024.5.1/PAMI/periodicCorrelatedPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27559 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicCorrelatedPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6691 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicCorrelatedPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.665207 pami-2024.5.1/PAMI/periodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.682575 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    17272 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    21302 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26383 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18106 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFPMC.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    36300 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17904 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28583 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      726 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6549 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26643 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.687595 pami-2024.5.1/PAMI/periodicFrequentPattern/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)    24417 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6539 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/closed/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.692951 pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6568 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    23867 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    18982 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.698907 pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/
+-rw-r--r--   0 vanithak   (502) staff       (20)    31832 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7869 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.702417 pami-2024.5.1/PAMI/periodicFrequentPattern/pyspark/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/pyspark/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     5219 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/pyspark/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26749 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.704239 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.710674 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/TopkPFP/
+-rw-r--r--   0 vanithak   (502) staff       (20)    19951 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/TopkPFP/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6862 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.714187 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4589 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    17514 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.715669 pami-2024.5.1/PAMI/recurringPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/recurringPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.719934 pami-2024.5.1/PAMI/recurringPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29135 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/recurringPattern/basic/RPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/recurringPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6637 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/recurringPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.721263 pami-2024.5.1/PAMI/relativeFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/relativeFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.727543 pami-2024.5.1/PAMI/relativeFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30349 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/relativeFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4261 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/relativeFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.729029 pami-2024.5.1/PAMI/relativeHighUtilityPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/relativeHighUtilityPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.732436 pami-2024.5.1/PAMI/relativeHighUtilityPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    35406 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/relativeHighUtilityPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6052 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/relativeHighUtilityPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.733838 pami-2024.5.1/PAMI/sequence/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequence/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.734495 pami-2024.5.1/PAMI/sequentialPatternMining/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequentialPatternMining/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.742926 pami-2024.5.1/PAMI/sequentialPatternMining/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    42265 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/sequentialPatternMining/basic/SPADE.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19986 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/sequentialPatternMining/basic/SPAM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequentialPatternMining/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6569 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequentialPatternMining/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    24786 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/sequentialPatternMining/basic/prefixSpan.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.746056 pami-2024.5.1/PAMI/sequentialPatternMining/closed/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequentialPatternMining/closed/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6285 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequentialPatternMining/closed/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/sequentialPatternMining/closed/bide.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.746629 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.753506 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    18431 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    26770 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19807 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7271 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.758825 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/topK/
+-rw-r--r--   0 vanithak   (502) staff       (20)    27859 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/topK/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7173 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.761037 pami-2024.5.1/PAMI/subgraphMining/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.773301 pami-2024.5.1/PAMI/subgraphMining/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1241 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2396 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/dfsCode.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/edge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2616 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/extendedEdge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      670 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/frequentSubgraph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4943 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/graph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28244 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/gspan.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1748 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      826 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/subgraphMining/basic/vertex.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.794423 pami-2024.5.1/PAMI/subgraphMining/topK/
+-rw-r--r--   0 vanithak   (502) staff       (20)     1949 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/DFSCode.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      593 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/DFSThread.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1316 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      772 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/edge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     2613 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/extendedEdge.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      674 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/frequentSubgraph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4295 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/graph.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     1486 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20979 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/tkg.py
+-rw-r--r--   0 vanithak   (502) staff       (20)      818 2024-03-29 21:11:29.000000 pami-2024.5.1/PAMI/subgraphMining/topK/vertex.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.797977 pami-2024.5.1/PAMI/uncertainFaultTolerantFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)    17694 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainFaultTolerantFrequentPattern/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6756 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.799172 pami-2024.5.1/PAMI/uncertainFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.815201 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    28251 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    27430 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20311 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/TUFP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    20112 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/TubeP.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    28047 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/TubeS.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    25675 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    19871 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4945 2024-04-09 02:02:26.000000 pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.816487 pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.821020 pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    30577 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4986 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.822263 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)      727 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.830065 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    33173 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    33178 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6536 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.831283 pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.834506 pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29414 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6603 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.835604 pami-2024.5.1/PAMI/weightedFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.839197 pami-2024.5.1/PAMI/weightedFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    25844 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/weightedFrequentPattern/basic/WFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     6659 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.840540 pami-2024.5.1/PAMI/weightedFrequentRegularPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentRegularPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.845171 pami-2024.5.1/PAMI/weightedFrequentRegularPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    29035 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentRegularPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     7495 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.846076 pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/__init__.py
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.848951 pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/basic/
+-rw-r--r--   0 vanithak   (502) staff       (20)    31245 2024-05-01 06:38:01.000000 pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py
+-rw-r--r--   0 vanithak   (502) staff       (20)        0 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/basic/__init__.py
+-rw-r--r--   0 vanithak   (502) staff       (20)     4771 2024-03-12 04:33:29.000000 pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py
+-rw-r--r--   0 vanithak   (502) staff       (20)    71688 2024-05-01 06:53:05.856971 pami-2024.5.1/PKG-INFO
+-rw-r--r--   0 vanithak   (502) staff       (20)    70199 2024-05-01 06:38:01.000000 pami-2024.5.1/README.md
+drwxr-xr-x   0 vanithak   (502) staff       (20)        0 2024-05-01 06:53:05.853639 pami-2024.5.1/pami.egg-info/
+-rw-r--r--   0 vanithak   (502) staff       (20)    71688 2024-05-01 06:53:05.000000 pami-2024.5.1/pami.egg-info/PKG-INFO
+-rw-r--r--   0 vanithak   (502) staff       (20)    18316 2024-05-01 06:53:05.000000 pami-2024.5.1/pami.egg-info/SOURCES.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)        1 2024-05-01 06:53:05.000000 pami-2024.5.1/pami.egg-info/dependency_links.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)      255 2024-05-01 06:53:05.000000 pami-2024.5.1/pami.egg-info/requires.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)        5 2024-05-01 06:53:05.000000 pami-2024.5.1/pami.egg-info/top_level.txt
+-rw-r--r--   0 vanithak   (502) staff       (20)       38 2024-05-01 06:53:05.858117 pami-2024.5.1/setup.cfg
+-rw-r--r--   0 vanithak   (502) staff       (20)     1533 2024-05-01 06:52:41.000000 pami-2024.5.1/setup.py
```

### Comparing `pami-2024.4.9.1/LICENSE` & `pami-2024.5.1/LICENSE`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithConfidence.py` & `pami-2024.5.1/PAMI/AssociationRules/basic/ARWithConfidence.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,13 +1,12 @@
-#  This code uses "confidence" metric to extract the association rules from given frequent patterns.
+# This code uses "confidence" metric to extract the association rules from given frequent patterns.
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
-#
 #             import PAMI.AssociationRules.basic import ARWithConfidence as alg
 #
 #             obj = alg.ARWithConfidence(iFile, minConf)
 #
 #             obj.mine()
 #
 #             associationRules = obj.getPatterns()
@@ -29,16 +28,14 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -50,22 +47,17 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
      
 """
 
-
-
-
 from PAMI.AssociationRules.basic import abstract as _ab
 from deprecated import deprecated
 
-
-
 class _Confidence:
     """
     :param  patterns: Dictionary containing patterns and its support value.
     :type patterns: dict
     :param  singleItems: List containing all the single frequent items.
     :type singleItems: list
     :param  minConf: Minimum confidence to mine all the satisfying association rules.
@@ -85,14 +77,15 @@
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
+
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             conf = self._generateWithConfidence(prefix, suffix[0])
@@ -103,14 +96,15 @@
                 self._generateWithConfidence(prefix + ' ' + suffix[i], suffix[j])
                 # self._generation(prefix+ ' ' +suffix[i], suffix[i+1:])
             self._generation(prefix1, suffix1)
 
     def _generateWithConfidence(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
+
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -135,66 +129,53 @@
             for j in range(i + 1, len(self._singleItems)):
                 self._generateWithConfidence(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 
 class ARWithConfidence:
     """
-    :Description: Association Rules are derived from frequent patterns using "confidence" metric.
-
-    :Reference:
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of association rules
-    :param  oFile: str :
-                   Name of the output file to store complete set of association rules
-    :param  minConf: float :
-                   The user can specify the minConf in float between the range of 0 to 1.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-        
-    :Attributes:
+    About this algorithm
+    ====================
 
+    :**Description**: Association Rules are derived from frequent patterns using "confidence" metric.
 
-        startTime : float
-            To record the start time of the mining process
+    :**Reference**:
 
-        endTime : float
-            To record the completion time of the mining process
+    :**Parameters**:    - **iFile** (*str*) -- *Name of the Input file to mine complete set of association rules*
+                        - **oFile** (*str*) -- *Name of the Output file to write association rules*
+                        - **minConf** (*float*) -- *Minimum confidence to mine all the satisfying association rules. The user can specify the minConf in float between the range of 0 to 1.*
+                        - **sep** (*str*) -- *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
 
-        finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
+                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
+                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
+                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
+                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
 
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
 
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+    Execution methods
+    =================
 
-
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 ARWithConfidence.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 ARWithConfidence.py sampleDB.txt patterns.txt 0.5 ' '
 
-    .. note:: minConf will be considered only in 0 to 1.
+    .. note:: minConf can be specified in a value between 0 and 1.
     
     
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    **Calling from a python program**
+
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import ARWithConfidence as alg
 
             obj = alg.ARWithConfidence(iFile, minConf)
 
             obj.mine()
@@ -216,18 +197,19 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    **Credits:**
-    -------------
+    Credits
+    =======
+
+            The complete program was written by P. Likhitha  under the supervision of Professor Rage Uday Kiran.
 
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
     """
 
     _minConf = float()
     _startTime = float()
     _endTime = float()
     _iFile = " "
     _oFile = " "
@@ -295,26 +277,15 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Association rule mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        k = self._readPatterns()
-        a = _Confidence(self._frequentPatterns, k, self._minConf)
-        a.run()
-        self._finalPatterns = a._finalPatterns
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Association rules successfully  generated from frequent patterns ")
+        self.mine()
 
 
 
     def mine(self):
         """
         Association rule mining process will start from here
         """
@@ -330,41 +301,45 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -372,26 +347,28 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the outputfile
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
```

### Comparing `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLeverage.py` & `pami-2024.5.1/PAMI/AssociationRules/basic/ARWithLift.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,16 +1,18 @@
-# This code uses "leverage" metric to extract the association rules from given frequent patterns.
+# This code uses "lift" metric to extract the association rules from given frequent patterns.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
 #
+#             import PAMI.AssociationRules.basic import ARWithLift as alg
 #
-#             import PAMI.AssociationRules.basic import ARWithLeverage as alg
+#             iFile = 'sampleDB.txt'
 #
-#             obj = alg.ARWithLeverage(iFile, minConf)
+#             minConf = 0.75  # can also be specified between 0 and 1
+#
+#             obj = alg.ARWithLift(iFile, minConf)
 #
 #             obj.mine()
 #
 #             associationRules = obj.getPatterns()
 #
 #             print("Total number of Association Rules:", len(associationRules))
 #
@@ -28,17 +30,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
-
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,36 +47,29 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
 from PAMI.AssociationRules.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 
-class _Leverage:
-
+class Lift:
     """
-    :param patterns: Dictionary containing patterns and its support value.
-
-    :type patterns: dict
-
+    :param  patterns: Dictionary containing patterns and its support value.
+    :type  patterns: dict
     :param  singleItems: List containing all the single frequent items.
-
-    :type singleItems: list
-
+    :type  singleItems: list
     :param  minConf: Minimum confidence to mine all the satisfying association rules.
-
-    :type minConf: int
+    :type  minConf: int
     """
-
+    
     def __init__(self, patterns, singleItems, minConf) -> None:
         """
         :param patterns: given frequent patterns
         :type patterns: dict
         :param singleItems: one-length frequent patterns
         :type singleItems: list
         :param minConf: minimum confidence
@@ -88,48 +80,50 @@
         self._singleItems = singleItems
         self._minConf = minConf
         self._finalPatterns = {}
 
     def _generation(self, prefix, suffix) -> None:
         """
         To generate the combinations all association rules.
+
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
+        :return: None
         """
-
-
         if len(suffix) == 1:
-            conf = self._generateWithLeverage(prefix, suffix[0])
+            self._generateWithLift(prefix, suffix[0])
         for i in range(len(suffix)):
             suffix1 = suffix[:i] + suffix[i + 1:]
             prefix1 = prefix + ' ' + suffix[i]
             for j in range(i + 1, len(suffix)):
-                self._generateWithLeverage(prefix + ' ' + suffix[i], suffix[j])
+                self._generateWithLift(prefix + ' ' + suffix[i], suffix[j])
+                # self._generation(prefix+ ' ' +suffix[i], suffix[i+1:])
             self._generation(prefix1, suffix1)
 
-    def _generateWithLeverage(self, lhs, rhs) -> float:
+    def _generateWithLift(self, lhs, rhs)  -> float:
         """
         To find association rules satisfying user-specified minConf
+
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         :return: the association rule
         :rtype: float
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
             return 0
         minimum = self._frequentPatterns[s]
         conf_lhs = minimum / self._frequentPatterns[lhs]
         conf_rhs = minimum / self._frequentPatterns[rhs]
-        lift_lhs = conf_lhs - self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
-        right_rhs = conf_rhs - self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
+        lift_lhs = conf_lhs / self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
+        right_rhs = conf_rhs / self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
         if lift_lhs >= self._minConf:
             s1 = lhs + '->' + rhs
             self._finalPatterns[s1] = conf_lhs
         if right_rhs >= self._minConf:
             s1 = rhs + '->' + lhs
             self._finalPatterns[s1] = conf_rhs
 
@@ -137,76 +131,68 @@
         """
         To generate the combinations all association rules.
         """
         for i in range(len(self._singleItems)):
             suffix = self._singleItems[:i] + self._singleItems[i + 1:]
             prefix = self._singleItems[i]
             for j in range(i + 1, len(self._singleItems)):
-                conf = self._generateWithLeverage(self._singleItems[i], self._singleItems[j])
+                self._generateWithLift(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 
-class ARWithLeverage:
+class ARWithLift:
     """
-    :Description: Association Rules are derived from frequent patterns using "leverage" metric.
-
-    :Reference:
+    About this algorithm
+    ====================
 
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of association rules
-    :param  oFile: str :
-                   Name of the output file to store complete set of association rules
-    :param  minConf: float :
-                   The user can specify the minConf in float between the range of 0 to 1.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-        
-        
-    :Attributes:
+    :**Description**: Association Rules are derived from frequent patterns using "lift" metric.
 
+    :**Reference**:
 
-        startTime : float
-            To record the start time of the mining process
+    :**Parameters**:    - **iFile** (*str*) -- *Name of the Input file to mine complete set of association rules.*
+                        - **oFile** (*str*) -- *Name of the output file to store complete set of association rules.*
+                        - **minConf** (*float*) -- *The user can specify the minConf in float between the range of 0 to 1.*
+                        - **sep** (*str*) --  *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
 
-        endTime : float
-            To record the completion time of the mining process
+    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
+                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
+                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
+                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
+                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
 
-        finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
 
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+    Execution methods
+    =================
 
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-
-
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ARWithLeverage.py <inputFile> <outputFile> <minConf> <sep>
+      (.venv) $ python3 ARWithLift.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
-      (.venv) $ python3 ARWithLeverage.py sampleDB.txt patterns.txt 10.0 ' '
-
-    .. note:: minConf will be considered only in 0 to 1.
+      (.venv) $ python3 ARWithLift.py sampleDB.txt patterns.txt 0.5 ' '
 
+    .. note:: minConf can be specified in a value between 0 and 1.
     
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    
+    **Calling from a python program**
+
     .. code-block:: python
 
-            import PAMI.AssociationRules.basic import ARWithLeverage as alg
+            import PAMI.AssociationRules.basic import ARWithLift as alg
+
+            iFile = 'sampleDB.txt'
 
-            obj = alg.ARWithLeverage(iFile, minConf)
+            minConf = 0.75  # can also be specified between 0 and 1
+
+            obj = alg.ARWithLift(iFile, minConf)
 
             obj.mine()
 
             associationRules = obj.getPatterns()
 
             print("Total number of Association Rules:", len(associationRules))
 
@@ -221,41 +207,43 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-            
-    **Credits:**
-    --------------------
+
+
+    Credits
+    =======
 
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     def __init__(self, iFile, minConf, sep) -> None:
         """
         :param iFile: input file name or path
         :type iFile: str
-        :param minConf: The user can specify the minConf in float between the range of 0 to 1.
+        :param minConf: minimum confidence
         :type minConf: float
-        :param sep: This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+        :param sep: Delimiter of input file
         :type sep: str
         :return: None
         """
         self._iFile = iFile
         self._minConf = minConf
         self._finalPatterns = {}
         self._sep = sep
 
     def _readPatterns(self) -> list:
         """
-        To read patterns  of leverage.
-        :return: List of patterns
+        Reading the input file and storing all the frequent patterns and their support respectively in a frequentPatterns variable.
+
+        :return: list of frequent patterns and their support respectively in a frequentPatterns
         :rtype: list
         """
         self._frequentPatterns = {}
         k = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             pattern, sup = [], []
             if self._iFile.empty:
@@ -295,74 +283,67 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Association rule mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        k = self._readPatterns()
-        a = _Leverage(self._frequentPatterns, k, self._minConf)
-        a.run()
-        self._finalPatterns = a._finalPatterns
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Association rules successfully  generated from frequent patterns ")
+        self.mine()
 
     def mine(self) -> None:
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
         k = self._readPatterns()
-        a = _Leverage(self._frequentPatterns, k, self._minConf)
+        a = Lift(self._frequentPatterns, k, self._minConf)
         a.run()
         self._finalPatterns = a._finalPatterns
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -370,27 +351,29 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> dict:
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
@@ -402,21 +385,19 @@
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ARWithLeverage(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
+            _ap = ARWithLift(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ARWithLeverage(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = ARWithLift(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.4.9.1/PAMI/AssociationRules/basic/ARWithLift.py` & `pami-2024.5.1/PAMI/frequentPattern/basic/Aprioribitset.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,22 +1,21 @@
-#  This code uses "lift" metric to extract the association rules from given frequent patterns.
-#
+# AprioriBitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# ---------------------------------------------------------
 #
-#             import PAMI.AssociationRules.basic import ARWithLift as alg
+#             import PAMI.frequentPattern.basic.AprioriBitset as alg
 #
-#             obj = alg.ARWithLift(iFile, minConf)
+#             obj = alg.AprioriBitset(iFile, minSup)
 #
 #             obj.mine()
 #
-#             associationRules = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Association Rules:", len(associationRules))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -28,17 +27,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
-
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,166 +44,84 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
-
-from PAMI.AssociationRules.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.frequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
-class Lift:
-
-    """
-    :param  patterns: Dictionary containing patterns and its support value.
-    :type  patterns: dict
-    :param  singleItems: List containing all the single frequent items.
-    :type  singleItems: list
-    :param  minConf: Minimum confidence to mine all the satisfying association rules.
-    :type  minConf: int
-    """
-    
-    def __init__(self, patterns, singleItems, minConf) -> None:
-        """
-        :param patterns: given frequent patterns
-        :type patterns: dict
-        :param singleItems: one-length frequent patterns
-        :type singleItems: list
-        :param minConf: minimum confidence
-        :type minConf: float
-        :return: None
-        """
-        self._frequentPatterns = patterns
-        self._singleItems = singleItems
-        self._minConf = minConf
-        self._finalPatterns = {}
-
-    def _generation(self, prefix, suffix) -> None:
-        """
-        To generate the combinations all association rules.
-        :param prefix: the prefix of association rule.
-        :type prefix: str
-        :param suffix: the suffix of association rule.
-        :type suffix: str
-        :return: None
-        """
-        if len(suffix) == 1:
-            self._generateWithLift(prefix, suffix[0])
-        for i in range(len(suffix)):
-            suffix1 = suffix[:i] + suffix[i + 1:]
-            prefix1 = prefix + ' ' + suffix[i]
-            for j in range(i + 1, len(suffix)):
-                self._generateWithLift(prefix + ' ' + suffix[i], suffix[j])
-                # self._generation(prefix+ ' ' +suffix[i], suffix[i+1:])
-            self._generation(prefix1, suffix1)
-
-    def _generateWithLift(self, lhs, rhs)  -> float:
-        """
-        To find association rules satisfying user-specified minConf
-        :param lhs: the prefix of association rule.
-        :type lhs: str
-        :param rhs: the suffix of association rule.
-        :type rhs: str
-        :return: the association rule
-        :rtype: float
-        """
-        s = lhs + '\t' + rhs
-        if self._frequentPatterns.get(s) == None:
-            return 0
-        minimum = self._frequentPatterns[s]
-        conf_lhs = minimum / self._frequentPatterns[lhs]
-        conf_rhs = minimum / self._frequentPatterns[rhs]
-        lift_lhs = conf_lhs / self._frequentPatterns[rhs] * self._frequentPatterns[lhs]
-        right_rhs = conf_rhs / self._frequentPatterns[lhs] * self._frequentPatterns[rhs]
-        if lift_lhs >= self._minConf:
-            s1 = lhs + '->' + rhs
-            self._finalPatterns[s1] = conf_lhs
-        if right_rhs >= self._minConf:
-            s1 = rhs + '->' + lhs
-            self._finalPatterns[s1] = conf_rhs
-
-    def run(self) -> None:
-        """
-        To generate the combinations all association rules.
-        """
-        for i in range(len(self._singleItems)):
-            suffix = self._singleItems[:i] + self._singleItems[i + 1:]
-            prefix = self._singleItems[i]
-            for j in range(i + 1, len(self._singleItems)):
-                self._generateWithLift(self._singleItems[i], self._singleItems[j])
-            self._generation(prefix, suffix)
-
 
-class ARWithLift:
+class AprioriBitset(_ab._frequentPatterns):
     """
-    :Description: Association Rules are derived from frequent patterns using "lift" metric.
+    :Description:  AprioriBitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
-    :Reference:
+    :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
+            372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of association rules
+                   Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of association rules
-    :param  minConf: float :
-                   The user can specify the minConf in float between the range of 0 to 1.
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-        
     :Attributes:
 
-
         startTime : float
-            To record the start time of the mining process
+          To record the start time of the mining process
 
         endTime : float
-            To record the completion time of the mining process
+          To record the completion time of the mining process
 
         finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+          Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+          To store the total amount of RSS memory consumed by the program
+
+        Database : list
+          To store the transactions of a database in list
 
 
     **Methods to execute code on terminal**
-    ----------------------------------------------------
+    ------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ARWithLift.py <inputFile> <outputFile> <minConf> <sep>
+      (.venv) $ python3 AprioriBitset.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ARWithLift.py sampleDB.txt patterns.txt 0.5 ' '
+      (.venv) $ python3 AprioriBitset.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup will be considered in percentage of database transactions
+
 
-    .. note:: minConf will be considered only in 0 to 1.
-    
-    
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
+    ---------------------------------------------------------
     .. code-block:: python
 
-            import PAMI.AssociationRules.basic import ARWithLift as alg
+            import PAMI.frequentPattern.basic.AprioriBitset as alg
 
-            obj = alg.ARWithLift(iFile, minConf)
+            obj = alg.AprioriBitset(iFile, minSup)
 
             obj.mine()
 
-            associationRules = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Association Rules:", len(associationRules))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
@@ -216,200 +130,279 @@
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
-            
+
     **Credits:**
-    -----------------------
+    -------------------
 
-             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+               The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    def __init__(self, iFile, minConf, sep) -> None:
-        """
-        :param iFile: input file name or path
-        :type iFile: str
-        :param minConf: minimum confidence
-        :type minConf: float
-        :param sep: Delimiter of input file
-        :type sep: str
-        :return: None
+    _startTime = float()
+    _endTime = float()
+    _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    _minSup = str()
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _Database = []
+    _mapSupport = {}
+    _lno = 0
+
+    def _convert(self, value):
+        """
+        To convert the user specified minSup value
+
+        :param value: user specified minSup value
+
+        :type value: int
+
+        :return: converted type
+
+        :rtype: int or float or string
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _creatingItemSets(self):
         """
-        self._iFile = iFile
-        self._minConf = minConf
-        self._finalPatterns = {}
-        self._sep = sep
-
-    def _readPatterns(self) -> list:
-        """
-        Reading the input file and storing all the frequent patterns and their support respectively in a frequentPatterns variable.
-        :return: list of frequent patterns and their support respectively in a frequentPatterns
-        :rtype: list
+        Storing the complete transactions of the database/input file in a database variable
         """
-        self._frequentPatterns = {}
-        k = []
+        self._Database = []
+        self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            pattern, sup = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'pattern' in i:
-                pattern = self._iFile['pattern'].tolist()
-            if 'support' in i:
-                support = self._iFile['support'].tolist()
-            for i in range(len(pattern)):
-                s = '\t'.join(pattern[i])
-                self._frequentPattern[s] = support[i]
+            if 'Transactions' in i:
+                self._Database = self._iFile['Transactions'].tolist()
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line = line.strip()
-                    line = line.split(':')
-                    s = line[0].split(self._sep)
-                    s = '\t'.join(s)
-                    self._frequentPatterns[s.strip()] = int(line[1])
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line = line.strip()
-                            line = line.split(':')
-                            s = line[0].split(self._sep)
-                            for j in s:
-                                if j not in k:
-                                    k.append(j)
-                            s = '\t'.join(s)
-                            self._frequentPatterns[s.strip()] = int(line[1])
+                            self._lno += 1
+                            splitter = [i.rstrip() for i in line.split(self._sep)]
+                            splitter = [x for x in splitter if x]
+                            self._Database.append(splitter)
                 except IOError:
                     print("File Not Found")
-                    quit()
-        return k
+        self._minSup = self._convert(self._minSup)
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
-        Association rule mining process will start from here
+        Frequent pattern mining process will start from here
+        We start with the scanning the itemSets and store the bitsets respectively.
+        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
         """
-        self._startTime = _ab._time.time()
-        k = self._readPatterns()
-        a = Lift(self._frequentPatterns, k, self._minConf)
-        a.run()
-        self._finalPatterns = a._finalPatterns
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Association rules successfully  generated from frequent patterns ")
+        self.mine()
+
+    def _bitPacker(self, data, maxIndex):
+        """
+        It takes the data and maxIndex as input and generates integer as output value.
+
+        :param data: it takes data as input.
+
+        :type data: int or float
+
+        :param maxIndex: It converts the data into bits By taking the maxIndex value as condition.
+
+        :type maxIndex: int
+
+        """
+        packed_bits = 0
+        for i in data:
+            packed_bits |= 1 << (maxIndex - i)
+
+        return packed_bits
 
     def mine(self) -> None:
         """
-        Association rule mining process will start from here
+        Frequent pattern mining process will start from here
+        # Bitset implementation
         """
         self._startTime = _ab._time.time()
-        k = self._readPatterns()
-        a = Lift(self._frequentPatterns, k, self._minConf)
-        a.run()
-        self._finalPatterns = a._finalPatterns
+
+        self._Database = []
+
+        self._creatingItemSets()
+
+        items = {}
+        index = 0
+        for line in self._Database:
+            for item in line:
+                if tuple([item]) in items:
+                    items[tuple([item])].append(index)
+                else:
+                    items[tuple([item])] = [index]
+            index += 1
+
+        # sort by length in descending order
+        items = dict(sorted(items.items(), key=lambda x: len(x[1]), reverse=True))
+        cands = []
+        for key in items:
+            if len(items[key]) >= self._minSup:
+                self._finalPatterns[key] = len(items[key])
+                cands.append(key)
+                items[key] = self._bitPacker(items[key], index)
+                # print(key, items[key])
+            else:
+                break
+
+        while cands:
+            newCands = []
+            for i in range(len(cands)):
+                for j in range(i + 1, len(cands)):
+                    if cands[i][:-1] == cands[j][:-1]:
+                        newCand = tuple(cands[i] + tuple([cands[j][-1]]))
+                        intersection = items[tuple([newCand[0]])]
+                        for k in range(1, len(newCand)):
+                            intersection &= items[tuple([newCand[k]])]
+                        count = int.bit_count(intersection)
+                        if count >= self._minSup:
+                            newCands.append(newCand)
+                            self._finalPatterns[newCand] = count
+                    else:
+                        break
+
+            cands = newCands
+
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Association rules successfully  generated from frequent patterns ")
+        print("Frequent patterns were generated successfully using Apriori algorithm ")
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
+
         """
 
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
+        # time = _ab._time.time()
+        # dataFrame = {}
+        # data = []
+        # for a, b in self._finalPatterns.items():
+        #     # data.append([a.replace('\t', ' '), b])
+        #     data.append([" ".join(a), b])
+        #     dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # print("Time taken to convert the frequent patterns into DataFrame is: ", _ab._time.time() - time)
+
+
+        dataFrame = _ab._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
+
         return dataFrame
 
-    def save(self, outFile) -> None:
+    def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the outputfile
-        :type outFile: file
+
+        :param outFile: name of the output file
+
+        :type outFile: csvfile
+
         :return: None
+
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+        with open(outFile, 'w') as f:
+            for x, y in self._finalPatterns.items():
+                x = self._sep.join(x)
+                f.write(f"{x} : {y}\n")
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
-        Function to send the result after completion of the mining process
+        This function is used to print the result
         """
-        print("Total number of Association Rules:", len(self.getPatterns()))
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ARWithLift(_ab._sys.argv[1], float(_ab._sys.argv[3]), _ab._sys.argv[4])
+            _ap = AprioriBitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ARWithLift(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = AprioriBitset(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Association Rules:", len(_ap.getPatterns()))
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

### Comparing `pami-2024.4.9.1/PAMI/AssociationRules/basic/RuleMiner.py` & `pami-2024.5.1/PAMI/AssociationRules/basic/RuleMiner.py`

 * *Files 4% similar despite different names*

```diff
@@ -64,14 +64,15 @@
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
+
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             conf = self._generaeWithConfidence(prefix, suffix[0])
@@ -82,14 +83,15 @@
                 self._generaeWithConfidence(prefix + ' ' + suffix[i], suffix[j])
                 #self._generation(prefix+ ' ' +suffix[i], suffix[i+1:]) 
             self._generation(prefix1, suffix1)
             
     def _generaeWithConfidence(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
+
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -135,14 +137,15 @@
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
+
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             self._generateWithLift(prefix, suffix[0])
@@ -153,14 +156,15 @@
                 self._generateWithLift(prefix + ' ' + suffix[i], suffix[j])
                 #self._generation(prefix+ ' ' +suffix[i], suffix[i+1:]) 
             self._generation(prefix1, suffix1)
             
     def _generateWithLift(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
+
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -208,14 +212,15 @@
         self._singleItems = singleItems
         self._threshold = threshold
         self._finalPatterns = {}
         
     def _generation(self, prefix, suffix):
         """
         To generate the combinations all association rules.
+
         :param prefix: the prefix of association rule.
         :type prefix: str
         :param suffix: the suffix of association rule.
         :type suffix: str
         """
         if len(suffix) == 1:
             conf = self._generateWithLeverage(prefix, suffix[0])
@@ -225,14 +230,15 @@
             for j in range(i+1, len(suffix)):
                 self._generateWithLeverage(prefix + ' ' + suffix[i], suffix[j])
             self._generation(prefix1, suffix1)
             
     def _generateWithLeverage(self, lhs, rhs):
         """
         To find association rules satisfying user-specified minConf
+
         :param lhs: the prefix of association rule.
         :type lhs: str
         :param rhs: the suffix of association rule.
         :type rhs: str
         """
         s = lhs + '\t' + rhs
         if self._frequentPatterns.get(s) == None:
@@ -258,14 +264,17 @@
             prefix = self._singleItems[i]
             for j in range(i+1, len(self._singleItems)):
                 conf = self._generateWithLeverage(self._singleItems[i], self._singleItems[j])
             self._generation(prefix, suffix)
 
 class RuleMiner:
     """
+    About this algorithm
+    ====================
+
     :Description: RuleMiner code is used to extract the association rules from given frequent patterns
 
     :Reference:
 
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of association rules
@@ -297,32 +306,34 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
 
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+    Execution methods
+    =================
+
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 RuleMiner.py <inputFile> <outputFile> <minConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 RuleMiner.py sampleDB.txt patterns.txt 0.5 ' '
 
-    .. note:: minConf will be considered only in 0 to 1.
+    .. note:: minConf can be specified in a value between 0 and 1.
+
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
             import PAMI.AssociationRules.basic import RuleMiner as alg
 
             obj = alg.RuleMiner(iFile, measure, o.5, "\t")
 
             obj.mine()
@@ -345,15 +356,15 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     :Methods:
 
-            startMine()
+            mine()
     """
 
     def __init__(self, iFile, measure, threshold, sep):
         """
         :param iFile: input file name or path
         :type iFile: str
         :param measure: measure
@@ -414,35 +425,15 @@
         return k
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Association rule mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        k = self._readPatterns()
-        if self._measure == 'confidence':
-            a = Confidence(self._frequentPatterns, k, self._threshold)
-            a.run()
-            self._finalPatterns = a._finalPatterns
-        if self._measure == 'lift':
-            a = Lift(self._frequentPatterns, k, self._threshold)
-            a.run()
-            self._finalPatterns = a._finalPatterns
-        if self._measure == 'leverage':
-            a = Leverage(self._frequentPatterns, k, self._threshold)
-            a.run()
-            self._finalPatterns = a._finalPatterns
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Association rules successfully  generated from frequent patterns ")
+        self.mine()
 
 
     def mine(self):
         """
         Association rule mining process will start from here
         """
         self._startTime = _ab._time.time()
@@ -466,41 +457,45 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Association rules successfully  generated from frequent patterns ")
     
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -508,26 +503,28 @@
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to a output file
+
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
         """
@@ -543,14 +540,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = RuleMiner(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = RuleMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Association Rules:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         _ap = RuleMiner('sensorOutput.txt', "lift", 0.5, '\t')
```

### Comparing `pami-2024.4.9.1/PAMI/AssociationRules/basic/abstract.py` & `pami-2024.5.1/PAMI/AssociationRules/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/correlatedPattern/__init__.py` & `pami-2024.5.1/PAMI/correlatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMine.py` & `pami-2024.5.1/PAMI/correlatedPattern/basic/CoMine.py`

 * *Files 10% similar despite different names*

```diff
@@ -87,16 +87,18 @@
         self.counter = 1
         self.parent = None
         self.child = []
         self.nodeLink = None
 
     def getChild(self, id1) -> Union[None, '_Node']:
         """
-        Param id1: give item id as input
-        type id1:
+        :param id1: give item id as input
+        :type id1: int
+        :return: the node with same itemId
+        :rtype: _Node
         """
         for i in self.child:
             if i.itemId == id1:
                 return i
         return None
 
 
@@ -134,15 +136,16 @@
         self.headerList = []
         self.mapItemNodes = {}
         self.mapItemLastNodes = {}
         self.root = _Node()
 
     def addTransaction(self, transaction: List[int]) -> None:
         """
-        adding transaction into tree
+        Adding transaction into tree
+
         :param transaction : it represents a single transaction in a database
         :type transaction : list
         :return: None
         """
 
         current = self.root
         for i in transaction:
@@ -157,31 +160,32 @@
             else:
                 child.counter += 1
                 current = child
 
     def fixNodeLinks(self, item: int, newNode: '_Node') -> None:
         """
         Fixing node link for the newNode that inserted into correlatedPatternTree
+
         :param item: it represents the item of newNode
         :type item : int
         :param newNode : it represents the newNode that inserted in correlatedPatternTree
         :type newNode : Node
         :return: None
-
         """
         if item in self.mapItemLastNodes.keys():
             lastNode = self.mapItemLastNodes[item]
             lastNode.nodeLink = newNode
         self.mapItemLastNodes[item] = newNode
         if item not in self.mapItemNodes.keys():
             self.mapItemNodes[item] = newNode
 
     def printTree(self, root: '_Node') -> None:
         """
         This method is to find the details of parent, children, and support of a Node
+
         :param root: it represents the Node in correlatedPatternTree
         :type root: Node
         :return: None
         """
 
         if root.child is None:
             return
@@ -189,14 +193,15 @@
             for i in root.child:
                 print(i.itemId, i.counter, i.parent.itemId)
                 self.printTree(i)
 
     def createHeaderList(self, mapSupport: Dict[int, int], minSup: int) -> None:
         """
         To create the headerList
+
         :param mapSupport : it represents the items with their supports
         :type mapSupport : dictionary
         :param minSup : it represents the minSup
         :param minSup : float
         :return: None
         """
         
@@ -206,14 +211,15 @@
                 t1.append(x)
         itemSetBuffer = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self.headerList = [i for i in t1 if i in itemSetBuffer]
 
     def addPrefixPath(self, prefix: List['_Node'], mapSupportBeta, minSup) -> None:
         """
         To construct the conditional tree with prefix paths of a node in correlatedPatternTree
+
         :param prefix : it represents the prefix items of a Node
         :type prefix : list
         :param mapSupportBeta : it represents the items with their supports
         :param mapSupportBeta : dictionary
         :param minSup : to check the item meets with minSup
         :param minSup : float
         :return: None
@@ -236,27 +242,29 @@
                 else:
                     child.counter += pathCount
                     current = child
 
 
 class CoMine(_ab._correlatedPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description: CoMine is one of the fundamental algorithm to discover correlated  patterns in a transactional database. It is based on the traditional FP-Growth algorithm. This algorithm uses depth-first search technique to find all correlated patterns in a transactional database.
 
     :Reference: Lee, Y.K., Kim, W.Y., Cao, D., Han, J. (2003). CoMine: efficient mining of correlated patterns. In ICDM (pp. 581–584).
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of correlated patterns
     :param  oFile: str :
                    Name of the output file to store complete set of correlated patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
     :param minAllConf: float :
                     The user can specify minAllConf values within the range (0, 1).
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
@@ -283,31 +291,33 @@
         finalPatterns : dict
             it represents to store the patterns
         itemSetBuffer : list
             it represents the store the items in mining
         maxPatternLength : int
            it represents the constraint for pattern length
 
-    **Methods to execute code on terminal**
-    ------------------------------------------
+    Execution methods
+    =================
+
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 CoMine.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 CoMine.py sampleTDB.txt output.txt 0.25 0.2
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup can be specified in support count or a value between 0 and 1.
+
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    --------------------------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.correlatedPattern.basic import CoMine as alg
 
             obj = alg.CoMine(iFile, minSup, minAllConf,sep)
 
             obj.mine()
@@ -328,16 +338,17 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    ----------------------------------------
+    Credits
+    =======
+
              The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _minSup = float()
@@ -400,14 +411,15 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _getRatio(self, prefix: List[int], prefixLength: int, s: int) -> float:
         """
         A Function to get itemSet Ratio
+
         :param prefix:the path
         :type prefix: list
         :param prefixLength: length
         :type prefixLength:int
         :s :current ratio
         :type s:float
         :return: minAllConf of prefix
@@ -431,35 +443,38 @@
                     self._mapSupport[j] = 1
                 else:
                     self._mapSupport[j] += 1
 
     def _saveItemSet(self, prefix, prefixLength, support) -> None:
         """
         To save the correlated patterns mined form correlatedPatternTree
+
         :param prefix: the correlated pattern
         :type prefix: list
         :param prefixLength : the length of a correlated pattern
         :type prefixLength : int
         :param support: the support of a pattern
         :type support :  int
         :return: None
-        :The correlated patterns were stored in a global variable finalPatterns
+
+        The correlated patterns were stored in a global variable finalPatterns
         """
         all_conf = self._getRatio(prefix, prefixLength, support)
         if all_conf < self._minAllConf:
             return
         l = []
         for i in range(prefixLength):
             l.append(prefix[i])
         self._itemSetCount += 1
         self._finalPatterns[tuple(l)] = [support, all_conf]
     
     def _convert(self, value: Union[int, float, str]) -> None:
         """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
         :return: None
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -470,39 +485,40 @@
             else:
                 value = int(value)
         return value
 
     def _saveAllCombinations(self, tempBuffer, s, position, prefix, prefixLength) -> None:
         """
         Generating all the combinations for items in single branch in correlatedPatternTree
+
         :param tempBuffer: items in a single branch
         :type tempBuffer: list
         :param s : support at leaf node of a branch
         :param position : the length of a tempBuffer
         :type position : int
         :param prefix : it represents the list of leaf node
         :type prefix : list
         :param prefixLength : the length of prefix
         :type prefixLength :int
         :return: None
-        
         """
         max1 = 1 << position
         for i in range(1, max1):
             newPrefixLength = prefixLength
             for j in range(position):
                 isSet = i & (1 << j)
                 if isSet > 0:
                     prefix.insert(newPrefixLength, tempBuffer[j].itemId)
                     newPrefixLength += 1
             self._saveItemSet(prefix, newPrefixLength, s)
 
     def _correlatedPatternGrowthGenerate(self, correlatedPatternTree, prefix, prefixLength, mapSupport) -> None:
         """
         Mining the fp tree
+
         :param correlatedPatternTree: it represents the correlatedPatternTree
         :type correlatedPatternTree: class Tree
         :param prefix : it represents an empty list and store the patterns that are mined
         :type prefix : list
         :param prefixLength : the length of prefix
         :type prefixLength :int
         :param mapSupport : it represents the support of item
@@ -563,42 +579,15 @@
                         self._correlatedPatternGrowthGenerate(treeBeta, prefix, prefixLength + 1, mapSupportBeta)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main method to start
         """
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._tree = _Tree()
-        self._finalPatterns = {}
-        self._correlatedOneItem()
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
-        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in self._Database:
-            _transaction = []
-            for j in i:
-                if j in _itemSetBuffer:
-                    _transaction.append(j)
-            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
-            self._tree.addTransaction(_transaction)
-        self._tree.createHeaderList(self._mapSupport, self._minSup)
-        if len(self._tree.headerList) > 0:
-            self._itemSetBuffer = []
-            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport)
-        print("Correlated patterns were generated successfully using CoMine algorithm")
-        self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self) -> None:
         """
         main method to start
         """
         self._startTime = _ab._time.time()
         if self._iFile is None:
@@ -628,41 +617,45 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _pd.DataFrame:
         """
         Storing final correlated patterns in a dataframe
+
         :return: returning correlated patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -672,14 +665,15 @@
             data.append([pat, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Confidence'])
         return dataframe
 
     def save(self, outFile) -> None:
         """
         Complete set of correlated patterns will be saved into an output file
+
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
@@ -688,22 +682,24 @@
                 pat += str(i) + "\t"
             patternsAndSupport = pat.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self) -> Dict[Tuple[int], List[Union[int, float]]]:
         """
         Function to send the set of correlated patterns after completion of the mining process
+
         :return: returning correlated patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         function to print the result after completing the process
+
         :return: None
         """
         print("Total number of Correlated Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
```

### Comparing `pami-2024.4.9.1/PAMI/correlatedPattern/basic/CoMinePlus.py` & `pami-2024.5.1/PAMI/correlatedPattern/basic/CoMinePlus.py`

 * *Files 6% similar despite different names*

```diff
@@ -88,14 +88,15 @@
         self.parent = None
         self.child = []
         self.nodeLink = None
 
     def getChild(self, itemName: int) -> Union['_Node', None]:
         """
         Retrieving the child from the tree
+
         :param itemName: name of the child
         :type itemName: list
         :return: returns the node with same itemName from correlatedPatternTree
         :rtype: list
         """
         for i in self.child:
             if i.itemId == itemName:
@@ -137,14 +138,15 @@
         self.mapItemNodes = {}
         self.mapItemLastNodes = {}
         self.root = _Node()
 
     def addTransaction(self, transaction: List[int]) -> None:
         """
         Adding a transaction into a tree
+
         :param transaction: it represents a transaction in a database
         :type transaction: list
         :return: None
         """
 
         # This method taken a transaction as input and returns the tree
         current = self.root
@@ -160,73 +162,73 @@
             else:
                 child.counter += 1
                 current = child
 
     def fixNodeLinks(self, item: int, newNode: _Node) -> None:
         """
         Fixing node link for the newNode that inserted into correlatedPatternTree
+
         :param item: it represents the item of newNode
         :type item: int
         :param newNode: it represents the newNode that inserted in correlatedPatternTree
         :type newNode: Node
         :return: None
-
         """
         if item in self.mapItemLastNodes.keys():
             lastNode = self.mapItemLastNodes[item]
             lastNode.nodeLink = newNode
         self.mapItemLastNodes[item] = newNode
         if item not in self.mapItemNodes.keys():
             self.mapItemNodes[item] = newNode
 
     def printTree(self, root: _Node) -> None:
         """
         Print the details of Node in correlatedPatternTree
+
         :param root: it represents the Node in correlatedPatternTree
         :type root: Node
         :return: None
-
         """
         # this method is used print the details of tree
         if not root.child:
             return
         else:
             for i in root.child:
                 print(i.itemId, i.counter, i.parent.itemId)
                 self.printTree(i)
 
     def createHeaderList(self, mapSupport: Dict[int, int], minSup: int) -> None:
         """
         To create the headerList
+
         :param mapSupport: it represents the items with their supports
         :type mapSupport: dictionary
         :param minSup: it represents the minSup
         :param minSup: float
         :return: None
-
         """
         # the correlatedPatternTree always maintains the header table to start the mining from leaf nodes
         t1 = []
         for x, y in mapSupport.items():
             if y >= minSup:
                 t1.append(x)
         itemSetBuffer = [k for k, v in sorted(mapSupport.items(), key=lambda val: val[1], reverse=True)]
         self.headerList = [i for i in t1 if i in itemSetBuffer]
 
     def addPrefixPath(self, prefix: List[_Node], mapSupportBeta: Dict[int, int], minSup: int) -> None:
         """
         To construct the conditional tree with prefix paths of a node in correlatedPatternTree
+
         :param prefix: it represents the prefix items of a Node
         :type prefix: list
         :param mapSupportBeta: it represents the items with their supports
         :param mapSupportBeta: dictionary
         :param minSup: to check the item meets with minSup
         :param minSup: float
         :return: None
-
         """
         # this method is used to add prefix paths in conditional trees of correlatedPatternTree
         pathCount = prefix[0].counter
         current = self.root
         prefix.reverse()
         for i in range(0, len(prefix) - 1):
             pathItem = prefix[i]
@@ -242,15 +244,18 @@
                     self.fixNodeLinks(pathItem.itemId, newNode)
                 else:
                     child.counter += pathCount
                     current = child
 
 
 class CoMinePlus(_ab._correlatedPatterns):
-    """ 
+    """
+    About this algorithm
+    ====================
+
     :Description: CoMinePlus is one of the efficient algorithm to discover correlated patterns in a transactional database. Using Item Support Intervals technique which is generating correlated patterns of higher order by combining only with items that have support within specified interval.
 
     :Reference:
         Uday Kiran R., Kitsuregawa M. (2012) Efficient Discovery of Correlated Patterns in Transactional Databases Using Items’ Support Intervals.
         In: Liddle S.W., Schewe KD., Tjoa A.M., Zhou X. (eds) Database and Expert Systems Applications. DEXA 2012. Lecture Notes in Computer Science, vol 7446. Springer, Berlin, Heidelberg.
         https://doi.org/10.1007/978-3-642-32600-4_18
 
@@ -294,32 +299,33 @@
         finalPatterns : dict
             it represents to store the patterns
         itemSetBuffer : list
             it represents the store the items in mining
         maxPatternLength : int
            it represents the constraint for pattern length
 
-    **Methods to execute code on terminal**
-    ----------------------------------------------
+    Execution methods
+    =================
+
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 CoMinePlus.py <inputFile> <outputFile> <minSup> <minAllConf> <sep>
 
       Example Usage:
 
       (.venv) $ python3 CoMinePlus.py sampleTDB.txt patterns.txt 0.4 0.5 ','
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup can be specified in support count or a value between 0 and 1.
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    -----------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.correlatedPattern.basic import CoMinePlus as alg
 
             obj = alg.CoMinePlus(iFile, minSup, minAllConf, sep)
 
             obj.mine()
@@ -341,16 +347,16 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    **Credits:**
-    -------------
+    Credits
+    =======
 
              The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
 
         """
 
     _startTime = float()
     _endTime = float()
@@ -370,14 +376,15 @@
     _itemSetCount = 0
     _maxPatternLength = 1000
     _sep = "\t"
 
     def __init__(self, iFile: Union[str, _pd.DataFrame], minSup: Union[int, float, str], minAllConf: str, sep: str="\t") -> None:
         """
         param iFile: input file name
+
         type iFile: str or DataFrame or url
         param minSup: user-specified minimum support
         type minSup: int or float
         param minAllConf: user-specified minimum all confidence
         type minAllConf: float
         param sep: delimiter of input file
         type sep : str
@@ -427,14 +434,15 @@
                     self._mapSupport[j] = 1
                 else:
                     self._mapSupport[j] += 1
 
     def _saveItemSet(self, prefix: List[_Node], prefixLength: int, support: int, ratio: float) -> None:
         """
         To save the correlated patterns mined form correlatedPatternTree
+
         :param prefix: the correlated pattern
         :type prefix: list
         :param prefixLength: the length of a correlated pattern
         :type prefixLength: int
         :param support: the support of a pattern
         :type support:  int
         :param ratio: float
@@ -446,14 +454,15 @@
             sample.append(prefix[i])
         self._itemSetCount += 1
         self._finalPatterns[tuple(sample)] = [support, ratio]
 
     def _saveAllCombinations(self, tempBuffer: List[_Node], s: int, position: int, prefix: List[_Node], prefixLength: int) -> None:
         """
         Generating all the combinations for items in single branch in correlatedPatternTree
+
         :param tempBuffer: items in a single branch
         :type tempBuffer: list
         :param s: support at leaf node of a branch
         :param position: the length of a tempBuffer
         :type position: int
         :param prefix: it represents the list of leaf node
         :type prefix: list
@@ -472,14 +481,15 @@
             ratio = s/self._mapSupport[self._getMaxItem(prefix, newPrefixLength)]
             if ratio >= self._minAllConf:
                 self._saveItemSet(prefix, newPrefixLength, s, ratio)
 
     def _correlatedPatternGrowthGenerate(self, correlatedPatternTree: _Tree, prefix: List[_Node], prefixLength: int, mapSupport: Dict[int, int], minConf: float)  -> None:
         """
         Mining the fp tree
+
         :param correlatedPatternTree: it represents the correlatedPatternTree
         :type correlatedPatternTree: class Tree
         :param prefix: it represents an empty list and store the patterns that are mined
         :type prefix: list
         :param param prefixLength: the length of prefix
         :type prefixLength: int
         :param mapSupport : it represents the support of item
@@ -550,14 +560,15 @@
                     if len(treeBeta.root.child) > 0:
                         treeBeta.createHeaderList(mapSupportBeta, self._minSup)
                         self._correlatedPatternGrowthGenerate(treeBeta, prefix, prefixLength + 1, mapSupportBeta, minConf)
 
     def _convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -569,45 +580,15 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
-
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        self._tree = _Tree()
-        self._minSup = self._convert(self._minSup)
-        self._correlatedOneItem()
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
-        _itemSetBuffer = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in self._Database:
-            _transaction = []
-            for j in i:
-                if j in _itemSetBuffer:
-                    _transaction.append(j)
-            _transaction.sort(key=lambda val: self._mapSupport[val], reverse=True)
-            self._tree.addTransaction(_transaction)
-        self._tree.createHeaderList(self._mapSupport, self._minSup)
-        if len(self._tree.headerList) > 0:
-            self._itemSetBuffer = []
-            self._correlatedPatternGrowthGenerate(self._tree, self._itemSetBuffer, 0, self._mapSupport, self._minAllConf)
-        print("Correlated Frequent patterns were generated successfully using CorrelatedPatternGrowth algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self) -> None:
         """
         Main program to start the operation
         """
 
         self._startTime = _ab._time.time()
@@ -640,23 +621,25 @@
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def _getMaxItem(self, prefix: List[_Node], prefixLength: int) -> int:
@@ -665,23 +648,25 @@
             if self._mapSupport[maxItem] < self._mapSupport[prefix[i]]:
                 maxItem = prefix[i]
         return maxItem
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _pd.DataFrame:
         """
         Storing final correlated patterns in a dataframe
+
         :return: returning correlated patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
@@ -691,30 +676,32 @@
             data.append([pat, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Confidence'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
         Complete set of correlated patterns will be loaded in to an output file
-        :param outFile: name of the outputfile
-        :type outFile: file
+
+        :param outFile: name of the output file
+        :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             pattern = str()
             for i in x:
                 pattern = pattern + i + "\t"
             s1 = str(pattern.strip()) + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> Dict[Tuple[str], List[Union[int, float]]]:
         """
         Function to send the set of correlated patterns after completion of the mining process
+
         :return: returning correlated patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.4.9.1/PAMI/correlatedPattern/basic/__init__.py` & `pami-2024.5.1/PAMI/correlatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/correlatedPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/correlatedPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/coveragePattern/basic/CMine.py` & `pami-2024.5.1/PAMI/coveragePattern/basic/CMine.py`

 * *Files 4% similar despite different names*

```diff
@@ -58,14 +58,16 @@
 
 from PAMI.coveragePattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 class CMine(_ab._coveragePatterns):
     """
+    About this algorithm
+    ====================
 
     :Description:  CMine algorithms aims to discover the coverage patterns in transactional databases.
 
     :Reference:    Bhargav Sripada, Polepalli Krishna Reddy, Rage Uday Kiran:
                    Coverage patterns for efficient banner advertisement placement. WWW (Companion Volume) 2011: 131-132
                    __https://dl.acm.org/doi/10.1145/1963192.1963259
 
@@ -100,32 +102,33 @@
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
         Database : list
             To store the transactions of a database in list
 
 
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+    Execution methods
+    =================
+
+    **Terminal command**
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 CMine.py <inputFile> <outputFile> <minRF> <minCS> <maxOR> <'\t'>
 
       Example Usage:
 
       (.venv) $ python3 CMine.py sampleTDB.txt patterns.txt 0.4 0.7 0.5 '\t'
 
     .. note: At the fixed minCS value, it can also be observed that the number of patterns increases as maxOR value increases.
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.coveragePattern.basic import CMine as alg
 
             obj = alg.CMine(iFile, minRF, minCS, maxOR, seperator)
 
             obj.mine()
@@ -147,16 +150,17 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
-    **Credits:**
-    --------------------------
+    Credits
+    =======
+
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
@@ -172,14 +176,15 @@
     _mapSupport = {}
     _lno = 0
 
 
     def _convert(self, value) -> Union[int, float]:
         """
         To convert the user specified minSup value
+
         :param value: user specified minSup value
         :return: converted type
         :rtype: Union[int, float]
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
@@ -189,15 +194,15 @@
                 value = float(value)
             else:
                 value = int(value)
         return value
 
     def _creatingItemSets(self) -> None:
         """
-            Storing the complete transactions of the database/input file in a database variable
+        Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
         self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -223,14 +228,15 @@
                             self._Database.append(splitter)
                 except IOError:
                     print("File Not Found")
 
     def creatingCoverageItems(self) -> Dict[str, List[str]]:
         """
         This function creates coverage items from _database.
+
         :return: coverageTidData that stores coverage items and their tid list.
         :rtype: dict
         """
         tidData = {}
         self._lno = 0
         for transaction in self._Database:
             self._lno = self._lno + 1
@@ -242,14 +248,15 @@
         coverageTidData = {k: v for k, v in tidData.items() if len(v) / len(self._Database) >= self._minRF}
         coverageTidData = dict(sorted(coverageTidData.items(), reverse=True, key=lambda x: len(x[1])))
         return coverageTidData
 
     def tidToBitset(self,item_set: Dict[str, int]) -> Dict[str, int]:
         """
         This function converts tid list to bitset.
+
         :param item_set:
         :return: Dictionary
         :rtype: dict
         """
         bitset = {}
 
         for k,v in item_set.items():
@@ -260,14 +267,15 @@
                 bitset[k] = (bitset[k] << diff) | 0b1
             bitset[k] = (bitset[k] << (self._lno - int(v[i])))
         return bitset
 
     def genPatterns(self,prefix: Tuple[str, int],tidData: List[Tuple[str, int]]) -> None:
         """
         This function generate coverage pattern about prefix.
+
         :param prefix: String
         :param tidData: list
         :return: None
         """
         # variables to store coverage item set and
         item_set = prefix[0]
 
@@ -283,46 +291,29 @@
                 if orCount / len(self._Database) >= self._minRF:
                     self._finalPatterns[coverageItem_set] = andCount
                 self.genPatterns((coverageItem_set,tid),tidData[i+1:length])
 
     def generateAllPatterns(self,coverageItems: Dict[str, int]) -> None:
         """
         This function generates all coverage patterns.
+
         :param coverageItems: coverage items
         :return: None
         """
         tidData = list(coverageItems.items())
         length = len(tidData)
         for i in range(length):
             #print(i,tidData[i][0])
             self.genPatterns(tidData[i],tidData[i+1:length])
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ Main method to start """
 
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self._creatingItemSets()
-        self._minCS = self._convert(self._minCS)
-        self._minRF =  self._convert(self._minRF)
-        self._maxOR = self._convert(self._maxOR)
-        coverageItems = self.creatingCoverageItems()
-        self._finalPatterns = {k: len(v) for k, v in coverageItems.items()}
-        coverageItemsBitset = self.tidToBitset(coverageItems)
-        self.generateAllPatterns(coverageItemsBitset)
-        self.save('output.txt')
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Coverage patterns were generated successfully using CMine  algorithm")
+        self.mine()
 
     def mine(self) -> None:
         """ Main method to start """
 
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
@@ -350,56 +341,66 @@
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
-        """Storing final coverage patterns in a dataframe
+        """
+        Storing final coverage patterns in a dataframe
+
         :return: returning coverage patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
-        """Complete set of coverage patterns will be loaded in to an output file
+        """
+        Complete set of coverage patterns will be loaded in to an output file
+
         :param outFile: name of the outputfile
         :type outFile: file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self) -> Dict[str, int]:
-        """ Function to send the set of coverage patterns after completion of the mining process
+        """
+        Function to send the set of coverage patterns after completion of the mining process
+
         :return: returning coverage patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.4.9.1/PAMI/coveragePattern/basic/CPPG.py` & `pami-2024.5.1/PAMI/coveragePattern/basic/CPPG.py`

 * *Files 4% similar despite different names*

```diff
@@ -352,49 +352,15 @@
                 value = int(value)
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ Mining process will start from this function
         """
-
-        #global _minSup, _maxPer, _lno
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minRF is None:
-            raise Exception("Please enter the Relative Frequency")
-        if self._maxOR is None:
-            raise Exception("Please enter the Overlap Ratio")
-        if self._minCS is None:
-            raise Exception("Please enter the Coverage Ratio")
-        self._creatingItemSets()
-        self._minRF = self._convert(self._minRF)
-        self._maxOR = self._convert(self._maxOR)
-        self._minCS = self._convert(self._minCS)
-        if self._minRF > len(self._Database) or self._minCS > len(self._Database) or self._maxOR > len(self._Database):
-            raise Exception("Please enter the constraints in range between 0 to 1")
-        generatedItems, pfList = self._coverageOneItem()
-        self._finalPatterns = {k: v for k, v in generatedItems.items()}
-        updatedDatabases = self._updateDatabases(pfList)
-        proData = self._buildProjectedDatabase(updatedDatabases, pfList)
-        for x, y in proData.items():
-            uniqueItems = [x]
-            for i in y:
-                for j in i:
-                    if j not in uniqueItems:
-                        uniqueItems.append(j)
-            self._generateFrequentPatterns(uniqueItems)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Coverage patterns were generated successfully using CPPG algorithm ")
+        self.mine()
 
     def mine(self) -> None:
         """ Mining process will start from this function
         """
 
         #global _minSup, _maxPer, _lno
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/coveragePattern/basic/abstract.py` & `pami-2024.5.1/PAMI/coveragePattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DB.py` & `pami-2024.5.1/PAMI/extras/DF2DB/DF2DB.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/DF2DBPlus.py` & `pami-2024.5.1/PAMI/extras/DF2DB/DF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/DenseFormatDF.py` & `pami-2024.5.1/PAMI/extras/DF2DB/DenseFormatDF.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/SparseFormatDF.py` & `pami-2024.5.1/PAMI/extras/DF2DB/SparseFormatDF.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/createTDB.py` & `pami-2024.5.1/PAMI/extras/DF2DB/createTDB.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DBPlus.py` & `pami-2024.5.1/PAMI/extras/DF2DB/denseDF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/denseDF2DB_dump.py` & `pami-2024.5.1/PAMI/extras/DF2DB/denseDF2DB_dump.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py` & `pami-2024.5.1/PAMI/extras/DF2DB/sparseDF2DBPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingBeta.py` & `pami-2024.5.1/PAMI/extras/calculateMISValues/usingBeta.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/calculateMISValues/usingSD.py` & `pami-2024.5.1/PAMI/extras/calculateMISValues/usingSD.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/convertMultiTSIntoFuzzy.py` & `pami-2024.5.1/PAMI/extras/convertMultiTSIntoFuzzy.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,19 +5,22 @@
 #
 #     from PAMI.extras.syntheticDataGenerator import convertMultiTSIntoFuzzy as fuz
 #
 #     obj = fuz.convertMultiTSIntoFuzzy(iFile, FuzFile)
 #
 #     obj.save()
 #
-#     obj.startMine()
+#     obj.mine()
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -49,16 +52,15 @@
 
                 from PAMI.extras.syntheticDataGenerator import convertMultiTSIntoFuzzy as fuz
 
                 obj = fuz.convertMultiTSIntoFuzzy(iFile, FuzFile)
 
                 obj.save()
 
-                obj.startMine()
-
+                obj.mine()
 
     """
 
 
     def __init__(self, iFile: str,  FuzFile: str) -> None:
         #super().__init__(iFile, nFile, FuzFile, minSup, maxPer, sep)
         self._iFile = iFile
@@ -136,15 +138,15 @@
                                 (self._RegionsCal[i][1] - quantity) / base)
                         else:
                             self.list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
                                 (quantity - self._RegionsCal[i][0]) / base)
             return
        
     def save(self, outputFile: str) -> None:
-        self.startMine()
+        self.mine()
         writer = open(outputFile, 'w+')
         for line in range(len(self._transactionsDB)):
             item_list = self._transactionsDB[line]
             fuzzyValues_list = self._fuzzyValuesDB[line]
             times = self._timeEvents[line]
             s = str()
             s2 = str()
@@ -163,22 +165,32 @@
                     s = s +  item + '.' + self._LabelKeyOne[k]  + '\t'
                     ss = ss + str(round(self.list[k], 2))+ '\t'
             s2 = s1 + ':' + s + ':' + ss
             # print(s2)
             # break
             writer.write("%s\n" %s2)
 
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
-        """ Frequent pattern mining process will start from here
+        """
+        Frequent pattern mining process will start from here
         """
         
         self._creatingItemSets()
         self._fuzzyMembershipFunc()
         self._finalPatterns = {}
+
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+
+        self._creatingItemSets()
+        self._fuzzyMembershipFunc()
+        self._finalPatterns = {}
         
 if __name__ == "__main__":
     convertMultipleTSIntoFuzzy(sys.argv[1], sys.argv[2])
```

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/FuzzyDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/FuzzyDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py` & `pami-2024.5.1/PAMI/extras/dbStats/MultipleTimeSeriesFuzzyDatabaseStats.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/SequentialDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/SequentialDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/TemporalDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/TemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/TransactionalDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/TransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/UncertainTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/UncertainTransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/dbStats/UtilityDatabase.py` & `pami-2024.5.1/PAMI/extras/dbStats/UtilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/abstract.py` & `pami-2024.5.1/PAMI/extras/fuzzyTransformation/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py` & `pami-2024.5.1/PAMI/extras/fuzzyTransformation/temporalToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py` & `pami-2024.5.1/PAMI/extras/fuzzyTransformation/transactionalToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py` & `pami-2024.5.1/PAMI/extras/fuzzyTransformation/utilityToFuzzy.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py` & `pami-2024.5.1/PAMI/extras/generateDatabase/generateSpatioTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py` & `pami-2024.5.1/PAMI/extras/generateDatabase/generateTemporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py` & `pami-2024.5.1/PAMI/extras/generateDatabase/generateTransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/generateLatexGraphFile.py` & `pami-2024.5.1/PAMI/extras/generateLatexGraphFile.py`

 * *Files 1% similar despite different names*

```diff
@@ -7,16 +7,18 @@
 #
 #     obj = fuz.generateLatexGraphFile(idf)
 #
 #     obj.save()
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.9.1/PAMI/extras/graph/DF2Fig.py` & `pami-2024.5.1/PAMI/extras/graph/DF2Fig.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/graph/DF2Tex.py` & `pami-2024.5.1/PAMI/extras/graph/DF2Tex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphFromDictionary.py` & `pami-2024.5.1/PAMI/extras/graph/plotLineGraphFromDictionary.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py` & `pami-2024.5.1/PAMI/extras/graph/plotLineGraphsFromDataFrame.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/graph/visualizeFuzzyPatterns.py` & `pami-2024.5.1/PAMI/extras/graph/visualizeFuzzyPatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/graph/visualizePatterns.py` & `pami-2024.5.1/PAMI/extras/graph/visualizePatterns.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/imageProcessing/imagery2Databases.py` & `pami-2024.5.1/PAMI/extras/imageProcessing/imagery2Databases.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/messaging/discord.py` & `pami-2024.5.1/PAMI/extras/messaging/discord.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/messaging/gmail.py` & `pami-2024.5.1/PAMI/extras/messaging/gmail.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py` & `pami-2024.5.1/PAMI/extras/neighbours/findNeighborsUsingEuclideanDistanceforPointInfo.py`

 * *Files 2% similar despite different names*

```diff
@@ -50,15 +50,15 @@
             This program find pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace
             and store the pairs.
         :param  seperator: str :
                     This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Methods:
 
-        startMine()
+        mine()
             find and store the pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace.
         getFileName()
             This function returns output file name.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
```

### Comparing `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py` & `pami-2024.5.1/PAMI/extras/neighbours/findNeighboursUsingEuclidean.py`

 * *Files 2% similar despite different names*

```diff
@@ -49,15 +49,15 @@
             This program find pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace
             and store the pairs.
         :param  seperator: str :
                     This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Methods:
 
-        startMine()
+        mine()
             find and store the pairs of values whose Euclidean distance is less than or equal to maxEucledianDistace.
         getFileName()
             This function returns output file name.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
```

### Comparing `pami-2024.4.9.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py` & `pami-2024.5.1/PAMI/extras/neighbours/findNeighboursUsingGeodesic.py`

 * *Files 3% similar despite different names*

```diff
@@ -51,15 +51,15 @@
             and store the pairs.
         :param  seperator: str :
                     This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Methods:
 
-        startMine()
+        mine()
             find and store the pairs of values whose Geodesic distance is less than or equal to maxDistace.
         getFileName()
             This function returns output file name.
 
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
```

### Comparing `pami-2024.4.9.1/PAMI/extras/plotPointOnMap.py` & `pami-2024.5.1/PAMI/extras/plotPointOnMap.py`

 * *Files 2% similar despite different names*

```diff
@@ -6,16 +6,19 @@
 #     from PAMI.extras.syntheticDataGenerator import plotPointOnMap as plt
 #
 #     obj = plt.plotPointOnMap(" ", 10, "\t")
 #
 #     obj.save()
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.9.1/PAMI/extras/plotPointOnMap_dump.py` & `pami-2024.5.1/PAMI/extras/plotPointOnMap_dump.py`

 * *Files 1% similar despite different names*

```diff
@@ -5,17 +5,20 @@
 #
 #     from PAMI.extras.syntheticDataGenerator import plotPointOnMap_dump as plt
 #
 #     obj = plt.plotPointOnMap_dump(" ", 10, "\t")
 #
 #     obj.save()
 #
-#
+
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.9.1/PAMI/extras/scatterPlotSpatialPoints.py` & `pami-2024.5.1/PAMI/extras/scatterPlotSpatialPoints.py`

 * *Files 5% similar despite different names*

```diff
@@ -6,16 +6,19 @@
 #   from PAMI.extras.syntheticDataGenerator import scatterPlotSpatialPoints as plt
 #
 #   obj = plt.scatterPlotSpatialPoints(iFile, "\t")
 #
 #   obj.save()
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -47,15 +50,14 @@
 
                 from PAMI.extras.syntheticDataGenerator import scatterPlotSpatialPoints as plt
 
                 obj = plt.scatterPlotSpatialPoints(iFile, "\t" )
 
                 obj.save(oFile)
 
-
     """
 
     def __init__(self, iFile: str, sep: str = '\t') ->None:
 
         self._iFile = iFile
         self._sep = sep
```

### Comparing `pami-2024.4.9.1/PAMI/extras/stats/TransactionalDatabase.py` & `pami-2024.5.1/PAMI/extras/stats/TransactionalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/stats/graphDatabase.py` & `pami-2024.5.1/PAMI/extras/stats/graphDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/stats/sequentialDatabase.py` & `pami-2024.5.1/PAMI/extras/stats/sequentialDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/stats/temporalDatabase.py` & `pami-2024.5.1/PAMI/extras/stats/temporalDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/stats/utilityDatabase.py` & `pami-2024.5.1/PAMI/extras/stats/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/TemporalDatabase.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,23 +1,20 @@
-# TemporalDatabase is a code used to create a synthetic temporal database.
+# TemporalDatabase is a collection of timestamps and along with data at particular time.
 #
 #  **Importing this algorithm into a python program**
 #  --------------------------------------------------------
 #
 #             from PAMI.extras.syntheticDataGenerator import TemporalDatabase as db
 #
-#             obj = db.TemporalDatabase(100, 10, 6, oFile, %, "\t")
+#             temporalDB = db(numOfTransactions, avgTransactionLength, numItems, outFileName)
 #
-#             obj.save()
+#             temporalDB.create(percentage)
 #
-#             obj.getFileName("outputFileName") # to create a file
 #
-#             obj.getDatabaseAsDataFrame("outputFileName") # to convert database into dataframe
-#
-#             obj.createTemporalFile("outputFileName") # to get outputfile
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -28,112 +25,98 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from typing import Tuple, List, Union
+
 import pandas as pd
 import numpy as np
-import random
 import sys
-import os
+
 
 class TemporalDatabase:
     """
-    :Description:   generateTemporalDatabase creates a temporal database and outputs a database or a frame depending on input
+    :Description: - creates a temporal database with required parameter (e.g.,numOfTransactions, avgLenOfTransactions, numItems and outputFile).
+                  - output can be printed in two ways either in text file or dataframe depending on the input type.
 
     :Attributes:
+
         :param numOfTransactions: int
             number of transactions
+
         :param avgLenOfTransactions: int
             average length of transactions
+
         :param numItems: int
             number of items
+
         :param outputFile: str
-            output file name
+            the name the output file
+
         :param percentage: int
             percentage of coinToss for TID of temporalDatabase
+
         :param sep: str
             seperator for database output file
+
         :param typeOfFile: str
             specify database or dataframe to get corresponding output
 
     :Methods:
         getFileName():
             returns filename
+
         createTemporalFile():
             creates temporal database file or dataframe
+
         getDatabaseAsDataFrame:
             returns dataframe
+
         performCoinFlip():
             Perform a coin flip with the given probability
+
         tuning():
             Tune the arrayLength to match avgLenOfTransactions
+
         createTemporalFile():
             create Temporal database or dataframe depending on input
 
+    **Methods to execute code on terminal**
+    ---------------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 TemporalDatabase.py <numOfTransactions> <avgLenOfTransactions> <numItems> <outputFile>
+
+      Example Usage:
+
+      (.venv) $ python3 TemporalDatabase.py 50.0 10.0 100 temporal.txt
+
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
 
-            from PAMI.extras.generateDatabase import generateTemporalDatabase as db
+            from PAMI.extras.syntheticDataGenerator import TemporalDatabase as db
+
+            temporalDB = db(numOfTransactions, avgTransactionLength, numItems, outFileName)
+
+            temporalDB.create(percentage)
 
-            numOfTransactions = 100
-            numItems = 15
-            avgTransactionLength = 6
-            outFileName = 'temporal_ot.txt'
-            sep = '\t'
-            percent = 75
-            frameOrBase = "dataframe" # if you want to get dataframe as output
-            frameOrBase = "database" # if you want to get database/csv/file as output
-
-            temporalDB = db.generateTemporalDatabase(numOfTransactions, avgTransactionLength, numItems, outFileName, percent, sep, frameOrBase )
-            temporalDB.createTemporalFile()
-            print(temporalDB.getDatabaseAsDataFrame())
 
     """
     def __init__(self, numOfTransactions: int, avgLenOfTransactions: int, 
                  numItems: int, outputFile: str, percentage: int=50,
                  sep: str='\t', typeOfFile: str="Database") -> None:
         
         """
-        :Description:   Initialize the generateTemporalDatabase class
-
-        :Attributes:
-            :param numOfTransactions: int
-                number of transactions
-            :param avgLenOfTransactions: int
-                average length of transactions
-            :param numItems: int
-                number of items
-            :param outputFile: str
-                output file name
-            :param percentage: int
-                percentage of coinToss for TID of temporalDatabase
-            :param sep: str
-                seperator for database output file
-            :param typeOfFile: str
-                specify database or dataframe to get corresponding output
-
-        :Methods:
-            getFileName():
-                returns filename
-            createTemporalFile():
-                creates temporal database file or dataframe
-            getDatabaseAsDataFrame:
-                returns dataframe
-            performCoinFlip():
-                Perform a coin flip with the given probability
-            tuning():
-                Tune the arrayLength to match avgLenOfTransactions
-            createTemporalFile():
-                create Temporal database or dataframe depending on input
-        
+        Initialize the generateTemporalDatabase class with required parameters.
         """
 
         self.numOfTransactions = numOfTransactions
         self.avgLenOfTransactions = avgLenOfTransactions
         self.numItems = numItems
         self.outputFile = outputFile
         if percentage > 1:
@@ -141,22 +124,23 @@
         else:
             self.percentage = percentage
         self.sep = sep
         self.typeOfFile = typeOfFile.lower()
 
     def getFileName(self) -> str:
         """
-        return filename
-        :return:
+        This function take the name of the outputfile.
+        :return: outputFile.
         """
         return self.outputFile
 
     def getDatabaseAsDataFrame(self) -> pd.DataFrame:
         """
-        return dataframe
+        This function return the database in dataframe format.
+
         return: pd.dataframe
         """
         return self.df
     
     def performCoinFlip(self, probability: float) -> bool:
         """Perform a coin flip with the given probability."""
         result = np.random.choice([0, 1], p=[1 - probability, probability])
@@ -164,21 +148,22 @@
 
 
     def tuning(self, array, sumRes) -> list:
         """
         Tune the array so that the sum of the values is equal to sumRes
 
         Parameters:
-        array: list - list of values
-        sumRes: int - target sum
+        :param array: list of values randomly generated.
+        :type array: list
+        :param sumRes: target sum
+        :type sumRes: int
 
         Returns:
         array: list - tuned array
         """
-
         # first generate a random array of length n whose values average to m
         values = np.random.randint(1, self.numItems, len(array))
 
         while np.sum(values) != sumRes:
             # get index of largest value
             # if sum is too large, decrease the largest value
             if np.sum(values) > sumRes:
@@ -194,16 +179,16 @@
         for i in range(len(array)):
             array[i][1] = values[i]
 
         return array
 
     def create(self) -> None:
         """
-        create Temporal database or dataframe depending on input
-        :return:
+        create Temporal database or dataframe depending on type of file specified.
+        :return: None
         """
 
         db = []
         lineSize = []
         for i in range(self.numOfTransactions):
             db.append([i])
             if self.performCoinFlip(self.percentage):
@@ -230,26 +215,11 @@
                 'timestamp': [line[0] for line in db],
                 'transactions': pd.Series([line[1:] for line in db])
             }
             self.df = pd.DataFrame(data)
 
         print("Temporal database created successfully")
 
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
 if __name__ == '__main__':
 
     obj = TemporalDatabase(sys.argv[1], sys.argv[2], sys.argv[3], sys.argv[4])
     obj.create(sys.argv[5])
```

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticGeoreferentialUncertainTransaction.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticUncertainTransactions.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/createSyntheticUtility.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUncertainTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUncertainTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUtilityTemporal.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/generateUtilityTransactional.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/syntheticUtilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/temporalDatabaseGen.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py` & `pami-2024.5.1/PAMI/extras/syntheticDataGenerator/utilityDatabase.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/extras/topKPatterns.py` & `pami-2024.5.1/PAMI/extras/topKPatterns.py`

 * *Files 0% similar despite different names*

```diff
@@ -6,16 +6,19 @@
 #     from PAMI.extras.syntheticDataGenerator import topKPatterns as tK
 #
 #     obj = tK.topKPatterns(" ", 10, "\t")
 #
 #     obj.save()
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.9.1/PAMI/extras/uncertaindb_convert.py` & `pami-2024.5.1/PAMI/extras/uncertaindb_convert.py`

 * *Files 0% similar despite different names*

```diff
@@ -7,16 +7,18 @@
 #
 #     obj = un.predictedClass2Transaction(predicted_classes, 0.8)
 #
 #     obj.save()
 #
 
 
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
```

### Comparing `pami-2024.4.9.1/PAMI/extras/visualize/graphs.py` & `pami-2024.5.1/PAMI/extras/visualize/graphs.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py` & `pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/FTApriori.py`

 * *Files 3% similar despite different names*

```diff
@@ -297,31 +297,16 @@
                     self._finalPatterns[tuple(j)] = res
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Fault-tolerant frequent pattern mining process will start from here
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._itemSup = self._convert(self._itemSup)
-        self._minLength = int(self._minLength)
-        self._faultTolerance = int(self._faultTolerance)
-        self._oneLengthFrequentItems()
 
-        self._getFaultPatterns()
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Fault-Tolerant Frequent patterns were generated successfully using FTApriori algorithm ")
+        self.mine()
 
     def mine(self) -> None:
         """
         Fault-tolerant frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py` & `pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -305,15 +305,15 @@
         tree : class
             it represents the Tree class
         finalPatterns : dict
             it represents to store the patterns
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -551,41 +551,15 @@
         return temp
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main program to start the operation
         """
-        global _minSup
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        _minSup = self._minSup
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self) -> None:
         """
         Main program to start the operation
         """
         global _minSup
         self.__startTime = _fp._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/faultTolerantFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/__init__.py` & `pami-2024.5.1/PAMI/frequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/basic/Apriori.py` & `pami-2024.5.1/PAMI/frequentPattern/basic/_Apriori.py`

 * *Files 0% similar despite different names*

```diff
@@ -443,8 +443,7 @@
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ap._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLAT.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cuEclat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-# ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# cuECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+#
 #
 # **Importing this algorithm into a python program**
-# ------------------------------------------------------------------
+# ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.basic.ECLAT as alg
+#             import PAMI.frequentPattern.cuda.cuEclat as alg
 #
-#             obj = alg.ECLAT(iFile, minSup)
+#             obj = alg.cuEclat(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -29,15 +30,14 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,34 +47,37 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.frequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+
+# from PAMI.frequentPattern.cuda import abstract as _ab
+import abstract as _ab
 from deprecated import deprecated
 
-class ECLAT(_ab._frequentPatterns):
+class cuEclat(_ab._frequentPatterns):
     """
     :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
+
+
     :Attributes:
 
         startTime : float
           To record the start time of the mining process
 
         endTime : float
           To record the completion time of the mining process
@@ -88,348 +91,305 @@
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
         Database : list
           To store the transactions of a database in list
 
 
+
     **Methods to execute code on terminal**
-    ------------------------------------------
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ECLAT.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cuEclat.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLAT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuEclat.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ------------------------------------------------------------------
+    ----------------------------------------------------
+
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLAT as alg
+             import PAMI.frequentPattern.cuda.cuEclat as alg
 
-            obj = alg.ECLAT(iFile, minSup)
+             obj = alg.cuEclat(iFile, minSup)
 
-            obj.mine()
+             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+             frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+             Df = obj.getPatternInDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+             memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+             print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+             memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+             print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+             run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    ----------------------
+    -------------
 
-             The complete program was written by Kundai  under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
+    _ab._cp.cuda.Device(0).use()
+
+
+
     _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
 
-    def _creatingItemSets(self) -> float:
+
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
-
-        :return: the complete transactions of the database/input file in a database variable
-
-        :rtype: float
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                temp = self._iFile['Transactions'].tolist()
+
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _getUniqueItemList(self) -> list:
-        """
-
-        Generating one frequent patterns
-
-        :return: list of unique patterns
-
-        :rtype: list
-
-        """
-        self._finalPatterns = {}
-        candidate = {}
-        uniqueItem = []
-        for i in range(len(self._Database)):
-            for j in range(len(self._Database[i])):
-                if self._Database[i][j] not in candidate:
-                    candidate[self._Database[i][j]] = {i}
-                else:
-                    candidate[self._Database[i][j]].add(i)
-        for key, value in candidate.items():
-            supp = len(value)
-            if supp >= self._minSup:
-                self._finalPatterns[key] = [value]
-                uniqueItem.append(key)
-        uniqueItem.sort()
-        return uniqueItem
-
-    def _generateFrequentPatterns(self, candidateFrequent: list) -> None:
-        """
-
-        It will generate the combinations of frequent items
-
-        :param candidateFrequent :it represents the items with their respective transaction identifiers
-
-        :type candidateFrequent: list
-
-        :return: None
-
-        """
-        new_freqList = []
-        for i in range(0, len(candidateFrequent)):
-            item1 = candidateFrequent[i]
-            i1_list = item1.split()
-            for j in range(i + 1, len(candidateFrequent)):
-                item2 = candidateFrequent[j]
-                i2_list = item2.split()
-                if i1_list[:-1] == i2_list[:-1]:
-                    interSet = self._finalPatterns[item1][0].intersection(self._finalPatterns[item2][0])
-                    if len(interSet) >= self._minSup:
-                        newKey = item1 + "\t" + i2_list[-1]
-                        self._finalPatterns[newKey] = [interSet]
-                        new_freqList.append(newKey)
-                else: break
-
-        if len(new_freqList) > 0:
-                self._generateFrequentPatterns(new_freqList)
-
-    def _convert(self, value) -> float:
+    def _convert(self, value):
         """
 
         To convert the user specified minSup value
 
         :param value: user specified minSup value
 
-        :return: converted type
+        :type value: int or float or str
 
-        :rtype: float
+        :return: converted type
 
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
+    
+    def _arraysAndItems(self):
+        """ 
+        Convert the items into arrays for cupy and store them in a dictionary variable
+        :return: dictionary variable
+        """
+        ArraysAndItems = {}
+
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                j = tuple([j])
+                if j not in ArraysAndItems:
+                    ArraysAndItems[j] = [i]
+                else:
+                    ArraysAndItems[j].append(i)
+
+        newArraysAndItems = {}
+
+        for k,v in ArraysAndItems.items():
+            ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
+            if len(v) >= self._minSup:
+                self._finalPatterns[k] = len(v)
+                newArraysAndItems[k] = ArraysAndItems[k]
+
+        return newArraysAndItems
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
+        self.mine()
 
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        uniqueItemList = self._getUniqueItemList()
-        self._generateFrequentPatterns(uniqueItemList)
-        for x, y in self._finalPatterns.items():
-            self._finalPatterns[x] = len(y[0])
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT algorithm")
-
-    def mine(self) -> None:
+    def mine(self):
         """
         Frequent pattern mining process will start from here
         """
-
+        self._Database = []
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        uniqueItemList = self._getUniqueItemList()
-        self._generateFrequentPatterns(uniqueItemList)
-        for x, y in self._finalPatterns.items():
-            self._finalPatterns[x] = len(y[0])
+
+        ArraysAndItems = self._arraysAndItems()
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                for j in range(i+1, len(ArraysAndItems)):
+                    # print(i, "/", len(ArraysAndItems), end="\r")
+                    jList = list(keys[j])
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
+                        if len(intersect) >= self._minSup:
+                            newArraysAndItems[union] = intersect
+                            self._finalPatterns[union] = len(intersect)
+                    else:
+                        break
+
+            ArraysAndItems = newArraysAndItems
+            # print()
+
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT algorithm")
+        print("Frequent patterns were generated successfully using cuEclat algorithm ")
+            
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
-
         Total amount of USS memory consumed by the mining process will be retrieved from this function
-
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
-
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
-
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
         :return: returning RSS memory consumed by the mining process
-
         :rtype: float
-
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
-
         :return: returning total amount of runtime taken by the mining process
-
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
-
         Storing final frequent patterns in a dataframe
-
         :return: returning frequent patterns in a dataframe
-
         :rtype: pd.DataFrame
-
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
-
         Complete set of frequent patterns will be loaded in to an output file
-
         :param outFile: name of the output file
-
         :type outFile: csvfile
-
-        :return: None
-
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
-
         :return: returning frequent patterns
-
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
-        Function used to print the results
+        This function is used to print the results
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in s:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLAT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print(_ap.getPatternsAsDataFrame())
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in s:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
+
+
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATDiffset.py` & `pami-2024.5.1/PAMI/frequentPattern/basic/ECLATDiffset.py`

 * *Files 4% similar despite different names*

```diff
@@ -272,38 +272,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
-        self._startTime = _ab._time.time()
-        self._Database = []
-        self._finalPatterns = {}
-        self._diffSets = {}
-        self._trans_set = set()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        #print(len(self._Database))
-        self._minSup = self._convert(self._minSup)
-        uniqueItemList = []
-        uniqueItemList = self._getUniqueItemList()
-        self._runDeclat(uniqueItemList)
-        self._finalPatterns = self._diffSets
-        #print(len(self._finalPatterns), len(uniqueItemList))
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using ECLAT Diffset algorithm")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/basic/ECLATbitset.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cuEclatBit.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,15 +1,16 @@
-# ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# cuECLATBit is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+#
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.basic.ECLATbitset as alg
+#             import PAMI.frequentPattern.cuda.cuEclatBit as alg
 #
-#             obj = alg.ECLATbitset(iFile, minSup)
+#             obj = alg.cuEclatBit(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -29,15 +30,14 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,30 +48,31 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
-from PAMI.frequentPattern.basic import abstract as _ab
+# from PAMI.frequentPattern.cuda import abstract as _ab
+import abstract as _ab
 from deprecated import deprecated
 
-class ECLATbitset(_ab._frequentPatterns):
+class cuEclatBit(_ab._frequentPatterns):
     """
-    :Description:  ECLATbitset is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+    :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
 
     :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
             372-390 (2000), https://ieeexplore.ieee.org/document/846291
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+    :param  minSup: int :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         startTime : float
           To record the start time of the mining process
@@ -88,283 +89,253 @@
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
         Database : list
           To store the transactions of a database in list
 
 
+
     **Methods to execute code on terminal**
-    ------------------------------------------
+    ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 ECLATbitset.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cuEclatBit.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 ECLATbitset.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cuEclatBit.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ---------------------------------------------------------
+    ----------------------------------------------------
+
     .. code-block:: python
 
-            import PAMI.frequentPattern.basic.ECLATbitset as alg
+             import PAMI.frequentPattern.cuda.cuEclatBit as alg
+
+             obj = alg.cuEclatBit(iFile, minSup)
 
-            obj = alg.ECLATbitset(iFile, minSup)
+             obj.mine()
 
-            obj.mine()
+             frequentPatterns = obj.getPatterns()
 
-            frequentPatterns = obj.getPatterns()
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+             obj.save(oFile)
 
-            obj.save(oFile)
+             Df = obj.getPatternInDataFrame()
 
-            Df = obj.getPatternInDataFrame()
+             memUSS = obj.getMemoryUSS()
 
-            memUSS = obj.getMemoryUSS()
+             print("Total Memory in USS:", memUSS)
 
-            print("Total Memory in USS:", memUSS)
+             memRSS = obj.getMemoryRSS()
 
-            memRSS = obj.getMemoryRSS()
+             print("Total Memory in RSS", memRSS)
 
-            print("Total Memory in RSS", memRSS)
+             run = obj.getRuntime()
 
-            run = obj.getRuntime()
+             print("Total ExecutionTime in seconds:", run)
 
-            print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------------
+    -------------
 
-               The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
+
+    _minSup = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _minSup = str()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _mapSupport = {}
-    _lno = 0
 
+    _sumKernel = _ab._cp.RawKernel(r'''
 
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
+    #define uint32_t unsigned int
+
+    extern "C" __global__
+
+    void sumKernel(uint32_t *d_a, uint32_t *sum, uint32_t numElements)
+    {
+        uint32_t i = blockDim.x * blockIdx.x + threadIdx.x;
+        if (i < numElements)
+        {  
+            atomicAdd(&sum[0], __popc(d_a[i]));
+        }
+        return;    
+    }
+
+    ''', 'sumKernel')
 
-        :param value: user specified minSup value
 
-        :type value: int
 
-        :return: converted type
 
-        :rtype: int or float or string
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
-        self._mapSupport = {}
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                temp = self._iFile['Transactions'].tolist()
 
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
-                            splitter = [i.rstrip() for i in line.split(self._sep)]
-                            splitter = [x for x in splitter if x]
-                            self._Database.append(splitter)
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
-        self._minSup = self._convert(self._minSup)
+                    quit()
 
-    def creatingFrequentItems(self):
-        """
-        This function creates frequent items from _database.
-
-        :return: frequentTidData that stores frequent items and their tid list.
-
-        :rtype: Dict
-
-        """
-        tidData = {}
-        self._lno = 0
-        for transaction in self._Database:
-            self._lno = self._lno + 1
-            for item in transaction:
-                if item not in tidData:
-                    tidData[item] = [self._lno]
-                else:
-                    tidData[item].append(self._lno)
-        frequentTidData = {k: v for k, v in tidData.items() if len(v) >= self._minSup}
-        frequentTidData = dict(sorted(frequentTidData.items(), reverse=True, key=lambda x: len(x[1])))
-        return frequentTidData
-
-    def tidToBitset(self,itemset):
+    def _convert(self, value):
         """
 
-        This function converts tid list to bitset.
-
-        :param itemset: frequent itemset that generated
-
-        :type itemset: Dict
+        To convert the user specified minSup value
 
-        :return: patterns with original item names
+        :param value: user specified minSup value
 
-        :rtype: Dict
+        :type value: int or float or str
 
-        """
-        bitset = {}
-
-        for k,v in itemset.items():
-            bitset[k] = 0b1
-            bitset[k] = (bitset[k] << int(v[0])) | 0b1
-            for i in range(1,len(v)):
-                diff = int(v[i]) - int(v[i-1])
-                bitset[k] = (bitset[k] << diff) | 0b1
-            bitset[k] = (bitset[k] << (self._lno - int(v[i])))
-        return bitset
+        :return: converted type
 
-    def genPatterns(self,prefix,tidData):
         """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+    
+    def arraysAndItems(self):
+        ArraysAndItems = {}
+
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                j = tuple([j])
+                if j not in ArraysAndItems:
+                    ArraysAndItems[j] = [i]
+                else:
+                    ArraysAndItems[j].append(i)
 
-        This function generate frequent pattern about prefix.
-
-        :param prefix: prefix of pattern to generate patterns
-
-        :type prefix: str
-
-        :param tidData: tidData for pattern generation
-
-        :type tidData: list
-
-        """
-        # variables to store frequent item set and
-        itemset = prefix[0]
+        newArraysAndItems = {}
 
-        # Get the length of tidData
-        length = len(tidData)
+        for k,v in ArraysAndItems.items():
+            ArraysAndItems[k] = _ab._np.array(v, dtype=_ab._np.uint32)
+            if len(v) >= self._minSup:
+                self._finalPatterns[k] = len(v)
+                newArraysAndItems[k] = ArraysAndItems[k]
 
-        for i in range(length):
-            #tid = prefix[1].intersection(tidData[i][1])
-            tid = prefix[1] & tidData[i][1]
-            count = bin(tid).count("1") - 1
-            #tidLength = len(tid)
-            if count >= self._minSup:
-                frequentItemset = itemset + '\t' + tidData[i][0]
-                self._finalPatterns[frequentItemset] = count
-                self.genPatterns((frequentItemset,tid),tidData[i+1:length])
+        return newArraysAndItems
+    
+    def createBitRepresentation(self, ArraysAndItems):
+        bitRep = {}
+        arraySize = len(self._Database) // 32 + 1 if len(self._Database) % 32 != 0 else len(self._Database) // 32
 
-    def genAllFrequentPatterns(self,frequentItems):
-        """
-        This function generates all frequent patterns.
 
-        :param frequentItems: frequent items
+        for k, v in ArraysAndItems.items():
+            bitRep[k] = _ab._np.zeros(arraySize, dtype=_ab._np.uint32)
+            for i in v:
+                bitRep[k][i // 32] |= 1 << 31 - (i % 32)
 
-        :type frequentItems: Dict
+        for k, v in bitRep.items():
+            bitRep[k] = _ab._cp.array(v)
 
-        """
-        tidData = list(frequentItems.items())
-        length = len(tidData)
-        for i in range(length):
-            #print(i,tidData[i][0])
-            self.genPatterns(tidData[i],tidData[i+1:length])
+        return bitRep
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
-        We start with the scanning the itemSets and store the bitsets respectively.
-        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
         """
-
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-
-        self._creatingItemSets()
-        frequentItems = self.creatingFrequentItems()
-        self._finalPatterns = {k: len(v) for k, v in frequentItems.items()}
-        frequentItemsBitset = self.tidToBitset(frequentItems)
-        self.genAllFrequentPatterns(frequentItemsBitset)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Eclat_bitset algorithm")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
-        We start with the scanning the itemSets and store the bitsets respectively.
-        We form the combinations of single items and  check with minSup condition to check the frequency of patterns
         """
-
+        self._Database = []
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-
         self._creatingItemSets()
-        frequentItems = self.creatingFrequentItems()
-        self._finalPatterns = {k: len(v) for k, v in frequentItems.items()}
-        frequentItemsBitset = self.tidToBitset(frequentItems)
-        self.genAllFrequentPatterns(frequentItemsBitset)
+        itemsList = sorted(list(set.union(*self._Database)))  # because Database is list
+        self._minSup = self._convert(self._minSup)
+
+        ArraysAndItems = self.arraysAndItems()
+
+        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
+
+        while len(ArraysAndItems) > 0:
+            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
+            newArraysAndItems = {}
+            keys = list(ArraysAndItems.keys())
+            for i in range(len(ArraysAndItems)):
+                iList = list(keys[i])
+                # print(i, "/", len(ArraysAndItems), end="\r")
+                for j in range(i+1, len(ArraysAndItems)):
+                    jList = list(keys[j])
+                    union = []
+                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
+                        union = iList + [jList[-1]]
+                        union = tuple(union)
+                        unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
+                        sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
+                        self._sumKernel((len(unionData) // 32 + 1,), (32,), (unionData, sum, _ab._cp.uint32(len(unionData))))
+                        sum = sum[0]
+                        if sum >= self._minSup and union not in self._finalPatterns:
+                            newArraysAndItems[union] = unionData
+                            string = "\t".join(union)
+                            self._finalPatterns[string] = sum
+            ArraysAndItems = newArraysAndItems
+            # print()
+
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Eclat_bitset algorithm")
+        print("Frequent patterns were generated successfully using cuEclatBit algorithm ")
+            
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
@@ -397,27 +368,28 @@
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the outputfile
-        :type outFile: file
+        :param outFile: name of the output file
+        :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
@@ -426,25 +398,36 @@
     def printResults(self):
         """
         This function is used to print the result
         """
         print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
 
-if __name__=="__main__":
+if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cuEclatBit(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = ECLATbitset(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cuEclatBit(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+"""_ap = cuEclat("/home/tarun/PAMI/PAMI/frequentPattern/cuda/test.txt", 2, " ")
+    _ap = cuEclat("/home/tarun/Transactional_T10I4D100K.csv", 450, "\t")
+    _ap.startMine()
+    _ap.mine()
+    print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+    print("Total Memory in USS:", _ap.getMemoryUSS())
+    print("Total Memory in RSS", _ap.getMemoryRSS())
+    print("Total ExecutionTime in s:", _ap.getRuntime())"""
+
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/basic/FPGrowth.py` & `pami-2024.5.1/PAMI/frequentPattern/basic/_FPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/frequentPattern/basic/abstract.py`

 * *Files 7% similar despite different names*

```diff
@@ -110,70 +110,24 @@
         self._finalPatterns = {}
         self._oFile = str()
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._startTime = float()
         self._endTime = float()
 
-    '''@abstractmethod
-    def iFile(self):
-        """Variable to store the input file path/file name"""
-
-        pass
-
-    @abstractmethod
-    def minSup(self):
-        """Variable to store the user-specified minimum support value"""
-
-        pass
-
-    @abstractmethod
-    def sep(self):
-        """Variable to store the user-specified minimum support value"""
-
-        pass
-
-    @abstractmethod
-    def startTime(self):
-        """Variable to store the start time of the mining process"""
-
-        pass
-
-    @abstractmethod
-    def endTime(self):
-        """Variable to store the end time of the complete program"""
-
-        pass
-
-    @abstractmethod
-    def memoryUSS(self):
-        """Variable to store USS memory consumed by the program"""
-
-        pass
-
-    @abstractmethod
-    def memoryRSS(self):
-        """Variable to store RSS memory consumed by the program"""
-
-        pass
-
-    @abstractmethod
-    def finalPatterns(self):
-        """Variable to store the complete set of patterns in a dictionary"""
+    @_abstractmethod
+    def startMine(self):
+        """
+        Code for the mining process will start from this function
+        """
 
         pass
 
-    @abstractmethod
-    def oFile(self):
-        """Variable to store the name of the output file to store the complete set of frequent patterns"""
-
-        pass'''
-
     @_abstractmethod
-    def startMine(self):
+    def mine(self):
         """
         Code for the mining process will start from this function
         """
 
         pass
 
     @_abstractmethod
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/closed/CHARM.py` & `pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,26 +1,25 @@
-# CHARM is an algorithm to discover closed frequent patterns in a transactional database. Closed frequent patterns are patterns if there exists no superset that has the same support count as this original itemset . This algorithm employs depth-first search technique to find the complete set of closed frequent patterns in a
+# GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
+# Lattice Traversal to mine the geo referenced peridoic frequent patterns.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------------
+# --------------------------------------------------------
 #
 #
-#             from PAMI.frequentPattern.closed import CHARM as alg
+#             import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
 #
-#             obj = alg.CHARM(iFile, minSup)
+#             obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of Closed Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
 #
-#             obj.savePatterns(oFile)
-#
-#             Df = obj.getPatternsAsDataFrame()
+#             obj.save("outFile")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -45,479 +44,454 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-"""
+     Copyright (C)  2021 Rage Uday Kiran
 
+"""
 
-from PAMI.frequentPattern.closed import abstract as _ab
+from  PAMI.geoReferencedPeriodicFrequentPattern.basic import abstract as _ab
 from deprecated import deprecated
 
 
-class CHARM(_ab._frequentPatterns):
+class GPFPMiner(_ab._geoReferencedPeriodicFrequentPatterns):
     """
-    :Description: CHARM is an algorithm to discover closed frequent patterns in a transactional database. Closed frequent patterns are patterns if there exists no superset that has the same support count as this original itemset. This algorithm employs depth-first search technique to find the complete set of closed frequent patterns in a
-
-
-    :Reference:   Mohammed J. Zaki and Ching-Jui Hsiao, CHARM: An Efficient Algorithm for Closed Itemset Mining,
-            Proceedings of the 2002 SIAM, SDM. 2002, 457-473, https://doi.org/10.1137/1.9781611972726.27
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-    :param  sep: str :
+    :Description:   GPFPMiner is an Extension of ÉCLAT algorithm,which  stands for Equivalence Class Clustering and
+    bottom-up Lattice Traversal to mine the geo referenced periodic frequent patterns.
+        
+    :Reference:
+
+    :param  iFile: str
+                   Name of the Input file to mine complete set of Geo-referenced periodic frequent patterns
+    :param  oFile: str
+                   Name of the output file to store complete set of Geo-referenced periodic frequent patterns
+    :param  minSup: int or float or str
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param nFile: str
+                   Name of the input file to mine complete set of Geo-referenced periodic frequent patterns
+    :param  sep: str
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
-
     :Attributes:
 
+        iFile : str
+            Input file name or path of the input file
+        nFile : str
+           Name of Neighbourhood file name
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : float or int or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
         startTime : float
-          To record the start time of the mining process
-
+            To record the start time of the mining process
         endTime : float
-          To record the completion time of the mining process
-
+            To record the completion time of the mining process
         finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
+            Storing the complete set of patterns in a dictionary variable
+        oFile : str
+            Name of the output file to store complete set of frequent patterns
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
-
+            To store the total amount of RSS memory consumed by the program
         Database : list
-          To store the transactions of a database in list
+            To store the complete set of transactions available in the input database/file
 
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        itemSetCount : int
-            it represents the total no of patterns
-        finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-        hashing : dict
-            stores the patterns with their support to check for the closed property
+    :Methods:
 
+            mine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            save(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrames()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            creatingItemSets(iFileName)
+                Storing the complete transactions of the database/input file in a database variable
+            frequentOneItem()
+                Generating one frequent patterns
+            convert(value)
+                To convert the given user specified value    
+            getNeighbourItems(keySet)
+                A function to get common neighbours of a itemSet
+            mapNeighbours(file)
+                A function to map items to their neighbours
 
-    **Methods to execute code on terminal**
-    --------------------------------------------------------------
+    **Executing the code on terminal :**
+    ----------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 CHARM.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 GPFPMiner.py <inputFile> <outputFile> <neighbourFile> <minSup> <maxPer>
 
       Example Usage:
 
-      (.venv) $ python3 CHARM.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 GPFPMiner.py sampleTDB.txt output.txt sampleN.txt 0.5 0.3
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup & maxPer will be considered in percentage of database transactions
 
 
-    **Importing this algorithm into a python program**
-    --------------------------------------------------------------
-    .. code-block:: python
 
-            from PAMI.frequentPattern.closed import CHARM as alg
+    **Sample run of importing the code :**
+    -----------------------------------------
+    .. code-block:: python
+    
+            import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
 
-            obj = alg.CHARM(iFile, minSup)
+            obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
 
             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
-
-            print("Total number of Closed Frequent Patterns:", len(frequentPatterns))
+            Patterns = obj.getPatterns()
 
-            obj.savePatterns(oFile)
+            print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
 
-            Df = obj.getPatternsAsDataFrame()
+            obj.save("outFile")
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-
     **Credits:**
-    -------------------------------
-
-                 The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-
+    --------------
+        The complete program was written by P.RaviKumar under the supervision of Professor Rage Uday Kiran.
     """
 
+    _minSup = " "
+    _maxPer = " "
     _startTime = float()
     _endTime = float()
-    _minSup = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _sep = " "
+    _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
+    _sep = "\t"
     _lno = 0
-    _mapSupport = {}
-    _hashing = {}
-    _itemSetCount = 0
-    _maxItemId = 0
-    _tableSize = 10000
-    _writer = None
-
-    def _convert(self, value):
-        """
-
-        To convert the type of user specified minSup value
 
-        :param value: user specified minSup value
-
-        :type value: int or float or str
-
-        :return: converted type
-
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._lno * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        return value
+    def __init__(self, iFile, nFile, minSup, maxPer, sep="\t"):
+        super().__init__(iFile, nFile, minSup, maxPer, sep)
+        self._NeighboursMap = {}
 
-    def _creatingItemsets(self):
+    def _creatingItemSets(self):
         """
-        Storing the complete frequent patterns of the database/input file in a database variable
+        Storing the complete transactions of the database/input file in a database variable
         """
-        self._mapSupport = {}
-        self._tidList = {}
-        self._lno = 0
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            for i in self._Database:
-                self._lno += 1
-                for j in i:
-                    if j not in self._mapSupport:
-                        self._mapSupport[j] = 1
-                        self._tidList[j] = [self._lno]
-                    else:
-                        self._mapSupport[j] += 1
-                        self._tidList[j].append(self._lno)
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
-                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    for j in temp:
-                        if j not in self._mapSupport:
-                            self._mapSupport[j] = 1
-                            self._tidList[j] = [self._lno]
-                        else:
-                            self._mapSupport[j] += 1
-                            self._tidList[j].append(self._lno)
+                    self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            i = [i.rstrip() for i in line.split(self._sep)]
-                            i = [x for x in i if x]
-                            self._lno += 1
-                            for j in i:
-                                if j not in self._mapSupport:
-                                    self._mapSupport[j] = 1
-                                    self._tidList[j] = [self._lno]
-                                else:
-                                    self._mapSupport[j] += 1
-                                    self._tidList[j].append(self._lno)
+                            line = line.rstrip()
+                            temp = [i.strip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
+
+    # function to get frequent one pattern
+    def _frequentOneItem(self):
+        """
+        Generating one frequent patterns
+        """
+
+        candidate = {}
+        for i in self._Database:
+            self._lno += 1
+            n = int(i[0])
+            for j in i[1:]:
+                if j not in candidate:
+                    candidate[j] = [1, abs(0-n), n, [n]]
+                else:
+                    candidate[j][0] += 1
+                    candidate[j][1] = max(candidate[j][1], abs(n - candidate[j][2]))
+                    candidate[j][2] = n
+                    candidate[j][3].append(n)
         self._minSup = self._convert(self._minSup)
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minSup}
-        _flist = {}
-        self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
-        for x, y in self._tidList.items():
-            t1 = 0
-            for i in y:
-                t1 += i
-            _flist[x] = t1
-        _flist = [key for key, value in sorted(_flist.items(), key=lambda x: x[1])]
-        return _flist
-
-    def _calculate(self, tidSet):
-        """
-        To calculate the hashcode of pattern
-
-        :param tidSet: the timestamps of a pattern
-
-        :type tidSet: list
-
-        :rtype: int
-        """
-
-        hashcode = 0
-        for i in tidSet:
-            hashcode += i
-        if hashcode < 0:
-            hashcode = abs(0 - hashcode)
-        return hashcode % self._tableSize
-
-    def _contains(self, itemSet, value, hashcode):
-        """
-        Check for the closed property(patterns with same support) by checking the hashcode(sum of timestamps),
-        if hashcode key in hashing dict is none then returns a false, else returns with true.
-        :param itemSet: frequent pattern
-        :type itemSet: list
-        :param value: support of the pattern
-        :type value: int
-        :param hashcode: calculated from the timestamps of pattern
-        :type hashcode: int
-        """
-        if self._hashing.get(hashcode) is None:
-            return False
-        for i in self._hashing[hashcode]:
-            itemSetx = i
-            if value == self._hashing[hashcode][itemSetx] and set(itemSetx).issuperset(itemSet):
-                return True
-        return False
+        self._maxPer = self._convert(self._maxPer)
+        #print(self._minSup, self._maxPer)
+        self._tidList = {k: v[3] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
+        candidate = {k: [v[0], v[1]] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
+        plist = [key for key, value in sorted(candidate.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        return plist
 
-    def _save(self, prefix, suffix, tidSetx):
+    def _convert(self, value):
         """
-        Check for the closed property (patterns with same support), if found deletes the subsets and stores
-        supersets and also saves the patterns that satisfy the closed property
+        To convert the given user specified value
 
-        :param prefix: the prefix of a pattern
+        :param value: user specified value
 
-        :type prefix: frequent item or pattern
+        :type value: int or float or str
 
-        :param suffix: the suffix of a patterns
+        :return: converted value
+
+        :rtype: float
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def _getSupportAndPeriod(self, timeStamps):
+        """
+        calculates the support and periodicity with list of timestamps
+
+        :param timeStamps: timestamps of a pattern
+        :type timeStamps: list
+        """
+        timeStamps.sort()
+        cur = 0
+        per = 0
+        sup = 0
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > self._maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+            sup += 1
+        per = max(per, self._lno - cur)
+        return [sup, per]
+
+    def _save(self, prefix, suffix, tidSetX):
+        """
+        Saves the patterns that satisfy the periodic frequent property.
 
+        :param prefix: the prefix of a pattern
+        :type prefix: list or None
+        :param suffix: the suffix of a patterns
         :type suffix: list
+        :param tidSetX: the timestamp of a patterns
+        :type tidSetX: list
 
-        :param tidSetx: the timestamp of a patterns
 
-        :type tidSetx: list
         """
-        if prefix is None:
+        if prefix == None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        prefix = list(set(prefix))
-        prefix.sort()
-        val = len(tidSetx)
-        if val >= self._minSup:
-            hashcode = self._calculate(tidSetx)
-            if self._contains(prefix, val, hashcode) is False:
-                sample = str()
-                for i in prefix:
-                    sample = sample + i + "\t"
-                self._itemSetCount += 1
-                self._finalPatterns[sample] = val
-            if hashcode not in self._hashing:
-                self._hashing[hashcode] = {tuple(prefix): val}
-            else:
-                self._hashing[hashcode][tuple(prefix)] = val
+        val = self._getSupportAndPeriod(tidSetX)
+        if val[0] >= self._minSup and val[1] <= self._maxPer:
+            self._finalPatterns[tuple(prefix)] = val
 
-    def _processEquivalenceClass(self, prefix, itemSets, tidSets):
+    def _Generation(self, prefix, itemSets, tidSets):
         """
-        Equivalence class is followed  and check for the patterns which satisfies frequent properties.
-        :param prefix:  main equivalence prefix
-        :type prefix: frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the minSup
+        Generates the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list or None
+        :param itemSets: the item sets of a patterns
         :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
+        :param tidSets: the timestamp of a patterns
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidI = tidSets[0]
             self._save(prefix, [i], tidI)
             return
-        if len(itemSets) == 2:
-            itemX = itemSets[0]
-            tidSetX = tidSets[0]
-            itemY = itemSets[1]
-            tidSetY = tidSets[1]
-            y1 = list(set(tidSetX).intersection(tidSetY))
-            if len(y1) >= self._minSup:
-                suffix = []
-                suffix += [itemX, itemY]
-                suffix = list(set(suffix))
-                self._save(prefix, suffix, y1)
-            if len(y1) != len(tidSetX):
-                self._save(prefix, [itemX], tidSetX)
-            if len(y1) != len(tidSetY):
-                self._save(prefix, [itemX], tidSetY)
-            return
         for i in range(len(itemSets)):
             itemX = itemSets[i]
-            if itemX is None:
+            if itemX == None:
                 continue
             tidSetX = tidSets[i]
             classItemSets = []
             classTidSets = []
-            itemSetx = [itemX]
+            itemSetX = [itemX]
+            neighboursItemsI = self._getNeighbourItems(itemSets[i])
             for j in range(i + 1, len(itemSets)):
-                itemY = itemSets[j]
-                if itemY is None:
+                neighboursItemsJ = self._getNeighbourItems(itemSets[i])
+                if not itemSets[j] in neighboursItemsI:
                     continue
-                tidSetY = tidSets[j]
-                y = list(set(tidSetX).intersection(tidSetY))
-                if len(y) < self._minSup:
-                    continue
-                if len(tidSetX) == len(tidSetY) and len(y) == len(tidSetX):
-                    itemSets.insert(j, None)
-                    tidSets.insert(j, None)
-                    itemSetx.append(itemY)
-                elif len(tidSetX) < len(tidSetY) and len(y) == len(tidSetX):
-                    itemSetx.append(itemY)
-                elif len(tidSetX) > len(tidSetY) and len(y) == len(tidSetY):
-                    itemSets.insert(j, None)
-                    tidSets.insert(j, None)
-                    classItemSets.append(itemY)
-                    classTidSets.append(y)
-                else:
-                    classItemSets.append(itemY)
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetX).intersection(tidSetJ))
+                if len(y) >= self._minSup:
+                    ne = list(set(neighboursItemsI).intersection(neighboursItemsJ))
+                    x = []
+                    x = x + [itemX]
+                    x = x + [itemJ]
+                    self._NeighboursMap[tuple(x)] = ne
+                    classItemSets.append(itemJ)
                     classTidSets.append(y)
-            if len(classItemSets) > 0:
-                newPrefix = list(set(itemSetx)) + prefix
-                self._processEquivalenceClass(newPrefix, classItemSets, classTidSets)
-                self._save(prefix, list(set(itemSetx)), tidSetX)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetX)
+
+    def _getNeighbourItems(self, keySet):
+        """
+        A function to get Neighbours of a item
+
+        :param keySet: itemSet
+        :type keySet: str or tuple
+        :return: set of common neighbours
+        :rtype: set
+        """
+        itemNeighbours = self._NeighboursMap.keys()
+        if isinstance(keySet, str):
+            if self._NeighboursMap.get(keySet) is None:
+                return []
+            itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
+        if isinstance(keySet, tuple):
+            keySet = list(keySet)
+            for j in range(0, len(keySet)):
+                i = keySet[j]
+                itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
+        return itemNeighbours
+
+    def mapNeighbours(self):
+        """
+        A function to map items to their Neighbours
+        """
+        self._NeighboursMap = {}
+        if isinstance(self._nFile, _ab._pd.DataFrame):
+            data = []
+            if self._nFile.empty:
+                print("its empty..")
+            i = self._nFile.columns.values.tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for i in data:
+                self._NeighboursMap[i[0]] = i[1:]
+        if isinstance(self._nFile, str):
+            if _ab._validators.url(self._nFile):
+                data = _ab._urlopen(self._nFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._NeighboursMap[temp[0]] = temp[1:]
+            else:
+                try:
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._NeighboursMap[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-        Mining process will start from here by extracting the frequent patterns from the database. It performs prefix
-        equivalence to generate the combinations and closed frequent patterns.
+        Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        _plist = self._creatingItemsets()
-        self._finalPatterns = {}
-        self._hashing = {}
-        for i in range(len(_plist)):
-            itemX = _plist[i]
-            if itemX is None:
-                continue
-            tidSetx = self._tidList[itemX]
-            itemSetx = [itemX]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemY = _plist[j]
-                if itemY is None:
-                    continue
-                tidSetY = self._tidList[itemY]
-                y1 = list(set(tidSetx).intersection(tidSetY))
-                if len(y1) < self._minSup:
-                    continue
-                if len(tidSetx) == len(tidSetY) and len(y1) == len(tidSetx):
-                    _plist.insert(j, None)
-                    itemSetx.append(itemY)
-                elif len(tidSetx) < len(tidSetY) and len(y1) == len(tidSetx):
-                    itemSetx.append(itemY)
-                elif len(tidSetx) > len(tidSetY) and len(y1) == len(tidSetY):
-                    _plist.insert(j, None)
-                    itemSets.append(itemY)
-                    tidSets.append(y1)
-                else:
-                    itemSets.append(itemY)
-                    tidSets.append(y1)
-            if len(itemSets) > 0:
-                self._processEquivalenceClass(itemSetx, itemSets, tidSets)
-            self._save(None, itemSetx, tidSetx)
-        print("Closed Frequent patterns were generated successfully using CHARM algorithm")
-        self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
+
+        self.mine()
 
     def mine(self):
         """
-        Mining process will start from here by extracting the frequent patterns from the database. It performs prefix
-        equivalence to generate the combinations and closed frequent patterns.
+        Frequent pattern mining process will start from here
         """
+
+        # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
-        _plist = self._creatingItemsets()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.mapNeighbours()
         self._finalPatterns = {}
-        self._hashing = {}
-        for i in range(len(_plist)):
-            itemX = _plist[i]
-            if itemX is None:
-                continue
-            tidSetx = self._tidList[itemX]
-            itemSetx = [itemX]
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemX = plist[i]
+            tidSetX = self._tidList[itemX]
+            itemSetX = [itemX]
             itemSets = []
             tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemY = _plist[j]
-                if itemY is None:
-                    continue
-                tidSetY = self._tidList[itemY]
-                y1 = list(set(tidSetx).intersection(tidSetY))
-                if len(y1) < self._minSup:
+            neighboursItems = self._getNeighbourItems(plist[i])
+            for j in range(i + 1, len(plist)):
+                if not plist[j] in neighboursItems:
                     continue
-                if len(tidSetx) == len(tidSetY) and len(y1) == len(tidSetx):
-                    _plist.insert(j, None)
-                    itemSetx.append(itemY)
-                elif len(tidSetx) < len(tidSetY) and len(y1) == len(tidSetx):
-                    itemSetx.append(itemY)
-                elif len(tidSetx) > len(tidSetY) and len(y1) == len(tidSetY):
-                    _plist.insert(j, None)
-                    itemSets.append(itemY)
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetX).intersection(tidSetJ))
+                if len(y1) >= self._minSup:
+                    itemSets.append(itemJ)
                     tidSets.append(y1)
-                else:
-                    itemSets.append(itemY)
-                    tidSets.append(y1)
-            if len(itemSets) > 0:
-                self._processEquivalenceClass(itemSetx, itemSets, tidSets)
-            self._save(None, itemSetx, tidSetx)
-        print("Closed Frequent patterns were generated successfully using CHARM algorithm")
+            self._Generation(itemSetX, itemSets, tidSets)
+            self._save(None, itemSetX, tidSetX)
         self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
@@ -528,75 +502,84 @@
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataframe
+            pat = ""
+            for i in a:
+                pat += str(i) + "\t"
+            data.append([pat, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Period'])
+        return dataFrame
 
     def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to a output file
+
         :param outFile: name of the output file
-        :type outFile: csvfile
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+            pat = ""
+            for i in x:
+                pat += str(i) + "\t"
+            patternsAndSupport = pat + ": " + str(y[0]) + ": " + str(y[1])
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
-
         return self._finalPatterns
-
+    
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Closed Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Spatial Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = CHARM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = CHARM(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Closed Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Spatial Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/closed/abstract.py` & `pami-2024.5.1/PAMI/frequentPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/abstract.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuApriori.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cuApriori.py`

 * *Files 4% similar despite different names*

```diff
@@ -261,46 +261,15 @@
         return newArraysAndItems
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self.arraysAndItems()
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                # print(i, "/", len(ArraysAndItems), end="\r")
-                iList = list(keys[i])
-                for j in range(i + 1, len(ArraysAndItems)):
-                    jList = list(keys[j])
-                    union = tuple(sorted(set(iList + jList)))
-                    intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]],
-                                                    assume_unique=True)
-                    if len(intersect) >= self._minSup and union not in self._finalPatterns:
-                        newArraysAndItems[union] = intersect
-                        self._finalPatterns[union] = len(intersect)
-            ArraysAndItems = newArraysAndItems
-            # print()
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuApriori algorithm ")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuAprioriBit.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cuAprioriBit.py`

 * *Files 9% similar despite different names*

```diff
@@ -273,51 +273,15 @@
         return bitRep
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self.arraysAndItems()
-        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                # print(i, "/", len(ArraysAndItems), end="\r")
-                iList = list(keys[i])
-                for j in range(i + 1, len(ArraysAndItems)):
-                    unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
-                    sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
-                    self._sumKernel((len(unionData) // 32 + 1,), (32,),
-                                    (unionData, sum, _ab._cp.uint32(len(unionData))))
-                    sum = sum[0]
-                    jList = list(keys[j])
-                    union = tuple(sorted(set(iList + jList)))
-                    if sum >= self._minSup and union not in self._finalPatterns:
-                        newArraysAndItems[union] = unionData
-                        string = "\t".join(union)
-                        self._finalPatterns[string] = sum
-            ArraysAndItems = newArraysAndItems
-            # print()
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuAprioriBit algorithm ")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclat.py` & `pami-2024.5.1/PAMI/frequentPattern/topk/FAE.py`

 * *Files 20% similar despite different names*

```diff
@@ -1,22 +1,21 @@
-# cuECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
-#
+# Top - K is and algorithm to discover top frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# ---------------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuEclat as alg
+#             import PAMI.frequentPattern.topK.FAE as alg
 #
-#             obj = alg.cuEclat(iFile, minSup)
+#             obj = alg.FAE(iFile, K)
 #
 #             obj.mine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             topKFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -30,14 +29,15 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,32 +47,35 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-
-# from PAMI.frequentPattern.cuda import abstract as _ab
-import abstract as _ab
+from PAMI.frequentPattern.topk import abstract as _ab
 from deprecated import deprecated
 
-class cuEclat(_ab._frequentPatterns):
+
+class FAE(_ab._frequentPatterns):
     """
-    :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+    :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
+
 
-    :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
-            372-390 (2000), https://ieeexplore.ieee.org/document/846291
+    :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261 · Source: IEEE Xplore
+                  https://ieeexplore.ieee.org/document/4370261
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param  k: int :
+                    User specified count of top frequent patterns
+    :param minimum: int :
+                    Minimum number of frequent patterns to consider in analysis
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
 
     :Attributes:
 
@@ -87,344 +90,371 @@
 
         memoryUSS : float
           To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
-        Database : list
-          To store the transactions of a database in list
-
+        finalPatterns : dict
+            it represents to store the patterns
 
 
     **Methods to execute code on terminal**
-    ----------------------------------------------------
+    -------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cuEclat.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
 
       Example Usage:
 
-      (.venv) $ python3 cuEclat.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: k will be considered as count of top frequent patterns to consider in analysis
 
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
 
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------------
     .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuEclat as alg
-
-             obj = alg.cuEclat(iFile, minSup)
-
-             obj.mine()
+        import PAMI.frequentPattern.topK.FAE as alg
 
-             frequentPatterns = obj.getPatterns()
+        obj = alg.FAE(iFile, K)
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+        obj.mine()
 
-             obj.save(oFile)
+        topKFrequentPatterns = obj.getPatterns()
 
-             Df = obj.getPatternInDataFrame()
+        print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
 
-             memUSS = obj.getMemoryUSS()
+        obj.save(oFile)
 
-             print("Total Memory in USS:", memUSS)
+        Df = obj.getPatternInDataFrame()
 
-             memRSS = obj.getMemoryRSS()
+        memUSS = obj.getMemoryUSS()
 
-             print("Total Memory in RSS", memRSS)
+        print("Total Memory in USS:", memUSS)
 
-             run = obj.getRuntime()
+        memRSS = obj.getMemoryRSS()
 
-             print("Total ExecutionTime in seconds:", run)
+        print("Total Memory in RSS", memRSS)
 
+        run = obj.getRuntime()
 
-    **Credits:**
-    -------------
+        print("Total ExecutionTime in seconds:", run)
 
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    Credits:
+    --------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _ab._cp.cuda.Device(0).use()
-
-
-
-    _minSup = float()
     _startTime = float()
     _endTime = float()
+    _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-
+    _tidList = {}
+    _minimum = int()
 
     def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+            Storing the complete transactions of the database/input file in a database variable
+
         """
+
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
 
-            for k in temp:
-                self._Database.append(set(k))
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
+    def _frequentOneItem(self):
         """
+        Generating one frequent patterns
+        """
+        candidate = {}
+        self._tidList = {}
+        for i in range(len(self._Database)):
+            for j in self._Database[i]:
+                if j not in candidate:
+                    candidate[j] = 1
+                    self._tidList[j] = [i]
+                else:
+                    candidate[j] += 1
+                    self._tidList[j].append(i)
+        self._finalPatterns = {}
+        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = candidate[i]
+        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
+        return plist
+
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
+
+            :param prefix: the prefix of a pattern
+            :type prefix: list
+            :param suffix: the suffix of a patterns
+            :type suffix: list
+            :param tidSetI: the timestamp of a patterns
+            :type tidSetI: list
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = len(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + "\t"
+        if len(self._finalPatterns) < self._k:
+            if val > self._minimum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([i for i in self._finalPatterns.values()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
+                if val > y:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[sample] = val
+                    self._finalPatterns = {k: v for k, v in
+                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
+                                                     reverse=True)}
+                    self._minimum = min([i for i in self._finalPatterns.values()])
+                    return
+
+    def _Generation(self, prefix, itemSets, tidSets):
+        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+            :param prefix:  main equivalence prefix
+            :type prefix: periodic-frequent item or pattern
+            :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
+                            and frequent with their timestamps
+            :type itemSets: list
+            :param tidSets: timestamps of the items in the argument itemSets
+            :type tidSets: list
+
+
+                    """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = list(set(tidSetI).intersection(tidSetJ))
+                if len(y) >= self._minimum:
+                    classItemSets.append(itemJ)
+                    classTidSets.append(y)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
-        To convert the user specified minSup value
-
+    def _convert(self, value):
+        """
+        to convert the type of user specified minSup value
         :param value: user specified minSup value
-
         :type value: int or float or str
-
         :return: converted type
-
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = ((len(self._Database)) * value)
             else:
                 value = int(value)
         return value
-    
-    def _arraysAndItems(self):
-        """ 
-        Convert the items into arrays for cupy and store them in a dictionary variable
-        :return: dictionary variable
-        """
-        ArraysAndItems = {}
-
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                j = tuple([j])
-                if j not in ArraysAndItems:
-                    ArraysAndItems[j] = [i]
-                else:
-                    ArraysAndItems[j].append(i)
-
-        newArraysAndItems = {}
-
-        for k,v in ArraysAndItems.items():
-            ArraysAndItems[k] = _ab._cp.array(v, dtype=_ab._np.uint32)
-            if len(v) >= self._minSup:
-                self._finalPatterns[k] = len(v)
-                newArraysAndItems[k] = ArraysAndItems[k]
-
-        return newArraysAndItems
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-        Frequent pattern mining process will start from here
+            Main function of the program
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self._arraysAndItems()
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                iList = list(keys[i])
-                for j in range(i+1, len(ArraysAndItems)):
-                    # print(i, "/", len(ArraysAndItems), end="\r")
-                    jList = list(keys[j])
-                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
-                        union = iList + [jList[-1]]
-                        union = tuple(union)
-                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
-                        if len(intersect) >= self._minSup:
-                            newArraysAndItems[union] = intersect
-                            self._finalPatterns[union] = len(intersect)
-                    else: 
-                        break
-
-            ArraysAndItems = newArraysAndItems
-            # print()
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuEclat algorithm ")
+        self.mine()
 
     def mine(self):
         """
-        Frequent pattern mining process will start from here
+            Main function of the program
         """
-        self._Database = []
         self._startTime = _ab._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self._arraysAndItems()
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                iList = list(keys[i])
-                for j in range(i+1, len(ArraysAndItems)):
-                    # print(i, "/", len(ArraysAndItems), end="\r")
-                    jList = list(keys[j])
-                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
-                        union = iList + [jList[-1]]
-                        union = tuple(union)
-                        intersect = _ab._cp.intersect1d(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]], assume_unique=True)
-                        if len(intersect) >= self._minSup:
-                            newArraysAndItems[union] = intersect
-                            self._finalPatterns[union] = len(intersect)
-                    else:
-                        break
-
-            ArraysAndItems = newArraysAndItems
-            # print()
-
+        self._k = self._convert(self._k)
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._tidList[itemJ]
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                if len(y1) >= self._minimum:
+                    itemSets.append(itemJ)
+                    tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
         self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuEclat algorithm ")
-            
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
+
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
+
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
+
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: csvfile
+
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
+
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printTOPK(self):
         """
         This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Top K Frequent  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in s:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cuEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in s:", _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
 
-
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cuEclatBit.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/_PFECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,26 +1,27 @@
-# cuECLATBit is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+# PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
 #
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cuEclatBit as alg
 #
-#             obj = alg.cuEclatBit(iFile, minSup)
+#             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 #
-#             obj.mine()
+#             obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 #
-#             frequentPatterns = obj.getPatterns()
+#             obj.startMine()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             obj.save(oFile)
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#             Df = obj.getPatternInDataFrame()
+#             obj.save("patterns")
+#
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -31,442 +32,440 @@
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-"""
+     Copyright (C)  2021 Rage Uday Kiran
 
+"""
 
-# from PAMI.frequentPattern.cuda import abstract as _ab
-import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
 
-class cuEclatBit(_ab._frequentPatterns):
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+
+
+class PFECLAT(_ab._periodicFrequentPatterns):
     """
-    :Description: ECLAT is one of the fundamental algorithm to discover frequent patterns in a transactional database.
+    :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
 
-    :Reference:  Mohammed Javeed Zaki: Scalable Algorithms for Association Mining. IEEE Trans. Knowl. Data Eng. 12(3):
-            372-390 (2000), https://ieeexplore.ieee.org/document/846291
+    :Reference:   P. Ravikumar, P.Likhitha, R. Uday kiran, Y. Watanobe, and Koji Zettsu, "Towards efficient discovery of
+                  periodic-frequent patterns in columnar temporal databases", 2021 IEA/AIE.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
-
-        endTime : float
-          To record the completion time of the mining process
-
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
-
+            To store the total amount of RSS memory consumed by the program
+        startTime : float
+            To record the start time of the mining process
+        endTime : float
+            To record the completion time of the mining process
         Database : list
-          To store the transactions of a database in list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
+        hashing : dict
+            stores the patterns with their support to check for the closed property
+
+    :Methods:
+
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingOneItemSets()
+            Scan the database and store the items with their timestamps which are periodic frequent 
+        getPeriodAndSupport()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
+            
 
+    **Methods to execute code on terminal**
+    ------------------------------------------
+    .. code-block:: console
 
 
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
+       Format:
 
-    .. code-block:: console
+       (.venv) $ python3 PFECLAT.py <inputFile> <outputFile> <minSup>
 
-      Format:
+       Example usage:
 
-      (.venv) $ python3 cuEclatBit.py <inputFile> <outputFile> <minSup>
+       (.venv) $ python3 PFECLAT.py sampleDB.txt patterns.txt 10.0
 
-      Example Usage:
 
-      (.venv) $ python3 cuEclatBit.py sampleDB.txt patterns.txt 10.0
 
-    .. note:: minSup will be considered in percentage of database transactions
+               .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
-    ----------------------------------------------------
-
+    --------------------------------------------------------
     .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuEclatBit as alg
+             from PAMI.periodicFrequentPattern.basic import PFECLAT as alg
 
-             obj = alg.cuEclatBit(iFile, minSup)
+                obj = alg.PFECLAT("../basic/sampleTDB.txt", "2", "5")
 
-             obj.mine()
+                obj.startMine()
 
-             frequentPatterns = obj.getPatterns()
+                periodicFrequentPatterns = obj.getPatterns()
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-             obj.save(oFile)
+                obj.save("patterns")
 
-             Df = obj.getPatternInDataFrame()
+                Df = obj.getPatternsAsDataFrame()
 
-             memUSS = obj.getMemoryUSS()
+                memUSS = obj.getMemoryUSS()
 
-             print("Total Memory in USS:", memUSS)
+                print("Total Memory in USS:", memUSS)
 
-             memRSS = obj.getMemoryRSS()
+                memRSS = obj.getMemoryRSS()
 
-             print("Total Memory in RSS", memRSS)
+                print("Total Memory in RSS", memRSS)
 
-             run = obj.getRuntime()
-
-             print("Total ExecutionTime in seconds:", run)
+                run = obj.getRuntime()
 
+                print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------
-
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+    --------------
+             The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
 
     """
-
-
-    _minSup = float()
-    _startTime = float()
-    _endTime = float()
-    _finalPatterns = {}
+    
     _iFile = " "
     _oFile = " "
     _sep = " "
+    _dbSize = None
+    _Database = None
+    _minSup = str()
+    _maxPer = str()
+    _tidSet = set()
+    _finalPatterns = {}
+    _startTime = None
+    _endTime = None
     _memoryUSS = float()
     _memoryRSS = float()
-    _Database = []
-
-    _sumKernel = _ab._cp.RawKernel(r'''
-
-    #define uint32_t unsigned int
-
-    extern "C" __global__
-
-    void sumKernel(uint32_t *d_a, uint32_t *sum, uint32_t numElements)
-    {
-        uint32_t i = blockDim.x * blockIdx.x + threadIdx.x;
-        if (i < numElements)
-        {  
-            atomicAdd(&sum[0], __popc(d_a[i]));
-        }
-        return;    
-    }
-
-    ''', 'sumKernel')
-
 
+    def _getPeriodic(self, tids: set) -> int:
+        tidList = list(tids)
+        tidList.sort()
+        tidList.append(self._dbSize)
+        cur = 0
+        per = 0
+        for tid in tidList:
+            per = max(per, tid - cur)
+            if per > self._maxPer:  # early stopping
+                break
+            cur = tid
+        return per
 
+    def _convert(self, value) -> float:
+        """
+        To convert the given user specified value
 
+        :param value: user specified value
+        :return: converted value
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbSize * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbSize * value)
+            else:
+                value = int(value)
+        return value
 
-    def _creatingItemSets(self):
+    def _creatingOneItemSets(self) -> list:
         """
         Storing the complete transactions of the database/input file in a database variable
+        :return: list
         """
-        self._Database = []
+        plist = []
+        Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
-
-            for k in temp:
-                self._Database.append(set(k))
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                Database.append(tr)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-
-    def _convert(self, value):
-        """
-
-        To convert the user specified minSup value
-
-        :param value: user specified minSup value
-
-        :type value: int or float or str
-
-        :return: converted type
-
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
-    
-    def arraysAndItems(self):
-        ArraysAndItems = {}
-
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                j = tuple([j])
-                if j not in ArraysAndItems:
-                    ArraysAndItems[j] = [i]
+        tid = 0
+        itemsets = {}  # {key: item, value: list of tids}
+        periodicHelper = {}  # {key: item, value: [period, last_tid]}
+        for line in Database:
+            tid = int(line[0])
+            self._tidSet.add(tid)
+            for item in line[1:]:
+                if item in itemsets:
+                    itemsets[item].add(tid)
+                    periodicHelper[item][0] = max(periodicHelper[item][0],
+                                                  abs(tid - periodicHelper[item][1]))  # update current max period
+                    periodicHelper[item][1] = tid  # update the last tid
                 else:
-                    ArraysAndItems[j].append(i)
+                    itemsets[item] = {tid}
+                    periodicHelper[item] = [abs(0 - tid), tid]  # initialize helper
 
-        newArraysAndItems = {}
-
-        for k,v in ArraysAndItems.items():
-            ArraysAndItems[k] = _ab._np.array(v, dtype=_ab._np.uint32)
-            if len(v) >= self._minSup:
-                self._finalPatterns[k] = len(v)
-                newArraysAndItems[k] = ArraysAndItems[k]
-
-        return newArraysAndItems
+        # finish all items' period
+        self._dbSize = len(Database)
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        del Database
+        for item, _ in periodicHelper.items():
+            periodicHelper[item][0] = max(periodicHelper[item][0],
+                                          abs(self._dbSize - periodicHelper[item][1]))  # tid of the last transaction
+        candidates = []
+        for item, tids in itemsets.items():
+            per = periodicHelper[item][0]
+            sup = len(tids)
+            if sup >= self._minSup and per <= self._maxPer:
+                candidates.append(item)
+                self._finalPatterns[item] = [sup, per, tids]
+        return candidates
     
-    def createBitRepresentation(self, ArraysAndItems):
-        bitRep = {}
-        arraySize = len(self._Database) // 32 + 1 if len(self._Database) % 32 != 0 else len(self._Database) // 32
+    def _generateEclat(self, candidates: list) -> None:
 
+        newCandidates = []
+        for i in range(0, len(candidates)):
+            prefixItem = candidates[i]
+            prefixItemSet = prefixItem.split()
+            for j in range(i + 1, len(candidates)):
+                item = candidates[j]
+                itemSet = item.split()
+                if prefixItemSet[:-1] == itemSet[:-1] and prefixItemSet[-1] != itemSet[-1]:
+                    _value = self._finalPatterns[item][2].intersection(self._finalPatterns[prefixItem][2])
+                    sup = len(_value)
+                    per = self._getPeriodic(_value)
+                    if sup >= self._minSup and per <= self._maxPer:
+                        newItem = prefixItem + "\t" + itemSet[-1]
+                        self._finalPatterns[newItem] = [sup, per, _value]
+                        newCandidates.append(newItem)
 
-        for k, v in ArraysAndItems.items():
-            bitRep[k] = _ab._np.zeros(arraySize, dtype=_ab._np.uint32)
-            for i in v:
-                bitRep[k][i // 32] |= 1 << 31 - (i % 32)
+        if len(newCandidates) > 0:
+            self._generateEclat(newCandidates)
 
-        for k, v in bitRep.items():
-            bitRep[k] = _ab._cp.array(v)
-
-        return bitRep
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    def startMine(self) -> None:
         """
-        Frequent pattern mining process will start from here
+        Mining process will start from this function
+        :return: None
         """
-        self._Database = []
         self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        itemsList = sorted(list(set.union(*self._Database)))  # because Database is list
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self.arraysAndItems()
-
-        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                iList = list(keys[i])
-                # print(i, "/", len(ArraysAndItems), end="\r")
-                for j in range(i+1, len(ArraysAndItems)):  
-                    jList = list(keys[j])
-                    union = []
-                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
-                        union = iList + [jList[-1]]
-                        union = tuple(union)
-                        unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
-                        sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
-                        self._sumKernel((len(unionData) // 32 + 1,), (32,), (unionData, sum, _ab._cp.uint32(len(unionData))))
-                        sum = sum[0]
-                        if sum >= self._minSup and union not in self._finalPatterns:
-                            newArraysAndItems[union] = unionData
-                            string = "\t".join(union)
-                            self._finalPatterns[string] = sum
-            ArraysAndItems = newArraysAndItems
-            # print()
-
+        self._finalPatterns = {}
+        frequentSets = self._creatingOneItemSets()
+        self._generateEclat(frequentSets)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
         self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuEclatBit algorithm ")
+        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
 
-    def mine(self):
+    def Mine(self) -> None:
         """
-        Frequent pattern mining process will start from here
+        Mining process will start from this function
+        :return: None
         """
-        self._Database = []
         self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        itemsList = sorted(list(set.union(*self._Database)))  # because Database is list
-        self._minSup = self._convert(self._minSup)
-
-        ArraysAndItems = self.arraysAndItems()
-
-        ArraysAndItems = self.createBitRepresentation(ArraysAndItems)
-
-        while len(ArraysAndItems) > 0:
-            # print("Total number of ArraysAndItems:", len(ArraysAndItems))
-            newArraysAndItems = {}
-            keys = list(ArraysAndItems.keys())
-            for i in range(len(ArraysAndItems)):
-                iList = list(keys[i])
-                # print(i, "/", len(ArraysAndItems), end="\r")
-                for j in range(i+1, len(ArraysAndItems)):
-                    jList = list(keys[j])
-                    union = []
-                    if iList[:-1] == jList[:-1] and iList[-1] != jList[-1]:
-                        union = iList + [jList[-1]]
-                        union = tuple(union)
-                        unionData = _ab._cp.bitwise_and(ArraysAndItems[keys[i]], ArraysAndItems[keys[j]])
-                        sum = _ab._cp.zeros(1, dtype=_ab._np.uint32)
-                        self._sumKernel((len(unionData) // 32 + 1,), (32,), (unionData, sum, _ab._cp.uint32(len(unionData))))
-                        sum = sum[0]
-                        if sum >= self._minSup and union not in self._finalPatterns:
-                            newArraysAndItems[union] = unionData
-                            string = "\t".join(union)
-                            self._finalPatterns[string] = sum
-            ArraysAndItems = newArraysAndItems
-            # print()
-
+        self._finalPatterns = {}
+        frequentSets = self._creatingOneItemSets()
+        self._generateEclat(frequentSets)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
         self._memoryRSS = float()
+        self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using cuEclatBit algorithm ")
-            
+        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+
+    def getMemoryUSS(self) -> float:
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """Calculating the total amount of runtime taken by the mining process
+
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
-        Storing final frequent patterns in a dataframe
-        :return: returning frequent patterns in a dataframe
+        Storing final periodic-frequent patterns in a dataframe
+
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
-        return dataFrame
+            data.append([a, b[0], b[1]])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+        return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of periodic-frequent patterns will be loaded in to a output file
+
         :param outFile: name of the output file
-        :type outFile: csvfile
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            #s1 = x.replace(' ', '\t') + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> dict:
         """
-        Function to send the set of frequent patterns after completion of the mining process
-        :return: returning frequent patterns
+        Function to send the set of periodic-frequent patterns after completion of the mining process
+
+        :return: returning periodic-frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
-        This function is used to print the result
+        This function is used to print the results
+        :return: None
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+                    
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
-            _ap = cuEclatBit(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = cuEclatBit(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = PFECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
-
-"""_ap = cuEclat("/home/tarun/PAMI/PAMI/frequentPattern/cuda/test.txt", 2, " ")
-    _ap = cuEclat("/home/tarun/Transactional_T10I4D100K.csv", 450, "\t")
-    _ap.startMine()
-    print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-    print("Total Memory in USS:", _ap.getMemoryUSS())
-    print("Total Memory in RSS", _ap.getMemoryRSS())
-    print("Total ExecutionTime in s:", _ap.getRuntime())"""
-
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+# cudaAprioriTID is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
 #
 # **Importing this algorithm into a python program**
 # ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
+#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 #
-#             obj = alg.cuAprioriGCT(iFile, minSup)
+#             obj = alg.cuAprioriBit(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,14 +30,15 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
+
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -47,227 +48,278 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
+
 from deprecated import deprecated
-from PAMI.frequentPattern.basic import abstract as _ab
-# import abstract as _ab
+import abstract as _ab
 
 import os
+import csv
 import time
 import numpy as np
-import pycuda.gpuarray as gpuarray
+import pycuda.gpuarray as _gpuarray
+import pycuda.autoinit
 import psutil
+import pycuda.driver as cuda
+from pycuda.compiler import SourceModule
+import pycuda
+
+deviceIntersection = SourceModule("""
+    __global__ void intersection(int *compareThis, int *compareThat, int *resultStart,
+                                 int *values, int *result, int resultX, int resultY){
+        const int tidX = blockIdx.x * blockDim.x + threadIdx.x;
+        const int tidY = blockIdx.y * blockDim.y + threadIdx.y;
+        int resultIndex = resultStart[tidX] + tidY;
+
+        // ignore if tidX or tidY is out of bounds or if the value comparing with is 0
+        if (tidX > resultX-1 || tidY > resultY-1 || values[compareThis[tidX] + tidY] == 0) return;
+
+        for (int i = 0; i < resultY; i++){
+            if ( values[compareThat[tidX] + i] == 0) return;
+            if ( values[compareThis[tidX] + tidY] == values[compareThat[tidX] + i]){
+                result[resultIndex] = values[compareThis[tidX] + tidY];
+                return;
+            }
+        }
+
+        //result[resultIndex] = values[compareThis[tidX] + tidY];
+
+    }
+
+"""
+                                  )
 
 
-class cudaAprioriGCT(_ab._frequentPatterns):
+class cudaAprioriTID:
     """
-    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
     :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-                In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
+            In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         startTime : float
-              To record the start time of the mining process
+          To record the start time of the mining process
 
         endTime : float
-              To record the completion time of the mining process
+          To record the completion time of the mining process
 
         finalPatterns : dict
-              Storing the complete set of patterns in a dictionary variable
+          Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-              To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-              To store the total amount of RSS memory consumed by the program
+          To store the total amount of RSS memory consumed by the program
 
         Database : list
-              To store the transactions of a database in list
+          To store the transactions of a database in list
 
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
+      (.venv) $ python3 cudaAprioriTID.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
+      (.venv) $ python3 cudaAprioriTID.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Importing this algorithm into a python program**
     ----------------------------------------------------
 
     .. code-block:: python
 
-            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
+             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
 
-            obj = alg.cuAprioriGCT(iFile, minSup)
+             obj = alg.cuAprioriBit(iFile, minSup)
 
-            obj.mine()
+             obj.mine()
 
-            frequentPatterns = obj.getPatterns()
+             frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+             Df = obj.getPatternInDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+             memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+             print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+             memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+             print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+             run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
     -------------
 
-                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
     __time = 0
     __memRSS = 0
     __memUSS = 0
     __GPU_MEM = 0
+    filePath = ""
+    _iFile = " "
+    _sep = ""
     _minSup = 0
-    _finalPatterns = {}
+    Patterns = {}
 
-    def __init__(self, filePath, minSup, sep):
-        self._iFile = filePath
-        self._sep = sep
-        self._minSup = minSup
+    def __init__(self, filePath, sep, minSup):
+        self.filePath = filePath
+        self.sep = sep
+        self.minSup = minSup
         self.__time = 0
         self.__memRSS = 0
         self.__memUSS = 0
 
-    def __creatingItemSets(self):
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self.__Database = []
+        self._Database = {}
+        lineNumber = 1
         if isinstance(self._iFile, _ab._pd.DataFrame):
+            temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
+                temp = self._iFile['Transactions'].tolist()
 
-            # print(self.Database)
+            for k in temp:
+                self._Database.append(set(k))
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
+                    for i in range(len(line)):
+                        if line[i] in self._Database:
+                            self._Database[i].append(lineNumber)
+                        else:
+                            self._Database[i] = [lineNumber]
+                    lineNumber += 1
+
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    self._Database.append(set(temp))
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.strip()
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self.__Database.append(temp)
+                            self._Database.append(set(temp))
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def __convert(self, value):
+    def _convert(self, value):
         """
 
         To convert the type of user specified minSup value
 
         :param value: user specified minSup value
 
         :type value: int or float or str
 
         :return: converted type
 
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self.__Database) * value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self.__Database) * value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def compute_vertical_bitvector_data(self):
-        """
-        Converting database into bit vector
+    """def _readFile(self, fileName, separator):
+        
+        Reads a file and stores the data in a dictionary
+
+        Args:
+            fileName: string
+            separator: string
+
+        Returns:
+            dictionary: dictionary
+        
+        file = open(fileName, 'r')
+        dictionary = {}
+        lineNumber = 1
+        for line in file:
+            line = line.strip()
+            line = line.split(separator)
+            for i in range(len(line)):
+                if line[i] in dictionary:
+                    dictionary[line[i]].append(lineNumber)
+                else:
+                    dictionary[line[i]] = [lineNumber]
+            lineNumber += 1
+
+        # sort dictionary by size of values
+        dictionary = dict(
+            sorted(dictionary.items(), key=lambda x: len(x[1]), reverse=True))
+        return dictionary, lineNumber
         """
-        # ---build item to idx mapping---#
-        idx = 0
-        item2idx = {}
-        for transaction in self.__Database:
-            for item in transaction:
-                if not item in item2idx:
-                    item2idx[item] = idx
-                    idx += 1
-        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
-        # ---build vertical data---#
-        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
-        for trans_id, transaction in enumerate(self.__Database):
-            for item in transaction:
-                vb_data[item2idx[item], trans_id] = 1
-        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
-        return vb_data, idx2item
-
     def getRuntime(self):
         """
         Calculating the total amount of time taken by the mining process
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+
         return self.__time
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-
         return self.__memRSS
 
     def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -285,174 +337,155 @@
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self.Patterns
 
     def get_numberOfPatterns(self):
-        return len(self._finalPatterns)
-
-    def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
-        :return: returning frequent patterns in a dataframe
-        :rtype: pd.DataFrame
-        """
-
-        dataFrame = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
-
-    def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: csvfile
-        """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            if type(x) == tuple:
-                pattern = ""
-                for item in x:
-                    pattern = pattern + str(item) + " "
-                s1 = pattern + ":" + str(y)
-            else:
-                s1 = str(x) + ":" + str(y)
-            writer.write("%s \n" % s1)
+        return len(self.Patterns)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        startTime = time.time()
-        basePattern = {}
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern[idx2item[i]] = [i]
-                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
-
-        while len(basePattern) > 0:
-            temp = {}
-            keysList = list(basePattern.keys())
-            valuesList = list(basePattern.values())
-            for i in range(len(basePattern) - 1):
-                keyI = keysList[i].split(" ")
-                keyI = [int(x) for x in keyI]
-
-                for j in range(i + 1, len(basePattern)):
-                    keyJ = keysList[j].split(" ")
-                    keyJ = [int(x) for x in keyJ]
-                    values = set(valuesList[i])
-                    for val in valuesList[j]:
-                        values.add(val)
-                    values = list(sorted(values))
-                    totalArray = vb_data[values[0]]
-                    for k in range(1, len(values)):
-                        totalArray = totalArray.__mul__(vb_data[values[k]])
-                    support = gpuarray.sum(totalArray).get()
-                    if support >= self._minSup:
-                        combinedKey = " ".join(
-                            str(x) for x in sorted(set(keyI) | set(keyJ)))
-                        temp[combinedKey] = values
-                        final[str(combinedKey)] = support
-            basePattern = temp
-
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
+        dev_Intersection = deviceIntersection.get_function("intersection")
         startTime = time.time()
-        basePattern = {}
         final = {}
 
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
         minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
 
-        for i in range(len(vb_data)):
-            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern[idx2item[i]] = [i]
-                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
-
-        while len(basePattern) > 0:
-            temp = {}
-            keysList = list(basePattern.keys())
-            valuesList = list(basePattern.values())
-            for i in range(len(basePattern) - 1):
-                keyI = keysList[i].split(" ")
-                keyI = [int(x) for x in keyI]
-
-                for j in range(i + 1, len(basePattern)):
-                    keyJ = keysList[j].split(" ")
-                    keyJ = [int(x) for x in keyJ]
-                    values = set(valuesList[i])
-                    for val in valuesList[j]:
-                        values.add(val)
-                    values = list(sorted(values))
-                    totalArray = vb_data[values[0]]
-                    for k in range(1, len(values)):
-                        totalArray = totalArray.__mul__(vb_data[values[k]])
-                    support = gpuarray.sum(totalArray).get()
-                    if support >= self._minSup:
-                        combinedKey = " ".join(
-                            str(x) for x in sorted(set(keyI) | set(keyJ)))
-                        temp[combinedKey] = values
-                        final[str(combinedKey)] = support
-            basePattern = temp
+
+        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
+        for key, value in data.items():
+            final[key] = len(value)
+
+        while len(data) > 1:
+            # sort data by size of values
+            data = dict(
+                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
+
+            values = list(data.values())
+            maxLength = values[0]
+            for i in range(1, len(values)):
+                while len(values[i]) != len(maxLength):
+                    values[i].append(0)
+
+            values = np.array(values)
+            resultSize = 0
+
+            compareThis = []
+            compareThat = []
+            resultStart = []
+            counter = 0
+
+            for i in range(len(values)):
+                for j in range(i+1, len(values)):
+                    resultSize += 1
+                    compareThis.append(i*len(maxLength))
+                    compareThat.append(j*len(maxLength))
+                    resultStart.append(counter)
+                    counter += len(maxLength)
+            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
+
+            # convert all to uint32
+            compareThis = np.array(compareThis, dtype=np.uint32)
+            compareThat = np.array(compareThat, dtype=np.uint32)
+            resultStart = np.array(resultStart, dtype=np.uint32)
+            values = np.array(values, dtype=np.uint32)
+            result = np.array(result, dtype=np.uint32)
+
+            # allocate memory on GPU
+            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
+            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
+            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
+            values_gpu = cuda.mem_alloc(values.nbytes)
+            result_gpu = cuda.mem_alloc(result.nbytes)
+
+            # add all nbytes to GPU_MEM
+            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
+            if sumBytes > self.__GPU_MEM:
+                self.__GPU_MEM = sumBytes
+
+            # copy data to GPU
+            cuda.memcpy_htod(compareThis_gpu, compareThis)
+            cuda.memcpy_htod(compareThat_gpu, compareThat)
+            cuda.memcpy_htod(resultStart_gpu, resultStart)
+            cuda.memcpy_htod(values_gpu, values)
+            cuda.memcpy_htod(result_gpu, result)
+
+            blockDim = (32, 32, 1)
+            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
+
+            dev_Intersection(compareThis_gpu, compareThat_gpu,
+                             resultStart_gpu, values_gpu, result_gpu,
+                             np.uint32(resultSize), np.uint32(len(maxLength)),
+                             block=blockDim, grid=gridDim)
+
+            # copy data back to CPU
+            cuda.Context.synchronize()
+            cuda.memcpy_dtoh(result, result_gpu)
+
+            # free GPU memory
+            cuda.DeviceAllocation.free(compareThis_gpu)
+            cuda.DeviceAllocation.free(compareThat_gpu)
+            cuda.DeviceAllocation.free(resultStart_gpu)
+            cuda.DeviceAllocation.free(values_gpu)
+            cuda.DeviceAllocation.free(result_gpu)
+
+            keys = list(data.keys())
+            # convert all to string and add " "
+            for i in range(len(keys)):
+                keys[i] = str(keys[i]) + " "
+            data = {}
+            index = 0
+            for i in range(len(keys)):
+                for j in range(i+1, len(keys)):
+                    newResult = list(sorted(set(result[index])))
+                    newResult = list(filter(lambda x: x > 0, newResult))
+                    if len(newResult) >= self.minSup:
+                        keyI = keys[i].split()
+                        keyJ = keys[j].split()
+                        combinedKey = " ".join(list(str(x) for x in (
+                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
+                        if combinedKey not in final:
+                            data[combinedKey] = newResult
+                            final[combinedKey] = len(newResult)
+                    index += 1
+
 
         self.__time = time.time() - startTime
         self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
         self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
-
-    def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of Coverage Patterns:", len(self.getPatterns()))
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        self.Patterns = final
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("GPU MEM: ", _ap.getGPUMemory())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaAprioriTID.py` & `pami-2024.5.1/PAMI/sequentialPatternMining/basic/SPAM.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,20 +1,21 @@
-# cudaAprioriTID is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
-#
-#
+# SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
+# This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
+#  This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------
+# --------------------------------------------------------
+#
 #
-#             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+#             import PAMI.sequentialPatternMining.basic.SPAM as alg
 #
-#             obj = alg.cuAprioriBit(iFile, minSup)
+#             obj = alg.SPAM(iFile, minSup)
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             frequentPatterns = obj.getPatterns()
+#             sequentialPatternMining = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
@@ -29,573 +30,494 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
 
+import pandas as pd
 from deprecated import deprecated
-import abstract as _ab
-
-import os
-import csv
-import time
-import numpy as np
-import pycuda.gpuarray as _gpuarray
-import pycuda.autoinit
-import psutil
-import pycuda.driver as cuda
-from pycuda.compiler import SourceModule
-import pycuda
-
-deviceIntersection = SourceModule("""
-    __global__ void intersection(int *compareThis, int *compareThat, int *resultStart,
-                                 int *values, int *result, int resultX, int resultY){
-        const int tidX = blockIdx.x * blockDim.x + threadIdx.x;
-        const int tidY = blockIdx.y * blockDim.y + threadIdx.y;
-        int resultIndex = resultStart[tidX] + tidY;
-
-        // ignore if tidX or tidY is out of bounds or if the value comparing with is 0
-        if (tidX > resultX-1 || tidY > resultY-1 || values[compareThis[tidX] + tidY] == 0) return;
-
-        for (int i = 0; i < resultY; i++){
-            if ( values[compareThat[tidX] + i] == 0) return;
-            if ( values[compareThis[tidX] + tidY] == values[compareThat[tidX] + i]){
-                result[resultIndex] = values[compareThis[tidX] + tidY];
-                return;
-            }
-        }
-
-        //result[resultIndex] = values[compareThis[tidX] + tidY];
-
-    }
-
-"""
-                                  )
 
+from PAMI.sequentialPatternMining.basic import abstract as _ab
+_ab._sys.setrecursionlimit(10000)
 
-class cudaAprioriTID:
+class SPAM(_ab._sequentialPatterns):
     """
-    :Description: Apriori is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
+    :Description:    SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
+                     This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
+                     This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
 
-    :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
-            In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
+    :Reference:   J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. Sequential Pattern Mining Using Bitmaps. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Edmonton, Alberta, Canada, July 2002.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of  Sequential frequent patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                   Name of the output file to store complete set of  Sequential frequent patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
-
-        endTime : float
-          To record the completion time of the mining process
-
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
-        memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
-        memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
+            iFile : str
+                Input file name or path of the input file
+            oFile : str
+                Name of the output file or the path of output file
+            minSup : float or int or str
+                The user can specify minSup either in count or proportion of database size.
+                If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                Otherwise, it will be treated as float.
+                Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+            sep : str
+                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+                However, the users can override their default separator.
+            startTime : float
+                To record the start time of the mining process
+            endTime : float
+                To record the completion time of the mining process
+            finalPatterns : dict
+                Storing the complete set of patterns in a dictionary variable
+            memoryUSS : float
+                To store the total amount of USS memory consumed by the program
+            memoryRSS : float
+                To store the total amount of RSS memory consumed by the program
+            Database : list
+                To store the sequences of a database in list
+            _idDatabase : dict
+                To store the sequences of a database by bit map
+            _maxSeqLen:
+                the maximum length of subsequence in sequence.
+
+    :Methods:
+
+            _creatingItemSets():
+                Storing the complete sequences of the database/input file in a database variable
+            _convert(value):
+                To convert the user specified minSup value
+            make2BitDatabase():
+                To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
+            DfsPruning(items,sStep,iStep):
+                the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
+            Sstep(s):
+                To convert bit to ssteo bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
+            startMine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            savePatterns(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrame()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            candidateToFrequent(candidateList)
+                Generates frequent patterns from the candidate patterns
+            frequentToCandidate(frequentList, length)
+                Generates candidate patterns from the frequent patterns
 
-        Database : list
-          To store the transactions of a database in list
-
-
-
-    **Methods to execute code on terminal**
-    ----------------------------------------------------
 
+    **Executing the code on terminal**:
+    ----------------------------------------
     .. code-block:: console
 
-      Format:
-
-      (.venv) $ python3 cudaAprioriTID.py <inputFile> <outputFile> <minSup>
-
-      Example Usage:
 
-      (.venv) $ python3 cudaAprioriTID.py sampleDB.txt patterns.txt 10.0
+       Format:
 
-    .. note:: minSup will be considered in percentage of database transactions
+       (.venv) $ python3 SPAM.py <inputFile> <outputFile> <minSup> (<separator>)
 
+       Examples usage:
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
+       (.venv) $ python3 SPAM.py sampleDB.txt patterns.txt 10.0
 
-    .. code-block:: python
 
-             import PAMI.frequentPattern.cuda.cuAprioriBit as alg
+               .. note:: minSup will be considered in times of minSup and count of database transactions
 
-             obj = alg.cuAprioriBit(iFile, minSup)
+    **Sample run of the importing code**:
+    -------------------------------------
+            import PAMI.sequentialPatternMining.basic.SPAM as alg
 
-             obj.mine()
+            obj = alg.SPAM(iFile, minSup)
 
-             frequentPatterns = obj.getPatterns()
+            obj.startMine()
 
-             print("Total number of Frequent Patterns:", len(frequentPatterns))
+            sequentialPatternMining = obj.getPatterns()
 
-             obj.save(oFile)
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-             Df = obj.getPatternInDataFrame()
+            obj.savePatterns(oFile)
 
-             memUSS = obj.getMemoryUSS()
+            Df = obj.getPatternInDataFrame()
 
-             print("Total Memory in USS:", memUSS)
+            memUSS = obj.getMemoryUSS()
 
-             memRSS = obj.getMemoryRSS()
+            print("Total Memory in USS:", memUSS)
 
-             print("Total Memory in RSS", memRSS)
+            memRSS = obj.getMemoryRSS()
 
-             run = obj.getRuntime()
+            print("Total Memory in RSS", memRSS)
 
-             print("Total ExecutionTime in seconds:", run)
+            run = obj.getRuntime()
 
+            print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    -------------
-
-             The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
-
+    **Credits**:
+    ------------
+            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
     """
 
-    __time = 0
-    __memRSS = 0
-    __memUSS = 0
-    __GPU_MEM = 0
-    filePath = ""
+    _minSup = float()
+    _startTime = float()
+    _endTime = float()
+    _finalPatterns = {}
     _iFile = " "
-    _sep = ""
-    _minSup = 0
-    Patterns = {}
-
-    def __init__(self, filePath, sep, minSup):
-        self.filePath = filePath
-        self.sep = sep
-        self.minSup = minSup
-        self.__time = 0
-        self.__memRSS = 0
-        self.__memUSS = 0
-
+    _oFile = " "
+    _sep = " "
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _Database = []
+    _idDatabase={}
+    _maxSeqLen=0
     def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        Storing the complete sequences of the database/input file in a database variable
         """
-        self._Database = {}
-        lineNumber = 1
+        self._Database = []
+
         if isinstance(self._iFile, _ab._pd.DataFrame):
             temp = []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 temp = self._iFile['Transactions'].tolist()
-
-            for k in temp:
-                self._Database.append(set(k))
+            if "tid" in i:
+                temp2=self._iFile[''].tolist()
+            addList=[]
+            addList.append(temp[0])
+            for k in range(len(temp)-1):
+                if temp2[k]==temp[k+1]:
+                    addList.append(temp[k+1])
+                else:
+                    self._Database.append(addList)
+                    addList=[]
+                    addList.append(temp[k+1])
+            self._Database.append(addList)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
-                    for i in range(len(line)):
-                        if line[i] in self._Database:
-                            self._Database[i].append(lineNumber)
-                        else:
-                            self._Database[i] = [lineNumber]
-                    lineNumber += 1
-
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(set(temp))
+                    temp.pop()
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._Database.append(set(temp))
+                            temp = [i.rstrip() for i in line.split('-1')]
+                            temp = [x for x in temp if x ]
+                            temp.pop()
+
+                            seq = []
+                            for i in temp:
+                                k = -2
+                                if len(i)>1:
+                                    seq.append(list(sorted(set(i.split()))))
+
+                                else:
+                                    seq.append(i)
+
+                            self._Database.append(seq)
+
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _convert(self, value):
         """
-
-        To convert the type of user specified minSup value
+        To convert the user specified minSup value
 
         :param value: user specified minSup value
-
-        :type value: int or float or str
-
         :return: converted type
-
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    """def _readFile(self, fileName, separator):
-        
-        Reads a file and stores the data in a dictionary
-
-        Args:
-            fileName: string
-            separator: string
-
-        Returns:
-            dictionary: dictionary
-        
-        file = open(fileName, 'r')
-        dictionary = {}
-        lineNumber = 1
-        for line in file:
-            line = line.strip()
-            line = line.split(separator)
-            for i in range(len(line)):
-                if line[i] in dictionary:
-                    dictionary[line[i]].append(lineNumber)
-                else:
-                    dictionary[line[i]] = [lineNumber]
-            lineNumber += 1
 
-        # sort dictionary by size of values
-        dictionary = dict(
-            sorted(dictionary.items(), key=lambda x: len(x[1]), reverse=True))
-        return dictionary, lineNumber
+    def make2BitDatabase(self):
         """
-    def getRuntime(self):
+        To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
         """
-        Calculating the total amount of time taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        self._maxSeqLen=max([len(i) for i in self._Database])
+        lineNumber=0
+        idDatabase={}
+        for line in self._Database:
+            seqNumber=1
+            for seq in line:
+
+                for data in seq:
+                    if data in idDatabase:
+                        while lineNumber+1!=len(idDatabase[data]):
+                            idDatabase[data].append(0)
+                        idDatabase[data][lineNumber]+=int(2**(self._maxSeqLen-seqNumber))
+
+                    else:
+                        idDatabase[data]=[]
+                        while lineNumber+1!=len(idDatabase[data]):
+                            idDatabase[data].append(0)
+                        idDatabase[data][lineNumber]+=(int(2 ** (self._maxSeqLen-seqNumber)))
+
+                seqNumber+=1
+            lineNumber+=1
+        for key,val in idDatabase.items():
+
+            sup=self.countSup(val)
+            while lineNumber+1!=len(idDatabase[key]):
+                            idDatabase[key].append(0)
+            if sup>=self._minSup:
+                self._finalPatterns[str(key)+self._sep+"-2"]=sup
+                self._idDatabase[str(key)]=val
+
+    def DfsPruning(self,items,sStep,iStep):
+        """
+        the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
+
+        :Attributes:
+
+        items : str
+            The pattrens I got before
+        sStep : list
+            Items presumed to have "sstep" relationship with "items".(sstep is What appears later like a-b and a-c)
+        iStep : list
+            Items presumed to have "istep" relationship with "items"(istep is What appears in same time like ab and ac)
+
+        """
+        Snext=[]
+        Inext=[]
+        ns = self.Sstep(self._idDatabase[items])
+        for i in sStep:
+            nnext=[]
+            for k in  range(len(self._idDatabase[items])):
+                nandi=ns[k] & self._idDatabase[i][k]
+                nnext.append(nandi)
+
+
+            sup=self.countSup(nnext)
+            if sup>=self._minSup:
+                key=items+self._sep+"-1"+self._sep+i
+                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
+                self._idDatabase[key]=nnext
+                Snext.append(i)
+
+        for i in Snext:
+            key = items+self._sep+"-1"+self._sep+i
+            self.DfsPruning(key,Snext,[k for k in Snext if self._Database.index(i)<self._Database.index(k)])
+        for i in iStep:
+            nnext = []
+
+            for k in range(len(self._idDatabase[items])):
+                nandi = self._idDatabase[items][k] & self._idDatabase[i][k]
+                nnext.append(nandi)
+            sup=self.countSup(nnext)
+            if sup>=self._minSup:
+                key=items+self._sep+str(i)
+                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
+                self._idDatabase[key]=nnext
+                Inext.append(i)
+        for i in Inext:
+            key = items +self._sep +str(i)
+            self.DfsPruning(key,Snext,[k for k in Inext if self._Database.index(i)<self._Database.index(k)])
+
+    def Sstep(self,s):
+        """
+        To convert bit to Sstep bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
+
+
+        :param s:list
+            to store each bit sequence
+        :return:
+            nextS:list to store the bit sequence converted by sstep
+
+        """
+        nextS=[]
+        for bins in s:
+            binS=str(bin(bins))
+
+
+            LenNum=2
+            for i in range(len(binS)-2):
+                if binS[LenNum] == "1":
+
+                    binS = binS[:LenNum] + "0" + binS[LenNum + 1:]
+                    while len(binS)-1!=LenNum:
+                        LenNum += 1
+                        binS = binS[:LenNum] + "1" + binS[LenNum + 1:]
+                    break
+                LenNum+=1
+            nextS.append(int(binS, 0))
+
+
+        return nextS
+
+    def countSup(self,n):
+        """
+        count support
+
+        :param n:list
+                to store each bit sequence
+        :return:
+            count: int support of this list
+        """
+        count=0
+        for i in n:
+            if "1" in str(bin(i)):
+                count+=1
+        return count
+
+    def startMine(self):
+        """
+        Frequent pattern mining process will start from here
+        """
+        self._Database = []
+        self._startTime = _ab._time.time()
+        self._creatingItemSets()
+        self._minSup = self._convert(self._minSup)
+        self.make2BitDatabase()
+        self._Database = [i for i in self._idDatabase.keys()]
+        for i in self._Database:
+            x=[]
+            for j in self._Database:
+                if self._Database.index(i)<self._Database.index(j):
+                    x.append(j)
+
+            self.DfsPruning(i,self._Database,x)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Frequent patterns were generated successfully using Apriori algorithm ")
+
+    def getMemoryUSS(self):
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__time
+        return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-        return self.__memRSS
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        return self._memoryRSS
+
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self.__memUSS
 
-    def getGPUMemory(self):
-        """
-        To calculate the total memory consumed by GPU
-        :return: return GPU memory
-        :rtype: int
-        """
+        return self._endTime - self._startTime
 
-        return self.__GPU_MEM
+    def getPatternsAsDataFrame(self):
+        """Storing final frequent patterns in a dataframe
+        :return: returning frequent patterns in a dataframe
+        :rtype: pd.DataFrame
+        """
+
+        dataFrame = {}
+        data = []
+        for a, b in self._finalPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def save(self, outFile):
+        """Complete set of frequent patterns will be loaded in to an output file
+        :param outFile: name of the output file
+        :type outFile: file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self.Patterns
-
-    def get_numberOfPatterns(self):
-        return len(self.Patterns)
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-        dev_Intersection = deviceIntersection.get_function("intersection")
-        startTime = time.time()
-        final = {}
-
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-
-
-        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
-        for key, value in data.items():
-            final[key] = len(value)
-
-        while len(data) > 1:
-            # sort data by size of values
-            data = dict(
-                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
-
-            values = list(data.values())
-            maxLength = values[0]
-            for i in range(1, len(values)):
-                while len(values[i]) != len(maxLength):
-                    values[i].append(0)
-
-            values = np.array(values)
-            resultSize = 0
-
-            compareThis = []
-            compareThat = []
-            resultStart = []
-            counter = 0
-
-            for i in range(len(values)):
-                for j in range(i+1, len(values)):
-                    resultSize += 1
-                    compareThis.append(i*len(maxLength))
-                    compareThat.append(j*len(maxLength))
-                    resultStart.append(counter)
-                    counter += len(maxLength)
-            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
-
-            # convert all to uint32
-            compareThis = np.array(compareThis, dtype=np.uint32)
-            compareThat = np.array(compareThat, dtype=np.uint32)
-            resultStart = np.array(resultStart, dtype=np.uint32)
-            values = np.array(values, dtype=np.uint32)
-            result = np.array(result, dtype=np.uint32)
-
-            # allocate memory on GPU
-            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
-            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
-            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
-            values_gpu = cuda.mem_alloc(values.nbytes)
-            result_gpu = cuda.mem_alloc(result.nbytes)
-
-            # add all nbytes to GPU_MEM
-            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
-            if sumBytes > self.__GPU_MEM:
-                self.__GPU_MEM = sumBytes
-
-            # copy data to GPU
-            cuda.memcpy_htod(compareThis_gpu, compareThis)
-            cuda.memcpy_htod(compareThat_gpu, compareThat)
-            cuda.memcpy_htod(resultStart_gpu, resultStart)
-            cuda.memcpy_htod(values_gpu, values)
-            cuda.memcpy_htod(result_gpu, result)
-
-            blockDim = (32, 32, 1)
-            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
-
-            dev_Intersection(compareThis_gpu, compareThat_gpu,
-                             resultStart_gpu, values_gpu, result_gpu,
-                             np.uint32(resultSize), np.uint32(len(maxLength)),
-                             block=blockDim, grid=gridDim)
-
-            # copy data back to CPU
-            cuda.Context.synchronize()
-            cuda.memcpy_dtoh(result, result_gpu)
-
-            # free GPU memory
-            cuda.DeviceAllocation.free(compareThis_gpu)
-            cuda.DeviceAllocation.free(compareThat_gpu)
-            cuda.DeviceAllocation.free(resultStart_gpu)
-            cuda.DeviceAllocation.free(values_gpu)
-            cuda.DeviceAllocation.free(result_gpu)
-
-            keys = list(data.keys())
-            # convert all to string and add " "
-            for i in range(len(keys)):
-                keys[i] = str(keys[i]) + " "
-            data = {}
-            index = 0
-            for i in range(len(keys)):
-                for j in range(i+1, len(keys)):
-                    newResult = list(sorted(set(result[index])))
-                    newResult = list(filter(lambda x: x > 0, newResult))
-                    if len(newResult) >= self.minSup:
-                        keyI = keys[i].split()
-                        keyJ = keys[j].split()
-                        combinedKey = " ".join(list(str(x) for x in (
-                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
-                        if combinedKey not in final:
-                            data[combinedKey] = newResult
-                            final[combinedKey] = len(newResult)
-                    index += 1
-
-
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self.Patterns = final
+        return self._finalPatterns
 
-    def mine(self):
+    def printResults(self):
         """
-        Frequent pattern mining process will start from here
+        This function is used to print the results
         """
-        dev_Intersection = deviceIntersection.get_function("intersection")
-        startTime = time.time()
-        final = {}
-
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-
-
-        data = dict(filter(lambda x: len(x[1]) >= self.minSup, self._Database()))
-        for key, value in data.items():
-            final[key] = len(value)
-
-        while len(data) > 1:
-            # sort data by size of values
-            data = dict(
-                sorted(data.items(), key=lambda x: len(x[1]), reverse=True))
-
-            values = list(data.values())
-            maxLength = values[0]
-            for i in range(1, len(values)):
-                while len(values[i]) != len(maxLength):
-                    values[i].append(0)
-
-            values = np.array(values)
-            resultSize = 0
-
-            compareThis = []
-            compareThat = []
-            resultStart = []
-            counter = 0
-
-            for i in range(len(values)):
-                for j in range(i+1, len(values)):
-                    resultSize += 1
-                    compareThis.append(i*len(maxLength))
-                    compareThat.append(j*len(maxLength))
-                    resultStart.append(counter)
-                    counter += len(maxLength)
-            result = np.zeros((resultSize, len(maxLength)), dtype=np.int32)
-
-            # convert all to uint32
-            compareThis = np.array(compareThis, dtype=np.uint32)
-            compareThat = np.array(compareThat, dtype=np.uint32)
-            resultStart = np.array(resultStart, dtype=np.uint32)
-            values = np.array(values, dtype=np.uint32)
-            result = np.array(result, dtype=np.uint32)
-
-            # allocate memory on GPU
-            compareThis_gpu = cuda.mem_alloc(compareThis.nbytes)
-            compareThat_gpu = cuda.mem_alloc(compareThat.nbytes)
-            resultStart_gpu = cuda.mem_alloc(resultStart.nbytes)
-            values_gpu = cuda.mem_alloc(values.nbytes)
-            result_gpu = cuda.mem_alloc(result.nbytes)
-
-            # add all nbytes to GPU_MEM
-            sumBytes = compareThis.nbytes + compareThat.nbytes + resultStart.nbytes + values.nbytes + result.nbytes
-            if sumBytes > self.__GPU_MEM:
-                self.__GPU_MEM = sumBytes
-
-            # copy data to GPU
-            cuda.memcpy_htod(compareThis_gpu, compareThis)
-            cuda.memcpy_htod(compareThat_gpu, compareThat)
-            cuda.memcpy_htod(resultStart_gpu, resultStart)
-            cuda.memcpy_htod(values_gpu, values)
-            cuda.memcpy_htod(result_gpu, result)
-
-            blockDim = (32, 32, 1)
-            gridDim = (resultSize//32 + 1, len(maxLength)//32 + 1, 1)
-
-            dev_Intersection(compareThis_gpu, compareThat_gpu,
-                             resultStart_gpu, values_gpu, result_gpu,
-                             np.uint32(resultSize), np.uint32(len(maxLength)),
-                             block=blockDim, grid=gridDim)
-
-            # copy data back to CPU
-            cuda.Context.synchronize()
-            cuda.memcpy_dtoh(result, result_gpu)
-
-            # free GPU memory
-            cuda.DeviceAllocation.free(compareThis_gpu)
-            cuda.DeviceAllocation.free(compareThat_gpu)
-            cuda.DeviceAllocation.free(resultStart_gpu)
-            cuda.DeviceAllocation.free(values_gpu)
-            cuda.DeviceAllocation.free(result_gpu)
-
-            keys = list(data.keys())
-            # convert all to string and add " "
-            for i in range(len(keys)):
-                keys[i] = str(keys[i]) + " "
-            data = {}
-            index = 0
-            for i in range(len(keys)):
-                for j in range(i+1, len(keys)):
-                    newResult = list(sorted(set(result[index])))
-                    newResult = list(filter(lambda x: x > 0, newResult))
-                    if len(newResult) >= self.minSup:
-                        keyI = keys[i].split()
-                        keyJ = keys[j].split()
-                        combinedKey = " ".join(list(str(x) for x in (
-                            sorted(int(x) for x in (set(keyI) | set(keyJ))))))
-                        if combinedKey not in final:
-                            data[combinedKey] = newResult
-                            final[combinedKey] = len(newResult)
-                    index += 1
-
-
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self.Patterns = final
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = cudaAprioriTID(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("GPU MEM: ", _ap.getGPUMemory())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _Patterns = _ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(_Patterns))
+        _ap.savePatterns(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
-
 
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cudaEclatGCT.py`

 * *Files 3% similar despite different names*

```diff
@@ -347,36 +347,15 @@
             self.eclat(newBasePattern, final, vb_data, idx2item, item2idx)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        startTime = time.time()
-        basePattern = []
-        final = {}
-
-        self.__creatingItemSets()
-        self._minSup = self.__convert(self._minSup)
-        minSup = self._minSup
-        vb_data, idx2item = self.compute_vertical_bitvector_data()
-
-        for i in range(len(vb_data)):
-            if _gpuarray.sum(vb_data[i]).get() >= self._minSup:
-                basePattern.append(idx2item[i])
-                final[idx2item[i]] = _gpuarray.sum(vb_data[i]).get()
-
-        # reverse idx2item
-        item2idx = {idx2item[i]: i for i in idx2item}
-        self.eclat(basePattern, final, vb_data, idx2item, item2idx)
-        self.__time = time.time() - startTime
-        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
-        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
-        self._finalPatterns = final
-        self.__GPU_MEM = vb_data.nbytes
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         startTime = time.time()
         basePattern = []
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py` & `pami-2024.5.1/PAMI/frequentPattern/maximal/MaxFPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -645,46 +645,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
-        global _minSup
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        generatedItems, pfList = self._frequentOneItem()
-        updatedTransactions = self._updateTransactions(generatedItems)
-        for x, y in self._rank.items():
-            self._rankdup[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        patterns = {}
-        self._finalPatterns = {}
-        self._maximalTree = _MPTree()
-        Tree = self._buildTree(updatedTransactions, info)
-        Tree.generatePatterns([], patterns, self._maximalTree)
-        for x, y in patterns.items():
-            pattern = str()
-            x = self._convertItems(x)
-            for i in x:
-                pattern = pattern + i + "\t"
-            self._finalPatterns[pattern] = y
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Maximal Frequent patterns were generated successfully using MaxFp-Growth algorithm ")
+        self.mine()
 
     def mine(self):
         """
         Mining process will start from this function
         """
 
         global _minSup
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/maximal/__init__.py` & `pami-2024.5.1/PAMI/frequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/maximal/abstract.py` & `pami-2024.5.1/PAMI/frequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/abstract.py` & `pami-2024.5.1/PAMI/frequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelApriori.py` & `pami-2024.5.1/PAMI/frequentPattern/pyspark/parallelApriori.py`

 * *Files 3% similar despite different names*

```diff
@@ -361,38 +361,15 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-
-        # setting SparkConf and SparkContext to process in parallel
-        conf = _ab._SparkConf().setAppName("parallelApriori").setMaster("local[*]")
-        sc = _ab._SparkContext(conf=conf)
-        # sc.addFile("file:///home/hadoopuser/Spark_code/abstract.py")
-
-        # read database from iFile
-        database = sc.textFile(self._iFile, self._numPartitions).map(
-            lambda x: {int(y) for y in x.rstrip().split(self._sep)})
-        self._lno = database.count()
-        # Calculating minSup as a percentage
-        self._minSup = self._convert(self._minSup)
-
-        oneFrequentItems = self._genFrequentItems(database)
-        self._finalPatterns = oneFrequentItems
-        self._getAllFrequentPatterns(database, oneFrequentItems)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel Apriori algorithm")
-        sc.stop()
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelECLAT.py` & `pami-2024.5.1/PAMI/frequentPattern/cuda/cudaAprioriGCT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,16 +1,16 @@
-# ParallelEclat is an algorithm to discover frequent patterns in a transactional database. This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+# cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns using CUDA in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 #
-# **Importing this algorithm into a python program**
-#  ----------------------------------------------------
 #
+# **Importing this algorithm into a python program**
+# ----------------------------------------------------
 #
-#             import PAMI.frequentPattern.pyspark.parallelECLAT as alg
+#             import PAMI.frequentPattern.cuda.cudaAprioriGCT as alg
 #
-#             obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+#             obj = alg.cuAprioriGCT(iFile, minSup)
 #
 #             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
@@ -30,15 +30,14 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,85 +47,87 @@
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from pyspark import SparkConf, SparkContext
-# import abstract as _ab
-from PAMI.frequentPattern.pyspark import abstract as _ab
-from abc import ABC as _ABC, abstractmethod as _abstractmethod
 from deprecated import deprecated
+from PAMI.frequentPattern.basic import abstract as _ab
+# import abstract as _ab
+
+import os
+import time
+import numpy as np
+import pycuda.gpuarray as gpuarray
+import psutil
 
 
-class parallelECLAT(_ab._frequentPatterns):
+class cudaAprioriGCT(_ab._frequentPatterns):
     """
-    :Description: ParallelEclat is an algorithm to discover frequent patterns in a transactional database.
-     This program employs parallel apriori property (or downward closure property) to  reduce the search space effectively.
+    :Description: cudaAprioriGCT is one of the fundamental algorithm to discover frequent patterns in a transactional database. This program employs apriori property (or downward closure property) to  reduce the search space effectively. This algorithm employs breadth-first search technique to find the complete set of frequent patterns in a transactional database.
 
-    :Reference:
+    :Reference:  Agrawal, R., Imieli ́nski, T., Swami, A.: Mining association rules between sets of items in large databases.
+                In: SIGMOD. pp. 207–216 (1993), https://doi.org/10.1145/170035.170072
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  numPartitions: int :
-                   The number of partitions. On each worker node, an executor process is started and this process performs processing.The processing unit of worker node is partition
-
 
     :Attributes:
 
         startTime : float
-            To record the start time of the mining process
+              To record the start time of the mining process
 
         endTime : float
-            To record the completion time of the mining process
+              To record the completion time of the mining process
 
         finalPatterns : dict
-            Storing the complete set of patterns in a dictionary variable
+              Storing the complete set of patterns in a dictionary variable
 
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+              To store the total amount of USS memory consumed by the program
 
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+              To store the total amount of RSS memory consumed by the program
+
+        Database : list
+              To store the transactions of a database in list
 
-        lno : int
-            the number of transactions
 
 
     **Methods to execute code on terminal**
     ----------------------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 parallelECLAT.py <inputFile> <outputFile> <minSup> <numWorkers>
+      (.venv) $ python3 cudaAprioriGCT.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 parallelECLAT.py sampleDB.txt patterns.txt 10.0 3
+      (.venv) $ python3 cudaAprioriGCT.py sampleDB.txt patterns.txt 10.0
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
-
     **Importing this algorithm into a python program**
     ----------------------------------------------------
+
     .. code-block:: python
 
-            import PAMI.frequentPattern.pyspark.parallelECLAT as alg
+            import PAMI.frequentPattern.cuda.cuAprioriGCT as alg
 
-            obj = alg.parallelECLAT(iFile, minSup, numWorkers)
+            obj = alg.cuAprioriGCT(iFile, minSup)
 
             obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
@@ -144,60 +145,158 @@
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
 
     **Credits:**
-    ----------------------------------------------------
-             The complete program was written by Yudai Masu under the supervision of Professor Rage Uday Kiran.
+    -------------
+
+                The complete program was written by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
 
     """
 
-    _minSup = float()
-    _numPartitions = int()
-    _startTime = float()
-    _endTime = float()
+    __time = 0
+    __memRSS = 0
+    __memUSS = 0
+    __GPU_MEM = 0
+    _minSup = 0
     _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _lno = int()
 
-    def __init__(self, iFile, minSup, numWorkers, sep="\t"):
-        super().__init__(iFile, minSup, int(numWorkers), sep)
+    def __init__(self, filePath, minSup, sep):
+        self._iFile = filePath
+        self._sep = sep
+        self._minSup = minSup
+        self.__time = 0
+        self.__memRSS = 0
+        self.__memUSS = 0
+
+    def __creatingItemSets(self):
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        self.__Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            if self._iFile.empty:
+                print("its empty..")
+            i = self._iFile.columns.values.tolist()
+            if 'Transactions' in i:
+                self.__Database = self._iFile['Transactions'].tolist()
+
+            # print(self.Database)
+        if isinstance(self._iFile, str):
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self.__Database.append(temp)
+            else:
+                try:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line = line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self.__Database.append(temp)
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-    def getMemoryUSS(self):
+    def __convert(self, value):
         """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
-        :rtype: float
+
+        To convert the type of user specified minSup value
+
+        :param value: user specified minSup value
+
+        :type value: int or float or str
+
+        :return: converted type
+
         """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self.__Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self.__Database) * value)
+            else:
+                value = int(value)
+        return value
 
-        return self._memoryUSS
+    def compute_vertical_bitvector_data(self):
+        """
+        Converting database into bit vector
+        """
+        # ---build item to idx mapping---#
+        idx = 0
+        item2idx = {}
+        for transaction in self.__Database:
+            for item in transaction:
+                if not item in item2idx:
+                    item2idx[item] = idx
+                    idx += 1
+        idx2item = {idx: str(int(item)) for item, idx in item2idx.items()}
+        # ---build vertical data---#
+        vb_data = np.zeros((len(item2idx), len(self.__Database)), dtype=np.uint16)
+        for trans_id, transaction in enumerate(self.__Database):
+            for item in transaction:
+                vb_data[item2idx[item], trans_id] = 1
+        vb_data = gpuarray.to_gpu(vb_data.astype(np.uint16))
+        return vb_data, idx2item
+
+    def getRuntime(self):
+        """
+        Calculating the total amount of time taken by the mining process
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
+        """
+        return self.__time
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memRSS
 
-    def getRuntime(self):
+    def getMemoryUSS(self):
         """
-        Calculating the total amount of runtime taken by the mining process
-        :return: returning total amount of runtime taken by the mining process
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
+        return self.__memUSS
+
+    def getGPUMemory(self):
+        """
+        To calculate the total memory consumed by GPU
+        :return: return GPU memory
+        :rtype: int
+        """
+
+        return self.__GPU_MEM
+
+    def getPatterns(self):
+        """
+        Function to send the set of frequent patterns after completion of the mining process
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        return self._endTime - self._startTime
+    def get_numberOfPatterns(self):
+        return len(self._finalPatterns)
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
@@ -214,206 +313,102 @@
         Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: csvfile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
+            if type(x) == tuple:
+                pattern = ""
+                for item in x:
+                    pattern = pattern + str(item) + " "
+                s1 = pattern + ":" + str(y)
+            else:
+                s1 = str(x) + ":" + str(y)
             writer.write("%s \n" % s1)
-            
-    def printResults(self):
-        """
-        This method prints all the stats
-        """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
 
-    def getPatterns(self):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
         """
-        Function to send the set of frequent patterns after completion of the mining process
-        :return: returning frequent patterns
-        :rtype: dict
+        Frequent pattern mining process will start from here
         """
-        return self._finalPatterns
+        self.mine()
 
-    def _genPatterns(self, suffix, pattern, data):
+    def mine(self):
         """
-        This function is used to generate patterns
-        :param suffix: the suffix of the generated ptterns
-
-        :type suffix: str
-
-        :param pattern: the pattern of the generated ptterns
-
-        :type pattern: str
-
-        :param data: the data of the generated ptterns after completion of the mining process
-
-        :type data: str
+        Frequent pattern mining process will start from here
         """
-        freqPatterns = {}
-        index = data.index(suffix)
-        for i in range(index + 1, len(data)):
-            tid = pattern[1].intersection(data[i][1])
-            if len(tid) >= self._minSup:
-                freqPattern = pattern[0] + ' ' + data[i][0]
-                freqPatterns[freqPattern] = len(tid)
-                freqPatterns.update(self._genPatterns(data[i], (freqPattern, tid), data))
-        return freqPatterns
+        startTime = time.time()
+        basePattern = {}
+        final = {}
+
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
+        minSup = self._minSup
+        vb_data, idx2item = self.compute_vertical_bitvector_data()
+
+        for i in range(len(vb_data)):
+            if gpuarray.sum(vb_data[i]).get() >= self._minSup:
+                basePattern[idx2item[i]] = [i]
+                final[idx2item[i]] = gpuarray.sum(vb_data[i]).get()
+
+        while len(basePattern) > 0:
+            temp = {}
+            keysList = list(basePattern.keys())
+            valuesList = list(basePattern.values())
+            for i in range(len(basePattern) - 1):
+                keyI = keysList[i].split(" ")
+                keyI = [int(x) for x in keyI]
+
+                for j in range(i + 1, len(basePattern)):
+                    keyJ = keysList[j].split(" ")
+                    keyJ = [int(x) for x in keyJ]
+                    values = set(valuesList[i])
+                    for val in valuesList[j]:
+                        values.add(val)
+                    values = list(sorted(values))
+                    totalArray = vb_data[values[0]]
+                    for k in range(1, len(values)):
+                        totalArray = totalArray.__mul__(vb_data[values[k]])
+                    support = gpuarray.sum(totalArray).get()
+                    if support >= self._minSup:
+                        combinedKey = " ".join(
+                            str(x) for x in sorted(set(keyI) | set(keyJ)))
+                        temp[combinedKey] = values
+                        final[str(combinedKey)] = support
+            basePattern = temp
+
+        self.__time = time.time() - startTime
+        self.__memRSS = psutil.Process(os.getpid()).memory_info().rss
+        self.__memUSS = psutil.Process(os.getpid()).memory_full_info().uss
+        self._finalPatterns = final
+        self.__GPU_MEM = vb_data.nbytes
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Coverage Patterns:", len(self.getPatterns()))
+        print("GPU MEM: ", _ap.getGPUMemory())
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
-    def _convert(self, value):
-        """
-        To convert the user specified minSup value
-        :param value: user specified minSup value
-        :type value: int or float or str
-        :return: converted type
-        """
-        print(value, type(value))
-        if type(value) is int:
-            value = int(value)
-        elif type(value) is float:
-            value = (self._lno * value)
-        elif type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (self._lno * value)
-            else:
-                value = int(value)
-        else:
-            print("None")
-        print(type(value), value)
-        return value
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-
-        self._startTime = _ab._time.time()
-        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
-        sc = SparkContext(conf=conf)
-
-        data = sc.textFile(self._iFile, self._numPartitions) \
-            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
-        self._lno = data.count()
-        self._minSup = self._convert(self._minSup)
-
-        frequentItems = None
-        frequentItems = data.zipWithIndex() \
-            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
-            .groupByKey() \
-            .filter(lambda x: len(x[1]) >= self._minSup) \
-            .sortBy(lambda x: len(x[1])) \
-            .mapValues(set) \
-            .persist()
-        data.unpersist()
-        # elif 'temporal' in self._iFile:
-        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
-        #         .groupByKey() \
-        #         .filter(lambda x: len(x[1]) >= self._minSup) \
-        #         .mapValues(set) \
-        #         .persist()
-        #     data.unpersist()
-        # else:
-        #     pass
-        #     # print("may be not able to process the input file")
-
-        freqItems = dict(frequentItems.collect())
-        # print(len(freqItems))
-        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
-
-        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
-                            .filter(lambda x: len(x) != 0).collect())
-        for value in freqPatterns:
-            self._finalPatterns.update(value)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
-        sc.stop()
-
-    def mine(self):
-        """
-        Frequent pattern mining process will start from here
-        """
-
-        self._startTime = _ab._time.time()
-        conf = SparkConf().setAppName("Parallel ECLAT").setMaster("local[*]")
-        sc = SparkContext(conf=conf)
-
-        data = sc.textFile(self._iFile, self._numPartitions) \
-            .map(lambda line: [int(y) for y in line.rstrip().split(self._sep)]).persist()
-        self._lno = data.count()
-        self._minSup = self._convert(self._minSup)
-
-        frequentItems = None
-        frequentItems = data.zipWithIndex() \
-            .flatMap(lambda x: [(str(item), x[1]) for item in x[0]]) \
-            .groupByKey() \
-            .filter(lambda x: len(x[1]) >= self._minSup) \
-            .sortBy(lambda x: len(x[1])) \
-            .mapValues(set) \
-            .persist()
-        data.unpersist()
-        # elif 'temporal' in self._iFile:
-        #     frequentItems = data.flatMap(lambda trans: [(str(item), trans[0]) for item in trans[1:]]) \
-        #         .groupByKey() \
-        #         .filter(lambda x: len(x[1]) >= self._minSup) \
-        #         .mapValues(set) \
-        #         .persist()
-        #     data.unpersist()
-        # else:
-        #     pass
-        #     # print("may be not able to process the input file")
-
-        freqItems = dict(frequentItems.collect())
-        # print(len(freqItems))
-        self._finalPatterns = {k: len(v) for k, v in freqItems.items()}
-
-        freqPatterns = list(frequentItems.map(lambda x: self._genPatterns(x, x, list(freqItems.items())))
-                            .filter(lambda x: len(x) != 0).collect())
-        for value in freqPatterns:
-            self._finalPatterns.update(value)
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Parallel ECLAT algorithm")
-        sc.stop()
-
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = parallelECLAT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = cudaAprioriGCT(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
         _ap.mine()
-        _finalPatterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_finalPatterns))
+        print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("GPU MEM: ", _ap.getGPUMemory())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py` & `pami-2024.5.1/PAMI/frequentPattern/pyspark/parallelFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -261,65 +261,28 @@
         super().__init__(iFile, minSup, int(numWorkers), sep)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-
-        self._startTime = _ab._time.time()
-
-        conf = _SparkConf().setAppName("Parallel FPGrowth").setMaster("local[*]")
-        sc = _SparkContext(conf=conf)
-
-        rdd = sc.textFile(self._iFile, self._numPartitions)\
-            .map(lambda x: x.rstrip().split('\t'))\
-            .persist()
-
-        self._lno = rdd.count()
-        self._minSup = self._convert(self._minSup)
-
-        freqItems = rdd.flatMap(lambda trans: [(item, 1) for item in trans])\
-            .reduceByKey(add)\
-            .filter(lambda x: x[1] >= self._minSup)\
-            .sortBy(lambda x: x[1], ascending=False)\
-            .collect()
-        self._finalPatterns = dict(freqItems)
-        self._FPList = [x[0] for x in freqItems]
-        rank = dict([(item, index) for (index, item) in enumerate(self._FPList)])
-
-        workByPartition = rdd.flatMap(lambda x: self.genCondTransaction(x, rank)).groupByKey()
-
-        trees = workByPartition.foldByKey(Tree(), lambda tree, data: self.buildTree(tree, data))
-        freqPatterns = trees.flatMap(lambda tree_tuple: self.genAllFrequentPatterns(tree_tuple))
-        result = freqPatterns.map(lambda ranks_count: (tuple([self._FPList[z] for z in ranks_count[0]]), ranks_count[1]))\
-            .collect()
-
-        self._finalPatterns.update(dict(result))
-
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        sc.stop()
-
-        print("Frequent patterns were generated successfully using Parallel FPGrowth algorithm")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         self._startTime = _ab._time.time()
 
         conf = _SparkConf().setAppName("Parallel FPGrowth").setMaster("local[*]")
         sc = _SparkContext(conf=conf)
 
         rdd = sc.textFile(self._iFile, self._numPartitions)\
-            .map(lambda x: x.rstrip().split('\t'))\
+            .map(lambda x: x.rstrip().split(self._sep))\
             .persist()
 
         self._lno = rdd.count()
         self._minSup = self._convert(self._minSup)
 
         freqItems = rdd.flatMap(lambda trans: [(item, 1) for item in trans])\
             .reduceByKey(add)\
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/topk/FAE.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,21 +1,20 @@
-# Top - K is and algorithm to discover top frequent patterns in a transactional database.
-#
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+# --------------------------------------------------------
 #
-#             import PAMI.frequentPattern.topK.FAE as alg
+
+#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#             obj = alg.FAE(iFile, K)
+#             obj = alg.kPFPMiner(iFile, k)
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             topKFrequentPatterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -28,150 +27,171 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+     Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-from PAMI.frequentPattern.topk import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
 
+from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
 
-class FAE(_ab._frequentPatterns):
-    """
-    :Description: Top - K is and algorithm to discover top frequent patterns in a transactional database.
 
+class kPFPMiner(_ab._periodicFrequentPatterns):
+    """
+    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
 
-    :Reference:   Zhi-Hong Deng, Guo-Dong Fang: Mining Top-Rank-K Frequent Patterns: DOI: 10.1109/ICMLC.2007.4370261 · Source: IEEE Xplore
-                  https://ieeexplore.ieee.org/document/4370261
+    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
+                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
+                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  k: int :
-                    User specified count of top frequent patterns
-    :param minimum: int :
-                    Minimum number of frequent patterns to consider in analysis
+                   Name of the output file to store complete set of periodic frequent pattern's
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
-
     :Attributes:
 
-        startTime : float
-          To record the start time of the mining process
-
-        endTime : float
-          To record the completion time of the mining process
-
-        finalPatterns : dict
-          Storing the complete set of patterns in a dictionary variable
-
+        iFile : str
+            Input file name or path of the input file
+        k: int
+            User specified counte of top-k periodic frequent patterns
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        oFile : str
+            Name of the output file or the path of the output file
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        finalPatterns: dict
+            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
-          To store the total amount of USS memory consumed by the program
-
+            To store the total amount of USS memory consumed by the program
         memoryRSS : float
-          To store the total amount of RSS memory consumed by the program
-
-        finalPatterns : dict
-            it represents to store the patterns
+            To store the total amount of RSS memory consumed by the program
 
+    :Methods:
 
-    **Methods to execute code on terminal**
-    -------------------------------------------
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        savePatterns(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
 
+    **Executing the code on terminal:**
+    ------------------------------------------
     .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 FAE.py <inputFile> <outputFile> <K>
+       Format:
 
-      Example Usage:
 
-      (.venv) $ python3 FAE.py sampleDB.txt patterns.txt 10
+       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
 
-    .. note:: k will be considered as count of top frequent patterns to consider in analysis
+       Examples :
 
+       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
 
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------------
+    **Sample run of the importing code:
+    --------------------------------------
     .. code-block:: python
 
-        import PAMI.frequentPattern.topK.FAE as alg
+            import PAMI.periodicFrequentPattern.kPFPMiner as alg
 
-        obj = alg.FAE(iFile, K)
+            obj = alg.kPFPMiner(iFile, k)
 
-        obj.mine()
+            obj.startMine()
 
-        topKFrequentPatterns = obj.getPatterns()
+            periodicFrequentPatterns = obj.getPatterns()
 
-        print("Total number of Frequent Patterns:", len(topKFrequentPatterns))
+            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-        obj.save(oFile)
+            obj.save(oFile)
 
-        Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternInDataFrame()
 
-        memUSS = obj.getMemoryUSS()
+            memUSS = obj.getMemoryUSS()
 
-        print("Total Memory in USS:", memUSS)
+            print("Total Memory in USS:", memUSS)
 
-        memRSS = obj.getMemoryRSS()
+            memRSS = obj.getMemoryRSS()
 
-        print("Total Memory in RSS", memRSS)
+            print("Total Memory in RSS", memRSS)
 
-        run = obj.getRuntime()
+            run = obj.getRuntime()
 
-        print("Total ExecutionTime in seconds:", run)
+            print("Total ExecutionTime in seconds:", run)
 
-    Credits:
-    --------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    **Credits:**
+    --------------
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _startTime = float()
     _endTime = float()
     _k = int()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _tidList = {}
-    _minimum = int()
+    lno = int()
+    _maximum = int()
 
     def _creatingItemSets(self):
         """
-            Storing the complete transactions of the database/input file in a database variable
-
+        Storing the complete transactions of the database/input file in a database variable
         """
 
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
@@ -195,88 +215,109 @@
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
+                    
+    def getPer_Sup(self, tids):
+        tids.sort()
+        cur=0
+        per=list()
+        sup=0
+        #print(tids)
+        for i in range(len(tids)-1):
+            j = i + 1
+            #if tids[j] - cur <= periodicity:
+                #return [0,0]
+            per.append(tids[j] - cur)
+            cur = tids[j]
+        per.append(self.lno - cur)
+        return max(per)
 
     def _frequentOneItem(self):
         """
         Generating one frequent patterns
         """
-        candidate = {}
+        self._mapSupport = {}
         self._tidList = {}
-        for i in range(len(self._Database)):
-            for j in self._Database[i]:
-                if j not in candidate:
-                    candidate[j] = 1
-                    self._tidList[j] = [i]
+        n = 0
+        for line in self._Database:
+            self.lno += 1
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._tidList[si] = [n]
                 else:
-                    candidate[j] += 1
-                    self._tidList[j].append(i)
-        self._finalPatterns = {}
-        plist = [key for key, value in sorted(candidate.items(), key=lambda x: x[1], reverse=True)]
+                    self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
+        for x, y in self._mapSupport.items():
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
         for i in plist:
             if len(self._finalPatterns) >= self._k:
                 break
             else:
-                self._finalPatterns[i] = candidate[i]
-        self._minimum = min([self._finalPatterns[i] for i in self._finalPatterns.keys()])
+                self._finalPatterns[i] = self._mapSupport[i][1]
+        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
         plist = list(self._finalPatterns.keys())
         return plist
 
+
     def _save(self, prefix, suffix, tidSetI):
         """Saves the patterns that satisfy the periodic frequent property.
 
-            :param prefix: the prefix of a pattern
-            :type prefix: list
-            :param suffix: the suffix of a patterns
-            :type suffix: list
-            :param tidSetI: the timestamp of a patterns
-            :type tidSetI: list
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: list
         """
 
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = len(tidSetI)
+        val = self.getPer_Sup(tidSetI)
         sample = str()
         for i in prefix:
-            sample = sample + i + "\t"
+            sample = sample + i + " "
         if len(self._finalPatterns) < self._k:
-            if val > self._minimum:
+            if val < self._maximum:
                 self._finalPatterns[sample] = val
                 self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([i for i in self._finalPatterns.values()])
+                self._maximum = max([i for i in self._finalPatterns.values()])
         else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1]):
-                if val > y:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
+                if val < y:
                     del self._finalPatterns[x]
                     self._finalPatterns[sample] = val
                     self._finalPatterns = {k: v for k, v in
                                               sorted(self._finalPatterns.items(), key=lambda item: item[1],
                                                      reverse=True)}
-                    self._minimum = min([i for i in self._finalPatterns.values()])
+                    self._maximum = max([i for i in self._finalPatterns.values()])
                     return
 
     def _Generation(self, prefix, itemSets, tidSets):
         """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
-            :param prefix:  main equivalence prefix
-            :type prefix: periodic-frequent item or pattern
-            :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                            and frequent with their timestamps
-            :type itemSets: list
-            :param tidSets: timestamps of the items in the argument itemSets
-            :type tidSets: list
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
 
-
-                    """
+        """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidI = tidSets[0]
             self._save(prefix, [i], tidI)
             return
         for i in range(len(itemSets)):
             itemI = itemSets[i]
@@ -286,78 +327,44 @@
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetI).intersection(tidSetJ))
-                if len(y) >= self._minimum:
+                if self.getPer_Sup(y) <= self._maximum:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def _convert(self, value):
         """
         to convert the type of user specified minSup value
+
         :param value: user specified minSup value
-        :type value: int or float or str
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = ((len(self._Database)) * value)
             else:
                 value = int(value)
         return value
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-            Main function of the program
-        """
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if len(y1) >= self._minimum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
-        self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        Main function of the program
 
-    def mine(self):
-        """
-            Main function of the program
         """
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._k is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
@@ -369,120 +376,109 @@
             itemSetX = [itemI]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(plist)):
                 itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetI).intersection(tidSetJ))
-                if len(y1) >= self._minimum:
+                if self.getPer_Sup(y1) <= self._maximum:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-        print(" TopK frequent patterns were successfully generated using FAE algorithm.")
+        print("kPFPMiner has successfully generated top-k frequent patterns")
         self._endTime = _ab._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
-
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-        Storing final frequent patterns in a dataframe
+        """Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
-
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
         return dataFrame
 
     def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
 
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
+            patternsAndSupport = x + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
-
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printTOPK(self):
-        """
-        This function is used to print the results
-        """
-        print("Top K Frequent  Patterns:", len(self.getPatterns()))
+    def printResults(self):
+        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = FAE(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        _ap.mine()
-        print("Top K Frequent Patterns:", len(_ap.getPatterns()))
+        _Patterns = _ap.getPatterns()
+        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.9.1/PAMI/frequentPattern/topk/abstract.py` & `pami-2024.5.1/PAMI/frequentPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/__init__.py` & `pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py` & `pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/FCPGrowth.py`

 * *Files 12% similar despite different names*

```diff
@@ -472,113 +472,15 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """ 
         Frequent pattern mining process will startTime from here
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        for tr in range(len(self._transactions)):
-            items = self._transactions[tr]
-            quantities = self._fuzzyValues[tr]
-            for i in range(0, len(items)):
-                item = items[i]
-                regions = _Regions(item, float(quantities[i]), 3, self._mapItemRegionSum)
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
-                else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfFFIList = []
-        mapItemsToFFLIST = {}
-        self._minSup = self._convert(self._minSup)
-        #minSup = self._minSup
-        self._minAllConf = float(self._minAllConf)
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            region = 'N'
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-                region = 'L'
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-                region = 'M'
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                region = 'H'
-                self._mapItemSum[item] = high
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item, region)
-                mapItemsToFFLIST[item] = fuList
-                listOfFFIList.append(fuList)
-        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for tr in range(len(self._transactions)):
-            items = self._transactions[tr]
-            quantities = self._fuzzyValues[tr]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                regions = _Regions(pair.item, float(quantities[i]), 3, self._temp)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                        pair.region = 'L'
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.region = 'M'
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                        pair.region = 'H'
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i - 1, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                if pair.quantity > remainUtil:
-                    remainingUtility = pair.quantity
-                else:
-                    remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Fuzzy Correlated Patterns Successfully generated using FCPGrowth algorithms")
+        self.mine()
 
 
     def mine(self) -> None:
         """
         Frequent pattern mining process will startTime from here
         """
         self._startTime = _ab._time.time()
@@ -732,15 +634,14 @@
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
@@ -851,15 +752,15 @@
     inputFile = 'https://u-aizu.ac.jp/~udayrage/datasets/fuzzyDatabases/Fuzzy_T10I4D100K.csv'
 
     minimumSupportCount=1200  #Users can also specify this constraint between 0 to 1.
     ratioExample=0.8
     seperator='\t' 
 
     obj = FCPGrowth(inputFile, minimumSupportCount,ratioExample,seperator)    #initialize
-    obj.startMine()       
+    obj.mine()
 
 
 if __name__ == "__main__":
     main()
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py` & `pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/fuzzyCorrelatedPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -102,15 +102,14 @@
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         :type minSup: int or float or str
         :param minAllConf: The user can specify minimum all confidence ratio
         :type minAllConf: float
         :param sep: separator used to distinguish items from each other. The default separator is tab space. However, users can override the default separator
         :type sep: str
         """
-
         self._iFile = iFile
         self._minSup = minSup
         self._minAllConf = minAllConf
         self._sep = sep
         self._startTime = float()
         self._endTime = float()
         self._finalPatterns = {}
@@ -121,15 +120,15 @@
         """Code for the mining process will start from this function"""
 
         pass
 
     @_abstractmethod
     def getPatterns(self):
         """Complete set of frequent patterns generated will be retrieved from this function"""
-
+        
         pass
 
     @_abstractmethod
     def save(self, oFile):
         """Complete set of frequent patterns will be saved in to an output file from this function
 
         :param oFile: Name of the output file
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/fuzzyFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py` & `pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py`

 * *Files 5% similar despite different names*

```diff
@@ -1,25 +1,25 @@
-# Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
+# Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
+# on-trivial and challenging problem to its huge search space.we are using efficient pruning
+# techniques to reduce the search space.
 #
-# to its huge search space.we are using efficient pruning techniques to reduce the search space.
+# Sample run of importing the code:
+# ----------------------------------------
 #
-# **Importing this algorithm into a python program**
-# ---------------------------------------------------------
+#             from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
 #
-#             from PAMI.fuzzyFrequentPattern import FFIMiner as alg
-#
-#             obj = alg.FFIMiner("input.txt", 2)
+#             obj =alg.FPFPMiner("input.txt",2,3)
 #
 #             obj.mine()
 #
-#             fuzzyFrequentPattern = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+#             print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#             obj.save("outputFile")
+#             obj.save("output.txt")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -44,143 +44,158 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 
+     This program is free software: you can redistribute it and/or modify
+     it under the terms of the GNU General Public License as published by
+     the Free Software Foundation, either version 3 of the License, or
+     (at your option) any later version.
+
+     This program is distributed in the hope that it will be useful,
+     but WITHOUT ANY WARRANTY; without even the implied warranty of
+     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+     GNU General Public License for more details.
+
+     You should have received a copy of the GNU General Public License
+     along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
-from PAMI.fuzzyFrequentPattern.basic import abstract as _ab
+
+from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
-
     :Attributes:
 
-        item: int
+        item : int
             the item name
-        sumIUtil: float
+        sumLUtil : float
             the sum of utilities of a fuzzy item in database
-        sumRUtil: float
+        sumRUtil : float
             the sum of resting values of a fuzzy item in database
-        elements: list
-            a list of elements contain tid,Utility and resting values of element in each transaction
+        elements : list
+            list of elements contain tid,Utility and resting values of element in each transaction
+        maxPeriod : int
+            it represents the max period of a item
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
             Method to print elements
+
     """
 
-    def __init__(self, itemName: int) -> None:
+    def __init__(self, itemName: str) -> None:
         self.item = itemName
-        self.sumIUtil = 0.0
+        self.sumLUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
+        self.maxPeriod = 0
 
     def addElement(self, element) -> None:
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-
         :type element: Element
-
         :return: None
         """
-        self.sumIUtil += element.iUtils
+        self.sumLUtil += element.lUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
+        self.maxPeriod = max(self.maxPeriod, element.period)
 
     def printElement(self) -> None:
         """
-        A method to print elements
+        A Method to Print elements in the FFList
+        :return: None
         """
         for ele in self.elements:
-            print(ele.tid, ele.iUtils, ele.rUtils)
+            print(ele.tid, ele.lUtils, ele.rUtils, ele.period)
 
 
 class _Element:
     """
-    A class represents an Element of a fuzzy list
+        A class represents an Element of a fuzzy list
 
-    :Attributes:
+        :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils: float
+        lUtils: float
             the utility of a fuzzy item in the transaction
         rUtils : float
-            the  resting value of a fuzzy item in the transaction
+            the resting value of a fuzzy item in the transaction
+        period: int
+            represent the period of the element
     """
 
-    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
+    def __init__(self, tid: int, iUtil: float, rUtil: float, period: int) -> None:
         self.tid = tid
-        self.iUtils = iUtil
+        self.lUtils = iUtil
         self.rUtils = rUtil
+        self.period = period
 
 
 class _Pair:
     """
-    A class to store item and it's quantity together
+    A class to store item name and quantity together.
     """
 
     def __init__(self) -> None:
         self.item = 0
         self.quantity = 0
 
 
-class FFIMiner(_ab._fuzzyFrequentPattenrs):
+class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
     """
-    :Description:   Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
-                    to its huge search space.we are using efficient pruning techniques to reduce the search space.
-
-    :Reference:   Lin, Chun-Wei & Li, Ting & Fournier Viger, Philippe & Hong, Tzung-Pei. (2015).
-                  A fast Algorithm for mining fuzzy frequent itemsets. Journal of Intelligent & Fuzzy Systems. 29.
-                  2373-2379. 10.3233/IFS-151936.
-                  https://www.researchgate.net/publication/286510908_A_fast_Algorithm_for_mining_fuzzy_frequent_itemSets
+    :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
+                    on-trivial and challenging problem to its huge search space.we are using efficient pruning
+                    techniques to reduce the search space.
+
+    :Reference:   R. U. Kiran et al., "Discovering Fuzzy Periodic-Frequent Patterns in Quantitative Temporal Databases,"
+                  2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), Glasgow, UK, 2020, pp.
+                  1-8, doi: 10.1109/FUZZ48607.2020.9177579.
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-    :param fuzFile: str :
-                    The user can specify fuzFile.
-
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
-        iFile : string
-            Name of the input file to mine complete set of fuzzy  frequent patterns
-        fmFile : string
-            Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
-        oFile : string
-            Name of the oFile file to store complete set of fuzzy  frequent patterns
+        iFile : file
+            Name of the input file to mine complete set of fuzzy spatial frequent patterns
+        oFile : file
+               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
-            The user given minimum support
+            The user given support
+        period: int
+            periodicity of an element
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
+                To store the total amount of RSS memory consumed by the program
         startTime:float
-            To record the start time of the mining process
+               To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
         mapItemsLowSum: map
             To keep track of low region values of items
         mapItemsMidSum: map
@@ -188,23 +203,29 @@
         mapItemsHighSum: map
             To keep track of high region values of items
         mapItemSum: map
             To keep track of sum of Fuzzy Values of items
         mapItemRegions: map
             To Keep track of fuzzy regions of item
         jointCnt: int
-            To keep track of the number of ffi-list that was constructed
+            To keep track of the number of FFI-list that was constructed
         BufferSize: int
             represent the size of Buffer
         itemBuffer list
             to keep track of items in buffer
+        maxTID: int
+            represent the maximum tid of the database
+        lastTIDs: map
+            represent the last tid of fuzzy items
+        itemsToRegion: map
+            represent items with respective regions
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -212,106 +233,104 @@
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         convert(value):
             To convert the given user specified value
-        compareItems(o1, o2)
-            A Function that sort all ffi-list in ascending order of Support
-        FSFIMining(prefix, prefixLen, FSFIM, minSup)
-            Method generate ffi from prefix
+        FSFIMining( prefix, prefixLen, fsFim, minSup)
+            Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        findElementWithTID(uList, tid)
+        findElementWithTID(UList, tid)
             To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil)
+        WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
 
-
     **Executing the code on terminal :**
-    ------------------------------------------
+    ----------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FFIMiner.py <inputFile> <outputFile> <minSup> <separator>
+      (.venv) $ python3 FPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
 
       Example Usage:
 
-      (.venv) $ python3  FFIMiner.py sampleTDB.txt output.txt 6
+      (.venv) $ python3  FPFPMiner.py sampleTDB.txt output.txt 2 3
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Sample run of importing the code:**
-    ------------------------------------------
-    .. code-block:: python
-
-            from PAMI.fuzzyFrequentPattern import FFIMiner as alg
+    --------------------------------------
 
-            obj = alg.FFIMiner("input.txt", 2)
+        from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
 
-            obj.mine()
+        obj =alg.FPFPMiner("input.txt",2,3)
 
-            fuzzyFrequentPattern = obj.getPatterns()
+        obj.mine()
 
-            print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+        periodicFrequentPatterns = obj.getPatterns()
 
-            obj.save("outputFile")
+        print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
-            memUSS = obj.getMemoryUSS()
+        obj.save("output.txt")
 
-            print("Total Memory in USS:", memUSS)
+        memUSS = obj.getMemoryUSS()
 
-            memRSS = obj.getMemoryRSS()
+        print("Total Memory in USS:", memUSS)
 
-            print("Total Memory in RSS", memRSS)
+        memRSS = obj.getMemoryRSS()
 
-            run = obj.getRuntime()
+        print("Total Memory in RSS", memRSS)
 
-            print("Total ExecutionTime in seconds:", run)
+        run = obj.getRuntime()
 
+        print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+    --------------
+            The complete program was written by Sai Chitra.B under the supervision of Professor Rage Uday Kiran.
 
     """
-
     _startTime = float()
     _endTime = float()
-    _minSup = str()
+    _minSup = float()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _fuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _sep = "\t"
-
-    def __init__(self, iFile: str, minSup: float, sep: str="\t") -> None:
-        super().__init__(iFile, minSup, sep)
-        self._startTime = 0
-        self._endTime = 0
-        self._itemsCnt = 0
-        self._mapItemSum = {}
-        self._joinsCnt = 0
+    _sep = " "
+    _Database = []
+    _transactions = []
+    _fuzzyValues = []
+    _ts = []
+
+    def __init__(self, iFile: Union[str, _ab._pd.DataFrame], minSup: Union[int, float], period: Union[int, float], sep: str="\t") -> None:
+        super().__init__(iFile, minSup, period, sep)
+        self._oFile = ""
         self._BufferSize = 200
         self._itemSetBuffer = []
-        self._transactions = []
-        self._fuzzyValues = []
+        self._mapItemSum = {}
         self._finalPatterns = {}
+        self._joinsCnt = 0
+        self._itemsCnt = 0
+        self._startTime = float()
+        self._endTime = float()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
         self._dbLen = 0
 
-    def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
+    def _compareItems(self, o1, o2) -> int:
         """
-        A Function that sort all ffi-list in ascending order of Support
+        A Function that sort all FFI-list in ascending order of Support
 
         :param o1: First FFI-list
 
         :type o1: _FFList
 
         :param o2: Second FFI-list
 
@@ -319,236 +338,204 @@
 
         :return: Comparision Value
 
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
-            if o1.item < o2.item:
-                return -1
-            elif o1.item > o2.item:
-                return 1
-            else:
-                return 0
+            return int(o1.item) - int(o2.item)
         else:
             return compare
 
-    def _convert(self, value) -> Union[int, float]:
+    def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
 
         :type value: int or float or str
 
         :return: converted value
 
-        :rtype: int or float
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
-                value = float(value)
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemsets(self) -> None:
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :return: None
         """
-        self._transactions, self._fuzzyValues, self._Database = [], [], []
+        data, self._transactions, self._fuzzyValues, ts = [], [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                self._ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 self._transactions = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
                 self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
-            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
+                count = 0
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
                     parts[1] = parts[1].strip()
                     items = parts[0].split(self._sep)
                     quantities = parts[1].split(self._sep)
-                    self._transactions.append([x for x in items])
+                    self._ts.append(int(items[0]))
+                    self._transactions.append([x for x in items[1:]])
                     self._fuzzyValues.append([float(x) for x in quantities])
+                    count += 1
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
+                        count = 0
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
                             parts[1] = parts[1].strip()
                             items = parts[0].split(self._sep)
                             quantities = parts[1].split(self._sep)
-                            self._transactions.append([x for x in items])
+                            self._ts.append(int(items[0]))
+                            self._transactions.append([x for x in items[1:]])
                             self._fuzzyValues.append([float(x) for x in quantities])
+                            count += 1
                 except IOError:
                     print("File Not Found")
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        fuzzy-Frequent pattern mining process will start from here
+        Fuzzy periodic Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfffilist = []
-        mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
-        # minSup = self.minSup
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.quantity = quantities[i]
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self) -> None:
         """
-        fuzzy-Frequent pattern mining process will start from here
+        Fuzzy periodic Frequent pattern mining process will start from here
         """
+        maxTID = 0
+        lastTIDs = {}
         self._startTime = _ab._time.time()
-        self._creatingItemsets()
+        self._creatingItemSets()
+        self._finalPatterns = {}
+        tid = int()
         for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
+            self._dbLen += 1
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
-            self._dbLen += 1
+            if tid < maxTID:
+                maxTID = tid
             for i in range(0, len(items)):
                 item = items[i]
                 if item in self._mapItemSum:
                     self._mapItemSum[item] += quantities[i]
                 else:
                     self._mapItemSum[item] = quantities[i]
-        listOfffilist = []
+        listOfFFIList = []
         mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
-        # minSup = self.minSup
+        # self._minSup = self._convert(self._minSup)
+        self._minSup = float(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
         for item1 in self._mapItemSum.keys():
             item = item1
             if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
+                fUList = _FFList(item)
+                k = tuple([item])
+                mapItemsToFFLIST[k] = fUList
+                listOfFFIList.append(fUList)
+                lastTIDs[item] = tid
+        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         for line in range(len(self._transactions)):
+            tid = int(self._ts[line])
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
-                pair.quantity = quantities[i]
                 item = pair.item
+                pair.quantity = quantities[i]
                 if self._mapItemSum[item] >= self._minSup:
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
                 remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
+                for j in range(len(revisedTransaction) - 1, i - 1, -1):
                     remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
+                if pair.quantity > remainUtil:
+                    remainingUtility = pair.quantity
+                else:
+                    remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
+                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
+                    if len(FFListOfItem.elements) == 0:
+                        element = _Element(tid, pair.quantity, remainingUtility, 0)
+                    else:
+                        if lastTIDs[pair.item] == tid:
+                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
+                        else:
+                            lastTid = FFListOfItem.elements[-1].tid
+                            curPer = tid - lastTid
+                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
                     FFListOfItem.addElement(element)
-            tid += 1
-        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
+        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FFIMining(self, prefix, prefixLen, FSFIM, minSup):
+    def _FPFPMining(self, prefix, prefixLen, fsFim):
+
         """
-        Generates ffi from prefix
+        Generates FPFP from prefix
 
-        :param prefix: the prefix patterns of ffi
+        :param prefix: the prefix patterns of FPFP
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
-        :param FSFIM: the Fuzzy list of prefix itemSets
-        :type FSFIM: list
-        :param minSup: the minimum support of
-        :type minSup: int or flaot
-        """
-        for i in range(0, len(FSFIM)):
-            X = FSFIM[i]
-            if X.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            if X.sumRUtil >= minSup:
+        :param fsFim: the Fuzzy list of prefix itemSets
+        :type fsFim: list
+        """
+        for i in range(0, len(fsFim)):
+            X = fsFim[i]
+            if X.sumLUtil >= self._minSup and X.maxPeriod <= self._maxPer:
+                self._WriteOut(prefix, prefixLen, X.item, X.sumLUtil, X.maxPeriod)
+            if X.sumRUtil >= self._minSup:
                 exULs = []
-                for j in range(i + 1, len(FSFIM)):
-                    Y = FSFIM[j]
+                for j in range(i + 1, len(fsFim)):
+                    Y = fsFim[j]
                     exULs.append(self._construct(X, Y))
                     self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
+                self._FPFPMining(self._itemSetBuffer, prefixLen + 1, exULs)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
@@ -570,147 +557,150 @@
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px, py) -> _FFList:
+    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
         """
-        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
+        A function to construct a new Fuzzy item set from 2 fuzzy itemSets
 
-        :param px:the itemSet px
-        :type px:ffi-List
-        :param py:itemSet py
-        :type py:ffi-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :ffi-List
+        :param px:the item set px
+        :type px:FFI-List
+        :param py:item set py
+        :type py:FFI-List
+        :return :the item set of pxy(px and py)
+        :rtype :FFI-List
         """
         pxyUL = _FFList(py.item)
+        prev = 0
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
+            eXY = _Element(ex.tid, min([ex.lUtils, ey.lUtils], key=lambda x: float(x)), ey.rUtils, ex.tid - prev)
             pxyUL.addElement(eXY)
+            prev = ex.tid
         return pxyUL
 
-    def _findElementWithTID(self, uList, tid) -> _Element:
+    def _findElementWithTID(self, UList, tid) -> _Element:
         """
         To find element with same tid as given
 
-        :param uList: fuzzyList
-        :type uList: ffi-List
-        :param tid: transaction id
+        :param UList: fuzzy list
+        :type UList: FFI-List
+        :param tid:transaction id
         :type tid: int
-        :return: element  tid as given
-        :rtype: element if exit or None
+        :return: element eith tid as given
+        :rtype: element if exist or None
         """
-        List = uList.elements
+        List = UList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
             if List[mid].tid < tid:
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: list, prefixLen: int, item: int, sumIUtil: float) -> None:
+    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumLUtil: float, period: int) -> None:
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
-        :param sumIUtil: sum of utility of itemSet
-        :type sumIUtil: float
+        :param sumLUtil: sum of utility of itemSet
+        :type sumLUtil: float
+        :param period: represent the period of itemSet
+        :type period: int
         :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i])  + "\t"
+            res += str(prefix[i]) +  "\t"
         res += str(item)
-        res1 = str(sumIUtil)
-        self._finalPatterns[res] = res1
+        #res1 = str(sumLUtil) + " : " + str(period)
+        self._finalPatterns[res] = [sumLUtil, period]
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def getPatterns(self) -> dict:
+    def getPatterns(self) -> Dict[str, str]:
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile) -> dict:
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: dictionary of frequent patterns
-        :rtype: dict
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
+            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:  # to  include a user specified separator
+            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
+            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
-        _ap = FFIMiner('sample.txt', 1, ' ')
+        _ap = FPFPMiner('sample.txt', 1, 10, ' ')
         _ap.startMine()
         _ap.mine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save('output.txt')
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py` & `pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner_old.py`

 * *Files 8% similar despite different names*

```diff
@@ -201,21 +201,21 @@
     :Attributes:
 
         iFile : string
             Name of the input file to mine complete set of fuzzy  frequent patterns
         fmFile : string
             Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
         oFile : string
-               Name of the oFile file to store complete set of fuzzy  frequent patterns
+            Name of the oFile file to store complete set of fuzzy  frequent patterns
         minSup : float
             The user given minimum support
         memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
+            To store the total amount of RSS memory consumed by the program
         startTime:float
-               To record the start time of the mining process
+            To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         itemsCnt: int
             To record the number of fuzzy spatial itemSets generated
         mapItemsLowSum: map
             To keep track of low region values of items
         mapItemsMidSum: map
@@ -231,29 +231,29 @@
         BufferSize: int
             represent the size of Buffer
         itemBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
+        convert(value)
             To convert the given user specified value
         compareItems(o1, o2)
             A Function that sort all ffi-list in ascending order of Support
         FSFIMining(prefix, prefixLen, FSFIM, minSup)
             Method generate ffi from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
@@ -348,23 +348,18 @@
         self._dbLen = 0
 
     def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
         """
         A Function that sort all ffi-list in ascending order of Support
 
         :param o1: First FFI-list
-
         :type o1: _FFList
-
         :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
+        :type o2: _FFList
+        :return: Comparison Value
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             if o1.item < o2.item:
                 return -1
             elif o1.item > o2.item:
@@ -375,19 +370,16 @@
             return compare
 
     def _convert(self, value: Union[int, float, str]) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
-
         :type value: int or float or str
-
         :return: converted value
-
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
@@ -469,17 +461,15 @@
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _Regions(self, quantity: float) -> None:
         """
         :param quantity: Quantity to calculate regions
-
         :type quantity: float
-
         :return: None
         """
         self.list = [0] * len(self._LabelKey)
         if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
             self.list[0] = 1
             return
         elif quantity >= self._RegionsCal[-1][0]:
@@ -499,103 +489,16 @@
             return
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         fuzzy-Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                regions = self._Regions(float(quantities[i]))
-                print(regions)
-                item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
-                else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfffilist = []
-        mapItemsToFFLIST = {}
-        self._minSup = self._convert(self._minSup)
-        # minSup = self.minSup
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                regions = self._Regions(float(quantities[i]), 3)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        self._FSFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
+
 
     def mine(self) -> None:
         """
         fuzzy-Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemsets()
@@ -714,15 +617,14 @@
                 self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
-
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,49 +46,63 @@
         employ in PAMI
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
+
         minSup: integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
+
         startTime:float
             To record the start time of the algorithm
+
         endTime:float
             To record the completion time of the algorithm
+
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
+
         oFile : str
             Name of the output file to store complete set of frequent patterns
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
     :Method:
 
         startMine()
             Calling this function will start the actual mining process
+
         getPatterns()
             This function will output all interesting patterns discovered by an algorithm
+
         save(oFile)
             This function will store the discovered patterns in an output file specified by the user
+
         getPatternsAsDataFrame()
             The function outputs the patterns generated by an algorithm as a data frame
+
         getMemoryUSS()
             This function outputs the total amount of USS memory consumed by a mining algorithm
+
         getMemoryRSS()
             This function outputs the total amount of RSS memory consumed by a mining algorithm
+
         getRuntime()
             This function outputs the total runtime of a mining algorithm
 
     """
 
     def __init__(self, iFile, minSup, sep="\t"):
         """
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner.py` & `pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,17 +1,20 @@
-# Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-# which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-# techniques to reduce the search space.
 #
 # **Importing this algorithm into a python program**
-# ---------------------------------------------------------
-# .. code-block:: python
+# --------------------------------------------------------
+#     .. code-block:: python
 #
 #             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 #
+#             minSup = str()
+#
+#             iFile = " "
+#
+#             sep = "\t"
+#
 #             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 #
 #             obj.mine()
 #
 #             fuzzySpatialFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
@@ -29,15 +32,14 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -50,61 +52,59 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
+        item: int
+            the item name
+        sumIUtil: float
+            the sum of utilities of a fuzzy item in database
+        sumRUtil: float
+            the sum of resting values of a fuzzy item in database
+        elements: list
+            a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
-            Method to print elements
+            Method to print elements            
 
     """
 
-    def __init__(self, itemName: str) -> None:
+    def __init__(self, itemName):
         self.item = itemName
         self.sumIUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
 
-    def addElement(self, element) -> None:
+    def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :param element: Element
-        :return: None
+        :type element: Element
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
-    def printElement(self) -> None:
+    def printElement(self):
         """
         A Method to Print elements in the FFList
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
@@ -112,245 +112,271 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils: float
+        iUtils : float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
-    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
+    def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
         self.iUtils = iUtil
         self.rUtils = rUtil
 
 
+class _Regions:
+    """
+    A class calculate the regions
+
+    :Attributes:
+
+        low : int
+            low region value
+        middle : int
+            middle region value
+        high : int
+            high region values
+    """
+
+    def __init__(self, quantity, regionsNumber):
+        self.low = 0
+        self.middle = 0
+        self.high = 0
+        if regionsNumber == 3:  # if we have 3 regions
+            if 0 < quantity <= 1:
+                self.low = 1
+                self.high = 0
+                self.middle = 0
+            elif 1 < quantity <= 6:
+                self.low = float((6 - quantity) / 5)
+                self.middle = float((quantity - 1) / 5)
+                self.high = 0
+            elif 6 < quantity <= 11:
+                self.low = 0
+                self.middle = float((11 - quantity) / 5)
+                self.high = float((quantity - 6) / 5)
+            else:
+                self.low = 0
+                self.middle = 0
+                self.high = 1
+
+
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
 class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
     """
-    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
-
-    :Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
-                  Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
-                  (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float :
-                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-    :param nFile: str :
-                   Name of the input file to mine complete set of frequent patterns
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+        About this algorithm
+        ====================
 
+        :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
+                        which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
+                        techniques to reduce the search space.
+
+        Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
+                     Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
+                     (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+
+        :param  iFile: str :
+                       Name of the Input file to mine complete set of frequent patterns
+        :param  oFile: str :
+                       Name of the output file to store complete set of frequent patterns
+        :param  minSup: int or float or str :
+                       The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+        :param maxPer: float :
+                       The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+        :param nFile: str :
+                       Name of the input file to mine complete set of frequent patterns
+        :param  sep: str :
+                       This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+        :Attributes:
+
+            iFile : file
+                Name of the input file to mine complete set of fuzzy spatial frequent patterns
+            oFile : file
+                Name of the oFile file to store complete set of fuzzy spatial frequent patterns
+            minSup : float
+                The user given minimum support
+            neighbors : map
+                keep track of neighbours of elements
+            memoryRSS : float
+                To store the total amount of RSS memory consumed by the program
+            startTime : float
+                To record the start time of the mining process
+            endTime : float
+                To record the completion time of the mining process
+            itemsCnt : int
+                To record the number of fuzzy spatial itemSets generated
+            mapItemsLowSum : map
+                To keep track of low region values of items
+            mapItemsMidSum : map
+                To keep track of middle region values of items
+            mapItemsHighSum : map
+                To keep track of high region values of items
+            mapItemSum : map
+                To keep track of sum of Fuzzy Values of items
+            mapItemRegions : map
+                To Keep track of fuzzy regions of item
+            joinsCnt : int
+                To keep track of the number of FFI-list that was constructed
+            BufferSize : int
+                represent the size of Buffer
+            itemSetBuffer : list
+                to keep track of items in buffer
 
-    :Attributes:
+        :Methods:
 
-        iFile : file
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
-        oFile : file
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given minimum support
-        neighbors: map
-            keep track of neighbours of elements
-        memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-        startTime:float
-               To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        itemsCnt: int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemBuffer list
-            to keep track of items in buffer
+            mine()
+                Mining process will start from here
+            getPatterns()
+                Complete set of patterns will be retrieved with this function
+            save(oFile)
+                Complete set of frequent patterns will be loaded in to a output file
+            getPatternsAsDataFrame()
+                Complete set of frequent patterns will be loaded in to a dataframe
+            getMemoryUSS()
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
+            getMemoryRSS()
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
+            getRuntime()
+                Total amount of runtime taken by the mining process will be retrieved from this function
+            convert(value)
+                To convert the given user specified value
+            FSFIMining( prefix, prefixLen, fsFim, minSup)
+                Method generate FFI from prefix
+            construct(px, py)
+                A function to construct Fuzzy itemSet from 2 fuzzy itemSets
+            Intersection(neighbourX,neighbourY)
+                Return common neighbours of 2 itemSet Neighbours
+            findElementWithTID(uList, tid)
+                To find element with same tid as given
+            WriteOut(prefix, prefixLen, item, sumIUtil,period)
+                To Store the patten
 
-    :Methods:
+        Execution methods
+        =================
 
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
-            To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        Intersection(neighbourX,neighbourY)
-            Return common neighbours of 2 itemSet Neighbours
-        findElementWithTID(uList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
-            To Store the patten
 
-    **Executing the code on terminal :**
-    ----------------------------------------
+        **Terminal command**
+        .. code-block:: console
 
-    .. code-block:: console
+          Format:
 
-      Format:
+          (.venv) $ python3 FFSPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
 
-      (.venv) $ python3 FFSPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
+          Example Usage:
 
-      Example Usage:
+          (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
 
-      (.venv) $ python3  FFSPMiner.py sampleTDB.txt output.txt sampleN.txt 3
+          (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 0.3
 
-    .. note:: minSup will be considered in percentage of database transactions
+          (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
 
-    **Sample run of importing the code:**
-    ----------------------------------------
-    .. code-block:: python
+        .. note:: minSup will be considered in percentage of database transactions
 
-            from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 
-            obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+        **Sample run of importing the code:**
+        ----------------------------------------
 
-            obj.mine()
+                from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
 
-            fuzzySpatialFrequentPatterns = obj.getPatterns()
+                obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
 
-            print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+                obj.mine()
 
-            obj.save("outputFile")
+                fuzzySpatialFrequentPatterns = obj.getPatterns()
 
-            memUSS = obj.getMemoryUSS()
+                print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
 
-            print("Total Memory in USS:", memUSS)
+                obj.save("outputFile")
 
-            memRSS = obj.getMemoryRSS()
+                memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in RSS", memRSS)
+                print("Total Memory in USS:", memUSS)
 
-            run = obj.getRuntime()
+                memRSS = obj.getMemoryRSS()
 
-            print("Total ExecutionTime in seconds:", run)
+                print("Total Memory in RSS", memRSS)
 
-    **Credits:**
-    --------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
-    """
+                run = obj.getRuntime()
+
+                print("Total ExecutionTime in seconds:", run)
 
-    _startTime = float()
-    _endTime = float()
+        Credits
+        =======
+                The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+    """
     _minSup = str()
-    _maxPer = float()
-    _finalPatterns = {}
     _iFile = " "
-    _oFile = " "
     _nFile = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
     _sep = "\t"
-    _transactions = []
-    _fuzzyValues = []
 
-    def __init__(self, iFile: str, nFile: str, minSup: float, sep: str="\t") -> None:
+    def __init__(self, iFile, nFile, minSup, sep="\t"):
         super().__init__(iFile, nFile, minSup, sep)
         self._mapItemNeighbours = {}
         self._startTime = 0
         self._endTime = 0
+        self._itemsCnt = 0
+        self._mapItemsLowSum = {}
+        self._mapItemsMidSum = {}
+        self._mapItemsHighSum = {}
         self._mapItemSum = {}
+        self._mapItemRegions = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
         self._finalPatterns = {}
         self._dbLen = 0
-        self._itemsCnt = 0
 
-    def _compareItems(self, o1, o2) -> int:
+    def _compareItems(self, o1, o2):
         """
-        A Function that sort all ffi-list in ascending order of Support
-
-        :param o1: First FFI-list
-
-        :type o1: _FFList
-
-        :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
-        :rtype: int
+        A Function that sort all FFI-list in ascending order of Support
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
-            return 0
+            return int(o1.item) - int(o2.item)
         else:
             return compare
 
-    def _convert(self, value) -> float:
+    def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
         :type value: int or float or str
         :return: converted value
-        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
-        :return: None
         """
         self._transactions, self._fuzzyValues = [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
@@ -362,36 +388,36 @@
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[1] = parts[1].strip()
+                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[1].split(self._sep)
-                    self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([float(x) for x in quantities])
+                    quantities = parts[2].split(self._sep)
+                    self.transactions.append([x for x in items])
+                    self.fuzzyValues.append([x for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
-                            parts[1] = parts[1].strip()
+                            parts[2] = parts[2].strip()
                             items = parts[0].split(self._sep)
-                            quantities = parts[1].split(self._sep)
+                            quantities = parts[2].split(self._sep)
                             self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([float(x) for x in quantities])
+                            self._fuzzyValues.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _mapNeighbours(self) -> None:
+    def _mapNeighbours(self):
         self._mapItemNeighbours = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
             data, items = [], []
             if self._nFile.empty:
                 print("its empty..")
             i = self._nFile.columns.values.tolist()
             if 'items' in i:
@@ -427,130 +453,102 @@
                                 neigh1.append(parts[i])
                             self._mapItemNeighbours[item] = neigh1
                 except IOError:
                     print("File Not Found")
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Frequent pattern mining process will start from here
-        :return: None
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        self._mapNeighbours()
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfFFList = []
-        mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.quantity = quantities[i]
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item[0]) is None:
-                        continue
-                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
-                        remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def mine(self) -> None:
+    def mine(self):
         """
         Frequent pattern mining process will start from here
-        :return: None
         """
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._finalPatterns = {}
         self._mapNeighbours()
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             self._dbLen += 1
             for i in range(0, len(items)):
+                regions = _Regions(int(quantities[i]), 3)
                 item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
+                else:
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemsMidSum.keys():
+                    mid = self._mapItemsMidSum[item]
+                    mid += regions.middle
+                    self._mapItemsMidSum[item] = mid
+                else:
+                    self._mapItemsMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
                 else:
-                    self._mapItemSum[item] = quantities[i]
+                    self._mapItemsHighSum[item] = regions.high
         listOfFFList = []
         mapItemsToFFLIST = {}
-        #self._minSup = self._convert(self._minSup)
-        for item1 in self._mapItemSum.keys():
+        self._minSup = self._convert(self._minSup)
+        for item1 in self._mapItemsLowSum.keys():
             item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemsMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
             if self._mapItemSum[item] >= self._minSup:
                 fuList = _FFList(item)
                 mapItemsToFFLIST[item] = fuList
                 listOfFFList.append(fuList)
         listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
         for line in range(len(self._transactions)):
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
-                pair.quantity = quantities[i]
+                regions = _Regions(int(quantities[i]), 3)
                 item = pair.item
                 if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
                 remainUtil = 0
                 for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item[0]) is None:
+                    if self._mapItemNeighbours.get(pair.item) is None:
                         continue
-                    if revisedTransaction[j].item[0] in self._mapItemNeighbours[pair.item[0]]:
+                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
                         remainUtil += revisedTransaction[j].quantity
                 remainingUtility = remainUtil
                 if mapItemsToFFLIST.get(pair.item) is not None:
                     FFListOfItem = mapItemsToFFLIST[pair.item]
                     element = _Element(tid, pair.quantity, remainingUtility)
                     FFListOfItem.addElement(element)
             tid += 1
@@ -559,121 +557,120 @@
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FSFIMining(self, prefix: List, prefixLen: int, FSFIM: List, minSup: float, itemNeighbours: List):
+    def _FSFIMining(self, prefix, prefixLen, FSFIM, minSup, itemNeighbours):
         """
         Generates FFSPMiner from prefix
 
         :param prefix: the prefix patterns of FFSPMiner
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup:int
+        :type minSup: int
         :param itemNeighbours: the set of common neighbours of prefix
         :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
             X = FSFIM[i]
             if X.sumIUtil >= minSup:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item[0]), itemNeighbours)
+            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item), itemNeighbours)
             if X.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(FSFIM)):
                     Y = FSFIM[j]
-                    if Y.item[0] in newNeighbours:
+                    if Y.item in newNeighbours:
                         exULs.append(self._construct(X, Y))
                         self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
                 self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
 
-    def _Intersection(self, neighbourX: List, neighbourY: List) -> List:
+    def _Intersection(self, neighbourX, neighbourY):
         """
         A function to get common neighbours from 2 itemSets
 
         :param neighbourX: the set of neighbours of itemSet 1
         :type neighbourX: set or list
         :param neighbourY: the set of neighbours of itemSet 2
         :type neighbourY: set or list
-        :return : set of common neighbours of 2 itemSets
-        :rtype :set
+        :return: set of common neighbours of 2 itemSets
+        :rtype: set
         """
         result = []
         if neighbourX is None or neighbourY is None:
             return result
         for i in range(0, len(neighbourX)):
             if neighbourX[i] in neighbourY:
                 result.append(neighbourX[i])
         return result
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
+    def _construct(self, px, py):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param px:the itemSet px
-        :type px:FFI-List
-        :param py:itemSet py
-        :type py:FFI-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :FFI-List
+        :param px: the itemSet px
+        :type px: FFI-List
+        :param py: itemSet py
+        :type py: FFI-List
+        :return: the itemSet of pxy(px and py)
+        :rtype: FFI-List
         """
         pxyUL = _FFList(py.item)
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
             eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
             pxyUL.addElement(eXY)
         return pxyUL
 
-    def _findElementWithTID(self, uList: _FFList, tid: int) -> _Element:
+    def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
-        :param uList:fuzzyList
-        :type uList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element tid as given
+        :param uList: fuzzyList
+        :type uList: FFI-List
+        :param tid: transaction id
+        :type tid: int
+        :return: element tid as given
         :rtype: element if exist or None
         """
         List = uList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
@@ -681,75 +678,73 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: List, prefixLen: int, item: int, sumIUtil: float) -> None:
+    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
-        :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i]) + "\t"
-        res += str(item)
+            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
+        res += str(item) + "." + str(self._mapItemRegions.get(item))
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
-    def getPatterns(self) -> Dict[str, str]:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
-        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + " : " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
         """
         print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
@@ -763,19 +758,11 @@
         if len(_ab._sys.argv) == 5:
             _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
         _ap.mine()
         print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
-        _ap = FFSPMiner('sample.txt', 'nei.txt', 1, ' ')
-        _ap.startMine()
-        _ap.mine()
-        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/FFSPMiner_old.py` & `pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,35 +1,26 @@
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
-#     .. code-block:: python
 #
-#             from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
+#             from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
 #
-#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
+#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
 #
 #             obj.mine()
 #
-#             fuzzySpatialFrequentPatterns = obj.getPatterns()
-#
-#             print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+#             print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
 #
 #             obj.save("outputFile")
 #
-#             memUSS = obj.getMemoryUSS()
-#
-#             print("Total Memory in USS:", memUSS)
-#
-#             memRSS = obj.getMemoryRSS()
+#             print("Total Memory in USS:", obj.getMemoryUSS())
 #
-#             print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", obj.getMemoryRSS())
 #
-#             run = obj.getRuntime()
-#
-#             print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", obj.getRuntime())
 #
 
 
 
 
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
@@ -45,379 +36,365 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-from PAMI.fuzzyGeoreferencedFrequentPattern.basic import abstract as _ab
+import pandas as pd
+import plotly.express as px
+import PAMI.fuzzyGeoreferencedPeriodicFrequentPattern.basic.abstract as _ab
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
 
     :Attributes:
 
-        item: int
-            the item name
-        sumIUtil: float
-            the sum of utilities of a fuzzy item in database
-        sumRUtil: float
-            the sum of resting values of a fuzzy item in database
-        elements: list
-            a list of elements contain tid,Utility and resting values of element in each transaction
-
+         item : int
+             the item name
+         sumIUtil : float
+             the sum of utilities of a fuzzy item in database
+         sumRUtil : float
+             the sum of resting values of a fuzzy item in database
+         elements : list
+             a list of elements contain tid,Utility and resting values of element in each transaction
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
-            Method to print elements            
-
+            Method to print elements
     """
 
     def __init__(self, itemName):
         self.item = itemName
+        self.isPeriodic = False
         self.sumIUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
 
     def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
-        :type element: Element
+        :param element: Element
         """
         self.sumIUtil += element.iUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
 
     def printElement(self):
         """
-        A Method to Print elements in the FFList
+        A Method to Print elements in the FFList object
         """
         for ele in self.elements:
             print(ele.tid, ele.iUtils, ele.rUtils)
 
 
 class _Element:
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils: float
+        iUtils : float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
         self.iUtils = iUtil
         self.rUtils = rUtil
 
 
-class _Regions:
-    """
-    A class calculate the regions
-
-    :Attributes:
-
-        low : int
-            low region value
-        middle: int
-            middle region value
-        high : int
-            high region values
-    """
-
-    def __init__(self, quantity, regionsNumber):
-        self.low = 0
-        self.middle = 0
-        self.high = 0
-        if regionsNumber == 3:  # if we have 3 regions
-            if 0 < quantity <= 1:
-                self.low = 1
-                self.high = 0
-                self.middle = 0
-            elif 1 < quantity <= 6:
-                self.low = float((6 - quantity) / 5)
-                self.middle = float((quantity - 1) / 5)
-                self.high = 0
-            elif 6 < quantity <= 11:
-                self.low = 0
-                self.middle = float((11 - quantity) / 5)
-                self.high = float((quantity - 6) / 5)
-            else:
-                self.low = 0
-                self.middle = 0
-                self.high = 1
-
-
 class _Pair:
     """
     A class to store item and it's quantity together
     """
 
     def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
-class FFSPMiner(_ab._fuzzySpatialFrequentPatterns):
+class FGPFPMiner(_ab._fuzzySpatialFrequentPatterns):
     """
     :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
                     which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
                     techniques to reduce the search space.
 
-    Reference:   Reference: P. Veena, B. S. Chithra, R. U. Kiran, S. Agarwal and K. Zettsu, "Discovering Fuzzy Frequent
-                 Spatial Patterns in Large Quantitative Spatiotemporal databases," 2021 IEEE International Conference on Fuzzy Systems
-                 (FUZZ-IEEE), 2021, pp. 1-8, doi: 10.1109/FUZZ45933.2021.9494594.
+    :Reference:
+
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
     :param maxPer: float :
                    The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
     :param nFile: str :
                    Name of the input file to mine complete set of frequent patterns
+    :param  FuzFile: str :
+                   The user can specify fuzFile.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
-        neighbors: map
+        neighbors : map
             keep track of neighbours of elements
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        itemsCnt: int
+        itemsCnt : int
             To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
+        mapItemSum : map
             To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
+        mapItemRegions : map
             To Keep track of fuzzy regions of item
-        jointCnt: int
+        joinsCnt : int
             To keep track of the number of FFI-list that was constructed
-        BufferSize: int
+        BufferSize : int
             represent the size of Buffer
-        itemBuffer list
+        itemSetBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function            
-        convert(value):
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        convert(value)
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
         findElementWithTID(uList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
-    
+
     **Executing the code on terminal :**
     ----------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FFSPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <sep>
+      (.venv) $ python3 FGPFPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <maxPer> <sep>
 
       Example Usage:
 
-      (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
-
-      (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 0.3
-
-      (.venv) $ python3  FFSPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3
+      (.venv) $ python3  FGPFPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3 4
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Sample run of importing the code:**
-    ----------------------------------------
-        
-            from PAMI.fuzzyGeoreferencedFrequentPattern import FFSPMiner as alg
-
-            obj = alg.FFSPMiner("input.txt", "neighbours.txt", 2)
-
-            obj.mine()
-
-            fuzzySpatialFrequentPatterns = obj.getPatterns()
+    ------------------------------------------
 
-            print("Total number of fuzzy frequent spatial patterns:", len(fuzzySpatialFrequentPatterns))
+        from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
 
-            obj.save("outputFile")
+        obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
 
-            memUSS = obj.getMemoryUSS()
+        obj.mine()
 
-            print("Total Memory in USS:", memUSS)
+        print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
 
-            memRSS = obj.getMemoryRSS()
+        obj.save("outputFile")
 
-            print("Total Memory in RSS", memRSS)
+        print("Total Memory in USS:", obj.getMemoryUSS())
 
-            run = obj.getRuntime()
+        print("Total Memory in RSS", obj.getMemoryRSS())
 
-            print("Total ExecutionTime in seconds:", run)
+        print("Total ExecutionTime in seconds:", obj.getRuntime())
 
     **Credits:**
-    ---------------
-            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+    ----------------
+            The complete program was written by B.Sai Chitra and Kundai Kwangwari under the supervision of Professor Rage Uday Kiran.
     """
-    
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
+    _FuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = "\t"
-    _transactions = []
-    _fuzzyValues = []
+    _transactionsDB = []
+    _fuzzyValuesDB = []
 
-    def __init__(self, iFile, nFile, minSup, sep="\t"):
-        super().__init__(iFile, nFile, minSup, sep)
+    def __init__(self, iFile, nFile, FuzFile, minSup, maxPer, sep):
+        super().__init__(iFile, nFile, FuzFile, minSup, maxPer, sep)
         self._mapItemNeighbours = {}
         self._startTime = 0
         self._endTime = 0
         self._itemsCnt = 0
-        self._mapItemsLowSum = {}
-        self._mapItemsMidSum = {}
-        self._mapItemsHighSum = {}
+        self._itemSupData = {}
         self._mapItemSum = {}
+        self._finalClosedPeriodicPatterns = {}
         self._mapItemRegions = {}
+        self._fuzzyRegionReferenceMap = {}
         self._joinsCnt = 0
         self._BufferSize = 200
         self._itemSetBuffer = []
         self._finalPatterns = {}
+        self._finalPeriodicPatterns = {}
+        self._tidList = {}
         self._dbLen = 0
+        self._regionsNumber = 0
+        self._RegionsCal = []
+        self._RegionsLabel = []
+        self._LabelKey = {}
 
     def _compareItems(self, o1, o2):
         """
         A Function that sort all FFI-list in ascending order of Support
+
+        :param o1: First FFI-list
+        :type o1: _FFList
+        :param o2: Second FFI-list
+        :type o2: _FFList
+        :return: Comparison Value
+        :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
-    def _convert(self, value):
+    def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
         :type value: int or float or str
         :return: converted value
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
-                value = (self._dbLen * value)
+                value = float(value)
             else:
                 value = int(value)
         return value
 
+    def _fuzzyMembershipFunc(self):
+        try:
+            with open(self._FuzFile, 'r', encoding='utf-8') as f:
+                count = 0
+                for line in f:
+                    line = line.split("\n")[0]
+                    parts = line.split(" ")
+                    lowerBound = parts[0].strip()
+                    upperBound = parts[1].strip()
+                    lb_Label = parts[2].strip()
+                    ub_Label = parts[3].strip()
+                    self._RegionsCal.append([int(lowerBound), int(upperBound)])
+                    self._RegionsLabel.append([lb_Label, ub_Label])
+                    for i in range(0, 2):
+                        if lb_Label.capitalize() not in self._LabelKey:
+                            self._LabelKey[lb_Label.capitalize()] = count
+                            count += 1
+                        if ub_Label.capitalize() not in self._LabelKey:
+                            self._LabelKey[ub_Label.capitalize()] = count
+                            count += 1
+        except IOError:
+            print("File Not Found")
+            quit()
+
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-        self._transactions, self._fuzzyValues = [], []
+        self._transactionsDB, self._fuzzyValuesDB = [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.transactions = self._iFile['Transactions'].tolist()
+                self._transactionsDB = self._iFile['Transactions'].tolist()
             if 'fuzzyValues' in i:
-                self.fuzzyValues = self._iFile['Utilities'].tolist()
+                self._fuzzyValuesDB = self._iFile['fuzzyValues'].tolist()
 
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
                     quantities = parts[2].split(self._sep)
-                    self.transactions.append([x for x in items])
-                    self.fuzzyValues.append([x for x in quantities])
+                    self._transactionsDB.append([x for x in items])
+                    self._fuzzyValuesDB.append([x for x in quantities])
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
                             parts[2] = parts[2].strip()
                             items = parts[0].split(self._sep)
                             quantities = parts[2].split(self._sep)
-                            self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([x for x in quantities])
+                            self._transactionsDB.append([x for x in items])
+                            self._fuzzyValuesDB.append([x for x in quantities])
                 except IOError:
                     print("File Not Found")
                     quit()
 
     def _mapNeighbours(self):
+        """
+        A function to map items to their Neighbours
+        """
         self._mapItemNeighbours = {}
         if isinstance(self._nFile, _ab._pd.DataFrame):
             data, items = [], []
             if self._nFile.empty:
                 print("its empty..")
             i = self._nFile.columns.values.tolist()
             if 'items' in i:
@@ -449,204 +426,149 @@
                             parts = [x for x in parts]
                             item = parts[0]
                             neigh1 = []
                             for i in range(1, len(parts)):
                                 neigh1.append(parts[i])
                             self._mapItemNeighbours[item] = neigh1
                 except IOError:
+                    print(self._nFile)
                     print("File Not Found")
                     quit()
 
+    def _Regions(self, quantity):
+        """
+        param quantity:
+        type quantity:
+        """
+
+        self._list = [0] * len(self._LabelKey)
+        if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
+            self._list[0] = 1
+            return
+        elif quantity >= self._RegionsCal[-1][0]:
+            self._list[-1] = 1
+            return
+        else:
+            for i in range(1, len(self._RegionsCal) - 1):
+                if self._RegionsCal[i][0] < quantity <= self._RegionsCal[i][1]:
+                    base = self._RegionsCal[i][1] - self._RegionsCal[i][0]
+                    for pos in range(0, 2):
+                        if self._RegionsLabel[i][pos].islower():
+                            self._list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
+                                (self._RegionsCal[i][1] - quantity) / base)
+                        else:
+                            self._list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
+                                (quantity - self._RegionsCal[i][0]) / base)
+        return
+
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        self._mapNeighbours()
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                regions = _Regions(int(quantities[i]), 3)
-                item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
-                else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfFFList = []
-        mapItemsToFFLIST = {}
-        self._minSup = self._convert(self._minSup)
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                regions = _Regions(int(quantities[i]), 3)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item) is None:
-                        continue
-                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
-                        remainUtil += revisedTransaction[j].quantity
-                remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
+        self._mapNeighbours()
         self._creatingItemSets()
+        self._fuzzyMembershipFunc()
         self._finalPatterns = {}
-        self._mapNeighbours()
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
+        recent_occur = {}
+        for line in range(len(self._transactionsDB)):
+            item_list = self._transactionsDB[line]
+            fuzzyValues_list = self._fuzzyValuesDB[line]
             self._dbLen += 1
-            for i in range(0, len(items)):
-                regions = _Regions(int(quantities[i]), 3)
-                item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemsMidSum.keys():
-                    mid = self._mapItemsMidSum[item]
-                    mid += regions.middle
-                    self._mapItemsMidSum[item] = mid
+            """
+            This section below is for:
+            1.Finding the support of each item's region in the entire database
+            2.Finding the periodic patterns of the data
+            3.Trimming off the patterns whose support is less than minSupport
+            """
+            for i in range(0, len(item_list)):
+                item = item_list[i]
+                if item in self._tidList:
+                    self._tidList[item].append(self._dbLen - recent_occur[item][-1])
+                    recent_occur[item].append(self._dbLen)
                 else:
-                    self._mapItemsMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
+                    self._tidList[item] = [self._dbLen]
+                    recent_occur[item] = [self._dbLen]
+                fuzzy_ref = fuzzyValues_list[i]
+                if item in self._mapItemNeighbours:
+                    if fuzzy_ref not in self._fuzzyRegionReferenceMap:
+                        self._Regions(int(fuzzy_ref))
+                        self._fuzzyRegionReferenceMap[fuzzy_ref] = self._list
+
+                    if item in self._itemSupData.keys():
+                        self._itemSupData[item] = [sum(i) for i in zip(self._itemSupData[item],
+                                                                       self._fuzzyRegionReferenceMap[fuzzy_ref])]
+                    else:
+                        self._itemSupData[item] = self._fuzzyRegionReferenceMap[fuzzy_ref]
+
+        for item in self._tidList.keys():
+            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
+        del recent_occur
+        """
+        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
+        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
+        Step2. prune out all items whose regional support is less than the given minSup
+        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
+        """
+
         listOfFFList = []
         mapItemsToFFLIST = {}
+        region_label = []
+        for i in range(0, len(self._RegionsLabel)):
+            if self._RegionsLabel[i][1] not in region_label:
+                region_label.append(str(self._RegionsLabel[i][1]))
+
         self._minSup = self._convert(self._minSup)
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemsMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
-            if self._mapItemSum[item] >= self._minSup:
+        for item in self._itemSupData.keys():
+            if max(self._itemSupData[item]) >= self._minSup:
+                self._mapItemSum[item] = max(self._itemSupData[item])
+                self._mapItemRegions[item] = region_label[self._itemSupData[item].index(self._mapItemSum[item])]
                 fuList = _FFList(item)
+                if int(self._maxPer) >= max(self._tidList[item]):
+                    fuList.isPeriodic = True
                 mapItemsToFFLIST[item] = fuList
                 listOfFFList.append(fuList)
+
+        del self._itemSupData
+        del self._tidList
         listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
+        for j in range(len(self._transactionsDB)):
+            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
             revisedTransaction = []
-            for i in range(0, len(items)):
+            for i in range(0, len(item_list)):
                 pair = _Pair()
-                pair.item = items[i]
-                regions = _Regions(int(quantities[i]), 3)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
+                pair.item = item_list[i]
+                fuzzy_ref = str(self._fuzzyValuesDB[j][self._transactionsDB[j].index(pair.item)])
+                pair.quantity = self._fuzzyRegionReferenceMap[fuzzy_ref][
+                    region_label.index(self._mapItemRegions[pair.item])]
+                if pair.quantity > 0:
+                    revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            qaunt = {}
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
+                qaunt[pair.item] = pair.quantity
                 remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i, -1):
-                    if self._mapItemNeighbours.get(pair.item) is None:
-                        continue
-                    if revisedTransaction[j].item in self._mapItemNeighbours[pair.item]:
-                        remainUtil += revisedTransaction[j].quantity
+                temp = list(set(self._mapItemNeighbours[pair.item]).intersection(set(qaunt.keys())))
+                for j in temp:
+                    remainUtil += float(qaunt[j])
+                del temp
                 remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity, remainingUtility)
-                    FFListOfItem.addElement(element)
+                FFListObject = mapItemsToFFLIST[pair.item]
+                element = _Element(tid, pair.quantity, remainingUtility)
+                FFListObject.addElement(element)
+            del qaunt
             tid += 1
         itemNeighbours = list(self._mapItemNeighbours.keys())
         self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
@@ -660,43 +582,43 @@
         :param prefix: the prefix patterns of FFSPMiner
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param FSFIM: the Fuzzy list of prefix itemSets
         :type FSFIM: list
         :param minSup: the minimum support of
-        :type minSup:int
+        :type minSup: int
         :param itemNeighbours: the set of common neighbours of prefix
         :type itemNeighbours: list or set
         """
         for i in range(0, len(FSFIM)):
-            X = FSFIM[i]
-            if X.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
-            newNeighbours = self._Intersection(self._mapItemNeighbours.get(X.item), itemNeighbours)
-            if X.sumRUtil >= minSup:
+            _FFListObject1 = FSFIM[i]
+            if _FFListObject1.sumIUtil >= minSup:
+                self._WriteOut(prefix, prefixLen, _FFListObject1, _FFListObject1.sumIUtil)
+            newNeighbourList = self._Intersection(self._mapItemNeighbours.get(_FFListObject1.item), itemNeighbours)
+            if _FFListObject1.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(FSFIM)):
-                    Y = FSFIM[j]
-                    if Y.item in newNeighbours:
-                        exULs.append(self._construct(X, Y))
+                    _FFListObject2 = FSFIM[j]
+                    if _FFListObject2.item in newNeighbourList:
+                        exULs.append(self._construct(_FFListObject1, _FFListObject2))
                         self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbours)
+                self._itemSetBuffer.insert(prefixLen, _FFListObject1.item)
+                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbourList)
 
     def _Intersection(self, neighbourX, neighbourY):
         """
         A function to get common neighbours from 2 itemSets
 
         :param neighbourX: the set of neighbours of itemSet 1
         :type neighbourX: set or list
         :param neighbourY: the set of neighbours of itemSet 2
         :type neighbourY: set or list
-        :return : set of common neighbours of 2 itemSets
-        :rtype :set
+        :return: set of common neighbours of 2 itemSets
+        :rtype: set
         """
         result = []
         if neighbourX is None or neighbourY is None:
             return result
         for i in range(0, len(neighbourX)):
             if neighbourX[i] in neighbourY:
                 result.append(neighbourX[i])
@@ -721,49 +643,63 @@
         """
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px, py):
+    def _construct(self, _FFListObject1, _FFListObject2):
         """
         A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
 
-        :param px:the itemSet px
-        :type px:FFI-List
-        :param py:itemSet py
-        :type py:FFI-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :FFI-List
-        """
-        pxyUL = _FFList(py.item)
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
+        :param _FFListObject1: the itemSet px
+        :type _FFListObject1: FFI-List
+        :param _FFListObject2: itemSet py
+        :type _FFListObject2: FFI-List
+        :return: the itemSet of pxy(px and py)
+        :rtype: FFI-List
+        """
+        recent_occur, first_occur, tid = 0, 0, 0
+        periodlist = []
+        _newFFListObject = _FFList(_FFListObject2.item)
+        for Ob1Element in _FFListObject1.elements:
+            Ob2Element = self._findElementWithTID(_FFListObject2, Ob1Element.tid)
+            if Ob2Element is None:
                 continue
-            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
-            pxyUL.addElement(eXY)
-        return pxyUL
+            tid = Ob1Element.tid
+            if len(periodlist) == 0:
+                periodlist.append(abs(first_occur - tid))
+                recent_occur = tid
+            else:
+                periodlist.append(tid - recent_occur)
+                recent_occur = tid
+            newElement = _Element(Ob1Element.tid, min([Ob1Element.iUtils, Ob2Element.iUtils], key=lambda x: float(x)),
+                                  Ob2Element.rUtils)
+            _newFFListObject.addElement(newElement)
+
+        if periodlist and int(self._maxPer) >= max(periodlist):
+            _newFFListObject.isPeriodic = True
+        else:
+            _newFFListObject.isPeriodic = False
+        return _newFFListObject
 
     def _findElementWithTID(self, uList, tid):
         """
         To find element with same tid as given
 
-        :param uList:fuzzyList
-        :type uList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element tid as given
+        :param uList: fuzzyList
+        :type uList: FFI-List
+        :param tid: transaction id
+        :type tid: int
+        :return: element tid as given
         :rtype: element if exist or None
         """
         List = uList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
@@ -771,91 +707,154 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix, prefixLen, item, sumIUtil):
+    def _WriteOut(self, prefix, prefixLen, _FFListObject, sumIUtil):
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
-        :param item: the last item
-        :type item: int
+        :param _FFListObject: the last item
+        :type _FFListObject: int
         :param sumIUtil: sum of utility of itemSet
         :type sumIUtil: float
         """
+        item = _FFListObject.item
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
             res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
         res += str(item) + "." + str(self._mapItemRegions.get(item))
         res1 = str(sumIUtil)
         self._finalPatterns[res] = res1
 
+        if _FFListObject.isPeriodic:
+            self._finalPeriodicPatterns[res] = res1
+
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
-        for a, b in self._finalPatterns.items():
+        for a, b in self._finalPeriodicPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataFrame
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return self._finalPeriodicPatterns
 
     def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
+        keylist = (self._finalPatterns.keys())
         writer = open(self.oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + " : " + str(y)
+        for x in keylist:
+            patternsAndSupport = x.strip() + ":" + str(self._finalPatterns[x])
             writer.write("%s \n" % patternsAndSupport)
 
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Spatial Fuzzy Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Spatial Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
+    def getPatternsAsDataframe(self):
+
+        """
+        :return: returning periodic frequent patterns in a dataframe
+        :rtype: pd.DataFrame
+        """
+
+        data = []
+        dataFrame = _ab._pd.DataFrame()
+        for a, b in self._finalPeriodicPatterns.items():
+            data.append([a, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
+
+    def generateLatexCode(self, result):
+
+        titles = result.columns.tolist()
+        titles.remove("minsup")
+        titles.remove("algorithm")
+        for i in range(0, len(titles)):
+            legendary = pd.unique(result[['algorithm']].values.ravel())
+            color = ['red', 'blue', 'green', 'black', 'yellow']
+            xaxis = result["minsup"].values.tolist()
+            yaxis = result[titles[i]].values.tolist()
+            algo = result["algorithm"].values.tolist()
+            x_label = "minsup"
+            filename = titles[i]
+            latexwriter = open(filename + "Latexfile.tex", "w")
+            latexwriter.write("")
+            latexwriter.write("\\begin{axis}[\n\txlabel={\\Huge{" + x_label + "}},")
+            latexwriter.write("\n\tylabel={\\Huge{" + titles[i] + "}},")
+            latexwriter.write("\n\txmin=" + str(min(xaxis)) + ", xmax=" + str(max(xaxis)) + ",")
+
+            for num in range(0, len(legendary)):
+                latexwriter.write("\n\\addplot+  [" + color[num] + "]\n\tcoordinates {\n")
+                for num2 in range(0, len(xaxis)):
+                    if (legendary[num] == algo[num2]):
+                        latexwriter.write("(" + str(xaxis[num2]) + "," + str(yaxis[num2]) + ")\n")
+                latexwriter.write("\t};   \\addlegendentry{" + legendary[num] + "}\n")
+                if (num + 1 == len(legendary)):
+                    latexwriter.write("\\end{axis}")
+        print("Latex file generated successfully")
+
+    def generateGraphs(result):
+
+        fig = px.line(result, x='minsup', y='patterns', color='algorithm', title='Patterns)', markers=True)
+        fig.show()
+        fig = px.line(result, x='minsup', y='runtime', color='algorithm', title='Runtime)', markers=True)
+        fig.show()
+        fig = px.line(result, x='minsup', y='memoryUSS', color='algorithm', title='MemoryUSS)', markers=True)
+        fig.show()
+        fig = px.line(result, x='minsup', y='memoryRSS', color='algorithm', title='MemoryRSS)', markers=True)
+        fig.show()
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 7:
         if len(_ab._sys.argv) == 6:
-            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5],
+                             _ab._sys.argv[6])
         if len(_ab._sys.argv) == 5:
-            _ap = FFSPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Spatial Fuzzy Frequent  Patterns:", len(_ap.getPatterns()))
+        print("Total number of Spatial Fuzzy Periodic Frequent  Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS",  _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        _ap.save("outputfile.txt")
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
+
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/fuzzyGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,27 +46,27 @@
                     employ in PAMI
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup: integer or float or str
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime:float
+        startTime : float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py` & `pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner.py`

 * *Files 4% similar despite different names*

```diff
@@ -3,14 +3,24 @@
 # techniques to reduce the search space.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #             from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
 #
+#             minSup = str()
+#
+#             maxPer = float()
+#
+#             iFile = " "
+#
+#             nFile = " "
+#
+#             sep = "\t"
+
 #             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
 #
 #             obj.mine()
 #
 #             print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
 #
 #             obj.save("outputFile")
@@ -20,15 +30,14 @@
 #             print("Total Memory in RSS", obj.getMemoryRSS())
 #
 #             print("Total ExecutionTime in seconds:", obj.getRuntime())
 #
 
 
 
-
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -48,23 +57,24 @@
 import PAMI.fuzzyGeoreferencedPeriodicFrequentPattern.basic.abstract as _ab
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
+
     :Attributes:
 
-         item: int
+         item : int
              the item name
-         sumIUtil: float
+         sumIUtil : float
              the sum of utilities of a fuzzy item in database
-         sumRUtil: float
+         sumRUtil : float
              the sum of resting values of a fuzzy item in database
-         elements: list
+         elements : list
              a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
@@ -101,15 +111,15 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils: float
+        iUtils : float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the neighbourhood resting value of a fuzzy item in the transaction
     """
 
     def __init__(self, tid, iUtil, rUtil):
         self.tid = tid
@@ -125,14 +135,17 @@
     def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
 class FGPFPMiner(_ab._fuzzySpatialFrequentPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
                     which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
                     techniques to reduce the search space.
          
     :Reference:
 
     :param  iFile: str :
@@ -155,88 +168,80 @@
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
             Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
-        neighbors: map
+        neighbors : map
             keep track of neighbours of elements
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        itemsCnt: int
+        itemsCnt : int
             To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
+        mapItemSum : map
             To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
+        joinsCnt : int
             To keep track of the number of FFI-list that was constructed
-        BufferSize: int
+        BufferSize : int
             represent the size of Buffer
-        itemBuffer list
+        itemSetBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
+        convert(value)
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         Intersection(neighbourX,neighbourY)
             Return common neighbours of 2 itemSet Neighbours
         findElementWithTID(uList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
 
-    **Executing the code on terminal :**
-    --------------------------------------------
+    Execution methods
+    =================-
 
     .. code-block:: console
 
       Format:
 
       (.venv) $ python3 FGPFPMiner.py <inputFile> <outputFile> <neighbours> <minSup> <maxPer> <sep>
 
       Example Usage:
 
       (.venv) $ python3  FGPFPMiner.py sampleTDB.txt output.txt sampleN.txt 3 4
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
-    **Sample run of importing the code:**
-    ----------------------------------------
+    **Calling from a python program**
+
     .. code-block:: python
     
         from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
         
         obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
         
         obj.mine()
@@ -247,34 +252,25 @@
         
         print("Total Memory in USS:", obj.getMemoryUSS())
         
         print("Total Memory in RSS", obj.getMemoryRSS())
         
         print("Total ExecutionTime in seconds:", obj.getRuntime())
     
-    **Credits:**
-    ----------------
-            The complete program was written by B.Sai Chitra and Kundai Kwangwari under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
+            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
     """
 
-    _startTime = float()
-    _endTime = float()
     _minSup = str()
     _maxPer = float()
-    _finalPatterns = {}
     _iFile = " "
-    _oFile = " "
     _nFile = " "
-    _FuzFile = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
     _sep = "\t"
-    _transactionsDB = []
-    _fuzzyValuesDB = []
-    _ts = []
+
 
     def __init__(self, iFile, nFile, minSup, maxPer, sep):
         super().__init__(iFile, nFile, minSup, maxPer, sep)
         self._mapItemNeighbours = {}
         self._startTime = 0
         self._endTime = 0
         self._itemsCnt = 0
@@ -289,41 +285,33 @@
         self._dbLen = 0
 
     def _compareItems(self, o1, o2) -> int:
         """
         A Function that sort all FFI-list in ascending order of Support
 
         :param o1: First FFI-list
-
         :type o1: _FFList
-
         :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
+        :type o2: _FFList
+        :return: Comparison Value
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
     def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
-
         :type value: int or float or str
-
         :return: converted value
-
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
@@ -424,106 +412,15 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._mapNeighbours()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        recent_occur = {}
-        for line in range(len(self._transactionsDB)):
-            item_list = self._transactionsDB[line]
-            fuzzyValues_list = self._fuzzyValuesDB[line]
-            ts = self._ts[line]
-            self._dbLen += 1
-            """
-            The section below is for:
-            1.Finding the support of each item's region in the entire database
-            2.Finding the periodic patterns of the data
-            3.Trimming off the patterns whose support is less than minSupport
-            """
-            for i in range(0, len(item_list)):
-                item = item_list[i]
-                if item in self._tidList:
-                    self._tidList[item].append(ts - recent_occur[item][-1])
-                    recent_occur[item].append(ts)
-                else:
-                    self._tidList[item] = [ts]
-                    recent_occur[item] = [ts]
-                fuzzy_ref = fuzzyValues_list[i]
-                if item[0] in self._mapItemNeighbours:
-                    if item in self._itemSupData.keys():
-                        self._itemSupData[item] += fuzzy_ref
-                    else:
-                        self._itemSupData[item] = fuzzy_ref
-        for item in self._tidList.keys():
-            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
-        del recent_occur
-        """
-        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
-        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
-        Step2. prune out all items whose regional support is less than the given minSup
-        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
-        """
-
-        listOfFFList = []
-        mapItemsToFFLIST = {}
-        region_label = []
-        #self._minSup = self._convert(self._minSup)
-        for item in self._itemSupData.keys():
-            if self._itemSupData[item] >= self._minSup:
-                self._mapItemSum[item] = self._itemSupData[item]
-                fuList = _FFList(item)
-                if int(self._maxPer) >= max(self._tidList[item]):
-                    fuList.isPeriodic = True
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-        del self._itemSupData
-        del self._tidList
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for j in range(len(self._transactionsDB)):
-            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
-            fuzzy_list = [self._fuzzyValuesDB[j][i] for i in range(len(self._fuzzyValuesDB[j])) if self._transactionsDB[j][i] in self._mapItemSum.keys()]
-            revisedTransaction = []
-            for i in range(0, len(item_list)):
-                pair = _Pair()
-                pair.item = item_list[i]
-                fuzzy_ref = fuzzy_list[i]
-                pair.quantity = fuzzy_ref
-                if pair.quantity > 0:
-                    revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            qaunt = {}
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                qaunt[pair.item[0]] = pair.quantity
-                remainUtil = 0
-                temp = list(set(self._mapItemNeighbours[pair.item[0]]).intersection(set(qaunt.keys())))
-                # print(temp, self._mapItemNeighbours[pair.item[0]], qaunt)
-                for j in temp:
-                    remainUtil += float(qaunt[j])
-                del temp
-                remainingUtility = remainUtil
-                FFListObject = mapItemsToFFLIST[pair.item]
-                element = _Element(tid, pair.quantity, remainingUtility)
-                FFListObject.addElement(element)
-            del qaunt
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._mapNeighbours()
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/FGPFPMiner_old.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py`

 * *Files 19% similar despite different names*

```diff
@@ -1,27 +1,41 @@
+# Top K Spatial High Utility ItemSet Mining (TKSHUIM) aims to discover Top-K Spatial High Utility Itemsets
+# (TKSHUIs) in a spatioTemporal database
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
 #
-#             obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
+#             from PAMI.highUtilitySpatialPattern.topk import TKSHUIM as alg
+#
+#             obj=alg.TKSHUIM("input.txt","Neighbours.txt",35)
 #
 #             obj.mine()
 #
-#             print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
+#             Patterns = obj.getPatterns()
+#
+#             print("Total number of  Patterns:", len(Patterns))
+#
+#             obj.save("output")
+#
+#             Df = obj.getPatternsAsDataFrame()
+#
+#             memUSS = obj.getMemoryUSS()
 #
-#             obj.save("outputFile")
+#             print("Total Memory in USS:", memUSS)
 #
-#             print("Total Memory in USS:", obj.getMemoryUSS())
+#             memRSS = obj.getMemoryRSS()
 #
-#             print("Total Memory in RSS", obj.getMemoryRSS())
+#             print("Total Memory in RSS", memRSS)
 #
-#             print("Total ExecutionTime in seconds:", obj.getRuntime())
+#             run = obj.getRuntime()
 #
+#             print("Total ExecutionTime in seconds:", run)
+#
+
 
 
 
 
 __copyright__ = """
 Copyright (C)  2021 Rage Uday Kiran
 
@@ -36,939 +50,904 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-import pandas as pd
-import plotly.express as px
-import PAMI.fuzzyGeoreferencedPeriodicFrequentPattern.basic.abstract as _ab
-from deprecated import deprecated
 
+from PAMI.highUtilitySpatialPattern.topk.abstract import *
+from functools import cmp_to_key
+import heapq
+from deprecated import deprecated
 
-class _FFList:
+class Transaction:
     """
-    A class represent a Fuzzy List of an element
+    A class to store Transaction of a database
 
     :Attributes:
 
-         item: int
-             the item name
-         sumIUtil: float
-             the sum of utilities of a fuzzy item in database
-         sumRUtil: float
-             the sum of resting values of a fuzzy item in database
-         elements: list
-             a list of elements contain tid,Utility and resting values of element in each transaction
+        items: list
+            A list of items in transaction 
+        utilities: list
+            A list of utilites of items in transaction
+        transactionUtility: int
+            represent total sum of all utilities in the database
+        pmus: list
+            represent the pmu (probable maximum utility) of each element in the transaction
+        prefixutility:
+            prefix Utility values of item
+        offset:
+            an offset pointer, used by projected transactions
+
     :Methods:
 
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
-        printElement(e)
-            Method to print elements
+        projectedTransaction(offsetE):
+            A method to create new Transaction from existing till offsetE
+        getItems():
+            return items in transaction
+        getUtilities():
+            return utilities in transaction
+        getPmus():
+            return pmus in transaction
+        getLastPosition():
+            return last position in a transaction
+        removeUnpromisingItems():
+            A method to remove items with low Utility than minUtil
+        insertionSort():
+            A method to sort all items in the transaction
     """
+    offset = 0
+    prefixUtility = 0
+    
+    def __init__(self, items, utilities, transactionUtility, pmus=None):
+        self.items = items
+        self.utilities = utilities
+        self.transactionUtility = transactionUtility
+        if pmus is not None:
+            self.pmus = pmus
 
-    def __init__(self, itemName):
-        self.item = itemName
-        self.isPeriodic = False
-        self.sumIUtil = 0.0
-        self.sumRUtil = 0.0
-        self.elements = []
+    def projectTransaction(self, offsetE):
+        """
+        A method to create new Transaction from existing till offsetE
 
-    def addElement(self, element):
+        :param offsetE: an offset over the original transaction for projecting the transaction
+        :type offsetE: int
         """
-        A Method that add a new element to FFList
+        new_transaction = Transaction(self.items, self.utilities, self.transactionUtility)
+        utilityE = self.utilities[offsetE]
+        new_transaction.prefixUtility = self.prefixUtility + utilityE
+        new_transaction.transactionUtility = self.transactionUtility - utilityE
+        for i in range(self.offset, offsetE):
+            new_transaction.transactionUtility -= self.utilities[i]
+        new_transaction.offset = offsetE + 1
+        return new_transaction
 
-        :param element: an element to be added to FFList
-        :param element: Element
+    def getItems(self):
         """
-        self.sumIUtil += element.iUtils
-        self.sumRUtil += element.rUtils
-        self.elements.append(element)
+        A method to return items in transaction
+        """
+        return self.items
 
-    def printElement(self):
+    def getPmus(self):
         """
-        A Method to Print elements in the FFList object
+        A method to return pmus in transaction
         """
-        for ele in self.elements:
-            print(ele.tid, ele.iUtils, ele.rUtils)
+        return self.pmus
 
+    def getUtilities(self):
+        """
+        A method to return utilities in transaction
+        """
+        return self.utilities
 
-class _Element:
-    """
-    A class represents an Element of a fuzzy list
+    # get the last position in this transaction
+    def getLastPosition(self):
+        """
+        A method to return last position in a transaction
+        """
+        return len(self.items) - 1
+
+    def removeUnpromisingItems(self, oldNamesToNewNames):
+        """
+        A method to remove items with low Utility than minUtil
+
+        :param oldNamesToNewNames: A map represent old namses to new names
+        :type oldNamesToNewNames: map
+        """
+        tempItems = []
+        tempUtilities = []
+        for idx, item in enumerate(self.items):
+            if item in oldNamesToNewNames:
+                tempItems.append(oldNamesToNewNames[item])
+                tempUtilities.append(self.utilities[idx])
+            else:
+                self.transactionUtility -= self.utilities[idx]
+        self.items = tempItems
+        self.utilities = tempUtilities
+        self.insertionSort()
+
+    def insertionSort(self):
+        """
+        A method to sort items in order
+        """
+        for i in range(1, len(self.items)):
+            key = self.items[i]
+            utilityJ = self.utilities[i]
+            j = i - 1
+            while j >= 0 and key < self.items[j]:
+                self.items[j + 1] = self.items[j]
+                self.utilities[j + 1] = self.utilities[j]
+                j -= 1
+            self.items[j + 1] = key
+            self.utilities[j + 1] = utilityJ
 
-    :Attributes:
 
-        tid : int
-            keep tact of transaction id
-        iUtils: float
-            the utility of a fuzzy item in the transaction
-        rUtils : float
-            the neighbourhood resting value of a fuzzy item in the transaction
+class Dataset:
     """
+    A class represent the list of transactions in this dataset
 
-    def __init__(self, tid, iUtil, rUtil):
-        self.tid = tid
-        self.iUtils = iUtil
-        self.rUtils = rUtil
+    :Attributes:
 
+    transactions:
+        the list of transactions in this dataset
+    maxItem:
+        the largest item name
+        
+    :methods:
+
+        createTransaction(line):
+            Create a transaction object from a line from the input file
+        getMaxItem():
+            return Maximum Item
+        getTransactions():
+            return transactions in database
 
-class _Pair:
-    """
-    A class to store item and it's quantity together
     """
+    transactions = []
+    maxItem = 0
+    
+    def __init__(self, datasetpath, sep):
+        self.strToint = {}
+        self.intTostr = {}
+        self.cnt = 1
+        self.sep = sep
+        with open(datasetpath, 'r') as f:
+            lines = f.readlines()
+            for line in lines:
+                self.transactions.append(self.createTransaction(line))
+        f.close()
+
+    def createTransaction(self, line):
+        """
+        A method to create Transaction from dataset given
+
+        :param line: represent a single line of database
+        :type line: string
+        :return : Transaction.
+        :rtype: int
+        """
+        trans_list = line.strip().split(':')
+        transactionUtility = int(trans_list[1])
+        itemsString = trans_list[0].strip().split(self.sep)
+        utilityString = trans_list[2].strip().split(self.sep)
+        if (len(trans_list) == 4):
+            pmuString = trans_list[3].strip().split(self.sep)
+        items = []
+        utilities = []
+        pmus = []
+        for idx, item in enumerate(itemsString):
+            if (self.strToint).get(item) is None:
+                self.strToint[item] = self.cnt
+                self.intTostr[self.cnt] = item
+                self.cnt += 1
+            item_int = self.strToint.get(item)
+            if item_int > self.maxItem:
+                self.maxItem = item_int
+            items.append(item_int)
+            utilities.append(int(utilityString[idx]))
+            if (len(trans_list) == 4):
+                pmus.append(int(pmuString[idx]))
+        return Transaction(items, utilities, transactionUtility, pmus)
+
+    def getMaxItem(self):
+        """
+        A method to return name of the largest item
+        """
+        return self.maxItem
 
-    def __init__(self):
-        self.item = 0
-        self.quantity = 0
+    def getTransactions(self):
+        """
+        A method to return transactions from database
+        """
+        return self.transactions
 
 
-class FGPFPMiner(_ab._fuzzySpatialFrequentPatterns):
+class TKSHUIM(utilityPatterns):
     """
-    :Description:   Fuzzy Frequent Spatial Pattern-Miner is desired to find all Spatially frequent fuzzy patterns
-                    which is on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
+    :Description:
+       Top K Spatial High Utility ItemSet Mining (TKSHUIM) aims to discover Top-K Spatial High Utility Itemsets
+       (TKSHUIs) in a spatioTemporal database
 
     :Reference:
 
+       P. Pallikila et al., "Discovering Top-k Spatial High Utility Itemsets in Very Large Quantitative Spatiotemporal 
+       databases," 2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 4925-4935, 
+       doi: 10.1109/BigData52589.2021.9671912.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of High Utility Spatial patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float :
-                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+                   Name of the output file to store complete set of High Utility Spatial patterns
+    :param minUtil: int :
+                   Minimum utility threshold given by User
+    :param maxMemory: int :
+                   Maximum memory used by this program for running
+    :param candidateCount: int :
+                   Number of candidates to consider when calculating a high utility spatial pattern
     :param nFile: str :
-                   Name of the input file to mine complete set of frequent patterns
-    :param  FuzFile: str :
-                   The user can specify fuzFile.
+                   Name of the input file to mine complete set of High Utility Spatial patterns
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
+            Name of the input file to mine complete set of frequent patterns
+        nFile : file
+            Name of the Neighbours file that contain neighbours of items
         oFile : file
-            Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given minimum support
-        neighbors: map
-            keep track of neighbours of elements
+            Name of the output file to store complete set of frequent patterns
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
-        itemsCnt: int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemBuffer list
-            to keep track of items in buffer
+        k : int
+            The user given k value
+        candidateCount: int
+             Number of candidates 
+        utilityBinArrayLU: list
+             A map to hold the pmu values of the items in database
+        utilityBinArraySU: list
+            A map to hold the subtree utility values of the items is database
+        oldNamesToNewNames: list
+            A map to hold the subtree utility values of the items is database
+        newNamesToOldNames: list
+            A map to store the old name corresponding to new name
+        Neighbours : map
+            A dictionary to store the neighbours of a item
+        maxMemory: float
+            Maximum memory used by this program for running
+        itemsToKeep: list
+            keep only the promising items ie items having twu >= minUtil
+        itemsToExplore: list
+            keep items that subtreeUtility grater than minUtil
 
     :Methods:
 
-        startMine()
-            Mining process will start from here
+        mine()
+                Mining process will start from here
         getPatterns()
-            Complete set of patterns will be retrieved with this function
+                Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+                Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+                Complete set of patterns will be loaded in to a dataframe
         getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
+                Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+                Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
-            To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        Intersection(neighbourX,neighbourY)
-            Return common neighbours of 2 itemSet Neighbours
-        findElementWithTID(uList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
-            To Store the patten
+               Total amount of runtime taken by the mining process will be retrieved from this function
+        calculateNeighbourIntersection(self, prefixLength)
+               A method to return common Neighbours of items
+        backtrackingEFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
+               A method to mine the TKSHUIs Recursively
+        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep, neighbourhoodList)
+               A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
+        output(tempPosition, utility)
+               A method ave a high-utility itemSet to file or memory depending on what the user chose
+        is_equal(transaction1, transaction2)
+               A method to Check if two transaction are identical
+        intersection(lst1, lst2)
+               A method that return the intersection of 2 list
+        useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(dataset)
+              Scan the initial database to calculate the subtree utility of each items using a utility-bin array
+        sortDatabase(self, transactions)
+              A Method to sort transaction in the order of PMU
+        sort_transaction(self, trans1, trans2)
+              A Method to sort transaction in the order of PMU
+        useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
+             A method to scan the database using utility bin array to calculate the pmus                   
 
-    **Executing the code on terminal :**
-    ----------------------------------------
+    **Executing the code on terminal:**
+    -------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FGPFPMiner_old.py <inputFile> <outputFile> <neighbours> <minSup> <maxPer> <sep>
+      (.venv) $ python3 TKSHUIM.py <inputFile> <outputFile> <Neighbours> <k> <sep>
 
       Example Usage:
 
-      (.venv) $ python3  FGPFPMiner_old.py sampleTDB.txt output.txt sampleN.txt 3 4
+      (.venv) $ python3 TKSHUIM.py sampleTDB.txt output.txt sampleN.txt 35
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: maxMemory will be considered as Maximum memory used by this program for running
 
 
     **Sample run of importing the code:**
-    ------------------------------------------
-
-        from PAMI.fuzzyGeoreferencedPeriodicFrequentPattern import FGPFPMiner as alg
-
-        obj = alg.FFSPMiner("input.txt", "neighbours.txt", 3, 4)
-
-        obj.mine()
-
-        print("Total number of fuzzy frequent spatial patterns:", len(obj.getPatterns()))
-
-        obj.save("outputFile")
-
-        print("Total Memory in USS:", obj.getMemoryUSS())
-
-        print("Total Memory in RSS", obj.getMemoryRSS())
-
-        print("Total ExecutionTime in seconds:", obj.getRuntime())
-
-    **Credits:**
-    ----------------
-            The complete program was written by B.Sai Chitra and Kundai Kwangwari under the supervision of Professor Rage Uday Kiran.
-    """
-
-    _startTime = float()
-    _endTime = float()
-    _minSup = str()
-    _maxPer = float()
-    _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _nFile = " "
-    _FuzFile = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _sep = "\t"
-    _transactionsDB = []
-    _fuzzyValuesDB = []
-
-    def __init__(self, iFile, nFile, FuzFile, minSup, maxPer, sep):
-        super().__init__(iFile, nFile, FuzFile, minSup, maxPer, sep)
-        self._mapItemNeighbours = {}
-        self._startTime = 0
-        self._endTime = 0
-        self._itemsCnt = 0
-        self._itemSupData = {}
-        self._mapItemSum = {}
-        self._finalClosedPeriodicPatterns = {}
-        self._mapItemRegions = {}
-        self._fuzzyRegionReferenceMap = {}
-        self._joinsCnt = 0
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._finalPatterns = {}
-        self._finalPeriodicPatterns = {}
-        self._tidList = {}
-        self._dbLen = 0
-        self._regionsNumber = 0
-        self._RegionsCal = []
-        self._RegionsLabel = []
-        self._LabelKey = {}
-
-    def _compareItems(self, o1, o2):
-        """
-        A Function that sort all FFI-list in ascending order of Support
-
-        :param o1: First FFI-list
+    ----------------------------------------
+    .. code-block:: python
+        
+            from PAMI.highUtilitySpatialPattern.topk import TKSHUIM as alg
 
-        :type o1: _FFList
+            obj=alg.TKSHUIM("input.txt","Neighbours.txt",35)
 
-        :param o2: Second FFI-list
+            obj.mine()
 
-        :type o1: _FFList
+            Patterns = obj.getPatterns()
 
-        :return: Comparision Value
+            obj.save("output")
 
-        :rtype: int
-        """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
-        if compare == 0:
-            return int(o1.item) - int(o2.item)
-        else:
-            return compare
+            memUSS = obj.getMemoryUSS()
 
-    def _convert(self, value) -> float:
-        """
-        To convert the given user specified value
+            print("Total Memory in USS:", memUSS)
 
-        :param value: user specified value
+            memRSS = obj.getMemoryRSS()
 
-        :type value: int or float or str
+            print("Total Memory in RSS", memRSS)
 
-        :return: converted value
+            run = obj.getRuntime()
 
-        :rtype: float
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbLen * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-            else:
-                value = int(value)
-        return value
+            print("Total ExecutionTime in seconds:", run)
 
-    def _fuzzyMembershipFunc(self):
+    **Credits:**
+    ---------------
+            The complete program was written by Pradeep Pallikila under the supervision of Professor Rage Uday Kiran.
+    """
+    candidateCount = 0
+    utilityBinArrayLU = {}
+    utilityBinArraySU = {}
+    oldNamesToNewNames = {}
+    newNamesToOldNames = {}
+    strToint = {}
+    intTostr = {}
+    Neighbours = {}
+    temp = [0] * 5000
+    maxMemory = 0
+    startTime = float()
+    endTime = float()
+    finalPatterns = {}
+    iFile = " "
+    oFile = " "
+    nFile = " "
+    sep = "\t"
+    minUtil = 0
+    memoryUSS = float()
+    memoryRSS = float()
+    heapList = []
 
-        try:
-            with open(self._FuzFile, 'r', encoding='utf-8') as f:
-                count = 0
-                for line in f:
-                    line = line.split("\n")[0]
-                    parts = line.split(" ")
-                    lowerBound = parts[0].strip()
-                    upperBound = parts[1].strip()
-                    lb_Label = parts[2].strip()
-                    ub_Label = parts[3].strip()
-                    self._RegionsCal.append([int(lowerBound), int(upperBound)])
-                    self._RegionsLabel.append([lb_Label, ub_Label])
-                    for i in range(0, 2):
-                        if lb_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[lb_Label.capitalize()] = count
-                            count += 1
-                        if ub_Label.capitalize() not in self._LabelKey:
-                            self._LabelKey[ub_Label.capitalize()] = count
-                            count += 1
-        except IOError:
-            print("File Not Found")
-            quit()
-
-    def _creatingItemSets(self):
-        """
-        Storing the complete transactions of the database/input file in a database variable
-        """
-        self._transactionsDB, self._fuzzyValuesDB = [], []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            if self._iFile.empty:
-                print("its empty..")
-            i = self._iFile.columns.values.tolist()
-            if 'Transactions' in i:
-                self._transactionsDB = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self._fuzzyValuesDB = self._iFile['fuzzyValues'].tolist()
-
-        if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                for line in data:
-                    line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
-                    self._transactionsDB.append([x for x in items])
-                    self._fuzzyValuesDB.append([x for x in quantities])
-            else:
-                try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.split("\n")[0]
-                            parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[2] = parts[2].strip()
-                            items = parts[0].split(self._sep)
-                            quantities = parts[2].split(self._sep)
-                            self._transactionsDB.append([x for x in items])
-                            self._fuzzyValuesDB.append([x for x in quantities])
-                except IOError:
-                    print("File Not Found")
-                    quit()
-
-    def _mapNeighbours(self):
-        self._mapItemNeighbours = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data, items = [], []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'items' in i:
-                items = self._nFile['items'].tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._mapItemNeighbours[items[k]] = data[k]
-
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
-                for line in data:
-                    line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = [i.rstrip() for i in line.split(self._sep)]
-                    parts = [x for x in parts]
-                    item = parts[0]
-                    neigh1 = []
-                    for i in range(1, len(parts)):
-                        neigh1.append(parts[i])
-                    self._mapItemNeighbours[item] = neigh1
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line = line.split("\n")[0]
-                            parts = [i.rstrip() for i in line.split(self._sep)]
-                            parts = [x for x in parts]
-                            item = parts[0]
-                            neigh1 = []
-                            for i in range(1, len(parts)):
-                                neigh1.append(parts[i])
-                            self._mapItemNeighbours[item] = neigh1
-                except IOError:
-                    print(self._nFile)
-                    print("File Not Found")
-                    quit()
-
-    def _Regions(self, quantity):
-        """
-        param quantity:
-        type quantity:
-        """
-
-        self._list = [0] * len(self._LabelKey)
-        if self._RegionsCal[0][0] < quantity <= self._RegionsCal[0][1]:
-            self._list[0] = 1
-            return
-        elif quantity >= self._RegionsCal[-1][0]:
-            self._list[-1] = 1
-            return
-        else:
-            for i in range(1, len(self._RegionsCal) - 1):
-                if self._RegionsCal[i][0] < quantity <= self._RegionsCal[i][1]:
-                    base = self._RegionsCal[i][1] - self._RegionsCal[i][0]
-                    for pos in range(0, 2):
-                        if self._RegionsLabel[i][pos].islower():
-                            self._list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (self._RegionsCal[i][1] - quantity) / base)
-                        else:
-                            self._list[self._LabelKey[self._RegionsLabel[i][pos].capitalize()]] = float(
-                                (quantity - self._RegionsCal[i][0]) / base)
-        return
+    def __init__(self, iFile, nFile, k, sep="\t"):
+        super().__init__(iFile, nFile, k, sep)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
-        Frequent pattern mining process will start from here
+        Main function of the program.
         """
-        self._startTime = _ab._time.time()
-        self._mapNeighbours()
-        self._creatingItemSets()
-        self._fuzzyMembershipFunc()
-        self._finalPatterns = {}
-        recent_occur = {}
-        for line in range(len(self._transactionsDB)):
-            item_list = self._transactionsDB[line]
-            fuzzyValues_list = self._fuzzyValuesDB[line]
-            self._dbLen += 1
-            """
-            This section below is for:
-            1.Finding the support of each item's region in the entire database
-            2.Finding the periodic patterns of the data
-            3.Trimming off the patterns whose support is less than minSupport
-            """
-            for i in range(0, len(item_list)):
-                item = item_list[i]
-                if item in self._tidList:
-                    self._tidList[item].append(self._dbLen - recent_occur[item][-1])
-                    recent_occur[item].append(self._dbLen)
-                else:
-                    self._tidList[item] = [self._dbLen]
-                    recent_occur[item] = [self._dbLen]
-                fuzzy_ref = fuzzyValues_list[i]
-                if item in self._mapItemNeighbours:
-                    if fuzzy_ref not in self._fuzzyRegionReferenceMap:
-                        self._Regions(int(fuzzy_ref))
-                        self._fuzzyRegionReferenceMap[fuzzy_ref] = self._list
-
-                    if item in self._itemSupData.keys():
-                        self._itemSupData[item] = [sum(i) for i in zip(self._itemSupData[item],
-                                                                       self._fuzzyRegionReferenceMap[fuzzy_ref])]
-                    else:
-                        self._itemSupData[item] = self._fuzzyRegionReferenceMap[fuzzy_ref]
-
-        for item in self._tidList.keys():
-            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
-        del recent_occur
-        """
-        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
-        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
-        Step2. prune out all items whose regional support is less than the given minSup
-        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
-        """
-
-        listOfFFList = []
-        mapItemsToFFLIST = {}
-        region_label = []
-        for i in range(0, len(self._RegionsLabel)):
-            if self._RegionsLabel[i][1] not in region_label:
-                region_label.append(str(self._RegionsLabel[i][1]))
-
-        self._minSup = self._convert(self._minSup)
-        for item in self._itemSupData.keys():
-            if max(self._itemSupData[item]) >= self._minSup:
-                self._mapItemSum[item] = max(self._itemSupData[item])
-                self._mapItemRegions[item] = region_label[self._itemSupData[item].index(self._mapItemSum[item])]
-                fuList = _FFList(item)
-                if int(self._maxPer) >= max(self._tidList[item]):
-                    fuList.isPeriodic = True
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-
-        del self._itemSupData
-        del self._tidList
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for j in range(len(self._transactionsDB)):
-            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
-            revisedTransaction = []
-            for i in range(0, len(item_list)):
-                pair = _Pair()
-                pair.item = item_list[i]
-                fuzzy_ref = str(self._fuzzyValuesDB[j][self._transactionsDB[j].index(pair.item)])
-                pair.quantity = self._fuzzyRegionReferenceMap[fuzzy_ref][
-                    region_label.index(self._mapItemRegions[pair.item])]
-                if pair.quantity > 0:
-                    revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            qaunt = {}
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                qaunt[pair.item] = pair.quantity
-                remainUtil = 0
-                temp = list(set(self._mapItemNeighbours[pair.item]).intersection(set(qaunt.keys())))
-                for j in temp:
-                    remainUtil += float(qaunt[j])
-                del temp
-                remainingUtility = remainUtil
-                FFListObject = mapItemsToFFLIST[pair.item]
-                element = _Element(tid, pair.quantity, remainingUtility)
-                FFListObject.addElement(element)
-            del qaunt
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self):
         """
-        Frequent pattern mining process will start from here
+        Main function of the program.
         """
-        self._startTime = _ab._time.time()
-        self._mapNeighbours()
-        self._creatingItemSets()
-        self._fuzzyMembershipFunc()
-        self._finalPatterns = {}
-        recent_occur = {}
-        for line in range(len(self._transactionsDB)):
-            item_list = self._transactionsDB[line]
-            fuzzyValues_list = self._fuzzyValuesDB[line]
-            self._dbLen += 1
-            """
-            This section below is for:
-            1.Finding the support of each item's region in the entire database
-            2.Finding the periodic patterns of the data
-            3.Trimming off the patterns whose support is less than minSupport
-            """
-            for i in range(0, len(item_list)):
-                item = item_list[i]
-                if item in self._tidList:
-                    self._tidList[item].append(self._dbLen - recent_occur[item][-1])
-                    recent_occur[item].append(self._dbLen)
-                else:
-                    self._tidList[item] = [self._dbLen]
-                    recent_occur[item] = [self._dbLen]
-                fuzzy_ref = fuzzyValues_list[i]
-                if item in self._mapItemNeighbours:
-                    if fuzzy_ref not in self._fuzzyRegionReferenceMap:
-                        self._Regions(int(fuzzy_ref))
-                        self._fuzzyRegionReferenceMap[fuzzy_ref] = self._list
-
-                    if item in self._itemSupData.keys():
-                        self._itemSupData[item] = [sum(i) for i in zip(self._itemSupData[item],
-                                                                       self._fuzzyRegionReferenceMap[fuzzy_ref])]
+        self.startTime = time.time()
+        self.finalPatterns = {}
+        self.dataset = Dataset(self.iFile, self.sep)
+        with open(self.nFile, 'r') as o:
+            lines = o.readlines()
+            for line in lines:
+                line = line.split("\n")[0]
+                line_split = line.split(self.sep)
+                item = self.dataset.strToint.get(line_split[0])
+                lst = []
+                for i in range(1, len(line_split)):
+                    lst.append(self.dataset.strToint.get(line_split[i]))
+                self.Neighbours[item] = lst
+        o.close()
+        InitialMemory = psutil.virtual_memory()[3]
+        self.useUtilityBinArrayToCalculateLocalUtilityFirstTime(self.dataset)
+        itemsToKeep = []
+        for key in self.utilityBinArrayLU.keys():
+            if self.utilityBinArrayLU[key] >= self.minUtil:
+                itemsToKeep.append(key)
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self.utilityBinArrayLU[x])
+        currentName = 1
+        for idx, item in enumerate(itemsToKeep):
+            self.oldNamesToNewNames[item] = currentName
+            self.newNamesToOldNames[currentName] = item
+            itemsToKeep[idx] = currentName
+            currentName += 1
+        for transaction in self.dataset.getTransactions():
+            transaction.removeUnpromisingItems(self.oldNamesToNewNames)
+        self.sortDatabase(self.dataset.getTransactions())
+        emptyTransactionCount = 0
+        for transaction in self.dataset.getTransactions():
+            if len(transaction.getItems()) == 0:
+                emptyTransactionCount += 1
+        self.dataset.transactions = self.dataset.transactions[emptyTransactionCount:]
+        self.useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self.dataset)
+        self.heapList = []
+        itemsToExplore = []
+        for item in itemsToKeep:
+            if self.utilityBinArraySU[item] >= self.minUtil:
+                itemsToExplore.append(item)
+        commonitems = []
+        for i in range(self.dataset.maxItem):
+            commonitems.append(i)
+        self.backtrackingEFIM(self.dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
+        finalMemory = psutil.virtual_memory()[3]
+        memory = (finalMemory - InitialMemory) / 10000
+        if memory > self.maxMemory:
+            self.maxMemory = memory
+        self.endTime = time.time()
+        process = psutil.Process(os.getpid())
+        self.memoryUSS = float()
+        self.memoryRSS = float()
+        self.memoryUSS = process.memory_full_info().uss
+        self.memoryRSS = process.memory_info().rss
+        for item in self.heapList:
+            self.finalPatterns[item[1]] = item[0]
+        print('TOP-K mining process is completed by TKSHUIM')
+
+    def backtrackingEFIM(self, transactionsOfP, itemsToKeep, itemsToExplore, prefixLength):
+        """
+        A method to mine the TKSHUIs Recursively
+
+        :param transactionsOfP: the list of transactions containing the current prefix P
+        :type transactionsOfP: list
+        :param itemsToKeep: the list of secondary items in the p-projected database
+        :type itemsToKeep: list
+        :param itemsToExplore: the list of primary items in the p-projected database
+        :type itemsToExplore: list
+        :param prefixLength: current prefixLength
+        :type prefixLength: int
+        """
+        self.candidateCount += len(itemsToExplore)
+        for idx, e in enumerate(itemsToExplore):
+            initialMemory = psutil.virtual_memory()[3]
+            transactionsPe = []
+            utilityPe = 0
+            if len(transactionsOfP) == 0:
+                break 
+            previousTransaction = transactionsOfP[0]
+            consecutiveMergeCount = 0
+            for transaction in transactionsOfP:
+                items = transaction.getItems()
+                if e in items:
+                    positionE = items.index(e)
+                    if transaction.getLastPosition() == positionE:
+                        utilityPe += transaction.getUtilities()[positionE] + transaction.prefixUtility
                     else:
-                        self._itemSupData[item] = self._fuzzyRegionReferenceMap[fuzzy_ref]
-
-        for item in self._tidList.keys():
-            self._tidList[item].append(len(self._transactionsDB) - recent_occur[item][-1])
-        del recent_occur
-        """
-        Using Maximum Scalar Cardinality Value strategy to narrow down search space and generate candidate fuzzy periodic-frequent items. 
-        Step1. Identify the regional representative (region with max support). This is the representative that will be tested to see if its greater than given minSup
-        Step2. prune out all items whose regional support is less than the given minSup
-        Step3. At the end, sort the list of stored Candidate Frequent-Periodic Patterns in ascending order
-        """
-
-        listOfFFList = []
-        mapItemsToFFLIST = {}
-        region_label = []
-        for i in range(0, len(self._RegionsLabel)):
-            if self._RegionsLabel[i][1] not in region_label:
-                region_label.append(str(self._RegionsLabel[i][1]))
-
-        self._minSup = self._convert(self._minSup)
-        for item in self._itemSupData.keys():
-            if max(self._itemSupData[item]) >= self._minSup:
-                self._mapItemSum[item] = max(self._itemSupData[item])
-                self._mapItemRegions[item] = region_label[self._itemSupData[item].index(self._mapItemSum[item])]
-                fuList = _FFList(item)
-                if int(self._maxPer) >= max(self._tidList[item]):
-                    fuList.isPeriodic = True
-                mapItemsToFFLIST[item] = fuList
-                listOfFFList.append(fuList)
-
-        del self._itemSupData
-        del self._tidList
-        listOfFFList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for j in range(len(self._transactionsDB)):
-            item_list = list(set(self._transactionsDB[j]).intersection(set(self._mapItemSum.keys())))
-            revisedTransaction = []
-            for i in range(0, len(item_list)):
-                pair = _Pair()
-                pair.item = item_list[i]
-                fuzzy_ref = str(self._fuzzyValuesDB[j][self._transactionsDB[j].index(pair.item)])
-                pair.quantity = self._fuzzyRegionReferenceMap[fuzzy_ref][
-                    region_label.index(self._mapItemRegions[pair.item])]
-                if pair.quantity > 0:
-                    revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            qaunt = {}
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                qaunt[pair.item] = pair.quantity
-                remainUtil = 0
-                temp = list(set(self._mapItemNeighbours[pair.item]).intersection(set(qaunt.keys())))
-                for j in temp:
-                    remainUtil += float(qaunt[j])
-                del temp
-                remainingUtility = remainUtil
-                FFListObject = mapItemsToFFLIST[pair.item]
-                element = _Element(tid, pair.quantity, remainingUtility)
-                FFListObject.addElement(element)
-            del qaunt
-            tid += 1
-        itemNeighbours = list(self._mapItemNeighbours.keys())
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFList, self._minSup, itemNeighbours)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-
-    def _FSFIMining(self, prefix, prefixLen, FSFIM, minSup, itemNeighbours):
-        """
-        Generates FFSPMiner from prefix
-
-        :param prefix: the prefix patterns of FFSPMiner
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param FSFIM: the Fuzzy list of prefix itemSets
-        :type FSFIM: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        :param itemNeighbours: the set of common neighbours of prefix
-        :type itemNeighbours: list or set
-        """
-        for i in range(0, len(FSFIM)):
-            _FFListObject1 = FSFIM[i]
-            if _FFListObject1.sumIUtil >= minSup:
-                self._WriteOut(prefix, prefixLen, _FFListObject1, _FFListObject1.sumIUtil)
-            newNeighbourList = self._Intersection(self._mapItemNeighbours.get(_FFListObject1.item), itemNeighbours)
-            if _FFListObject1.sumRUtil >= minSup:
-                exULs = []
-                for j in range(i + 1, len(FSFIM)):
-                    _FFListObject2 = FSFIM[j]
-                    if _FFListObject2.item in newNeighbourList:
-                        exULs.append(self._construct(_FFListObject1, _FFListObject2))
-                        self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, _FFListObject1.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, newNeighbourList)
-
-    def _Intersection(self, neighbourX, neighbourY):
-        """
-        A function to get common neighbours from 2 itemSets
-
-        :param neighbourX: the set of neighbours of itemSet 1
-        :type neighbourX: set or list
-        :param neighbourY: the set of neighbours of itemSet 2
-        :type neighbourY: set or list
-        :return : set of common neighbours of 2 itemSets
-        :rtype :set
-        """
-        result = []
-        if neighbourX is None or neighbourY is None:
-            return result
-        for i in range(0, len(neighbourX)):
-            if neighbourX[i] in neighbourY:
-                result.append(neighbourX[i])
-        return result
-
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-
-        :return: returning USS memory consumed by the mining process
-        :rtype: float
-        """
-
-        return self._memoryUSS
-
-    def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
-        """
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
-
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
-        """
-        return self._endTime - self._startTime
-
-    def _construct(self, _FFListObject1, _FFListObject2):
+                        projectedTransaction = transaction.projectTransaction(positionE)
+                        utilityPe += projectedTransaction.prefixUtility
+                        if previousTransaction == transactionsOfP[0]:
+                            previousTransaction = projectedTransaction
+                        elif self.is_equal(projectedTransaction, previousTransaction):
+                            if consecutiveMergeCount == 0:
+                                items = previousTransaction.items[previousTransaction.offset:]
+                                utilities = previousTransaction.utilities[previousTransaction.offset:]
+                                itemsCount = len(items)
+                                positionPrevious = 0
+                                positionProjection = projectedTransaction.offset
+                                while positionPrevious < itemsCount:
+                                    utilities[positionPrevious] += projectedTransaction.utilities[positionProjection]
+                                    positionPrevious += 1
+                                    positionProjection += 1
+                                previousTransaction.prefixUtility += projectedTransaction.prefixUtility
+                                sumUtilities = previousTransaction.prefixUtility
+                                previousTransaction = Transaction(items, utilities, previousTransaction.transactionUtility + projectedTransaction.transactionUtility)
+                                previousTransaction.prefixUtility = sumUtilities
+                            else:
+                                positionPrevious = 0
+                                positionProjected = projectedTransaction.offset
+                                itemsCount = len(previousTransaction.items)
+                                while positionPrevious < itemsCount:
+                                    previousTransaction.utilities[positionPrevious] += projectedTransaction.utilities[
+                                        positionProjected]
+                                    positionPrevious += 1
+                                    positionProjected += 1
+                                previousTransaction.transactionUtility += projectedTransaction.transactionUtility
+                                previousTransaction.prefixUtility += projectedTransaction.prefixUtility
+                            consecutiveMergeCount += 1
+                        else:
+                            transactionsPe.append(previousTransaction)
+                            previousTransaction = projectedTransaction
+                            consecutiveMergeCount = 0
+                    transaction.offset = positionE
+            if previousTransaction != transactionsOfP[0]:
+                transactionsPe.append(previousTransaction)
+            self.temp[prefixLength] = self.newNamesToOldNames[e]
+            if utilityPe >= self.minUtil:
+                self.output(prefixLength, utilityPe)
+            neighbourhoodList = self.calculateNeighbourIntersection(prefixLength)
+            self.useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep, neighbourhoodList)
+            newItemsToKeep = []
+            newItemsToExplore = []
+            for l in range(idx + 1, len(itemsToKeep)):
+                itemK = itemsToKeep[l]
+                if self.utilityBinArraySU[itemK] >= self.minUtil:
+                    if itemK in neighbourhoodList:
+                        newItemsToExplore.append(itemK)
+                        newItemsToKeep.append(itemK)
+                elif self.utilityBinArrayLU[itemK] >= self.minUtil:
+                    if itemK in neighbourhoodList:
+                        newItemsToKeep.append(itemK)
+            self.backtrackingEFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
+            finalMemory = psutil.virtual_memory()[3]
+            memory = (finalMemory - initialMemory) / 10000
+            if self.maxMemory < memory:
+                self.maxMemory = memory
+
+    def useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe, j, itemsToKeep, neighbourhoodList):
+        """
+        A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P U {e}
+
+        :param transactionsPe: transactions the projected database for P U {e}
+        :type transactionsPe: list
+        :param j:the position of j in the list of promising items
+        :type j:int
+        :param itemsToKeep :the list of promising items
+        :type itemsToKeep: list
+        :param neighbourhoodList: list of neighbourhood elements
+        :type neighbourhoodList: list
+        """
+        for i in range(j + 1, len(itemsToKeep)):
+            item = itemsToKeep[i]
+            self.utilityBinArrayLU[item] = 0
+            self.utilityBinArraySU[item] = 0
+        for transaction in transactionsPe:
+            length = len(transaction.getItems())
+            i = length - 1
+            while i >= transaction.offset:
+                item = transaction.getItems()[i]
+                if item in itemsToKeep:
+                    remainingUtility = 0
+                    if self.newNamesToOldNames[item] in self.Neighbours:
+                        item_neighbours = self.Neighbours[self.newNamesToOldNames[item]]
+                        for k in range(i, length):
+                            transaction_item = transaction.getItems()[k]
+                            if self.newNamesToOldNames[transaction_item] in item_neighbours and transaction_item in neighbourhoodList:
+                                remainingUtility += transaction.getUtilities()[k]
+
+                    remainingUtility += transaction.getUtilities()[i]
+                    self.utilityBinArraySU[item] += remainingUtility + transaction.prefixUtility
+                    self.utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
+                i -= 1
+
+    def calculateNeighbourIntersection(self, prefixLength):
+        """
+        A method to find common Neighbours
+
+        :param prefixLength: the prefix itemSet
+        :type prefixLength:int
+        """
+        intersectionList = self.Neighbours.get(self.temp[0])
+        for i in range(1, prefixLength+1):
+            intersectionList = self.intersection(self.Neighbours[self.temp[i]], intersectionList)
+        finalIntersectionList = []
+        if intersectionList is None:
+            return finalIntersectionList
+        for item in intersectionList:
+            if item in self.oldNamesToNewNames:
+                finalIntersectionList.append(self.oldNamesToNewNames[item])
+        return finalIntersectionList
+    
+    def output(self, tempPosition, utility):
+        """
+        A method save all high-utility itemSet to file or memory depending on what the user chose
+
+        :param tempPosition: position of last item
+        :type tempPosition : int
+        :param utility: total utility of itemSet
+        :type utility: int
+        """
+        s1 = str()
+        for i in range(0, tempPosition+1):
+            s1 += self.dataset.intTostr.get((self.temp[i]))
+            if i != tempPosition:
+                s1 += "\t"
+        self.additemset(s1, utility)
+
+    def is_equal(self, transaction1, transaction2):
+        """
+        A method to Check if two transaction are identical
+
+        :param  transaction1: the first transaction.
+        :type  transaction1: Transaction
+        :param  transaction2:   the second transaction.
+        :type  transaction2: Transaction
+        :return : whether both are identical or not
+        :rtype: bool
+        """
+
+        length1 = len(transaction1.items) - transaction1.offset
+        length2 = len(transaction2.items) - transaction2.offset
+        if length1 != length2:
+            return False
+        position1 = transaction1.offset
+        position2 = transaction2.offset
+        while position1 < len(transaction1.items):
+            if transaction1.items[position1] != transaction2.items[position2]:
+                return False
+            position1 += 1
+            position2 += 1
+        return True
+    
+    def intersection(self, lst1, lst2):
+        """
+        A method that return the intersection of 2 list
+
+        :param  lst1: items neighbour to item1
+        :type lst1: list
+        :param lst2: items neighbour to item2
+        :type lst2: list
+        :return :intersection of two lists
+        :rtype : list
+        """
+        temp = set(lst2)
+        lst3 = [value for value in lst1 if value in temp]
+        return lst3
+
+    def useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset):
+        """
+        Scan the initial database to calculate the subtree utility of each item using a utility-bin array
+
+        :param dataset: the transaction database
+        :type dataset: Dataset
+        """
+        for transaction in dataset.getTransactions():
+            items = transaction.getItems()
+            utilities = transaction.getUtilities()
+            for idx, item in enumerate(items):
+                if item not in self.utilityBinArraySU:
+                    self.utilityBinArraySU[item] = 0
+                if self.newNamesToOldNames[item] not in self.Neighbours:
+                    self.utilityBinArraySU[item] += utilities[idx]
+                    continue
+                i = idx + 1
+                sumSu = utilities[idx]
+                while i < len(items):
+                    if self.newNamesToOldNames[items[i]] in self.Neighbours[self.newNamesToOldNames[item]]:
+                        sumSu += utilities[i]
+                    i += 1
+                self.utilityBinArraySU[item] += sumSu
+
+    def sortDatabase(self, transactions):
+        """
+        A Method to sort transaction in the order of PMU
+
+        :param transactions: transaction of items
+        :type transactions: Transaction
+        :return: sorted transaction
+        :rtype: Transaction
+        """
+        cmp_items = cmp_to_key(self.sort_transaction)
+        transactions.sort(key=cmp_items)
+
+    def sort_transaction(self, trans1, trans2):
+        """
+        A Method to sort transaction in the order of PMU
+
+        :param trans1: the first transaction.
+        :type trans1: Transaction
+        :param trans2:the second transaction.
+        :type trans2: Transaction
+        :return: sorted transaction.
+        :rtype: int
         """
-        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
-
-        :param _FFListObject1:the itemSet px
-        :type _FFListObject1:FFI-List
-        :param _FFListObject2:itemSet py
-        :type _FFListObject2:FFI-List
-        :return :the itemSet of pxy(px and py)
-        :rtype :FFI-List
-        """
-        recent_occur, first_occur, tid = 0, 0, 0
-        periodlist = []
-        _newFFListObject = _FFList(_FFListObject2.item)
-        for Ob1Element in _FFListObject1.elements:
-            Ob2Element = self._findElementWithTID(_FFListObject2, Ob1Element.tid)
-            if Ob2Element is None:
-                continue
-            tid = Ob1Element.tid
-            if len(periodlist) == 0:
-                periodlist.append(abs(first_occur - tid))
-                recent_occur = tid
-            else:
-                periodlist.append(tid - recent_occur)
-                recent_occur = tid
-            newElement = _Element(Ob1Element.tid, min([Ob1Element.iUtils, Ob2Element.iUtils], key=lambda x: float(x)),
-                                  Ob2Element.rUtils)
-            _newFFListObject.addElement(newElement)
-
-        if periodlist and int(self._maxPer) >= max(periodlist):
-            _newFFListObject.isPeriodic = True
+        trans1_items = trans1.getItems()
+        trans2_items = trans2.getItems()
+        pos1 = len(trans1_items) - 1
+        pos2 = len(trans2_items) - 1
+        if len(trans1_items) < len(trans2_items):
+            while pos1 >= 0:
+                sub = trans2_items[pos2] - trans1_items[pos1]
+                if sub != 0:
+                    return sub
+                pos1 -= 1
+                pos2 -= 1
+            return -1
+        elif len(trans1_items) > len(trans2_items):
+            while pos2 >= 0:
+                sub = trans2_items[pos2] - trans1_items[pos1]
+                if sub != 0:
+                    return sub
+                pos1 -= 1
+                pos2 -= 1
+            return 1
         else:
-            _newFFListObject.isPeriodic = False
-        return _newFFListObject
+            while pos2 >= 0:
+                sub = trans2_items[pos2] - trans1_items[pos1]
+                if sub != 0:
+                    return sub
+                pos1 -= 1
+                pos2 -= 1
+            return 0
+
+    def useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset):
+        """
+        A method to scan the database using utility bin array to calculate the pmus
+
+        :param dataset: the transaction database.
+        :type dataset: database
+        """
+        utilityMatrix = defaultdict(lambda: defaultdict(int))
+        for transaction in dataset.getTransactions():
+            for idx, item in enumerate(transaction.getItems()):
+                pmu = transaction.getUtilities()[idx]
+                if item in self.Neighbours:
+                    neighbors = self.Neighbours[item]
+                    for idx, item in enumerate(transaction.getItems()):
+                        if item in neighbors:
+                            pmu += transaction.getUtilities()[idx]
+                if item in self.utilityBinArrayLU:
+                    # self.utilityBinArrayLU[item] += transaction.getPmus()[idx]
+                    self.utilityBinArrayLU[item] += pmu
+                else:
+                    # self.utilityBinArrayLU[item] = transaction.getPmus()[idx]
+                    self.utilityBinArrayLU[item] = pmu
+                utilityMatrix[item][item] += transaction.getUtilities()[idx]
+                if item in self.Neighbours:
+                    neighbors = self.Neighbours[item]
+                    utility = transaction.getUtilities()[idx]
+                    for i, itemj in enumerate(transaction.getItems()):
+                        if (itemj != item) and (itemj in neighbors):
+                            utilityMatrix[item][itemj] += (utility + transaction.getUtilities()[i])
+
+        for item in utilityMatrix.keys():
+            for itemj in utilityMatrix[item].keys():
+                if itemj >= item:
+                    val = utilityMatrix[item][itemj]
+                    if val != 0 and val > self.minUtil:
+                        if itemj == item:
+                            itemset = str(item)
+                        else:
+                            itemset = str(item) + str(itemj)
+                        self.additemset(itemset, val)
 
-    def _findElementWithTID(self, uList, tid):
+    def additemset(self, itemset, utility):
         """
-        To find element with same tid as given
+        adds the itemset to the priority queue
 
-        :param uList:fuzzyList
-        :type uList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element tid as given
-        :rtype: element if exist or None
-        """
-        List = uList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
-            else:
-                return List[mid]
-        return None
+        :param itemset: the itemset to be added
 
-    def _WriteOut(self, prefix, prefixLen, _FFListObject, sumIUtil):
-        """
-        To Store the patten
+        :type itemset: str
 
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param _FFListObject: the last item
-        :type _FFListObject: int
-        :param sumIUtil: sum of utility of itemSet
-        :type sumIUtil: float
-        """
-        item = _FFListObject.item
-        self._itemsCnt += 1
-        res = ""
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
-        res += str(item) + "." + str(self._mapItemRegions.get(item))
-        res1 = str(sumIUtil)
-        self._finalPatterns[res] = res1
+        :param utility: utility matrix for the itemset to be added
 
-        if _FFListObject.isPeriodic:
-            self._finalPeriodicPatterns[res] = res1
+        :type utility: numpy.array
+        """
+        heapq.heappush(self.heapList, (utility, itemset))
+        if len(self.heapList) > self.k:
+            while len(self.heapList) > self.k:
+                heapq.heappop(self.heapList)
+                if len(self.heapList) == 0:
+                    break
+            self.minUtil = heapq.nsmallest(1, self.heapList)[0][0]
 
     def getPatternsAsDataFrame(self):
         """
-        Storing final frequent patterns in a dataframe
+        Storing final patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
-
         dataFrame = {}
         data = []
-        for a, b in self._finalPeriodicPatterns.items():
+        for a, b in self.finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Utility'])
 
+        return dataFrame
+    
     def getPatterns(self):
         """
-        Function to send the set of frequent patterns after completion of the mining process
+        Function to send the set of patterns after completion of the mining process
 
-        :return: returning frequent patterns
+        :return: returning patterns
         :rtype: dict
         """
-        return self._finalPeriodicPatterns
+        return self.finalPatterns
 
     def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to an output file
+        Complete set of patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
-        keylist = (self._finalPatterns.keys())
         writer = open(self.oFile, 'w+')
-        for x in keylist:
-            patternsAndSupport = x.strip() + ":" + str(self._finalPatterns[x])
+        for x, y in self.finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self):
+    def getMemoryUSS(self):
         """
-        This function is used to print the results
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning USS memory consumed by the mining process
+        :rtype: float
         """
-        print("Total number of Spatial Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
 
-    def getPatternsAsDataframe(self):
+        return self.memoryUSS
 
+    def getMemoryRSS(self):
         """
-        :return: returning periodic frequent patterns in a dataframe
-        :rtype: pd.DataFrame
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
         """
+        return self.memoryRSS
 
-        data = []
-        dataFrame = _ab._pd.DataFrame()
-        for a, b in self._finalPeriodicPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
 
-    def generateLatexCode(self, result):
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
+        """
+        return self.endTime-self.startTime
 
-        titles = result.columns.tolist()
-        titles.remove("minsup")
-        titles.remove("algorithm")
-        for i in range(0, len(titles)):
-            legendary = pd.unique(result[['algorithm']].values.ravel())
-            color = ['red', 'blue', 'green', 'black', 'yellow']
-            xaxis = result["minsup"].values.tolist()
-            yaxis = result[titles[i]].values.tolist()
-            algo = result["algorithm"].values.tolist()
-            x_label = "minsup"
-            filename = titles[i]
-            latexwriter = open(filename + "Latexfile.tex", "w")
-            latexwriter.write("")
-            latexwriter.write("\\begin{axis}[\n\txlabel={\\Huge{" + x_label + "}},")
-            latexwriter.write("\n\tylabel={\\Huge{" + titles[i] + "}},")
-            latexwriter.write("\n\txmin=" + str(min(xaxis)) + ", xmax=" + str(max(xaxis)) + ",")
-
-            for num in range(0, len(legendary)):
-                latexwriter.write("\n\\addplot+  [" + color[num] + "]\n\tcoordinates {\n")
-                for num2 in range(0, len(xaxis)):
-                    if (legendary[num] == algo[num2]):
-                        latexwriter.write("(" + str(xaxis[num2]) + "," + str(yaxis[num2]) + ")\n")
-                latexwriter.write("\t};   \\addlegendentry{" + legendary[num] + "}\n")
-                if (num + 1 == len(legendary)):
-                    latexwriter.write("\\end{axis}")
-        print("Latex file generated successfully")
-
-    def generateGraphs(result):
-
-        fig = px.line(result, x='minsup', y='patterns', color='algorithm', title='Patterns)', markers=True)
-        fig.show()
-        fig = px.line(result, x='minsup', y='runtime', color='algorithm', title='Runtime)', markers=True)
-        fig.show()
-        fig = px.line(result, x='minsup', y='memoryUSS', color='algorithm', title='MemoryUSS)', markers=True)
-        fig.show()
-        fig = px.line(result, x='minsup', y='memoryRSS', color='algorithm', title='MemoryRSS)', markers=True)
-        fig.show()
-
-
-if __name__ == "__main__":
-    _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 6:
-            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5],
-                             _ab._sys.argv[6])
-        if len(_ab._sys.argv) == 5:
-            _ap = FGPFPMiner(_ab._sys.argv[1], _ab._sys.argv[2], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        _ap.startMine()
-        _ap.mine()
-        print("Total number of Spatial Fuzzy Periodic Frequent  Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
-        _ap.save("outputfile.txt")
-    else:
-        print("Error! The number of input parameters do not match the total number of parameters provided")
+    def printResults(self):
+        """
+        This function is used to print the results
+        """
+        print("Top K Spatial  High Utility Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", self.getRuntime())
 
+def main():
+    inputFile = 'mushroom_utility_spmf.txt'
+    neighborFile = 'mushroom_neighbourhood.txt' #Users can also specify this constraint between 0 to 1.
+    k = 1000
+    seperator = ' ' 
+    obj = TKSHUIM(iFile=inputFile, nFile=neighborFile, k=k,  sep=seperator)    #initialize
+    obj.startMine()
+    obj.mine()
+    obj.printResults()
+    print(obj.getPatterns())
+
+if __name__ == '__main__':
+    main()
+    # _ap = str()
+    # if len(sys.argv) == 5 or len(sys.argv) == 6:
+    #     if len(sys.argv) == 6:
+    #         _ap = TKSHUIM(sys.argv[1], sys.argv[3], int(sys.argv[4]), sys.argv[5])
+    #     if len(sys.argv) == 5:
+    #         _ap = TKSHUIM(sys.argv[1], sys.argv[3], int(sys.argv[4]))
+    #     _ap.startMine()
+    #     _ap.mine()
+    #     print("Top K Spatial  High Utility Patterns:", len(_ap.getPatterns()))
+    #     _ap.save(sys.argv[2])
+    #     print("Total Memory in USS:", _ap.getMemoryUSS())
+    #     print("Total Memory in RSS",  _ap.getMemoryRSS())
+    #     print("Total ExecutionTime in seconds:", _ap.getRuntime())
+    # else:
+    #     for i in [1000, 5000]:
+    #         _ap = TKSHUIM('/Users/Likhitha/Downloads/mushroom_main_2000.txt',
+    #                 '/Users/Likhitha/Downloads/mushroom_neighbors_2000.txt', i, ' ')
+    #         _ap.startMine()
+    #         _ap.mine()
+    #         print("Total number of Spatial High Utility Patterns:", len(_ap.getPatterns()))
+    #         print("Total Memory in USS:", _ap.getMemoryUSS())
+    #         print("Total Memory in RSS", _ap.getMemoryRSS())
+    #         print("Total ExecutionTime in seconds:", _ap.getRuntime())
+    #     print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/fuzzyGeoreferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -40,32 +40,31 @@
 from urllib.request import urlopen as _urlopen
 import functools as _functools
 
 class _fuzzySpatialFrequentPatterns(_ABC):
     """ This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
         employ in PAMI
 
-
     Attributes :
     ----------
         iFile : str
             Input file name or path of the input file
-        minSup: integer or float or str
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime:float
+        startTime : float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py` & `pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/basic/F3PMiner.py`

 * *Files 5% similar despite different names*

```diff
@@ -28,17 +28,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
-
-
 __copyright__ = """
 
 Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -72,28 +69,27 @@
 
 class _FFList:
     """
      A class represent a Fuzzy List of an element
 
     :Attributes:
 
-         item: int
+         item : int
              the item name
-         sumIUtil: float
+         sumIUtil : float
              the sum of utilities of an fuzzy item in database
-         sumRUtil: float
+         sumRUtil : float
              the sum of resting values of a fuzzy item in database
-         elements: list
+         elements : list
              a list of elements contain tid,Utility and resting values of element in each transaction
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
-
         printElement(e)
             Method to print elements
 
     """
 
     def __init__(self, itemName):
         self.item = itemName
@@ -122,15 +118,15 @@
     """
     A class represents an Element of a fuzzy list
 
     :Attributes:
 
         tid : int
             keep tact of transaction id
-        iUtils: float
+        iUtils : float
             the utility of an fuzzy item in the transaction
         rUtils : float
             the  resting value of an fuzzy item in the transaction
     """
 
     def __init__(self, tid, iUtil):
         self.tid = tid
@@ -173,54 +169,54 @@
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : string
                Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given minimum support
         memoryRSS : float
                 To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
                To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        itemsCnt: int
+        itemsCnt : int
             To record the number of fuzzy spatial itemSets generated
-        mapItemsGSum: map
+        mapItemsGSum : map
             To keep track of G region values of items
         mapItemsMidSum: map
             To keep track of M region values of items
         mapItemsHSum: map
             To keep track of H region values of items
         mapItemSum: map
             To keep track of sum of Fuzzy Values of items
         mapItemRegions: map
             To Keep track of fuzzy regions of item
-        jointCnt: int
+        joinsCnt: int
             To keep track of the number of ffi-list that was constructed
         BufferSize: int
             represent the size of Buffer
-        itemBuffer list
+        itemSetBuffer list
             to keep track of items in buffer
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
+        convert(value)
             To convert the given user specified value
         compareItems(o1, o2)
             A Function that sort all ffi-list in ascending order of Support
         F3PMining(prefix, prefixLen, FSFIM, minSup)
             Method generate ffi from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
@@ -305,23 +301,18 @@
         self._dbLen = 0
 
     def _compareItems(self, o1, o2):
         """
         A Function that sort all ffi-list in ascending order of Support
 
         :param o1: First FFI-list
-
         :type o1: _FFList
-
         :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
+        :type o2: _FFList
+        :return: Comparison Value
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             if o1.item < o2.item:
                 return -1
             elif o1.item > o2.item:
@@ -332,19 +323,16 @@
             return compare
 
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
-
         :type value: int or float or str
-
         :return: converted value
-
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
@@ -410,68 +398,15 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         fuzzy-Frequent pattern mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        for line in range(len(self._transactions)):
-            times = self._ts[line]
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            self._dbLen += 1
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfffilist = []
-        mapItemsToFFLIST = {}
-        #self._minSup = float(self._minSup)
-        self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            # print(type(self._mapItemSum[item]))
-            if self._mapItemSum[item] >= self._minSup:
-                fuList = _FFList(item)
-                mapItemsToFFLIST[item] = fuList
-                listOfffilist.append(fuList)
-        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        tid = 0
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.quantity = quantities[i]
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                if mapItemsToFFLIST.get(pair.item) is not None:
-                    FFListOfItem = mapItemsToFFLIST[pair.item]
-                    element = _Element(tid, pair.quantity)
-                    FFListOfItem.addElement(element)
-            tid += 1
-        self._F3PMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self):
         """
         fuzzy-Frequent pattern mining process will start from here
         """
         self._startTime = _ab._time.time()
         self._creatingItemsets()
@@ -572,15 +507,14 @@
        """
         return self._memoryRSS
 
     def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
-
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
     def _construct(self, px, py):
         """
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py` & `pami-2024.5.1/PAMI/fuzzyPartialPeriodicPatterns/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -46,27 +46,27 @@
     :Description: This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                   employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup: integer or float or str
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime:float
+        startTime : float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner.py` & `pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,13 +1,9 @@
-# Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
-# on-trivial and challenging problem to its huge search space.we are using efficient pruning
-# techniques to reduce the search space.
-#
 # Sample run of importing the code:
-# ----------------------------------------
+# -------------------------------------
 #
 #             from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
 #
 #             obj =alg.FPFPMiner("input.txt",2,3)
 #
 #             obj.mine()
 #
@@ -61,114 +57,148 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
 """
 
 
 from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
 from deprecated import deprecated
 
 
 class _FFList:
     """
     A class represent a Fuzzy List of an element
+
     :Attributes:
 
-        item: int
+        item : int
             the item name
-        sumLUtil: float
+        sumLUtil : float
             the sum of utilities of a fuzzy item in database
-        sumRUtil: float
+        sumRUtil : float
             the sum of resting values of a fuzzy item in database
-        elements: list
+        elements : list
             list of elements contain tid,Utility and resting values of element in each transaction
-        maxPeriod: int
+        maxPeriod : int
             it represents the max period of a item
 
     :Methods:
 
         addElement(element)
             Method to add an element to this fuzzy list and update the sums at the same time.
         printElement(e)
             Method to print elements
 
     """
 
-    def __init__(self, itemName: str) -> None:
+    def __init__(self, itemName):
         self.item = itemName
         self.sumLUtil = 0.0
         self.sumRUtil = 0.0
         self.elements = []
         self.maxPeriod = 0
 
-    def addElement(self, element) -> None:
+    def addElement(self, element):
         """
         A Method that add a new element to FFList
 
         :param element: an element to be added to FFList
         :type element: Element
-        :return: None
         """
         self.sumLUtil += element.lUtils
         self.sumRUtil += element.rUtils
         self.elements.append(element)
         self.maxPeriod = max(self.maxPeriod, element.period)
 
-    def printElement(self) -> None:
+    def printElement(self):
         """
         A Method to Print elements in the FFList
-        :return: None
         """
         for ele in self.elements:
             print(ele.tid, ele.lUtils, ele.rUtils, ele.period)
 
 
 class _Element:
     """
-        A class represents an Element of a fuzzy list
+    A class represents an Element of a fuzzy list
 
-        :Attributes:
+    :Attributes:
 
         tid : int
             keep tact of transaction id
-        lUtils: float
+        lUtils : float
             the utility of a fuzzy item in the transaction
         rUtils : float
             the resting value of a fuzzy item in the transaction
-        period: int
+        period : int
             represent the period of the element
     """
 
-    def __init__(self, tid: int, iUtil: float, rUtil: float, period: int) -> None:
+    def __init__(self, tid, iUtil, rUtil, period):
         self.tid = tid
         self.lUtils = iUtil
         self.rUtils = rUtil
         self.period = period
 
 
+class _Regions:
+    """
+    A class calculate the regions
+
+    :Attributes:
+
+        low : int
+            low region value
+        middle : int
+            middle region value
+        high : int
+            high region values
+        """
+
+    def __init__(self, quantity, regionsNumber):
+        self.low = 0
+        self.middle = 0
+        self.high = 0
+        if regionsNumber == 3:  # if we have 3 regions
+            if 0 < quantity <= 1:
+                self.low = 1
+                self.high = 0
+                self.middle = 0
+            elif 1 < quantity <= 6:
+                self.low = float((6 - quantity) / 5)
+                self.middle = float((quantity - 1) / 5)
+                self.high = 0
+            elif 6 < quantity <= 11:
+                self.low = 0
+                self.middle = float((11 - quantity) / 5)
+                self.high = float((quantity - 6) / 5)
+            else:
+                self.low = 0
+                self.middle = 0
+                self.high = 1
+
+
 class _Pair:
     """
     A class to store item name and quantity together.
     """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.item = 0
         self.quantity = 0
 
 
 class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
     """
     :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
                     on-trivial and challenging problem to its huge search space.we are using efficient pruning
                     techniques to reduce the search space.
 
-    :Reference:   R. U. Kiran et al., "Discovering Fuzzy Periodic-Frequent Patterns in Quantitative Temporal Databases,"
-                  2020 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), Glasgow, UK, 2020, pp.
-                  1-8, doi: 10.1109/FUZZ48607.2020.9177579.
+    :Reference:
+
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of frequent patterns
     :param  oFile: str :
                    Name of the output file to store complete set of frequent patterns
     :param  minSup: int or float or str :
                    The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
@@ -182,86 +212,86 @@
 
         iFile : file
             Name of the input file to mine complete set of fuzzy spatial frequent patterns
         oFile : file
                Name of the oFile file to store complete set of fuzzy spatial frequent patterns
         minSup : float
             The user given support
-        period: int
+        period : int
             periodicity of an element
         memoryRSS : float
                 To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
                To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        itemsCnt: int
+        itemsCnt : int
             To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
+        mapItemsLowSum : map
             To keep track of low region values of items
-        mapItemsMidSum: map
+        mapItemsMidSum : map
             To keep track of middle region values of items
-        mapItemsHighSum: map
+        mapItemsHighSum : map
             To keep track of high region values of items
-        mapItemSum: map
+        mapItemSum : map
             To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
+        mapItemRegions : map
             To Keep track of fuzzy regions of item
-        jointCnt: int
+        joinsCnt : int
             To keep track of the number of FFI-list that was constructed
-        BufferSize: int
+        BufferSize : int
             represent the size of Buffer
-        itemBuffer list
+        itemSetBuffer list
             to keep track of items in buffer
-        maxTID: int
+        maxTID : int
             represent the maximum tid of the database
-        lastTIDs: map
+        lastTIDs : map
             represent the last tid of fuzzy items
-        itemsToRegion: map
+        itemsToRegion : map
             represent items with respective regions
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
+        convert(value)
             To convert the given user specified value
         FSFIMining( prefix, prefixLen, fsFim, minSup)
             Method generate FFI from prefix
         construct(px, py)
             A function to construct Fuzzy itemSet from 2 fuzzy itemSets
         findElementWithTID(UList, tid)
             To find element with same tid as given
         WriteOut(prefix, prefixLen, item, sumIUtil,period)
             To Store the patten
 
     **Executing the code on terminal :**
-    ----------------------------------------
+    ---------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 FPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+      (.venv) $ python3 FPFPMiner_old.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
 
       Example Usage:
 
-      (.venv) $ python3  FPFPMiner.py sampleTDB.txt output.txt 2 3
+      (.venv) $ python3  FPFPMiner_old.py sampleTDB.txt output.txt 2 3
 
     .. note:: minSup will be considered in percentage of database transactions
 
 
     **Sample run of importing the code:**
     --------------------------------------
 
@@ -286,98 +316,92 @@
         print("Total Memory in RSS", memRSS)
 
         run = obj.getRuntime()
 
         print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
+    ---------------
             The complete program was written by Sai Chitra.B under the supervision of Professor Rage Uday Kiran.
 
     """
     _startTime = float()
     _endTime = float()
-    _minSup = float()
+    _minSup = str()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _sep = " "
     _Database = []
     _transactions = []
     _fuzzyValues = []
     _ts = []
 
-    def __init__(self, iFile: Union[str, _ab._pd.DataFrame], minSup: Union[int, float], period: Union[int, float], sep: str="\t") -> None:
+    def __init__(self, iFile, minSup, period, sep="\t"):
         super().__init__(iFile, minSup, period, sep)
         self._oFile = ""
         self._BufferSize = 200
         self._itemSetBuffer = []
+        self._mapItemRegions = {}
         self._mapItemSum = {}
+        self._mapItemsHighSum = {}
         self._finalPatterns = {}
         self._joinsCnt = 0
         self._itemsCnt = 0
+        self._mapItemMidSum = {}
         self._startTime = float()
         self._endTime = float()
+        self._mapItemsLowSum = {}
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._dbLen = 0
 
-    def _compareItems(self, o1, o2) -> int:
+    def _compareItems(self, o1, o2):
         """
         A Function that sort all FFI-list in ascending order of Support
 
         :param o1: First FFI-list
-
         :type o1: _FFList
-
         :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
+        :type o2: _FFList
+        :return: Comparison Value
         :rtype: int
         """
         compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
         if compare == 0:
             return int(o1.item) - int(o2.item)
         else:
             return compare
 
-    def _convert(self, value) -> float:
+    def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
-
-        :type value: int or float or str
-
+        :type value: int  or float or str
         :return: converted value
-
         :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (self._dbLen * value)
         if type(value) is str:
             if '.' in value:
                 value = (self._dbLen * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self) -> None:
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
-
-        :return: None
         """
         data, self._transactions, self._fuzzyValues, ts = [], [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
@@ -391,122 +415,48 @@
                 data = _ab._urlopen(self._iFile)
                 count = 0
                 for line in data:
                     line = line.decode("utf-8")
                     line = line.split("\n")[0]
                     parts = line.split(":")
                     parts[0] = parts[0].strip()
-                    parts[1] = parts[1].strip()
+                    parts[2] = parts[2].strip()
                     items = parts[0].split(self._sep)
-                    quantities = parts[1].split(self._sep)
-                    self._ts.append(int(items[0]))
-                    self._transactions.append([x for x in items[1:]])
-                    self._fuzzyValues.append([float(x) for x in quantities])
+                    quantities = parts[2].split(self._sep)
+                    self._ts.append(count)
+                    self._transactions.append([x for x in items])
+                    self._fuzzyValues.append([x for x in quantities])
                     count += 1
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         count = 0
                         for line in f:
                             line = line.split("\n")[0]
                             parts = line.split(":")
                             parts[0] = parts[0].strip()
-                            parts[1] = parts[1].strip()
+                            parts[2] = parts[2].strip()
                             items = parts[0].split(self._sep)
-                            quantities = parts[1].split(self._sep)
-                            self._ts.append(int(items[0]))
-                            self._transactions.append([x for x in items[1:]])
-                            self._fuzzyValues.append([float(x) for x in quantities])
+                            quantities = parts[2].split(self._sep)
+                            self._ts.append(count)
+                            self._transactions.append([x for x in items])
+                            self._fuzzyValues.append([x for x in quantities])
                             count += 1
                 except IOError:
                     print("File Not Found")
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self) -> None:
+    def startMine(self):
         """
         Fuzzy periodic Frequent pattern mining process will start from here
         """
-        maxTID = 0
-        lastTIDs = {}
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        tid = int()
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            self._dbLen += 1
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            if tid < maxTID:
-                maxTID = tid
-            for i in range(0, len(items)):
-                item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
-                else:
-                    self._mapItemSum[item] = quantities[i]
-        listOfFFIList = []
-        mapItemsToFFLIST = {}
-        # self._minSup = self._convert(self._minSup)
-        self._minSup = float(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        for item1 in self._mapItemSum.keys():
-            item = item1
-            if self._mapItemSum[item] >= self._minSup:
-                fUList = _FFList(item)
-                k = tuple([item])
-                mapItemsToFFLIST[k] = fUList
-                listOfFFIList.append(fUList)
-                lastTIDs[item] = tid
-        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                item = pair.item
-                pair.quantity = quantities[i]
-                if self._mapItemSum[item] >= self._minSup:
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i - 1, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                if pair.quantity > remainUtil:
-                    remainingUtility = pair.quantity
-                else:
-                    remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
-                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
-                    if len(FFListOfItem.elements) == 0:
-                        element = _Element(tid, pair.quantity, remainingUtility, 0)
-                    else:
-                        if lastTIDs[pair.item] == tid:
-                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
-                        else:
-                            lastTid = FFListOfItem.elements[-1].tid
-                            curPer = tid - lastTid
-                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
-                    FFListOfItem.addElement(element)
-        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def mine(self) -> None:
+    def mine(self):
         """
         Fuzzy periodic Frequent pattern mining process will start from here
         """
         maxTID = 0
         lastTIDs = {}
         self._startTime = _ab._time.time()
         self._creatingItemSets()
@@ -516,160 +466,198 @@
             tid = int(self._ts[line])
             self._dbLen += 1
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             if tid < maxTID:
                 maxTID = tid
             for i in range(0, len(items)):
+                regions = _Regions(int(quantities[i]), 3)
                 item = items[i]
-                if item in self._mapItemSum:
-                    self._mapItemSum[item] += quantities[i]
+                if item in self._mapItemsLowSum.keys():
+                    low = self._mapItemsLowSum[item]
+                    low += regions.low
+                    self._mapItemsLowSum[item] = low
                 else:
-                    self._mapItemSum[item] = quantities[i]
+                    self._mapItemsLowSum[item] = regions.low
+                if item in self._mapItemMidSum.keys():
+                    mid = self._mapItemMidSum[item]
+                    mid += regions.middle
+                    self._mapItemMidSum[item] = mid
+                else:
+                    self._mapItemMidSum[item] = regions.middle
+                if item in self._mapItemsHighSum.keys():
+                    high = self._mapItemsHighSum[item]
+                    high += regions.high
+                    self._mapItemsHighSum[item] = high
+                else:
+                    self._mapItemsHighSum[item] = regions.high
         listOfFFIList = []
         mapItemsToFFLIST = {}
-        # self._minSup = self._convert(self._minSup)
-        self._minSup = float(self._minSup)
+        itemsToRegion = {}
+        self._minSup = self._convert(self._minSup)
         self._maxPer = self._convert(self._maxPer)
-        for item1 in self._mapItemSum.keys():
+        for item1 in self._mapItemsLowSum.keys():
             item = item1
+            low = self._mapItemsLowSum[item]
+            mid = self._mapItemMidSum[item]
+            high = self._mapItemsHighSum[item]
+            if low >= mid and low >= high:
+                self._mapItemSum[item] = low
+                self._mapItemRegions[item] = "L"
+                itemsToRegion[item] = "L"
+            elif mid >= low and mid >= high:
+                self._mapItemSum[item] = mid
+                self._mapItemRegions[item] = "M"
+                itemsToRegion[item] = "M"
+            elif high >= low and high >= mid:
+                self._mapItemRegions[item] = "H"
+                self._mapItemSum[item] = high
+                itemsToRegion[item] = "H"
             if self._mapItemSum[item] >= self._minSup:
                 fUList = _FFList(item)
-                k = tuple([item])
+                k = tuple([item, itemsToRegion.get(item)])
                 mapItemsToFFLIST[k] = fUList
                 listOfFFIList.append(fUList)
                 lastTIDs[item] = tid
         listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
         for line in range(len(self._transactions)):
             tid = int(self._ts[line])
             items = self._transactions[line]
             quantities = self._fuzzyValues[line]
             revisedTransaction = []
             for i in range(0, len(items)):
                 pair = _Pair()
                 pair.item = items[i]
+                regions = _Regions(int(quantities[i]), 3)
                 item = pair.item
-                pair.quantity = quantities[i]
                 if self._mapItemSum[item] >= self._minSup:
+                    if self._mapItemRegions[pair.item] == "L":
+                        pair.quantity = regions.low
+                    elif self._mapItemRegions[pair.item] == "M":
+                        pair.quantity = regions.middle
+                    elif self._mapItemRegions[pair.item] == "H":
+                        pair.quantity = regions.high
                     if pair.quantity > 0:
                         revisedTransaction.append(pair)
             revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
             for i in range(len(revisedTransaction) - 1, -1, -1):
                 pair = revisedTransaction[i]
                 remainUtil = 0
                 for j in range(len(revisedTransaction) - 1, i - 1, -1):
                     remainUtil += revisedTransaction[j].quantity
                 if pair.quantity > remainUtil:
                     remainingUtility = pair.quantity
                 else:
                     remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(tuple([pair.item])) is not None:
-                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item])]
+                if mapItemsToFFLIST.get(tuple([pair.item, itemsToRegion[pair.item]])) is not None:
+                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item, itemsToRegion[pair.item]])]
                     if len(FFListOfItem.elements) == 0:
                         element = _Element(tid, pair.quantity, remainingUtility, 0)
                     else:
                         if lastTIDs[pair.item] == tid:
                             element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
                         else:
                             lastTid = FFListOfItem.elements[-1].tid
                             curPer = tid - lastTid
                             element = _Element(tid, pair.quantity, remainingUtility, curPer)
                     FFListOfItem.addElement(element)
-        self._FPFPMining(self._itemSetBuffer, 0, listOfFFIList)
+        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def _FPFPMining(self, prefix, prefixLen, fsFim):
+    def _FSFIMining(self, prefix, prefixLen, fsFim, minSup):
 
         """
         Generates FPFP from prefix
 
         :param prefix: the prefix patterns of FPFP
         :type prefix: len
         :param prefixLen: the length of prefix
         :type prefixLen: int
         :param fsFim: the Fuzzy list of prefix itemSets
         :type fsFim: list
+        :param minSup: the minimum support of
+        :type minSup:int
         """
         for i in range(0, len(fsFim)):
             X = fsFim[i]
-            if X.sumLUtil >= self._minSup and X.maxPeriod <= self._maxPer:
+            if X.sumLUtil >= minSup and X.maxPeriod <= self._maxPer:
                 self._WriteOut(prefix, prefixLen, X.item, X.sumLUtil, X.maxPeriod)
-            if X.sumRUtil >= self._minSup:
+            if X.sumRUtil >= minSup:
                 exULs = []
                 for j in range(i + 1, len(fsFim)):
                     Y = fsFim[j]
                     exULs.append(self._construct(X, Y))
                     self._joinsCnt += 1
                 self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FPFPMining(self._itemSetBuffer, prefixLen + 1, exULs)
+                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, )
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
-    def _construct(self, px: _FFList, py: _FFList) -> _FFList:
+    def _construct(self, px, py):
         """
         A function to construct a new Fuzzy item set from 2 fuzzy itemSets
 
-        :param px:the item set px
-        :type px:FFI-List
-        :param py:item set py
-        :type py:FFI-List
-        :return :the item set of pxy(px and py)
-        :rtype :FFI-List
+        :param px: the item set px
+        :type px: FFI-List
+        :param py: item set py
+        :type py: FFI-List
+        :return: the item set of pxy(px and py)
+        :rtype: FFI-List
         """
         pxyUL = _FFList(py.item)
         prev = 0
         for ex in px.elements:
             ey = self._findElementWithTID(py, ex.tid)
             if ey is None:
                 continue
             eXY = _Element(ex.tid, min([ex.lUtils, ey.lUtils], key=lambda x: float(x)), ey.rUtils, ex.tid - prev)
             pxyUL.addElement(eXY)
             prev = ex.tid
         return pxyUL
 
-    def _findElementWithTID(self, UList, tid) -> _Element:
+    def _findElementWithTID(self, UList, tid):
         """
         To find element with same tid as given
 
         :param UList: fuzzy list
-        :type UList:FFI-List
-        :param tid:transaction id
-        :type tid:int
-        :return:element eith tid as given
+        :type UList: FFI-List
+        :param tid: transaction id
+        :type tid: int
+        :return: element with tid as given
         :rtype: element if exist or None
         """
         List = UList.elements
         first = 0
         last = len(List) - 1
         while first <= last:
             mid = (first + last) >> 1
@@ -677,77 +665,75 @@
                 first = mid + 1
             elif List[mid].tid > tid:
                 last = mid - 1
             else:
                 return List[mid]
         return None
 
-    def _WriteOut(self, prefix: List[int], prefixLen: int, item: int, sumLUtil: float, period: int) -> None:
+    def _WriteOut(self, prefix, prefixLen, item, sumLUtil, period):
         """
         To Store the patten
 
         :param prefix: prefix of itemSet
         :type prefix: list
         :param prefixLen: length of prefix
         :type prefixLen: int
         :param item: the last item
         :type item: int
         :param sumLUtil: sum of utility of itemSet
         :type sumLUtil: float
         :param period: represent the period of itemSet
         :type period: int
-        :return: None
         """
         self._itemsCnt += 1
         res = ""
         for i in range(0, prefixLen):
-            res += str(prefix[i]) +  "\t"
-        res += str(item)
+            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
+        res += str(item) + "." + str(self._mapItemRegions.get(item))
         #res1 = str(sumLUtil) + " : " + str(period)
         self._finalPatterns[res] = [sumLUtil, period]
 
-    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def getPatterns(self) -> Dict[str, str]:
+    def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: csv ile
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
         """
         print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in seconds:", self.getRuntime())
@@ -764,17 +750,9 @@
         _ap.mine()
         print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
-        _ap = FPFPMiner('sample.txt', 1, 10, ' ')
-        _ap.startMine()
-        _ap.mine()
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/FPFPMiner_old.py` & `pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,21 +1,28 @@
-# Sample run of importing the code:
-# -------------------------------------
+# SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 #
-#             from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+# **Importing this algorithm into a python program**
 #
-#             obj =alg.FPFPMiner("input.txt",2,3)
+#             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
+#
+#             iFile = 'sampleDB.txt'
+#
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, sep)
 #
 #             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save("output.txt")
+#             obj.save(oFile)
+#
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -23,855 +30,799 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+"""
 
-     This program is free software: you can redistribute it and/or modify
-     it under the terms of the GNU General Public License as published by
-     the Free Software Foundation, either version 3 of the License, or
-     (at your option) any later version.
+from PAMI.weightedFrequentNeighbourhoodPattern.basic import abstract as _fp
+import pandas as pd
+from deprecated import deprecated
+from typing import List, Dict, Tuple, Union, Iterable
 
-     This program is distributed in the hope that it will be useful,
-     but WITHOUT ANY WARRANTY; without even the implied warranty of
-     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
-     GNU General Public License for more details.
+_minWS = str()
+_weights = {}
+_rank = {}
+_neighbourList = {}
 
-     You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-"""
+_fp._sys.setrecursionlimit(20000)
 
 
-from PAMI.fuzzyPeriodicFrequentPattern.basic import abstract as _ab
-from deprecated import deprecated
+class _WeightedItem:
+    """
+    A class used to represent the weight of the item
+
+    :Attributes:
+
+        item: str
+            storing item of the frequent pattern
 
+        weight: float
+            stores the weight of the item
 
-class _FFList:
     """
-    A class represent a Fuzzy List of an element
+    def __init__(self, item: str, weight: float) -> None:
+        self.item = item
+        self.weight = weight
+
+
+class _Node:
+    """
+    A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
-        item: int
-            the item name
-        sumLUtil: float
-            the sum of utilities of a fuzzy item in database
-        sumRUtil: float
-            the sum of resting values of a fuzzy item in database
-        elements: list
-            list of elements contain tid,Utility and resting values of element in each transaction
-        maxPeriod: int
-            it represents the max period of a item
+        itemId: int
+            storing item of a node
 
-    :Methods:
+        counter: int
+            To maintain the support of node
 
-        addElement(element)
-            Method to add an element to this fuzzy list and update the sums at the same time.
+        parent: node
+            To maintain the parent of node
 
-        printElement(e)
-            Method to print elements
+        children: list
+            To maintain the children of node
+
+    :Methods:
+
+        addChild(node)
+            Updates the nodes children list and parent for the given node
 
     """
 
-    def __init__(self, itemName):
-        self.item = itemName
-        self.sumLUtil = 0.0
-        self.sumRUtil = 0.0
-        self.elements = []
-        self.maxPeriod = 0
+    def __init__(self, item: str, children: Dict[str, '_Node']) -> None:
+        self.itemId = item
+        self.counter = 1
+        self.weight = 0
+        self.parent = None
+        self.children = children
 
-    def addElement(self, element):
+    def addChild(self, node: '_Node') -> None:
         """
-        A Method that add a new element to FFList
+        Retrieving the child from the tree
 
-        :param element: an element to be added to FFList
-        :type element: Element
-        """
-        self.sumLUtil += element.lUtils
-        self.sumRUtil += element.rUtils
-        self.elements.append(element)
-        self.maxPeriod = max(self.maxPeriod, element.period)
+        :param node: Children node.
+        :type node: Node
+        :return: Updates the children nodes and parent nodes
+        :return: None
 
-    def printElement(self):
         """
-        A Method to Print elements in the FFList
-        """
-        for ele in self.elements:
-            print(ele.tid, ele.lUtils, ele.rUtils, ele.period)
+        self.children[node.itemId] = node
+        node.parent = self
 
 
-class _Element:
+class _Tree:
     """
-    A class represents an Element of a fuzzy list
+    A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
-    tid : int
-        keep tact of transaction id
-    lUtils: float
-        the utility of a fuzzy item in the transaction
-    rUtils : float
-        the resting value of a fuzzy item in the transaction
-    period: int
-        represent the period of the element
-    """
+        root : Node
+            The first node of the tree set to Null.
 
-    def __init__(self, tid, iUtil, rUtil, period):
-        self.tid = tid
-        self.lUtils = iUtil
-        self.rUtils = rUtil
-        self.period = period
+        summaries : dictionary
+            Stores the nodes itemId which shares same itemId
 
+        info : dictionary
+            frequency of items in the transactions
 
-class _Regions:
-    """
-    A class calculate the regions
+    :Methods:
 
-    :Attributes:
+        addTransaction(transaction, freq)
+            adding items of  transactions into the tree as nodes and freq is the count of nodes
+        getFinalConditionalPatterns(node)
+            getting the conditional patterns from fp-tree for a node
+        getConditionalPatterns(patterns, frequencies)
+            sort the patterns by removing the items with lower minWS
+        generatePatterns(prefix)
+            generating the patterns from fp-tree
+    """
 
-        low : int
-            low region value
-        middle: int
-            middle region value
-        high : int
-            high region values
-        """
-
-    def __init__(self, quantity, regionsNumber):
-        self.low = 0
-        self.middle = 0
-        self.high = 0
-        if regionsNumber == 3:  # if we have 3 regions
-            if 0 < quantity <= 1:
-                self.low = 1
-                self.high = 0
-                self.middle = 0
-            elif 1 < quantity <= 6:
-                self.low = float((6 - quantity) / 5)
-                self.middle = float((quantity - 1) / 5)
-                self.high = 0
-            elif 6 < quantity <= 11:
-                self.low = 0
-                self.middle = float((11 - quantity) / 5)
-                self.high = float((quantity - 6) / 5)
+    def __init__(self) -> None:
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction: List[_WeightedItem], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return: None
+        """
+
+        # This method takes transaction as input and returns the tree
+        global _neighbourList, _rank
+        currentNode = self.root
+        for i in range(len(transaction)):
+            wei = 0
+            l1 = i
+            while l1 >= 0:
+                wei += transaction[l1].weight
+                l1 -= 1
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                newNode.freq = count
+                newNode.weight = wei
+                currentNode.addChild(newNode)
+                if _rank[transaction[i].item] in self.summaries:
+                    self.summaries[_rank[transaction[i].item]].append(newNode)
+                else:
+                    self.summaries[_rank[transaction[i].item]] = [newNode]
+                currentNode = newNode
             else:
-                self.low = 0
-                self.middle = 0
-                self.high = 1
-
+                currentNode = currentNode.children[transaction[i].item]
+                currentNode.freq += count
+                currentNode.weight += wei
+
+    def addConditionalPattern(self, transaction: List[_WeightedItem], count: int) -> None:
+        """
+        Adding transaction into tree
+
+        :param transaction: it represents the one transaction in database
+        :type transaction: list
+        :param count: frequency of item
+        :type count: int
+        :return : None
+        """
+        # This method takes transaction as input and returns the tree
+        global _neighbourList, _rank
+        currentNode = self.root
+        for i in range(len(transaction)):
+            wei = 0
+            l1 = i
+            while l1 >= 0:
+                wei += transaction[l1].weight
+                l1 -= 1
+            if transaction[i].itemId not in currentNode.children:
+                newNode = _Node(transaction[i].itemId, {})
+                newNode.freq = count
+                newNode.weight = wei
+                currentNode.addChild(newNode)
+                if _rank[transaction[i].itemId] in self.summaries:
+                    self.summaries[_rank[transaction[i].itemId]].append(newNode)
+                else:
+                    self.summaries[_rank[transaction[i].itemId]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].itemId]
+                currentNode.freq += count
+                currentNode.weight += wei
 
-class _Pair:
-    """
-    A class to store item name and quantity together.
-    """
+    def printTree(self, root: _Node) -> None:
+        """
+        To print the details of tree
 
-    def __init__(self):
-        self.item = 0
-        self.quantity = 0
+        :param root: root node of the tree
+        :return: details of tree
+        """
+        if len(root.children) == 0:
+            return
+        else:
+            for x, y in root.children.items():
+                #print(y.itemId, y.parent.itemId, y.freq, y.weight)
+                self.printTree(y)
+
+
+    def getFinalConditionalPatterns(self, alpha: int) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+        """
+        Generates the conditional patterns for a node
+
+        :param alpha: node to generate conditional patterns
+        :return: returns conditional patterns, frequency of each item in conditional patterns
+        """
+        finalPatterns = []
+        finalFreq = []
+        global _neighbourList
+        for i in self.summaries[alpha]:
+            set1 = i.weight
+            set2 = []
+            while i.parent.itemId is not None:
+                if i.parent.itemId in _neighbourList[i.itemId]:
+                    set2.append(i.parent)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalFreq.append(set1)
+        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
+        return finalPatterns, finalFreq, info
+
+    @staticmethod
+    def getConditionalTransactions(ConditionalPatterns: List[List[_Node]], conditionalFreq: List[float]) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+        """
+        To calculate the frequency of items in conditional patterns and sorting the patterns
+
+        :param ConditionalPatterns: paths of a node
+        :param conditionalFreq: frequency of each item in the path
+        :return: conditional patterns and frequency of each item in transactions
+        """
+        global _rank
+        pat = []
+        freq = []
+        data1 = {}
+        for i in range(len(ConditionalPatterns)):
+            for j in ConditionalPatterns[i]:
+                if j.itemId in data1:
+                    data1[j.itemId] += conditionalFreq[i]
+                else:
+                    data1[j.itemId] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minWS}
+        count = 0
+        for p in ConditionalPatterns:
+            p1 = [v for v in p if v.itemId in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x)), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                freq.append(conditionalFreq[count])
+            count += 1
+        up_dict = {_rank[k]: v for k, v in up_dict.items()}
+        return pat, freq, up_dict
+
+    def generatePatterns(self, prefix: List[int]) -> Iterable[Tuple[List[int], float]]:
+        """
+        To generate the frequent patterns
+
+        :param prefix: an empty list
+        :return: Frequent patterns that are extracted from fp-tree
+
+        """
+        global _minWS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, freq, info = self.getFinalConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addConditionalPattern(patterns[pat], freq[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
 
 
-class FPFPMiner(_ab._fuzzyPeriodicFrequentPatterns):
+class SWFPGrowth(_fp._weightedFrequentSpatialPatterns):
     """
-    :Description:   Fuzzy Periodic Frequent Pattern Miner is desired to find all fuzzy periodic frequent patterns which is
-                    on-trivial and challenging problem to its huge search space.we are using efficient pruning
-                    techniques to reduce the search space.
+    About this algorithm
+    ====================
 
-    :Reference:
+    :Description: SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
 
+    :Reference: R. Uday Kiran, P. P. C. Reddy, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
+                "Discovering Spatial Weighted Frequent Itemsets in Spatiotemporal Databases," 2019 International
+                Conference on Data Mining Workshops (ICDMW), 2019, pp. 987-996, doi: 10.1109/ICDMW.2019.00143.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent patterns
+                   Name of the Input file to mine complete set of weighted Frequent Neighbourhood Patterns.
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float :
-                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+                   Name of the output file to store complete set of weighted Frequent Neighbourhood Patterns.
+    :param  minSup: int or str or float:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  maxper: floot :
+                   where maxPer represents the maximum periodicity threshold value specified by the user.
 
 
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of fuzzy spatial frequent patterns
+            Input file name or path of the input file
+
+        minWS: float or int or str
+            The user can specify minWS either in count or proportion of database size.
+            If the program detects the data type of minWS is integer, then it treats minWS is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minWS=10 will be treated as integer, while minWS=10.0 will be treated as float
+
+        minWeight: float or int or str
+            The user can specify minWeight either in count or proportion of database size.
+            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
+
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            However, the users can override their default separator.
+
         oFile : file
-               Name of the oFile file to store complete set of fuzzy spatial frequent patterns
-        minSup : float
-            The user given support
-        period: int
-            periodicity of an element
-        memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
+            Name of the output file or the path of the output file
+
         startTime:float
-               To record the start time of the mining process
+            To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
-        itemsCnt: int
-            To record the number of fuzzy spatial itemSets generated
-        mapItemsLowSum: map
-            To keep track of low region values of items
-        mapItemsMidSum: map
-            To keep track of middle region values of items
-        mapItemsHighSum: map
-            To keep track of high region values of items
-        mapItemSum: map
-            To keep track of sum of Fuzzy Values of items
-        mapItemRegions: map
-            To Keep track of fuzzy regions of item
-        jointCnt: int
-            To keep track of the number of FFI-list that was constructed
-        BufferSize: int
-            represent the size of Buffer
-        itemBuffer list
-            to keep track of items in buffer
-        maxTID: int
-            represent the maximum tid of the database
-        lastTIDs: map
-            represent the last tid of fuzzy items
-        itemsToRegion: map
-            represent items with respective regions
 
-    :Methods:
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+        Database : list
+            To store the transactions of a database in list
 
-        startMine()
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+
+        lno : int
+            it represents the total no of transactions
+
+        tree : class
+            it represents the Tree class
+
+        finalPatterns : dict
+            it represents to store the patterns
+
+    :Methods :
+
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        convert(value):
-            To convert the given user specified value
-        FSFIMining( prefix, prefixLen, fsFim, minSup)
-            Method generate FFI from prefix
-        construct(px, py)
-            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
-        findElementWithTID(UList, tid)
-            To find element with same tid as given
-        WriteOut(prefix, prefixLen, item, sumIUtil,period)
-            To Store the patten
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
+        frequentOneItem()
+            Extracts the one-frequent patterns from transactions
+
+    Execution methods
+    =================
+
+
+    **Terminal command**
 
-    **Executing the code on terminal :**
-    ---------------------------------------
 
     .. code-block:: console
 
-      Format:
+       Format:
 
-      (.venv) $ python3 FPFPMiner_old.py <inputFile> <outputFile> <minSup> <maxPer> <sep>
+       (.venv) $ python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
 
-      Example Usage:
+       Example usage :
 
-      (.venv) $ python3  FPFPMiner_old.py sampleTDB.txt output.txt 2 3
+       (.venv) $ python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
 
-    .. note:: minSup will be considered in percentage of database transactions
+    .. note:: minSup will be considered in support count or frequency
 
 
-    **Sample run of importing the code:**
-    --------------------------------------
+    **Calling from a python program**
 
-        from PAMI.fuzzyPeriodicFrequentPattern.basic import FPFPMiner as alg
+    .. code-block:: python
 
-        obj =alg.FPFPMiner("input.txt",2,3)
+            from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
 
-        obj.mine()
+            obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, seperator)
 
-        periodicFrequentPatterns = obj.getPatterns()
+            iFile = 'sampleDB.txt'
 
-        print("Total number of Fuzzy Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+            minSup = 10  # can also be specified between 0 and 1
 
-        obj.save("output.txt")
+            obj.mine()
 
-        memUSS = obj.getMemoryUSS()
+            frequentPatterns = obj.getPatterns()
 
-        print("Total Memory in USS:", memUSS)
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-        memRSS = obj.getMemoryRSS()
+            obj.save(oFile)
 
-        print("Total Memory in RSS", memRSS)
+            Df = obj.getPatternsAsDataFrame()
 
-        run = obj.getRuntime()
+            memUSS = obj.getmemoryUSS()
 
-        print("Total ExecutionTime in seconds:", run)
+            print("Total Memory in USS:", memUSS)
 
-    **Credits:**
-    ---------------
-            The complete program was written by Sai Chitra.B under the supervision of Professor Rage Uday Kiran.
+            memRSS = obj.getMemoryRSS()
 
-    """
-    _startTime = float()
-    _endTime = float()
-    _minSup = str()
-    _maxPer = float()
-    _finalPatterns = {}
-    _iFile = " "
-    _oFile = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _sep = " "
-    _Database = []
-    _transactions = []
-    _fuzzyValues = []
-    _ts = []
-
-    def __init__(self, iFile, minSup, period, sep="\t"):
-        super().__init__(iFile, minSup, period, sep)
-        self._oFile = ""
-        self._BufferSize = 200
-        self._itemSetBuffer = []
-        self._mapItemRegions = {}
-        self._mapItemSum = {}
-        self._mapItemsHighSum = {}
-        self._finalPatterns = {}
-        self._joinsCnt = 0
-        self._itemsCnt = 0
-        self._mapItemMidSum = {}
-        self._startTime = float()
-        self._endTime = float()
-        self._mapItemsLowSum = {}
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._dbLen = 0
-
-    def _compareItems(self, o1, o2):
-        """
-        A Function that sort all FFI-list in ascending order of Support
-
-        :param o1: First FFI-list
-
-        :type o1: _FFList
-
-        :param o2: Second FFI-list
-
-        :type o1: _FFList
-
-        :return: Comparision Value
-
-        :rtype: int
-        """
-        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
-        if compare == 0:
-            return int(o1.item) - int(o2.item)
-        else:
-            return compare
+            print("Total Memory in RSS", memRSS)
 
-    def _convert(self, value):
-        """
-        To convert the given user specified value
+            run = obj.getRuntime()
 
-        :param value: user specified value
+            print("Total ExecutionTime in seconds:", run)
 
-        :type value: int  or float or str
+    Credits
+    =======
 
-        :return: converted value
 
-        :rtype: float
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (self._dbLen * value)
-        if type(value) is str:
-            if '.' in value:
-                value = (self._dbLen * value)
-            else:
-                value = int(value)
-        return value
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
+    """
 
-    def _creatingItemSets(self):
+    __startTime = float()
+    __endTime = float()
+    _Weights = {}
+    _minWS = str()
+    __finalPatterns = {}
+    _neighbourList = {}
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __tree = _Tree()
+    __rank = {}
+    __rankDup = {}
+
+    def __init__(self, iFile: Union[str, _fp._pd.DataFrame], nFile: Union[str, _fp._pd.DataFrame], minWS: Union[int, float, str], sep='\t') -> None:
+        super().__init__(iFile, nFile, minWS, sep)
+
+    def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
+        :return: None
         """
-        data, self._transactions, self._fuzzyValues, ts = [], [], [], []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
+        self._Database = []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                self._ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._transactions = self._iFile['Transactions'].tolist()
-            if 'fuzzyValues' in i:
-                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
+            # print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
-                count = 0
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
+                    line.strip()
                     line = line.decode("utf-8")
-                    line = line.split("\n")[0]
-                    parts = line.split(":")
-                    parts[0] = parts[0].strip()
-                    parts[2] = parts[2].strip()
-                    items = parts[0].split(self._sep)
-                    quantities = parts[2].split(self._sep)
-                    self._ts.append(count)
-                    self._transactions.append([x for x in items])
-                    self._fuzzyValues.append([x for x in quantities])
-                    count += 1
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
-                        count = 0
                         for line in f:
-                            line = line.split("\n")[0]
-                            parts = line.split(":")
-                            parts[0] = parts[0].strip()
-                            parts[2] = parts[2].strip()
-                            items = parts[0].split(self._sep)
-                            quantities = parts[2].split(self._sep)
-                            self._ts.append(count)
-                            self._transactions.append([x for x in items])
-                            self._fuzzyValues.append([x for x in quantities])
-                            count += 1
+                            line = line.strip()
+                            line = line.split(':')
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [int(i.strip()) for i in line[1].split(self._sep)]
+                            tr = []
+                            for i in range(len(temp1)):
+                                we = _WeightedItem(temp1[i], temp2[i])
+                                tr.append(we)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
+    def _scanNeighbours(self) -> None:
         """
-        Fuzzy periodic Frequent pattern mining process will start from here
+        Scans the neighbors file and creates a dictionary of items and their corresponding neighbor lists.
+
+        :return: None
         """
-        maxTID = 0
-        lastTIDs = {}
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        tid = int()
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            self._dbLen += 1
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            if tid < maxTID:
-                maxTID = tid
-            for i in range(0, len(items)):
-                regions = _Regions(int(quantities[i]), 3)
-                item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemMidSum.keys():
-                    mid = self._mapItemMidSum[item]
-                    mid += regions.middle
-                    self._mapItemMidSum[item] = mid
-                else:
-                    self._mapItemMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfFFIList = []
-        mapItemsToFFLIST = {}
-        itemsToRegion = {}
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-                itemsToRegion[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-                itemsToRegion[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
-                itemsToRegion[item] = "H"
-            if self._mapItemSum[item] >= self._minSup:
-                fUList = _FFList(item)
-                k = tuple([item, itemsToRegion.get(item)])
-                mapItemsToFFLIST[k] = fUList
-                listOfFFIList.append(fUList)
-                lastTIDs[item] = tid
-        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                regions = _Regions(int(quantities[i]), 3)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i - 1, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                if pair.quantity > remainUtil:
-                    remainingUtility = pair.quantity
-                else:
-                    remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(tuple([pair.item, itemsToRegion[pair.item]])) is not None:
-                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item, itemsToRegion[pair.item]])]
-                    if len(FFListOfItem.elements) == 0:
-                        element = _Element(tid, pair.quantity, remainingUtility, 0)
-                    else:
-                        if lastTIDs[pair.item] == tid:
-                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
-                        else:
-                            lastTid = FFListOfItem.elements[-1].tid
-                            curPer = tid - lastTid
-                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
-                    FFListOfItem.addElement(element)
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-
-    def mine(self):
-        """
-        Fuzzy periodic Frequent pattern mining process will start from here
-        """
-        maxTID = 0
-        lastTIDs = {}
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        tid = int()
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            self._dbLen += 1
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            if tid < maxTID:
-                maxTID = tid
-            for i in range(0, len(items)):
-                regions = _Regions(int(quantities[i]), 3)
-                item = items[i]
-                if item in self._mapItemsLowSum.keys():
-                    low = self._mapItemsLowSum[item]
-                    low += regions.low
-                    self._mapItemsLowSum[item] = low
-                else:
-                    self._mapItemsLowSum[item] = regions.low
-                if item in self._mapItemMidSum.keys():
-                    mid = self._mapItemMidSum[item]
-                    mid += regions.middle
-                    self._mapItemMidSum[item] = mid
-                else:
-                    self._mapItemMidSum[item] = regions.middle
-                if item in self._mapItemsHighSum.keys():
-                    high = self._mapItemsHighSum[item]
-                    high += regions.high
-                    self._mapItemsHighSum[item] = high
-                else:
-                    self._mapItemsHighSum[item] = regions.high
-        listOfFFIList = []
-        mapItemsToFFLIST = {}
-        itemsToRegion = {}
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        for item1 in self._mapItemsLowSum.keys():
-            item = item1
-            low = self._mapItemsLowSum[item]
-            mid = self._mapItemMidSum[item]
-            high = self._mapItemsHighSum[item]
-            if low >= mid and low >= high:
-                self._mapItemSum[item] = low
-                self._mapItemRegions[item] = "L"
-                itemsToRegion[item] = "L"
-            elif mid >= low and mid >= high:
-                self._mapItemSum[item] = mid
-                self._mapItemRegions[item] = "M"
-                itemsToRegion[item] = "M"
-            elif high >= low and high >= mid:
-                self._mapItemRegions[item] = "H"
-                self._mapItemSum[item] = high
-                itemsToRegion[item] = "H"
-            if self._mapItemSum[item] >= self._minSup:
-                fUList = _FFList(item)
-                k = tuple([item, itemsToRegion.get(item)])
-                mapItemsToFFLIST[k] = fUList
-                listOfFFIList.append(fUList)
-                lastTIDs[item] = tid
-        listOfFFIList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        for line in range(len(self._transactions)):
-            tid = int(self._ts[line])
-            items = self._transactions[line]
-            quantities = self._fuzzyValues[line]
-            revisedTransaction = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                regions = _Regions(int(quantities[i]), 3)
-                item = pair.item
-                if self._mapItemSum[item] >= self._minSup:
-                    if self._mapItemRegions[pair.item] == "L":
-                        pair.quantity = regions.low
-                    elif self._mapItemRegions[pair.item] == "M":
-                        pair.quantity = regions.middle
-                    elif self._mapItemRegions[pair.item] == "H":
-                        pair.quantity = regions.high
-                    if pair.quantity > 0:
-                        revisedTransaction.append(pair)
-            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-            for i in range(len(revisedTransaction) - 1, -1, -1):
-                pair = revisedTransaction[i]
-                remainUtil = 0
-                for j in range(len(revisedTransaction) - 1, i - 1, -1):
-                    remainUtil += revisedTransaction[j].quantity
-                if pair.quantity > remainUtil:
-                    remainingUtility = pair.quantity
+        self._neighbourList = {}
+        if isinstance(self._nFile, _fp._pd.DataFrame):
+            data, items = [], []
+            if self._nFile.empty:
+                print("its empty..")
+            i = self._nFile.columns.values.tolist()
+            if 'item' in i:
+                items = self._nFile['items'].tolist()
+            if 'Neighbours' in i:
+                data = self._nFile['Neighbours'].tolist()
+            for k in range(len(items)):
+                self._neighbourList[items[k][0]] = data[k]
+            # print(self.Database)
+        if isinstance(self._nFile, str):
+            if _fp._validators.url(self._nFile):
+                data = _fp._urlopen(self._nFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._neighbourList[temp[0]] = temp[1:]
+            else:
+                try:
+                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._neighbourList[temp[0]] = temp[1:]
+                except IOError:
+                    print("File Not Found2")
+                    quit()
+
+    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
+        """
+        To convert the type of user specified minWS value
+
+        :param value: user specified minWS value
+        :return: converted type
+        """
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
+
+    def __frequentOneItem(self) -> List[str]:
+        """
+        Generating One frequent items sets
+
+        :return: None
+        """
+        global _maxWeight
+        self._mapSupport = {}
+        for tr in self._Database:
+            for i in tr:
+                nn = [j for j in tr if j.item in self._neighbourList[i.item]]
+                if i.item not in self._mapSupport:
+                    self._mapSupport[i.item] = i.weight
                 else:
-                    remainingUtility = remainUtil
-                if mapItemsToFFLIST.get(tuple([pair.item, itemsToRegion[pair.item]])) is not None:
-                    FFListOfItem = mapItemsToFFLIST[tuple([pair.item, itemsToRegion[pair.item]])]
-                    if len(FFListOfItem.elements) == 0:
-                        element = _Element(tid, pair.quantity, remainingUtility, 0)
-                    else:
-                        if lastTIDs[pair.item] == tid:
-                            element = _Element(tid, pair.quantity, remainingUtility, maxTID - tid)
-                        else:
-                            lastTid = FFListOfItem.elements[-1].tid
-                            curPer = tid - lastTid
-                            element = _Element(tid, pair.quantity, remainingUtility, curPer)
-                    FFListOfItem.addElement(element)
-        self._FSFIMining(self._itemSetBuffer, 0, listOfFFIList, self._minSup)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-
-    def _FSFIMining(self, prefix, prefixLen, fsFim, minSup):
-
-        """
-        Generates FPFP from prefix
-
-        :param prefix: the prefix patterns of FPFP
-        :type prefix: len
-        :param prefixLen: the length of prefix
-        :type prefixLen: int
-        :param fsFim: the Fuzzy list of prefix itemSets
-        :type fsFim: list
-        :param minSup: the minimum support of
-        :type minSup:int
-        """
-        for i in range(0, len(fsFim)):
-            X = fsFim[i]
-            if X.sumLUtil >= minSup and X.maxPeriod <= self._maxPer:
-                self._WriteOut(prefix, prefixLen, X.item, X.sumLUtil, X.maxPeriod)
-            if X.sumRUtil >= minSup:
-                exULs = []
-                for j in range(i + 1, len(fsFim)):
-                    Y = fsFim[j]
-                    exULs.append(self._construct(X, Y))
-                    self._joinsCnt += 1
-                self._itemSetBuffer.insert(prefixLen, X.item)
-                self._FSFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup, )
+                    self._mapSupport[i.item] += i.weight
+                for k in nn:
+                    self._mapSupport[i.item] += k.weight
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minWS}
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        return genList
+
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[_WeightedItem]]:
+        """
+        Updates the items in transactions with rank of items according to their support
+
+        :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
+                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
+        :param itemSet: list of one-frequent items
+        :return: list
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = []
+            for i in range(len(tr)):
+                if tr[i].item in itemSet:
+                    list2.append(tr[i])
+            if len(list2) >= 1:
+                basket = list2
+                basket.sort(key=lambda val: self.__rank[val.item])
+                list1.append(basket)
+        return list1
+
+    @staticmethod
+    def __buildTree(transactions: List[List[_WeightedItem]], info: Dict[int, float]) -> _Tree:
+        """
+        Builds the tree with updated transactions
+
+        :param transactions: updated transactions
+        :param info: support details of each item in transactions.
+        :return: transactions compressed in fp-tree.
+        """
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(transactions)):
+            rootNode.addTransaction(transactions[i], 1)
+        return rootNode
+
+    def __savePeriodic(self, itemSet: List[str]) -> str:
+        """
+        The duplication items and their ranks
+
+        :param itemSet: frequent itemSet that generated
+        :return: patterns with original item names.
+
+        """
+        temp = str()
+        for i in itemSet:
+            temp = temp + self.__rankDup[i] + "\t"
+        return temp
+
+    @deprecated(
+         "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+        self.mine()
+
+    def mine(self) -> None:
+        """
+        Frequent pattern mining process will start from here
+        """
+        global _minWS, _neighbourList, _rank
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minWS is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._scanNeighbours()
+        self._minWS = self.__convert(self._minWS)
+        _minWS = self._minWS
+        itemSet = self.__frequentOneItem()
+        updatedTransactions = self.__updateTransactions(itemSet)
+        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
+        _rank = self.__rank
+        for x, y in self.__rank.items():
+            self.__rankDup[y] = x
+        _neighbourList = self._neighbourList
+        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
+        # for x, y in self._neighbourList.items():
+        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
+        #     _neighbourList[self.__rank[x]] = xx
+        # print(_neighbourList)
+        __Tree = self.__buildTree(updatedTransactions, info)
+        patterns = __Tree.generatePatterns([])
+        self.__finalPatterns = {}
+        for k in patterns:
+            s = self.__savePeriodic(k[0])
+            self.__finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-        return self._memoryRSS
 
-    def getRuntime(self):
+        return self.__memoryRSS
+
+    def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def _construct(self, px, py):
-        """
-        A function to construct a new Fuzzy item set from 2 fuzzy itemSets
+        return self.__endTime - self.__startTime
 
-        :param px:the item set px
-        :type px:FFI-List
-        :param py:item set py
-        :type py:FFI-List
-        :return :the item set of pxy(px and py)
-        :rtype :FFI-List
-        """
-        pxyUL = _FFList(py.item)
-        prev = 0
-        for ex in px.elements:
-            ey = self._findElementWithTID(py, ex.tid)
-            if ey is None:
-                continue
-            eXY = _Element(ex.tid, min([ex.lUtils, ey.lUtils], key=lambda x: float(x)), ey.rUtils, ex.tid - prev)
-            pxyUL.addElement(eXY)
-            prev = ex.tid
-        return pxyUL
-
-    def _findElementWithTID(self, UList, tid):
-        """
-        To find element with same tid as given
-
-        :param UList: fuzzy list
-        :type UList: FFI-List
-        :param tid: transaction id
-        :type tid: int
-        :return: element with tid as given
-        :rtype: element if exist or None
-        """
-        List = UList.elements
-        first = 0
-        last = len(List) - 1
-        while first <= last:
-            mid = (first + last) >> 1
-            if List[mid].tid < tid:
-                first = mid + 1
-            elif List[mid].tid > tid:
-                last = mid - 1
-            else:
-                return List[mid]
-        return None
-
-    def _WriteOut(self, prefix, prefixLen, item, sumLUtil, period):
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
-        To Store the patten
-
-        :param prefix: prefix of itemSet
-        :type prefix: list
-        :param prefixLen: length of prefix
-        :type prefixLen: int
-        :param item: the last item
-        :type item: int
-        :param sumLUtil: sum of utility of itemSet
-        :type sumLUtil: float
-        :param period: represent the period of itemSet
-        :type period: int
-        """
-        self._itemsCnt += 1
-        res = ""
-        for i in range(0, prefixLen):
-            res += str(prefix[i]) + "." + str(self._mapItemRegions[prefix[i]]) + "\t"
-        res += str(item) + "." + str(self._mapItemRegions.get(item))
-        #res1 = str(sumLUtil) + " : " + str(period)
-        self._finalPatterns[res] = [sumLUtil, period]
 
-    def getPatternsAsDataFrame(self):
-        """
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+        for a, b in self.__finalPatterns.items():
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def getPatterns(self):
+    def save(self, outFile: str) -> None:
         """
-        Function to send the set of frequent patterns after completion of the mining process
 
-        :return: returning frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
-
-    def save(self, outFile):
-        """
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv ile
+        :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % patternsAndSupport)
+        for x, y in self.__finalPatterns.items():
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
+
+    def getPatterns(self) -> Dict[str, float]:
+        """
+
+        Function to send the set of frequent patterns after completion of the mining process
 
-    def printResults(self):
+        :return: returning frequent patterns
+        :rtype: dict
+        """
+        return self.__finalPatterns
+
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Spatial Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:  # to  include a user specified separator
-            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:  # to consider "\t" as a separator
-            _ap = FPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_fp._sys.argv) == 7 or len(_fp._sys.argv) == 8:
+        if len(_fp._sys.argv) == 8:
+            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6],
+                             _fp._sys.argv[7])
+        if len(_fp._sys.argv) == 7:
+            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Fuzzy Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
+        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Total Memory in RSS",  _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        _ap = SWFPGrowth('sample.txt', 'neighbourSample.txt', 150, ' ')
+        _ap.startMine()
+        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py` & `pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/fuzzyPeriodicFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,29 +46,29 @@
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        minSup: integer or float or str
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=0.1 will be treated as float
-        maxPer: int
+        maxPer : int
             The user specified Maximum periodicity
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
         startTime:float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
```

### Comparing `pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/GPFPMiner.py` & `pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,23 +1,21 @@
-# GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
-# Lattice Traversal to mine the geo referenced peridoic frequent patterns.
+# STEclat is one of the fundamental algorithm to discover geo refereneced partial periodic-frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 #
-#             import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
-#
-#             obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
+#             obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
 #
 #             obj.mine()
 #
-#             Patterns = obj.getPatterns()
+#             partialPeriodicSpatialPatterns = obj.getPatterns()
 #
-#             print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
+#             print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
 #
 #             obj.save("outFile")
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -48,130 +46,130 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from  PAMI.geoReferencedPeriodicFrequentPattern.basic import abstract as _ab
+
+from PAMI.georeferencedPartialPeriodicPattern.basic import abstract as _ab
 from deprecated import deprecated
 
 
-class GPFPMiner(_ab._geoReferencedPeriodicFrequentPatterns):
-    """ 
-    :Description:   GPFPMiner is a Extension of ECLAT algorithm,which  stands for Equivalence Class Clustering and bottom-up
-                    Lattice Traversal to mine the geo referenced peridoic frequent patterns.
-        
-    :Reference:
+class STEclat(_ab._partialPeriodicSpatialPatterns):
+    """
+    :Description:   STEclat is one of the fundamental algorithm to discover georefereneced partial periodic-frequent patterns in a transactional database.
+
+    :Reference:   R. Uday Kiran, C. Saideep, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
+                 "Discovering Partial Periodic Spatial Patterns in Spatiotemporal Databases," 2019 IEEE International
+                  Conference on Big Data (Big Data), 2019, pp. 233-238, doi: 10.1109/BigData47090.2019.9005693.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of Geo-referenced periodic frequent patterns
+                   Name of the Input file to mine complete set of Geo-referenced Partial Periodic patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of Geo-referenced periodic frequent patterns
-    :param  minSup: int or float or str :
-                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
-    :param maxPer: float :
-                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+                   Name of the output file to store complete set of Geo-referenced Partial Periodic patterns
+    :param  minPS: int or float or str :
+                   The user can specify minPS either in count or proportion of database size. If the program detects the data type of minPS is integer, then it treats minPS is expressed in count. Otherwise, it will be treated as float.
+    :param maxIAT: int or float or str :
+                   The user can specify maxIAT either in count or proportion of database size. If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count. Otherwise, it will be treated as float.
     :param nFile: str :
-                   Name of the input file to mine complete set of Geo-referenced periodic frequent patterns
+                   Name of the input file to mine complete set of Geo-referenced Partial Periodic patterns
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
-
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str:
+        nFile : str
            Name of Neighbourhood file name
-        minSup: float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+        maxIAT : float or int or str
+            The user can specify maxIAT either in count or proportion of database size.
+            If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: float or int or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats minSup is expressed in count.
+            Example: maxIAT=10 will be treated as integer, while maxIAT=10.0 will be treated as float
+        minPS : float or int or str
+            The user can specify minPS either in count or proportion of database size.
+            If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+            Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         Database : list
             To store the complete set of transactions available in the input database/file
 
     :Methods:
 
-            startMine()
-                Mining process will start from here
-            getPatterns()
-                Complete set of patterns will be retrieved with this function
-            save(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
-            getPatternsAsDataFrames()
-                Complete set of frequent patterns will be loaded in to a dataframe
-            getMemoryUSS()
-                Total amount of USS memory consumed by the mining process will be retrieved from this function
-            getMemoryRSS()
-                Total amount of RSS memory consumed by the mining process will be retrieved from this function
-            getRuntime()
-                Total amount of runtime taken by the mining process will be retrieved from this function
-            creatingItemSets(iFileName)
-                Storing the complete transactions of the database/input file in a database variable
-            frequentOneItem()
-                Generating one frequent patterns
-            convert(value):
-                To convert the given user specified value    
-            getNeighbourItems(keySet):
-                A function to get common neighbours of a itemSet
-             mapNeighbours(file):
-                A function to map items to their neighbours
+        mine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrames()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(iFileName)
+            Storing the complete transactions of the database/input file in a database variable
+        frequentOneItem()
+            Generating one frequent patterns
+        convert(value):
+            To convert the given user specified value
+        getNeighbourItems(keySet)
+            A function to get common neighbours of a itemSet
+        mapNeighbours(file)
+            A function to map items to their neighbours
 
     **Executing the code on terminal :**
     ----------------------------------------
 
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 GPFPMiner.py <inputFile> <outputFile> <neighbourFile> <minSup> <maxPer>
+      (.venv) $ python3 STEclat.py <inputFile> <outputFile> <neighbourFile>  <minPS>  <maxIAT>
 
       Example Usage:
 
-      (.venv) $ python3 GPFPMiner.py sampleTDB.txt output.txt sampleN.txt 0.5 0.3
-
-    .. note:: minSup & maxPer will be considered in percentage of database transactions
+      (.venv) $ python3 STEclat.py sampleTDB.txt output.txt sampleN.txt 0.2 0.5
 
+    .. note:: maxIAT & minPS will be considered in percentage of database transactions
 
 
     **Sample run of importing the code :**
-    -----------------------------------------
+    --------------------------------------
     .. code-block:: python
     
-            import PAMI.geoReferencedPeridicFrequentPattern.GPFPMiner as alg
+            import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 
-            obj = alg.GPFPMiner("sampleTDB.txt", "sampleN.txt", 5, 3)
+            obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
 
             obj.mine()
 
-            Patterns = obj.getPatterns()
+            partialPeriodicSpatialPatterns = obj.getPatterns()
 
-            print("Total number of Geo Referenced Periodic-Frequent Patterns:", len(Patterns))
+            print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
 
             obj.save("outFile")
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -180,34 +178,34 @@
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    --------------
-        The complete program was written by P.RaviKumar under the supervision of Professor Rage Uday Kiran.
+    -------------
+        The complete program was written by P. Likhitha under the supervision of Professor Rage Uday Kiran.
     """
 
-    _minSup = " "
-    _maxPer = " "
+    _maxIAT = " "
+    _minPS = " "
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _nFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _sep = "\t"
     _lno = 0
 
-    def __init__(self, iFile, nFile, minSup, maxPer, sep="\t"):
-        super().__init__(iFile, nFile, minSup, maxPer, sep)
+    def __init__(self, iFile, nFile, minPS, maxIAT, sep="\t"):
+        super().__init__(iFile, nFile, minPS, maxIAT,  sep)
         self._NeighboursMap = {}
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
@@ -220,59 +218,61 @@
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
                 self._Database.append(tr)
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.rstrip()
-                            temp = [i.strip() for i in line.split(self._sep)]
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
     # function to get frequent one pattern
     def _frequentOneItem(self):
         """
         Generating one frequent patterns
         """
-
-        candidate = {}
-        for i in self._Database:
-            self._lno += 1
-            n = int(i[0])
-            for j in i[1:]:
-                if j not in candidate:
-                    candidate[j] = [1, abs(0-n), n, [n]]
+        self._tidList = {}
+        self._mapSupport = {}
+        self._maxIAT = self._convert(self._maxIAT)
+        for line in self._Database:
+            s = line
+            n = int(s[0])
+            for i in range(1, len(s)):
+                si = s[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [0, n]
+                    self._tidList[si] = [n]
                 else:
-                    candidate[j][0] += 1
-                    candidate[j][1] = max(candidate[j][1], abs(n - candidate[j][2]))
-                    candidate[j][2] = n
-                    candidate[j][3].append(n)
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        #print(self._minSup, self._maxPer)
-        self._tidList = {k: v[3] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
-        candidate = {k: [v[0], v[1]] for k, v in candidate.items() if v[0] >= self._minSup and v[1] <= self._maxPer}
-        plist = [key for key, value in sorted(candidate.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+                    lp = n - self._mapSupport[si][1]
+                    if lp <= self._maxIAT:
+                        self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = n
+                    self._tidList[si].append(n)
+        self._minPS = self._convert(self._minPS)
+        self._mapSupport = {k: v[0] for k, v in self._mapSupport.items() if v[0] >= self._minPS}
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
         return plist
 
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
@@ -291,53 +291,47 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _getSupportAndPeriod(self, timeStamps):
+    def _getPeriodicSupport(self, timeStamps):
         """
         calculates the support and periodicity with list of timestamps
 
         :param timeStamps: timestamps of a pattern
         :type timeStamps: list
         """
         timeStamps.sort()
-        cur = 0
         per = 0
-        sup = 0
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > self._maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-            sup += 1
-        per = max(per, self._lno - cur)
-        return [sup, per]
+        for i in range(len(timeStamps) - 1):
+            j = i + 1
+            if abs(timeStamps[j] - timeStamps[i]) <= self._maxIAT:
+                per += 1
+        return per
 
     def _save(self, prefix, suffix, tidSetX):
         """
         Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
         :type prefix: list or None
         :param suffix: the suffix of a patterns
         :type suffix: list
         :param tidSetX: the timestamp of a patterns
         :type tidSetX: list
 
-
         """
-        if prefix == None:
+        if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = self._getSupportAndPeriod(tidSetX)
-        if val[0] >= self._minSup and val[1] <= self._maxPer:
+        val = self._getPeriodicSupport(tidSetX)
+        if val >= self._minPS:
             self._finalPatterns[tuple(prefix)] = val
 
     def _Generation(self, prefix, itemSets, tidSets):
         """
         Generates the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
@@ -345,53 +339,45 @@
         :param itemSets: the item sets of a patterns
         :type itemSets: list
         :param tidSets: the timestamp of a patterns
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
+            tidi = tidSets[0]
+            self._save(prefix, [i], tidi)
             return
         for i in range(len(itemSets)):
-            itemX = itemSets[i]
-            if itemX == None:
+            itemI = itemSets[i]
+            if itemI is None:
                 continue
             tidSetX = tidSets[i]
             classItemSets = []
             classTidSets = []
-            itemSetX = [itemX]
-            neighboursItemsI = self._getNeighbourItems(itemSets[i])
+            itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
-                neighboursItemsJ = self._getNeighbourItems(itemSets[i])
-                if not itemSets[j] in neighboursItemsI:
-                    continue
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) >= self._minSup:
-                    ne = list(set(neighboursItemsI).intersection(neighboursItemsJ))
-                    x = []
-                    x = x + [itemX]
-                    x = x + [itemJ]
-                    self._NeighboursMap[tuple(x)] = ne
+                val = self._getPeriodicSupport(y)
+                if val >= self._minPS:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
+            newprefix = list(set(itemSetX)) + prefix
+            self._Generation(newprefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
     def _getNeighbourItems(self, keySet):
         """
-        A function to get Neighbours of a item
+        A function to get Neighbours of an item
 
-        :param keySet:itemSet
-        :type keySet:str or tuple
+        :param keySet: itemSet
+        :type keySet: str or tuple
         :return: set of common neighbours
-        :rtype:set
+        :rtype: set
         """
         itemNeighbours = self._NeighboursMap.keys()
         if isinstance(keySet, str):
             if self._NeighboursMap.get(keySet) is None:
                 return []
             itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
         if isinstance(keySet, tuple):
@@ -438,60 +424,27 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
-        # global items_sets, endTime, startTime
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self.mapNeighbours()
-        self._finalPatterns = {}
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemX = plist[i]
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
-            itemSets = []
-            tidSets = []
-            neighboursItems = self._getNeighbourItems(plist[i])
-            for j in range(i + 1, len(plist)):
-                if not plist[j] in neighboursItems:
-                    continue
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) >= self._minSup:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetX)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        #self._minSup = self._convert(self._minSup)
         self.mapNeighbours()
         self._finalPatterns = {}
         plist = self._frequentOneItem()
         for i in range(len(plist)):
             itemX = plist[i]
             tidSetX = self._tidList[itemX]
             itemSetX = [itemX]
@@ -500,15 +453,16 @@
             neighboursItems = self._getNeighbourItems(plist[i])
             for j in range(i + 1, len(plist)):
                 if not plist[j] in neighboursItems:
                     continue
                 itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) >= self._minSup:
+                val = self._getPeriodicSupport(y1)
+                if val >= self._minPS:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
             self._save(None, itemSetX, tidSetX)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
@@ -526,14 +480,15 @@
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
@@ -555,64 +510,66 @@
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             pat = ""
             for i in a:
-                pat += str(i) + "\t"
-            data.append([pat, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Period'])
+                pat += str(i) + ' '
+            data.append([pat, b])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
         return dataFrame
 
     def save(self, outFile):
         """
-        Complete set of frequent patterns will be loaded in to a output file
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             pat = ""
             for i in x:
-                pat += str(i) + "\t"
-            patternsAndSupport = pat + ": " + str(y[0]) + ": " + str(y[1])
+                pat += str(i) + '\t'
+            patternsAndSupport = pat.strip() + ": " + str(y)
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
         """
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-    
+
     def printResults(self):
         """
         This function is used to print the results
         """
-        print("Total number of Spatial Periodic-Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Spatial Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
         if len(_ab._sys.argv) == 7:
-            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
-            _ap = GPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
         _ap.mine()
-        print("Total number of Spatial Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Total ExecutionTime in seconds:",  _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
+
```

### Comparing `pami-2024.4.9.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/geoReferencedPeriodicFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -31,34 +31,34 @@
     """
     :Description: This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                   employ in PAMI
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str
-            Neighbourhoof file name
-        minSup: integer or float or str
+        nFile : str
+            Neighbourhood file name
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: integer or float or str
+        maxPer : integer or float or str
             The user can specify maxPer either in count or proportion of database size.
             If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxPer = 10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator
-        startTime:float
+        startTime : float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
@@ -124,14 +124,15 @@
         """Complete set of frequent patterns generated will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def save(self, oFile):
         """Complete set of frequent patterns will be saved in to an output file from this function
+
         :param oFile: Name of the output file
         :type oFile: csv file
         """
 
         pass
 
     @_abstractmethod
```

### Comparing `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/georeferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py` & `pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/FSPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -288,15 +288,15 @@
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
     :Methods:
 
-        startMine()
+        mine()
             This function starts pattern mining.
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -508,30 +508,15 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start pattern mining from here
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._readDatabase()
-        print(len(self._Database), len(self._neighbourList))
-        self._minSup = self._convert(self._minSup)
-        self._getFrequentItems()
-        self._sortTransaction()
-        _FPTree = self._createFPTree()
-        self._finalPatterns.update(dict(_FPTree.mining(self._minSup, self._neighbourList)))
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Frequent Spatial Patterns successfully generated using FSPGrowth")
+        self.mine()
 
     def mine(self):
         """
         Start pattern mining from here
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
```

### Comparing `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py` & `pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/SpatialECLAT.py`

 * *Files 4% similar despite different names*

```diff
@@ -79,39 +79,39 @@
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str
+        nFile : str
             Name of Neighbourhood file name
-        minSup: int or float or str
+        minSup : int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
         Database : list
             To store the complete set of transactions available in the input database/file
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -127,19 +127,19 @@
             Generating one frequent patterns
         dictKeysToInt(iList)
             Converting dictionary keys to integer elements
         eclatGeneration(cList)
             It will generate the combinations of frequent items
         generateSpatialFrequentPatterns(tidList)
             It will generate the combinations of frequent items from a list of items
-        convert(value):
+        convert(value)
             To convert the given user specified value    
-        getNeighbourItems(keySet):
+        getNeighbourItems(keySet)
             A function to get common neighbours of a itemSet
-        mapNeighbours(file):
+        mapNeighbours(file)
             A function to map items to their neighbours
 
     **Executing the code on terminal :**
     ----------------------------------------
 
     .. code-block:: console
 
@@ -320,15 +320,15 @@
                         tidList[tuple(set(itemList))] = intersectionList
         return tidList
 
     def _generateSpatialFrequentPatterns(self, tidList):
         """
         It will generate the combinations of frequent items from a list of items
 
-        :param tidList :it represents the items with their respective transaction identifiers
+        :param tidList: it represents the items with their respective transaction identifiers
         :type tidList: dictionary
         :return: returning transaction dictionary
         :rtype: dict
         """
         tidList1 = {}
         if len(tidList) == 0:
             print("There are no more candidate sets")
@@ -347,18 +347,19 @@
                         tidList1[tuple(itemList)] = intersectionList
 
         return tidList1
 
     def _getNeighbourItems(self, keySet):
         """
         A function to get Neighbours of a item
-        :param keySet:itemSet
-        :type keySet:str or tuple
+
+        :param keySet: itemSet
+        :type keySet: str or tuple
         :return: set of common neighbours
-        :rtype:set
+        :rtype: set
         """
         itemNeighbours = self._NeighboursMap.keys()
         if isinstance(keySet, str):
             if self._NeighboursMap.get(keySet) is None:
                 return []
             itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
         if isinstance(keySet, tuple):
@@ -409,41 +410,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
 
-        # global items_sets, endTime, startTime
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._mapNeighbours()
-        self._finalPatterns = {}
-        self._frequentOneItem()
-        frequentSet = self._generateSpatialFrequentPatterns(self._finalPatterns)
-        for x, y in frequentSet.items():
-            if x not in self._finalPatterns:
-                self._finalPatterns[x] = y
-        while 1:
-            frequentSet = self._eclatGeneration(frequentSet)
-            for x, y in frequentSet.items():
-                if x not in self._finalPatterns:
-                    self._finalPatterns[x] = y
-            if len(frequentSet) == 0:
-                break
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Spatial Frequent patterns were generated successfully using SpatialECLAT algorithm")
+        self.mine()
 
     def mine(self):
         """
         Frequent pattern mining process will start from here
         """
 
         # global items_sets, endTime, startTime
```

### Comparing `pami-2024.4.9.1/PAMI/georeferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/georeferencedFrequentPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,36 +46,37 @@
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str
+        nFile : str
             Neighbourhood file name
-        minSup: integer or float or str
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime:float
+        startTime : float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
     :Methods:
 
         startMine()
             Calling this function will start the actual mining process
         getPatterns()
             This function will output all interesting patterns discovered by an algorithm
         save(oFile)
```

### Comparing `pami-2024.4.9.1/PAMI/georeferencedFrequentSequencePattern/abstract.py` & `pami-2024.5.1/PAMI/georeferencedFrequentSequencePattern/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -46,29 +46,29 @@
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str
+        nFile : str
             Neighbourhood file name
-        minSup: integer or float or str
+        minSup : integer or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator
-        startTime:float
+        startTime : float
             To record the start time of the algorithm
-        endTime:float
+        endTime : float
             To record the completion time of the algorithm
-        finalPatterns: dict
+        finalPatterns : dict
             Storing the complete set of patterns in a dictionary variable
         oFile : str
             Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
@@ -164,11 +164,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-
         """ To print the results of execution"""
 
         pass
```

### Comparing `pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/STEclat.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py`

 * *Files 8% similar despite different names*

```diff
@@ -1,23 +1,24 @@
-# STEclat is one of the fundamental algorithm to discover geo refereneced partial periodic-frequent patterns in a transactional database.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 #
-#             obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
+#             import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+#
+#             obj = alg.TopkPFPGrowth(iFile, k, maxPer,oFile)
+#
+#             obj.startMine()
 #
-#             obj.mine()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             partialPeriodicSpatialPatterns = obj.getPatterns()
+#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
-#             print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
+#             obj.save(oFile)
 #
-#             obj.save("outFile")
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -26,17 +27,16 @@
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -46,187 +46,170 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-from PAMI.georeferencedPartialPeriodicPattern.basic import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+import pandas as pd
 from deprecated import deprecated
 
 
-class STEclat(_ab._partialPeriodicSpatialPatterns):
+class TopkPFPGrowth(_ab._periodicFrequentPatterns):
     """
-    :Description:   STEclat is one of the fundamental algorithm to discover georefereneced partial periodic-frequent patterns in a transactional database.
+    :Description:   Top - K is and algorithm to discover top periodic frequent patterns in a temporal database.
 
-    :Reference:   R. Uday Kiran, C. Saideep, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
-                 "Discovering Partial Periodic Spatial Patterns in Spatiotemporal Databases," 2019 IEEE International
-                  Conference on Big Data (Big Data), 2019, pp. 233-238, doi: 10.1109/BigData47090.2019.9005693.
+    :Reference:   Komate Amphawan, Philippe Lenca, Athasit Surarerks: "Mining Top-K Periodic-Frequent Pattern from Transactional Databases without Support Threshold"
+                  International Conference on Advances in Information Technology: https://link.springer.com/chapter/10.1007/978-3-642-10392-6_3
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of Geo-referenced Partial Periodic patterns
+                   Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of Geo-referenced Partial Periodic patterns
-    :param  minPS: int or float or str :
-                   The user can specify minPS either in count or proportion of database size. If the program detects the data type of minPS is integer, then it treats minPS is expressed in count. Otherwise, it will be treated as float.
-    :param maxIAT: int or float or str :
-                   The user can specify maxIAT either in count or proportion of database size. If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count. Otherwise, it will be treated as float.
-    :param nFile: str :
-                   Name of the input file to mine complete set of Geo-referenced Partial Periodic patterns
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  maxPer: str:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
-        nFile: str:
-           Name of Neighbourhood file name
-        maxIAT: float or int or str
-            The user can specify maxIAT either in count or proportion of database size.
-            If the program detects the data type of maxIAT is integer, then it treats maxIAT is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxIAT=10 will be treated as integer, while maxIAT=10.0 will be treated as float
-        minPS: float or int or str
-            The user can specify minPS either in count or proportion of database size.
-            If the program detects the data type of minPS is integer, then it treats minPS is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minPS=10 will be treated as integer, while minPS=10.0 will be treated as float
+        k: int
+            User specified counte of top frequent patterns
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+        oFile : str
+            Name of the output file or the path of the output file
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
-        oFile : str
-            Name of the output file to store complete set of frequent patterns
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        Database : list
-            To store the complete set of transactions available in the input database/file
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrames()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(iFileName)
-            Storing the complete transactions of the database/input file in a database variable
+        creatingItemSets()
+            Scans the dataset or dataframes and stores in list format
         frequentOneItem()
-            Generating one frequent patterns
-        convert(value):
-            To convert the given user specified value
-        getNeighbourItems(keySet):
-            A function to get common neighbours of a itemSet
-         mapNeighbours(file):
-            A function to map items to their neighbours
-
-    **Executing the code on terminal :**
-    ----------------------------------------
+            Generates one frequent patterns
+        eclatGeneration(candidateList)
+            It will generate the combinations of frequent items
+        generateFrequentPatterns(tidList)
+            It will generate the combinations of frequent items from a list of items
 
-    .. code-block:: console
+    **Executing the code on terminal:**
+    -------------------------------------
+   .. code-block:: console
 
-      Format:
 
-      (.venv) $ python3 STEclat.py <inputFile> <outputFile> <neighbourFile>  <minPS>  <maxIAT>
+       Format:
 
-      Example Usage:
+       (.venv) $ python3 TopkPFP.py <inputFile> <outputFile> <k> <maxPer>
 
-      (.venv) $ python3 STEclat.py sampleTDB.txt output.txt sampleN.txt 0.2 0.5
+       Examples:
 
-    .. note:: maxIAT & minPS will be considered in percentage of database transactions
+       (.venv) $ python3 TopkPFP.py sampleDB.txt patterns.txt 10 3
 
 
-    **Sample run of importing the code :**
-    --------------------------------------
+    **Sample run of the importing code:**
+    ---------------------------------------
     .. code-block:: python
-    
-            import PAMI.georeferencedPartialPeriodicPattern.STEclat as alg
 
-            obj = alg.STEclat("sampleTDB.txt", "sampleN.txt", 3, 4)
+            import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
 
-            obj.mine()
+            obj = alg.TopkPFPGrowth(iFile, k, maxPer)
 
-            partialPeriodicSpatialPatterns = obj.getPatterns()
+            obj.startMine()
 
-            print("Total number of Periodic Spatial Frequent Patterns:", len(partialPeriodicSpatialPatterns))
+            periodicFrequentPatterns = obj.getPatterns()
 
-            obj.save("outFile")
+            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+
+            obj.save(oFile)
+
+            Df = obj.getPatternInDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    -------------
-        The complete program was written by P. Likhitha under the supervision of Professor Rage Uday Kiran.
+    --------------
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    _maxIAT = " "
-    _minPS = " "
     _startTime = float()
     _endTime = float()
+    _k = int()
+    _maxPer = " "
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _nFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _sep = "\t"
-    _lno = 0
-
-    def __init__(self, iFile, nFile, minPS, maxIAT, sep="\t"):
-        super().__init__(iFile, nFile, minPS, maxIAT,  sep)
-        self._NeighboursMap = {}
+    _tidList = {}
+    _lno = int()
+    _minimum = int()
+    _mapSupport = {}
 
     def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
+
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
+            if 'Patterns' in i:
+                data = self._iFile['Patterns'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
                 self._Database.append(tr)
-
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -240,364 +223,310 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    # function to get frequent one pattern
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
-        """
-        self._tidList = {}
-        self._mapSupport = {}
-        self._maxIAT = self._convert(self._maxIAT)
-        for line in self._Database:
-            s = line
-            n = int(s[0])
-            for i in range(1, len(s)):
-                si = s[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [0, n]
-                    self._tidList[si] = [n]
-                else:
-                    lp = n - self._mapSupport[si][1]
-                    if lp <= self._maxIAT:
-                        self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = n
-                    self._tidList[si].append(n)
-        self._minPS = self._convert(self._minPS)
-        self._mapSupport = {k: v[0] for k, v in self._mapSupport.items() if v[0] >= self._minPS}
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        return plist
-
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
-
-        :type value: int or float or str
-
         :return: converted value
-
-        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _getPeriodicSupport(self, timeStamps):
+    def _frequentOneItem(self):
         """
-        calculates the support and periodicity with list of timestamps
+        Generating one frequent patterns
+        """
+
+        self._mapSupport = {}
+        self._tidList = {}
+        n = 0
+        for line in self._Database:
+            self._lno += 1
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
+                if self._mapSupport.get(si) is None:
+                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._tidList[si] = [n]
+                else:
+                    self._mapSupport[si][0] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    self._mapSupport[si][2] = n
+                    self._tidList[si].append(n)
+        for x, y in self._mapSupport.items():
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
+        self._maxPer = self._convert(self._maxPer)
+        self._k = self._convert(self._k)
+        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if v[1] <= self._maxPer}
+        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._finalPatterns = {}
+        #print(len(plist))
+        for i in plist:
+            if len(self._finalPatterns) >= self._k:
+                break
+            else:
+                self._finalPatterns[i] = [self._mapSupport[i][0], self._mapSupport[i][1]]
+        self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+        plist = list(self._finalPatterns.keys())
+        return plist
+
+    def _getSupportAndPeriod(self, timeStamps):
+        """To calculate the periodicity and support
 
-        :param timeStamps: timestamps of a pattern
-        :type timeStamps: list
+        :param timeStamps: Timestamps of an item set
+        :return: support, periodicity
         """
+
+        global lno
         timeStamps.sort()
-        per = 0
-        for i in range(len(timeStamps) - 1):
-            j = i + 1
-            if abs(timeStamps[j] - timeStamps[i]) <= self._maxIAT:
-                per += 1
-        return per
+        cur = 0
+        per = list()
+        sup = 0
+        for j in range(len(timeStamps)):
+            per.append(timeStamps[j] - cur)
+            cur = timeStamps[j]
+            sup += 1
+        per.append(self._lno - cur)
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per)]
 
-    def _save(self, prefix, suffix, tidSetX):
-        """
-        Saves the patterns that satisfy the periodic frequent property.
+    def _save(self, prefix, suffix, tidSetI):
+        """Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
-        :type prefix: list or None
+        :type prefix: list
         :param suffix: the suffix of a patterns
         :type suffix: list
-        :param tidSetX: the timestamp of a patterns
-        :type tidSetX: list
-
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: list
         """
+
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = self._getPeriodicSupport(tidSetX)
-        if val >= self._minPS:
-            self._finalPatterns[tuple(prefix)] = val
+        val = self._getSupportAndPeriod(tidSetI)
+        sample = str()
+        for i in prefix:
+            sample = sample + i + " "
+        if len(self._finalPatterns) < self._k:
+            if val[0] >= self._minimum:
+                self._finalPatterns[sample] = val
+                self._finalPatterns = {k: v for k, v in
+                                  sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+        else:
+            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1][0]):
+                if val[0] > y[0]:
+                    del self._finalPatterns[x]
+                    self._finalPatterns[x] = y
+                    self._finalPatterns = {k: v for k, v in
+                                          sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
+                    self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
+                    return
 
     def _Generation(self, prefix, itemSets, tidSets):
         """
-        Generates the patterns that satisfy the periodic frequent property.
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
-        :param prefix: the prefix of a pattern
-        :type prefix: list or None
-        :param itemSets: the item sets of a patterns
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
+                            and frequent with their timestamps
         :type itemSets: list
-        :param tidSets: the timestamp of a patterns
+        :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
-            tidi = tidSets[0]
-            self._save(prefix, [i], tidi)
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
             return
         for i in range(len(itemSets)):
             itemI = itemSets[i]
             if itemI is None:
                 continue
-            tidSetX = tidSets[i]
+            tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
-                y = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y)
-                if val >= self._minPS:
+                y = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
-            newprefix = list(set(itemSetX)) + prefix
-            self._Generation(newprefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetX)
-
-    def _getNeighbourItems(self, keySet):
-        """
-        A function to get Neighbours of an item
-
-        :param keySet:itemSet
-        :type keySet:str or tuple
-        :return: set of common neighbours
-        :rtype:set
-        """
-        itemNeighbours = self._NeighboursMap.keys()
-        if isinstance(keySet, str):
-            if self._NeighboursMap.get(keySet) is None:
-                return []
-            itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(keySet))))
-        if isinstance(keySet, tuple):
-            keySet = list(keySet)
-            for j in range(0, len(keySet)):
-                i = keySet[j]
-                itemNeighbours = list(set(itemNeighbours).intersection(set(self._NeighboursMap.get(i))))
-        return itemNeighbours
-
-    def mapNeighbours(self):
-        """
-        A function to map items to their Neighbours
-        """
-        self._NeighboursMap = {}
-        if isinstance(self._nFile, _ab._pd.DataFrame):
-            data = []
-            if self._nFile.empty:
-                print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for i in data:
-                self._NeighboursMap[i[0]] = i[1:]
-        if isinstance(self._nFile, str):
-            if _ab._validators.url(self._nFile):
-                data = _ab._urlopen(self._nFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._NeighboursMap[temp[0]] = temp[1:]
-            else:
-                try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._NeighboursMap[temp[0]] = temp[1:]
-                except IOError:
-                    print("File Not Found")
-                    quit()
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            self._save(prefix, list(set(itemSetX)), tidSetI)
 
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Frequent pattern mining process will start from here
+        Main function of the program
         """
-
-        # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        #self._minSup = self._convert(self._minSup)
-        self.mapNeighbours()
-        self._finalPatterns = {}
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemX = plist[i]
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
+        _plist = self._frequentOneItem()
+        for i in range(len(_plist)):
+            itemI = _plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            neighboursItems = self._getNeighbourItems(plist[i])
-            for j in range(i + 1, len(plist)):
-                if not plist[j] in neighboursItems:
-                    continue
-                itemJ = plist[j]
+            for j in range(i + 1, len(_plist)):
+                itemJ = _plist[j]
                 tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y1)
-                if val >= self._minPS:
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y1)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetX)
+        print("TopK Periodic Frequent patterns were generated successfully")
         self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
+        _process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
+        self._memoryUSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
 
-    def mine(self):
+    def Mine(self):
         """
-        Frequent pattern mining process will start from here
+        Main function of the program
         """
-
-        # global items_sets, endTime, startTime
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
+        if self._k is None:
+            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        #self._minSup = self._convert(self._minSup)
-        self.mapNeighbours()
-        self._finalPatterns = {}
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemX = plist[i]
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
+        _plist = self._frequentOneItem()
+        for i in range(len(_plist)):
+            itemI = _plist[i]
+            tidSetI = self._tidList[itemI]
+            itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            neighboursItems = self._getNeighbourItems(plist[i])
-            for j in range(i + 1, len(plist)):
-                if not plist[j] in neighboursItems:
-                    continue
-                itemJ = plist[j]
+            for j in range(i + 1, len(_plist)):
+                itemJ = _plist[j]
                 tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y1)
-                if val >= self._minPS:
+                y1 = list(set(tidSetI).intersection(tidSetJ))
+                val = self._getSupportAndPeriod(y1)
+                if val[0] >= self._minimum and val[1] <= self._maxPer:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetX)
+        print("TopK Periodic Frequent patterns were generated successfully")
         self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
+        _process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Spatial Periodic Frequent patterns were generated successfully using SpatialEclat algorithm")
+        self._memoryUSS = float()
+        self._memoryUSS = _process.memory_full_info().uss
+        self._memoryRSS = _process.memory_info().rss
 
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
+        """Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            pat = ""
-            for i in a:
-                pat += str(i) + ' '
-            data.append([pat, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
+            data.append([a, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
     def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
+
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            pat = ""
-            for i in x:
-                pat += str(i) + '\t'
-            patternsAndSupport = pat.strip() + ": " + str(y)
+            patternsAndSupport = x.replace(' ', '\t') + ":" + f'{y[0]}:{y[1]}'
             writer.write("%s \n" % patternsAndSupport)
 
     def getPatterns(self):
-        """
-        Function to send the set of frequent patterns after completion of the mining process
+        """ Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of  Spatial Partial Periodic Patterns:", len(self.getPatterns()))
+        print("Top K Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
-            _ap = STEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _ap.mine()
-        print("Total number of Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        print("Top K Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in seconds:",  _ap.getRuntime())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
-
```

### Comparing `pami-2024.4.9.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/georeferencedPartialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py` & `pami-2024.5.1/PAMI/highUtilityFrequentPattern/basic/HUFIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -73,29 +73,30 @@
             represent total sum of all utilities in the database
         prefixUtility:
             prefix Utility values of item
         offset:
             an offset pointer, used by projected transactions
         support:
             maintains the support of the transaction
+
     :Methods:
 
-        projectedTransaction(offsetE):
+        projectedTransaction(offsetE)
             A method to create new Transaction from existing starting from offsetE until the end
-        getItems():
+        getItems()
             return items in transaction
-        getUtilities():
+        getUtilities()
             return utilities in transaction
-        getLastPosition():
+        getLastPosition()
             return last position in a transaction
-        removeUnpromisingItems():
+        removeUnpromisingItems()
             A method to remove items which are having low values when compared with minUtil
-        insertionSort():
+        insertionSort()
             A method to sort all items in the transaction
-        getSupport():
+        getSupport()
             returns the support of the transaction
     """
     offset = 0
     prefixUtility = 0
     support = 1
 
     def __init__(self, items: List[int], utilities: List[int], transactionUtility: int) -> None:
@@ -105,19 +106,16 @@
         self.support = 1
 
     def projectTransaction(self, offsetE: int) -> '_Transaction':
         """
         A method to create new Transaction from existing transaction starting from offsetE until the end
 
         :param offsetE: an offset over the original transaction for projecting the transaction
-
         :type offsetE: int
-
         :return: a new transaction starting from offsetE until the end of the transaction
-
         :rtype: _Transaction
         """
         new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
         new_transaction.prefixUtility = self.prefixUtility + utilityE
         new_transaction.transactionUtility = self.transactionUtility - utilityE
         new_transaction.support = self.support
@@ -127,59 +125,52 @@
         return new_transaction
 
     def getItems(self) -> List[int]:
         """
         A method to return items in transaction
 
         :return: the list of items in transaction starting from offsetE until the end of the transactions
-
         :rtype: list
         """
         return self.items
 
     def getUtilities(self) -> List[int]:
         """
         A method to return utilities in transaction
 
         :return: the list of utilities in transaction starting from offsetE until the end of the transaction
-
         :rtype: list
         """
         return self.utilities
 
     def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
-
         :return: the last position in a transaction
-
         :rtype: int
         """
 
         return len(self.items) - 1
 
     def getSupport(self) -> int:
         """
         A method to return support in a transaction
 
         :return: the support in a transaction
-
         :rtype: int
         """
 
         return self.support
 
     def removeUnpromisingItems(self, oldNamesToNewNames: Dict[int, int]) -> None:
         """
         A method to remove items which are not present in the map passed to the function
 
         :param oldNamesToNewNames: A map represent old names to new names
-
         :type oldNamesToNewNames: map
-
         :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
@@ -189,14 +180,15 @@
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
     def insertionSort(self) -> None:
         """
         A method to sort items in order
+
         :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
@@ -216,19 +208,19 @@
         transactions :
             the list of transactions in this dataset
         maxItem:
             the largest item name
         
     :methods:
 
-        createTransaction(line):
+        createTransaction(line)
             Create a transaction object from a line from the input file
-        getMaxItem():
+        getMaxItem()
             return Maximum Item
-        getTransactions():
+        getTransactions()
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
     
     def __init__(self, datasetPath: Union[str, _ab._pd.DataFrame], sep: str) -> None:
@@ -239,17 +231,15 @@
         self.createItemSets(datasetPath)
 
     def createItemSets(self, datasetPath: List[str]) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
 
         :param datasetPath: list of paths to the input file to store
-
         :type datasetPath: list
-
         :return: None
 
         """
         self.Database = []
         self.transactions = []
         if isinstance(datasetPath, _ab._pd.DataFrame):
             utilities, data, utilitySum = [], [], []
@@ -292,27 +282,20 @@
                     quit()
 
     def createTransaction(self, items: List[str], utilities: List[str], utilitySum: int) -> _Transaction:
         """
         A method to create Transaction from dataset given
 
         :param items: represent a single line of database
-
         :type items: list
-
         :param utilities: represent the utilities of items
-
         :type utilities: list
-
         :param utilitySum: represent  the utilitySum
-
         :type utilitySum: int
-
         :return: a Transaction from given dataset
-
         :rtype: _Transaction
         """
         transactionUtility = utilitySum
         itemsString = items
         utilityString = utilities
         items = []
         utilities = []
@@ -329,25 +312,23 @@
         return _Transaction(items, utilities, transactionUtility)
 
     def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
 
         :return: the name of the largest item in the dataset
-
         :rtype: int
         """
         return self.maxItem
 
     def getTransactions(self) -> List[_Transaction]:
         """
         A method to return transactions from database
 
         :return: the list of transactions from database which have the highest utility
-
         :rtype: list
         """
         return self.transactions
 
 
 class HUFIM(_ab._utilityPatterns):
     """
@@ -416,15 +397,15 @@
         itemsToKeep: list
             keep only the promising items i.e items that can extend other items to form RHUIs
         itemsToExplore: list
             list of items that needs to be explored
 
     :Methods:
 
-        startMine()
+        mine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of patterns will be loaded in to a dataframe
@@ -531,14 +512,15 @@
 
     def __init__(self, iFile: str, minUtil: Union[int, float], minSup: Union[int, float], sep: str="\t") -> None:
         super().__init__(iFile, minUtil, minSup, sep)
 
     def _convert(self, value) -> Union[int, float]:
         """
         To convert the given user specified value
+
         :param value: user specified value
         :type value: int or float or str
         :return: converted value
         :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
@@ -552,73 +534,23 @@
                 value = int(value)
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         High Utility Frequent Pattern mining start here
+
         :return: None
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._dataset = []
-        self._dataset = _Dataset(self._iFile, self._sep)
-        self._singleItemSetsSupport = _ab._defaultdict(int)
-        self._singleItemSetsUtility = _ab._defaultdict(int)
-        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        self._minUtil = int(self._minUtil)
-        self._minSup = self._convert(self._minSup)
-        itemsToKeep = []
-        for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
-                itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
-        currentName = 1
-        for idx, item in enumerate(itemsToKeep):
-            self._oldNamesToNewNames[item] = currentName
-            self._newNamesToOldNames[currentName] = item
-            itemsToKeep[idx] = currentName
-            currentName += 1
-        for transaction in self._dataset.getTransactions():
-            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
-        self._sortDatabase(self._dataset.getTransactions())
-        emptyTransactionCount = 0
-        for transaction in self._dataset.getTransactions():
-            if len(transaction.getItems()) == 0:
-                emptyTransactionCount += 1
-        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
-        # calculating suffix utility values
-        totalUtility = 0
-        for item in itemsToKeep:
-            totalUtility += self._singleItemSetsUtility[self._newNamesToOldNames[item]]
-        # piItems
-        piItems = []
-        for item in itemsToKeep:
-            if totalUtility >= self._minUtil:
-                piItems.append(item)
-                totalUtility -= self._singleItemSetsUtility[self._newNamesToOldNames[item]]
-            else:
-                break
-        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
-        itemsToExplore = []
-        for item in piItems:
-            if self._utilityBinArraySU[item] >= self._minUtil:
-                itemsToExplore.append(item)
-        self._backTrackingHUFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
+        self.mine()
 
     def mine(self) -> None:
         """
         High Utility Frequent Pattern mining start here
+
         :return: None
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
         self._dataset = []
         self._dataset = _Dataset(self._iFile, self._sep)
         self._singleItemSetsSupport = _ab._defaultdict(int)
@@ -670,14 +602,15 @@
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("High Utility Frequent patterns were generated successfully using HUFIM algorithm")
 
     def _backTrackingHUFIM(self, transactionsOfP: List[_Transaction], itemsToKeep: List[int], itemsToExplore: List[int], prefixLength: int) -> None:
         """
         A method to mine the HUFIs Recursively
+
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
@@ -816,14 +749,15 @@
             if i != tempPosition:
                 s1 += "\t"
         self._finalPatterns[s1] = [utility, support]
 
     def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
         """
         A method to Check if two transaction are identical
+
         :param  transaction1: the first transaction
         :type  transaction1: Trans
         :param  transaction2:    the second transaction
         :type  transaction2: Trans
         :return : whether both are identical or not
         :rtype: bool
         """
@@ -839,14 +773,15 @@
             position1 += 1
             position2 += 1
         return True
 
     def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
+
         :param dataset: the transaction database
         :type dataset: Dataset
         :return : None
         """
         for transaction in dataset.getTransactions():
             sumSU = 0
             i = len(transaction.getItems()) - 1
@@ -859,24 +794,26 @@
                 else:
                     self._utilityBinArraySU[item] = sumSU
                 i -= 1
 
     def _sortDatabase(self, transactions: List[_Transaction]) -> None:
         """
         A Method to sort transaction
+
         :param transactions: transactions of items
         :type transactions: list
         :return: None
         """
         compareItems = _ab._functools.cmp_to_key(self._sortTransaction)
         transactions.sort(key=compareItems)
 
     def _sortTransaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
         """
         A Method to sort transaction
+
         :param trans1: the first transaction
         :type trans1: Trans
         :param trans2:the second transaction
         :type trans2: Trans
         :return: sorted transaction
         :rtype: int
         """
@@ -908,14 +845,15 @@
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
     def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         A method to calculate local utility of single itemSets
+
         :param dataset: the transaction database
         :type dataset: databases
         :return: None
         """
         for transaction in dataset.getTransactions():
             for idx, item in enumerate(transaction.getItems()):
                 self._singleItemSetsSupport[item] += 1
@@ -924,66 +862,72 @@
                     self._utilityBinArrayLU[item] += transaction.transactionUtility
                 else:
                     self._utilityBinArrayLU[item] = transaction.transactionUtility
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
         Storing final patterns in a dataframe
+
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
         """
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'Support'])
 
         return dataFrame
     
     def getPatterns(self) -> Dict[str, List[Union[int, float]]]:
         """
         Function to send the set of patterns after completion of the mining process
+
         :return: returning patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
     def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime-self._startTime
     
     def printResults(self) -> None:
         """
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/highUtilityFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py` & `pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/SHUFIM.py`

 * *Files 3% similar despite different names*

```diff
@@ -390,15 +390,15 @@
         itemsToKeep: list
             keep only the promising items ie items whose supersets can be required patterns
         itemsToExplore: list
             keep items that subtreeUtility grater than minUtil
 
     :Methods :
 
-        startMine()
+        mine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of frequent patterns will be loaded in to a dataframe
@@ -529,87 +529,15 @@
         return value
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         High Utility Frequent Pattern mining start here
         """
-        self._startTime = _ab._time.time()
-        self._patternCount = 0
-        self._finalPatterns = {}
-        self._dataset = _Dataset(self._iFile, self._sep)
-        self._singleItemSetsSupport = _ab._defaultdict(int)
-        self._singleItemSetsUtility = _ab._defaultdict(int)
-        self._minUtil = int(self._minUtil)
-        self._minSup = self._convert(self._minSup)
-        with open(self._nFile, 'r') as o:
-            lines = o.readlines()
-            for line in lines:
-                line = line.split("\n")[0]
-                line_split = line.split(self._sep)
-                item = self._dataset.strToInt.get(line_split[0])
-                lst = []
-                for i in range(1, len(line_split)):
-                    lst.append(self._dataset.strToInt.get(line_split[i]))
-                self._Neighbours[item] = lst
-        o.close()
-        InitialMemory = _ab._psutil.virtual_memory()[3]
-        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        _itemsToKeep = []
-        for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= self._minUtil and self._singleItemSetsSupport[key] >= self._minSup:
-                _itemsToKeep.append(key)
-        # sorting items in decreasing order of their utilities
-        _itemsToKeep = sorted(_itemsToKeep, key=lambda x: self._singleItemSetsUtility[x], reverse=True)
-        _currentName = 1
-        for idx, item in enumerate(_itemsToKeep):
-            self._oldNamesToNewNames[item] = _currentName
-            self._newNamesToOldNames[_currentName] = item
-            _itemsToKeep[idx] = _currentName
-            _currentName += 1
-        for transaction in self._dataset.getTransactions():
-            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
-        self._sortDatabase(self._dataset.getTransactions())
-        _emptyTransactionCount = 0
-        for transaction in self._dataset.getTransactions():
-            if len(transaction.getItems()) == 0:
-                _emptyTransactionCount += 1
-        self._dataset.transactions = self._dataset.transactions[_emptyTransactionCount:]
-        # calculating neighborhood suffix utility values
-        _secondary = []
-        for idx, item in enumerate(_itemsToKeep):
-            _cumulativeUtility = self._singleItemSetsUtility[self._newNamesToOldNames[item]]
-            if self._newNamesToOldNames[item] in self._Neighbours:
-                neighbors = [self._oldNamesToNewNames[y] for y in self._Neighbours[self._newNamesToOldNames[item]] if y in self._oldNamesToNewNames]
-                for i in range(idx+1, len(_itemsToKeep)):
-                    _nextItem = _itemsToKeep[i]
-                    if _nextItem in neighbors:
-                        _cumulativeUtility += self._singleItemSetsUtility[self._newNamesToOldNames[_nextItem]]
-            if _cumulativeUtility >= self._minUtil:
-                _secondary.append(item)         
-        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
-        _itemsToExplore = []
-        for item in _secondary:
-            if self._utilityBinArraySU[item] >= self._minUtil:
-                _itemsToExplore.append(item)
-        _commonitems = []
-        for i in range(self._dataset.maxItem):
-            _commonitems.append(i)
-        self._backtrackingEFIM(self._dataset.getTransactions(), _itemsToKeep, _itemsToExplore, 0)
-        _finalMemory = _ab._psutil.virtual_memory()[3]
-        memory = (_finalMemory - InitialMemory) / 10000
-        if memory > self._maxMemory:
-            self._maxMemory = memory
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print('Spatial High Utility Frequent Itemsets generated successfully using SHUFIM algorithm')
+        self.mine()
 
     def mine(self):
         """
         High Utility Frequent Pattern mining start here
         """
         self._startTime = _ab._time.time()
         self._patternCount = 0
@@ -1067,15 +995,15 @@
     inputFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
     neighborFile = '/home/nakamura/workspace/labwork/PAMI/PAMI/highUtilityGeoreferencedFrequentPattern/basic/mushroom_utility_spmf.txt'
 
     minUtilCount = 10000
     minSup = 100
     seperator = ' '  
     obj = SHUFIM(iFile=inputFile, nFile=neighborFile, minUtil=minUtilCount, minSup=minSup, sep=seperator)    #initialize
-    obj.startMine()   
+    obj.mine()
     obj.printResults()
     print(obj.getPatterns())
 
 
 if __name__ == '__main__':
     main()
     # _ap = str()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/highUtilityGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/EFIM.py` & `pami-2024.5.1/PAMI/highUtilityPattern/basic/EFIM.py`

 * *Files 3% similar despite different names*

```diff
@@ -361,15 +361,15 @@
         itemsToKeep: list
             keep only the promising items ie items having local utility values greater than or equal to minUtil
         itemsToExplore: list
             list of items that have subtreeUtility value greater than or equal to minUtil
 
     :Methods :
 
-        startMine()
+        mine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of patterns will be loaded in to a dataframe
@@ -492,50 +492,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Start the EFIM algorithm.
         :return: None
         """
-        self._startTime = _ab._time.time()
-        self._dataset = _Dataset(self._iFile, self._sep)
-        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        self._minUtil = int(self._minUtil)
-        itemsToKeep = []
-        for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= self._minUtil:
-                itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
-        currentName = 1
-        for idx, item in enumerate(itemsToKeep):
-            self._oldNamesToNewNames[item] = currentName
-            self._newNamesToOldNames[currentName] = item
-            itemsToKeep[idx] = currentName
-            currentName += 1
-        for transaction in self._dataset.getTransactions():
-            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
-        self._sortDatabase(self._dataset.getTransactions())
-        emptyTransactionCount = 0
-        for transaction in self._dataset.getTransactions():
-            if len(transaction.getItems()) == 0:
-                emptyTransactionCount += 1
-        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
-        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
-        itemsToExplore = []
-        for item in itemsToKeep:
-            if self._utilityBinArraySU[item] >= self._minUtil:
-                itemsToExplore.append(item)
-        self._backTrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("High Utility patterns were generated successfully using EFIM algorithm")
+        self.mine()
 
     def mine(self) -> None:
         """
         Start the EFIM algorithm.
         :return: None
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/HMiner.py` & `pami-2024.5.1/PAMI/highUtilityPattern/basic/HMiner.py`

 * *Files 12% similar despite different names*

```diff
@@ -180,15 +180,15 @@
         huiCnt: int
             huis created
         neighbors: map
             keep track of nighboues of elements
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -350,104 +350,15 @@
                     quit()
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Main program to start the operation
         """
-        self._startTime = _ab._time.time()
-        self._creteItemsets()
-        self._finalPatterns = {}
-        for line in range(len(self._transactions)):
-            items_str = self._transactions[line]
-            utility_str = self._utilities[line]
-            transUtility = self._utilitySum[line]
-            for i in range(0, len(items_str)):
-                item = items_str[i]
-                twu = self._mapOfTWU.get(item)
-                if twu == None:
-                    twu = transUtility
-                else:
-                    twu += transUtility
-                self._mapOfTWU[item] = twu
-        listOfCUList = []
-        hashTable = {}
-        mapItemsToCUList = {}
-        minutil = self._minUtil
-        for item in self._mapOfTWU.keys():
-            if self._mapOfTWU.get(item) >= self._minUtil:
-                uList = _CUList(item)
-                mapItemsToCUList[item] = uList
-                listOfCUList.append(uList)
-        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._HMiner))
-        tid = 1
-        for line in range(len(self._transactions)):
-            items = self._transactions[line]
-            utilities = self._utilities[line]
-            ru = 0
-            newTwu = 0
-            tx_key = []
-            revisedTrans = []
-            for i in range(0, len(items)):
-                pair = _Pair()
-                pair.item = items[i]
-                pair.utility = int(utilities[i])
-                if self._mapOfTWU.get(pair.item) >= self._minUtil:
-                    revisedTrans.append(pair)
-                    tx_key.append(pair.item)
-                    newTwu += pair.utility
-            revisedTrans.sort(key=_ab._functools.cmp_to_key(self._HMiner))
-            tx_key1 = tuple(tx_key)
-            if len(revisedTrans) > 0:
-                if tx_key1 not in hashTable.keys():
-                    hashTable[tx_key1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
-                    for i in range(len(revisedTrans) - 1, -1, -1):
-                        pair = revisedTrans[i]
-                        cuListoFItems = mapItemsToCUList.get(pair.item)
-                        element = _Element(tid, pair.utility, ru, 0, 0)
-                        if i > 0:
-                            element.ppos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
-                        else:
-                            element.ppos = - 1
-                        cuListoFItems.addElements(element)
-                        ru += pair.utility
-                else:
-                    pos = hashTable[tx_key1]
-                    ru = 0
-                    for i in range(len(revisedTrans) - 1, -1, -1):
-                        cuListoFItems = mapItemsToCUList[revisedTrans[i].item]
-                        cuListoFItems.elements[pos].nu += revisedTrans[i].utility
-                        cuListoFItems.elements[pos].nru += ru
-                        cuListoFItems.sumnu += revisedTrans[i].utility
-                        cuListoFItems.sumnru += ru
-                        ru += revisedTrans[i].utility
-                        pos = cuListoFItems.elements[pos].ppos
-                    # EUCS
-            for i in range(len(revisedTrans) - 1, -1, -1):
-                pair = revisedTrans[i]
-                mapFMAPItem = self._mapFMAP.get(pair.item)
-                if mapFMAPItem == None:
-                    mapFMAPItem = {}
-                    self._mapFMAP[pair.item] = mapFMAPItem
-                for j in range(i + 1, len(revisedTrans)):
-                    pairAfter = revisedTrans[j]
-                    twuSUm = mapFMAPItem.get(pairAfter.item)
-                    if twuSUm is None:
-                        mapFMAPItem[pairAfter.item] = newTwu
-                    else:
-                        mapFMAPItem[pairAfter.item] = twuSUm + newTwu
-            tid += 1
-        self._ExploreSearchTree([], listOfCUList, minutil)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("High Utility patterns were generated successfully using HMiner algorithm")
+        self.mine()
 
     def mine(self):
         """
         Main program to start the operation
         """
         self._startTime = _ab._time.time()
         self._creteItemsets()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/UPGrowth.py` & `pami-2024.5.1/PAMI/highUtilityPattern/basic/UPGrowth.py`

 * *Files 3% similar despite different names*

```diff
@@ -378,15 +378,15 @@
         phuis : list
             A list to store the phuis
         MapItemToTwu : map
             A map to store the twu of each item in database
 
     :Methods:
 
-        startMine()
+        mine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         createLocalTree(tree, item)
             A Method to Construct conditional pattern base
         UPGrowth( tree, alpha)
             A Method to Mine UP Tree recursively
@@ -512,96 +512,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process will start from here
         :return: None
         """
-        self._startTime = _ab._time.time()
-        tree = _UPTree()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        for line in self._Database:
-            line = line.split("\n")[0]
-            transaction = line.strip().split(':')
-            items = transaction[0].split(self._sep)
-            transactionUtility = int(transaction[1])
-            for item in items:
-                Item = int(item)
-                if Item in self._MapItemToTwu:
-                    self._MapItemToTwu[Item] += transactionUtility
-                else:
-                    self._MapItemToTwu[Item] = transactionUtility
-        for line in self._Database:
-            line = line.split("\n")[0]
-            transaction = line.strip().split(':')
-            items = transaction[0].split(self._sep)
-            utilities = transaction[2].split(self._sep)
-            remainingUtility = 0
-            revisedTransaction = []
-            for idx, item in enumerate(items):
-                Item = int(item)
-                utility = int(utilities[idx])
-                if self._MapItemToTwu[Item] >= self._minUtil:
-                    element = _UPItem(Item, utility)
-                    revisedTransaction.append(element)
-                    remainingUtility += utility
-                    if Item in self._MapItemToMinimumUtility:
-                        minItemUtil = self._MapItemToMinimumUtility[Item]
-                        if minItemUtil >= utility:
-                            self._MapItemToMinimumUtility[Item] = utility
-                    else:
-                        self._MapItemToMinimumUtility[Item] = utility
-            revisedTransaction = sorted(revisedTransaction, key=lambda x: self._MapItemToTwu[x.name], reverse=True)
-            self._ParentNumberOfNodes += tree.addTransaction(revisedTransaction, remainingUtility)
-        tree.createHeaderList(self._MapItemToTwu)
-        alpha = []
-        self._finalPatterns = {}
-        # print("number of nodes in parent tree", self.ParentNumberOfNodes)
-        self._UPGrowth(tree, alpha)
-        # self.phuis = sorted(self.phuis, key=lambda x: len(x))
-        # print(self.phuis[0:10])
-        for line in self._Database:
-            line = line.split("\n")[0]
-            transaction = line.strip().split(':')
-            items = transaction[0].split(self._sep)
-            utilities = transaction[2].split(self._sep)
-            mapItemToUtility = {}
-            for idx, item in enumerate(items):
-                Item = int(item)
-                utility = int(utilities[idx])
-                if self._MapItemToTwu[Item] >= self._minUtil:
-                    mapItemToUtility[Item] = utility
-            for itemset in self._phuis:
-                l = len(itemset)
-                count = 0
-                utility = 0
-                for item in itemset:
-                    item = int(item)
-                    if item in mapItemToUtility:
-                        utility += mapItemToUtility[item]
-                        count += 1
-                if count == l:
-                    self._MapItemsetsToUtilities[tuple(itemset)] += utility
-
-        for itemset in self._phuis:
-            util = self._MapItemsetsToUtilities[tuple(itemset)]
-            if util >= self._minUtil:
-                s = str()
-                for item in itemset:
-                    s = s + str(item)
-                    s = s + "\t"
-                self._finalPatterns[s] = util
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("High Utility patterns were generated successfully using UPGrowth algorithm")
+        self.mine()
 
     def mine(self) -> None:
         """
         Mining process will start from here
         :return: None
         """
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/highUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/basic/efimParallel.py` & `pami-2024.5.1/PAMI/highUtilityPattern/basic/efimParallel.py`

 * *Files 2% similar despite different names*

```diff
@@ -126,15 +126,15 @@
             Read the input file and return the filtered transactions, primary items, and secondary items.
         binarySearch(arr, item):
             Perform a binary search on the given array to find the given item.
         project(beta, file_data, secondary):
             Project the given beta itemset on the given database.
         search(collections):
             Search for high utility itemsets in the given collections.
-        startMine():
+        mine():
             Start the EFIM algorithm.
         savePatterns(outputFile):
             Save the patterns discovered by the algorithm to an output file.
         getPatterns():
             Get the patterns discovered by the algorithm.
         getRuntime():
             Get the runtime of the algorithm.
@@ -447,29 +447,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start the EFIM algorithm.
         """
 
-        ps = psutil.Process(os.getpid())
-
-        self.start = time.time()
-
-        fileData, primary, secondary = self._read_file()
-
-        collection = [[[], fileData, primary, secondary]]
-
-        self._search(collection)
-
-        self.memoryRSS = ps.memory_info().rss
-        self.memoryUSS = ps.memory_full_info().uss
-
-        end = time.time()
-        self.runtime = end - self.start
+        self.mine()
 
     def mine(self):
         """
         Start the EFIM algorithm.
         """
 
         ps = psutil.Process(os.getpid())
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/abstract.py` & `pami-2024.5.1/PAMI/highUtilityPattern/parallel/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPattern/parallel/efimparallel.py` & `pami-2024.5.1/PAMI/highUtilityPattern/parallel/efimparallel.py`

 * *Files 2% similar despite different names*

```diff
@@ -108,15 +108,15 @@
             Read the input file and return the filtered transactions, primary items, and secondary items.
         binarySearch(arr, item):
             Perform a binary search on the given array to find the given item.
         project(beta, file_data, secondary):
             Project the given beta itemset on the given database.
         search(collections):
             Search for high utility itemsets in the given collections.
-        startMine():
+        mine():
             Start the EFIM algorithm.
         savePatterns(outputFile):
             Save the patterns discovered by the algorithm to an output file.
         getPatterns():
             Get the patterns discovered by the algorithm.
         getRuntime():
             Get the runtime of the algorithm.
@@ -386,29 +386,15 @@
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Start the EFIM algorithm.
         """
 
-        ps = psutil.Process(os.getpid())
-
-        self.start = time.time()
-
-        fileData, primary, secondary = self._read_file()
-
-        collection = [[[], fileData, primary, secondary]]
-
-        self._search(collection)
-
-        self.memoryRSS = ps.memory_info().rss
-        self.memoryUSS = ps.memory_full_info().uss
-
-        end = time.time()
-        self.runtime = end - self.start
+        self.mine()
 
     def mine(self):
         """
         Start the EFIM algorithm.
         """
 
         ps = psutil.Process(os.getpid())
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/HUPMS.py` & `pami-2024.5.1/PAMI/highUtilityPatternsInStreams/HUPMS.py`

 * *Files 4% similar despite different names*

```diff
@@ -468,15 +468,15 @@
 
         contains(superset, subset)
             Checks if the superset contains the subset
 
         treeGenerations(root, netUtil, candidatePattern, curItem)
             Generates the tree of the high utility patterns
 
-        startMine()
+        mine()
             Starts the mining process
 
         printTree(root, level)
             Prints the HUS-tree in a readable format
 
         getMemoryRSS()
             Returns the memory usage of the algorithm in resident set size
@@ -748,85 +748,15 @@
                     self.treeGenerations(conditionalTree, netUtil, candidatePattern, newItemset)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         This function will start the mining process
         """
-        global _minUtil
-        self.__startTime = _hus._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minUtil is None:
-            raise Exception("Please enter the Minimum Support")
-        if self._windowSize is None:
-            raise Exception("Please enter the Window Size")
-        if self._paneSize is None:
-            raise Exception("Please enter the Pane Size")
-        self.__windowSize = int(self._windowSize)
-        self.__paneSize = int(self._paneSize)
-        
-        self._createItemsets()
-        self._minUtil = float(self._minUtil)
-        self.__tree = _HUSTree(self.__windowSize, self.__paneSize)
-        
-        transactionwiseUtility = []
-
-        for i in range(len(self._transactions)):
-            curTrans = {}
-            for j in range(len(self._transactions[i])):
-                curTrans[self._transactions[i][j]] = self._utilities[i][j]
-            transactionwiseUtility.append(curTrans)
-
-        for i in range(0, self.__windowSize):
-            self.__tree.batchIndex = i
-            for j in range(0, self.__paneSize):
-                self.__tree.addTransaction(self._transactions[i * self.__paneSize + j], self._utilitySum[i * self.__paneSize + j])
-
-        startIndex = 0
-        endIndex = self.__windowSize * self.__paneSize
-
-        while(endIndex <= len(self._transactions)):
-            
-            filteredItemsets = {}
-
-            self.treeGenerations(self.__tree, self._minUtil, filteredItemsets)
-
-            results = []
-
-            for itemSetLen in filteredItemsets:
-                for itemSet in filteredItemsets[itemSetLen]:
-                    itemSetUtility = 0
-                    for transId in range(startIndex, endIndex):
-                        if(self.contains(list(transactionwiseUtility[transId].keys()), itemSet)):
-                            for item in itemSet:
-                                itemSetUtility += transactionwiseUtility[transId][item]
-                        
-                    if(itemSetUtility >= self._minUtil):
-                        results.append([itemSet, itemSetUtility])
-
-            self.__finalPatterns[(startIndex, endIndex)] = results
-
-            if(endIndex >= len(self._transactions)):
-                break
-
-            self.__tree.removeBatch()
-
-            for i in range(0, self.__paneSize):
-                self.__tree.addTransaction(self._transactions[endIndex + i], self._utilitySum[endIndex + i])
-
-            startIndex += self.__paneSize
-            endIndex += self.__paneSize
-
-        self.__endTime = _hus._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _hus._psutil.Process(_hus._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self):
         """
         This function will start the mining process
         """
         global _minUtil
         self.__startTime = _hus._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py` & `pami-2024.5.1/PAMI/highUtilityPatternsInStreams/SHUGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -510,15 +510,15 @@
 
         contains(superset, subset)
             Checks if the superset contains the subset
 
         treeGenerations(root, netUtil, candidatePattern, curItem)
             Generates the tree of the high utility patterns
 
-        startMine()
+        mine()
             Starts the mining process
 
         printTree(root, level)
             Prints the SHU-tree in a readable format
 
         getMemoryRSS()
             Returns the memory usage of the algorithm in resident set size
@@ -828,85 +828,15 @@
                     self.treeGenerations(conditionalTree, netUtil, candidatePattern, newItemset)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         This function will start the mining process
         """
-        global _minUtil
-        self.__startTime = _hus._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minUtil is None:
-            raise Exception("Please enter the Minimum Support")
-        if self._windowSize is None:
-            raise Exception("Please enter the Window Size")
-        if self._paneSize is None:
-            raise Exception("Please enter the Pane Size")
-        self.__windowSize = int(self._windowSize)
-        self.__paneSize = int(self._paneSize)
-        
-        self._createItemsets()
-        self._minUtil = float(self._minUtil)
-        self.__tree = _SHUTree(self.__windowSize, self.__paneSize)
-        
-        transactionwiseUtility = []
-
-        for i in range(len(self._transactions)):
-            curTrans = {}
-            for j in range(len(self._transactions[i])):
-                curTrans[self._transactions[i][j]] = self._utilities[i][j]
-            transactionwiseUtility.append(curTrans)
-
-        for i in range(0, self.__windowSize):
-            self.__tree.batchIndex = i
-            for j in range(0, self.__paneSize):
-                self.__tree.addTransaction(self._transactions[i * self.__paneSize + j], self._utilitySum[i * self.__paneSize + j], self._utilities[i * self.__paneSize + j])
-
-        startIndex = 0
-        endIndex = self.__windowSize * self.__paneSize
-
-        while(endIndex <= len(self._transactions)):
-            
-            filteredItemsets = {}
-            
-            self.treeGenerations(self.__tree, self._minUtil, filteredItemsets)
-
-            results = []
-
-            for itemSetLen in filteredItemsets:
-                for itemSet in filteredItemsets[itemSetLen]:
-                    itemSetUtility = 0
-                    for transId in range(startIndex, endIndex):
-                        if(self.contains(list(transactionwiseUtility[transId].keys()), itemSet)):
-                            for item in itemSet:
-                                itemSetUtility += transactionwiseUtility[transId][item]
-                        
-                    if(itemSetUtility >= self._minUtil):
-                        results.append([itemSet, itemSetUtility])
-
-            self.__finalPatterns[(startIndex, endIndex)] = results
-
-            if(endIndex >= len(self._transactions)):
-                break
-
-            self.__tree.removeBatch()
-
-            for i in range(0, self.__paneSize):
-                self.__tree.addTransaction(self._transactions[endIndex + i], self._utilitySum[endIndex + i], self._utilities[endIndex + i])
-
-            startIndex += self.__paneSize
-            endIndex += self.__paneSize
-
-        self.__endTime = _hus._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _hus._psutil.Process(_hus._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self):
         """
         This function will start the mining process
         """
         global _minUtil
         self.__startTime = _hus._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilityPatternsInStreams/abstract.py` & `pami-2024.5.1/PAMI/highUtilityPatternsInStreams/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/__init__.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/abstract.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/HDSHUIM.py`

 * *Files 10% similar despite different names*

```diff
@@ -198,15 +198,15 @@
             huis created
         neighbors: map
             keep track of neighbours of elements
         mapOfPMU: map
             a map to keep track of Probable Maximum utility(PMU) of each item
     :Methods:
 
-            startMine()
+            mine()
                 Mining process will start from here
             getPatterns()
                 Complete set of patterns will be retrieved with this function
             save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
             constructCUL(x, compactUList, st, minUtil, length, exNeighbours)
                 A method to construct CUL's database
@@ -325,129 +325,17 @@
             return compare
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
         """
-        minUtil = self._minUtil
-        self._startTime = _ab._time.time()
-        with open(self._nFile, 'r') as file1:
-            for line in file1:
-                line = line.split("\n")[0]
-                parts = line.split(self._sep)
-                parts = [i.strip() for i in parts]
-                item = parts[0]
-                neigh1 = list()
-                for i in range(1, len(parts)):
-                    neigh1.append(parts[i])
-                self._neighbors[item] = set(neigh1)
-        with open(self._iFile, 'r') as file:
-            for line in file:
-                parts = line.split(":")
-                itemString = (parts[0].split("\n")[0]).split(self._sep)
-                utilityString = (parts[2].split("\n")[0]).split(self._sep)
-                transUtility = int(parts[1])
-                trans1 = set()
-                for i in range(0, len(itemString)):
-                    trans1.add(itemString[i])
-                for i in range(0, len(itemString)):
-                    item = itemString[i]
-                    twu = self._mapOfPMU.get(item)
-                    if twu is None:
-                        twu = int(utilityString[i])
-                    else:
-                        twu += int(utilityString[i])
-                    self._mapOfPMU[item] = twu
-                    if self._neighbors.get(item) is None:
-                        continue
-                    neighbours2 = trans1.intersection(self._neighbors.get(item))
-                    for item2 in neighbours2:
-                        if self._mapOfPMU.get(item2) is None:
-                            self._mapOfPMU[item2] = int(utilityString[i])
-                        else:
-                            self._mapOfPMU[item2] += int(utilityString[i])
-
-        listOfCUList = []
-        hashTable = {}
-        mapItemsToCUList = {}
-        for item in self._mapOfPMU.keys():
-            if self._mapOfPMU.get(item) >= minUtil:
-                uList = _CUList(item)
-                mapItemsToCUList[item] = uList
-                listOfCUList.append(uList)
-        listOfCUList.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-        ts = 1
-        with open(self._iFile, 'r') as file:
-            for line in file:
-                parts = line.split(":")
-                items = (parts[0].split("\n")[0]).split(self._sep)
-                utilities = (parts[2].split("\n")[0]).split(self._sep)
-                ru = 0
-                newTwu = 0
-                txKey = []
-                revisedTrans = []
-                for i in range(0, len(items)):
-                    pair = _Pair()
-                    pair.item = items[i]
-                    pair.utility = int(utilities[i])
-                    if self._mapOfPMU.get(pair.item) >= minUtil:
-                        revisedTrans.append(pair)
-                        txKey.append(pair.item)
-                        newTwu += pair.utility
-                revisedTrans.sort(key=_ab._functools.cmp_to_key(self._compareItems))
-                txKey1 = tuple(txKey)
-                if len(revisedTrans) > 0:
-                    if txKey1 not in hashTable.keys():
-                        hashTable[txKey1] = len(mapItemsToCUList[revisedTrans[len(revisedTrans) - 1].item].elements)
-                        for i in range(len(revisedTrans) - 1, -1, -1):
-                            pair = revisedTrans[i]
-                            cuListOfItems = mapItemsToCUList.get(pair.item)
-                            element = _Element(ts, pair.utility, ru, 0, 0)
-                            if i > 0:
-                                element.prevPos = len(mapItemsToCUList[revisedTrans[i - 1].item].elements)
-                            else:
-                                element.prevPos = -1
-                            cuListOfItems.addElements(element)
-                            ru += pair.utility
-                    else:
-                        pos = hashTable[txKey1]
-                        ru = 0
-                        for i in range(len(revisedTrans) - 1, -1, -1):
-                            cuListOfItems = mapItemsToCUList[revisedTrans[i].item]
-                            cuListOfItems.elements[pos].snu += revisedTrans[i].utility
-                            cuListOfItems.elements[pos].remainingUtility += ru
-                            cuListOfItems.sumSnu += revisedTrans[i].utility
-                            cuListOfItems.sumRemainingUtility += ru
-                            ru += revisedTrans[i].utility
-                            pos = cuListOfItems.elements[pos].prevPos
-                # EUCS
-                for i in range(len(revisedTrans) - 1, -1, -1):
-                    pair = revisedTrans[i]
-                    mapFMAPItem = self._mapFMAP.get(pair.item)
-                    if mapFMAPItem is None:
-                        mapFMAPItem = {}
-                        self._mapFMAP[pair.item] = mapFMAPItem
-                    for j in range(i + 1, len(revisedTrans)):
-                        pairAfter = revisedTrans[j]
-                        twuSUm = mapFMAPItem.get(pairAfter.item)
-                        if twuSUm is None:
-                            mapFMAPItem[pairAfter.item] = newTwu
-                        else:
-                            mapFMAPItem[pairAfter.item] = twuSUm + newTwu
-                ts += 1
-        exNeighbours = set(self._mapOfPMU.keys())
-        # print(self.Neighbours)
-        self._ExploreSearchTree([], listOfCUList, exNeighbours, minUtil)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
+
+
 
     def mine(self) -> None:
         """
         main program to start the operation
         """
         minUtil = self._minUtil
         self._startTime = _ab._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/SHUIM.py`

 * *Files 2% similar despite different names*

```diff
@@ -348,15 +348,15 @@
         itemsToKeep: list
             keep only the promising items ie items having twu >= minUtil
         itemsToExplore: list
             keep items that subtreeUtility grater than minUtil
 
     :Methods:
 
-        startMine()
+        mine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
         save(oFile)
                 Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
                 Complete set of frequent patterns will be loaded in to a dataframe
@@ -463,71 +463,15 @@
         super().__init__(iFile, nFile, minUtil, sep)
 
     @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
         """
-        self._startTime = _ab._time.time()
-        self._patternCount = 0
-        self._finalPatterns = {}
-        self._dataset = _Dataset(self._iFile, self._sep)
-        with open(self._nFile, 'r') as o:
-            lines = o.readlines()
-            for line in lines:
-                line = line.split("\n")[0]
-                line_split = line.split(self._sep)
-                line_split = [i.strip() for i in line_split]
-                item = self._dataset.strToInt.get(line_split[0])
-                lst = []
-                for i in range(1, len(line_split)):
-                    lst.append(self._dataset.strToInt.get(line_split[i]))
-                self._Neighbours[item] = lst
-        o.close()
-        #print(len(self._Neighbours))
-        InitialMemory = _ab._psutil.virtual_memory()[3]
-        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
-        itemsToKeep = []
-        for key in self._utilityBinArrayLU.keys():
-            if self._utilityBinArrayLU[key] >= self._minUtil:
-                itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
-        currentName = 1
-        for idx, item in enumerate(itemsToKeep):
-            self._oldNamesToNewNames[item] = currentName
-            self._newNamesToOldNames[currentName] = item
-            itemsToKeep[idx] = currentName
-            currentName += 1
-        for transaction in self._dataset.getTransactions():
-            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
-        self._sortDatabase(self._dataset.getTransactions())
-        emptyTransactionCount = 0
-        for transaction in self._dataset.getTransactions():
-            if len(transaction.getItems()) == 0:
-                emptyTransactionCount += 1
-        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
-        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
-        itemsToExplore = []
-        for item in itemsToKeep:
-            if self._utilityBinArraySU[item] >= self._minUtil:
-                itemsToExplore.append(item)
-        commonitems = []
-        for i in range(self._dataset.maxItem):
-            commonitems.append(i)
-        self._backtrackingEFIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
-        finalMemory = _ab._psutil.virtual_memory()[3]
-        memory = (finalMemory - InitialMemory) / 10000
-        if memory > self._maxMemory:
-            self._maxMemory = memory
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def mine(self) -> None:
         """
         main program to start the operation
         """
         self._startTime = _ab._time.time()
         self._patternCount = 0
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/TKSHUIM.py` & `pami-2024.5.1/PAMI/relativeHighUtilityPattern/basic/RHUIM.py`

 * *Files 13% similar despite different names*

```diff
@@ -1,25 +1,24 @@
-# Top K Spatial High Utility ItemSet Mining (TKSHUIM) aims to discover Top-K Spatial High Utility Itemsets
-# (TKSHUIs) in a spatioTemporal database
+# RHUIM algorithm helps us to mine Relative High Utility itemSets from transactional databases.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.highUtilitySpatialPattern.topk import TKSHUIM as alg
+#             from PAMI.relativeHighUtilityPattern.basic import RHUIM as alg
 #
-#             obj=alg.TKSHUIM("input.txt","Neighbours.txt",35)
+#             obj = alg.RHUIM("input.txt", 35, 20)
 #
-#             obj.mine()
+#             obj.startMine()
 #
-#             Patterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of  Patterns:", len(Patterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save("output")
+#             obj.savePatterns(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -31,17 +30,16 @@
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
 
 
-
 __copyright__ = """
-Copyright (C)  2021 Rage Uday Kiran
+ Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -51,283 +49,315 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.highUtilitySpatialPattern.topk.abstract import *
-from functools import cmp_to_key
-import heapq
+import pandas as pd
 from deprecated import deprecated
+from PAMI.relativeHighUtilityPattern.basic import abstract as _ab
+
 
-class Transaction:
+class _Transaction:
     """
     A class to store Transaction of a database
 
     :Attributes:
 
         items: list
             A list of items in transaction 
         utilities: list
-            A list of utilites of items in transaction
+            A list of utilities of items in transaction
         transactionUtility: int
             represent total sum of all utilities in the database
-        pmus: list
-            represent the pmu (probable maximum utility) of each element in the transaction
-        prefixutility:
+        prefixUtility:
             prefix Utility values of item
         offset:
-            an offset pointer, used by projected transactions
+            an offset pointer, used by projected transaction
 
     :Methods:
 
         projectedTransaction(offsetE):
-            A method to create new Transaction from existing till offsetE
+            A method to create new Transaction from existing starting from offsetE until the end
         getItems():
             return items in transaction
         getUtilities():
             return utilities in transaction
-        getPmus():
-            return pmus in transaction
         getLastPosition():
             return last position in a transaction
         removeUnpromisingItems():
-            A method to remove items with low Utility than minUtil
+            A method to remove items which are having low values when compared with minUtil
         insertionSort():
             A method to sort all items in the transaction
     """
     offset = 0
     prefixUtility = 0
-    
-    def __init__(self, items, utilities, transactionUtility, pmus=None):
+
+    def __init__(self, items: list, utilities: list, transactionUtility: int) -> None:
         self.items = items
         self.utilities = utilities
         self.transactionUtility = transactionUtility
-        if pmus is not None:
-            self.pmus = pmus
 
-    def projectTransaction(self, offsetE):
+    def projectTransaction(self, offsetE: int) -> '_Transaction':
         """
-        A method to create new Transaction from existing till offsetE
+        A method to create new Transaction from existing transaction starting from offsetE until the end
 
         :param offsetE: an offset over the original transaction for projecting the transaction
         :type offsetE: int
         """
-        new_transaction = Transaction(self.items, self.utilities, self.transactionUtility)
+        new_transaction = _Transaction(self.items, self.utilities, self.transactionUtility)
         utilityE = self.utilities[offsetE]
         new_transaction.prefixUtility = self.prefixUtility + utilityE
         new_transaction.transactionUtility = self.transactionUtility - utilityE
         for i in range(self.offset, offsetE):
             new_transaction.transactionUtility -= self.utilities[i]
         new_transaction.offset = offsetE + 1
         return new_transaction
 
-    def getItems(self):
+    def getItems(self) -> list:
         """
         A method to return items in transaction
+        :return: list
         """
         return self.items
 
-    def getPmus(self):
-        """
-        A method to return pmus in transaction
-        """
-        return self.pmus
-
-    def getUtilities(self):
+    def getUtilities(self) -> list:
         """
         A method to return utilities in transaction
+        :return: list
         """
         return self.utilities
 
-    # get the last position in this transaction
-    def getLastPosition(self):
+    def getLastPosition(self) -> int:
         """
         A method to return last position in a transaction
+        :return: int
         """
+
         return len(self.items) - 1
 
-    def removeUnpromisingItems(self, oldNamesToNewNames):
+    def removeUnpromisingItems(self, oldNamesToNewNames: dict) -> None:
         """
-        A method to remove items with low Utility than minUtil
+        A method to remove items which are not present in the map passed to the function
 
-        :param oldNamesToNewNames: A map represent old namses to new names
+        :param oldNamesToNewNames: A map represent old names to new names
         :type oldNamesToNewNames: map
+        :return: None
         """
         tempItems = []
         tempUtilities = []
         for idx, item in enumerate(self.items):
             if item in oldNamesToNewNames:
                 tempItems.append(oldNamesToNewNames[item])
                 tempUtilities.append(self.utilities[idx])
             else:
                 self.transactionUtility -= self.utilities[idx]
         self.items = tempItems
         self.utilities = tempUtilities
         self.insertionSort()
 
-    def insertionSort(self):
+    def insertionSort(self) -> None:
         """
         A method to sort items in order
+        :return: None
         """
         for i in range(1, len(self.items)):
             key = self.items[i]
             utilityJ = self.utilities[i]
             j = i - 1
             while j >= 0 and key < self.items[j]:
                 self.items[j + 1] = self.items[j]
                 self.utilities[j + 1] = self.utilities[j]
                 j -= 1
             self.items[j + 1] = key
             self.utilities[j + 1] = utilityJ
+        
 
-
-class Dataset:
+class _Dataset:
     """
     A class represent the list of transactions in this dataset
 
-    :Attributes:
+   :Attributes:
 
-    transactions:
-        the list of transactions in this dataset
-    maxItem:
-        the largest item name
+        transactions:
+            the list of transactions in this dataset
+        maxItem:
+            the largest item name
         
-    :methods:
+   :methods:
 
         createTransaction(line):
             Create a transaction object from a line from the input file
         getMaxItem():
             return Maximum Item
         getTransactions():
             return transactions in database
 
     """
     transactions = []
     maxItem = 0
-    
-    def __init__(self, datasetpath, sep):
-        self.strToint = {}
-        self.intTostr = {}
+    def __init__(self, datasetPath: str, sep: str) -> None:
+        self.strToInt = {}
+        self.intToStr = {}
         self.cnt = 1
         self.sep = sep
-        with open(datasetpath, 'r') as f:
-            lines = f.readlines()
-            for line in lines:
-                self.transactions.append(self.createTransaction(line))
-        f.close()
+        self.createItemSets(datasetPath)
+
+    def createItemSets(self, datasetPath: str) -> None:
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        :return: None
+        """
+        self.transactions = []
+        itemsets, utilities, utilityValues = [], [], []
+        if isinstance(datasetPath, _ab._pd.DataFrame):
+            utilities, data, utilityValues = [], [], []
+            if datasetPath.empty:
+                print("its empty..")
+            i = datasetPath.columns.values.tolist()
+            if 'Transactions' in i:
+                itemsets = datasetPath['Transactions'].tolist()
+            if 'Utilities' in i:
+                utilities = datasetPath['Patterns'].tolist()
+            if 'UtilitySum' in i:
+                utilityValues = datasetPath['utilitySum'].tolist()
+            for k in range(len(itemsets)):
+                self.transactions.append(self.createTransaction(itemsets[k], utilities[k], utilityValues[k]))
+        if isinstance(datasetPath, str):
+            if _ab._validators.url(datasetPath):
+                data = _ab._urlopen(datasetPath)
+                for line in data:
+                    line = line.decode("utf-8")
+                    trans_list = line.strip().split(':')
+                    transactionUtility = int(trans_list[1])
+                    itemsString = trans_list[0].strip().split(self.sep)
+                    itemsString = [x for x in itemsString if x]
+                    utilityString = trans_list[2].strip().split(self.sep)
+                    utilityString = [x for x in utilityString if x]
+                    self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
+            else:
+                try:
+                    with open(datasetPath, 'r', encoding='utf-8') as f:
+                        for line in f:
+                            trans_list = line.strip().split(':')
+                            transactionUtility = int(trans_list[1])
+                            itemsString = trans_list[0].strip().split(self.sep)
+                            itemsString = [x for x in itemsString if x]
+                            utilityString = trans_list[2].strip().split(self.sep)
+                            utilityString = [x for x in utilityString if x]
+                            self.transactions.append(self.createTransaction(itemsString, utilityString, transactionUtility))
+                except IOError:
+                    print("File Not Found")
+                    quit()
 
-    def createTransaction(self, line):
+    def createTransaction(self, itemSet: list, utilities: list, utilitySum: int) -> _Transaction:
         """
         A method to create Transaction from dataset given
+            
+        :Attributes:
 
-        :param line: represent a single line of database
-        :type line: string
+        :param itemSet: represent a transactions itemset in database
+        :type itemSet: list
+        :param utilities: utility values of respective transaction itemSets
+        :type utilities: list
+        :param utilitySum: represent the sum of utility Sum
+        :type utilitySum: int
         :return : Transaction.
-        :rtype: int
+        :rtype: Transaction
         """
-        trans_list = line.strip().split(':')
-        transactionUtility = int(trans_list[1])
-        itemsString = trans_list[0].strip().split(self.sep)
-        utilityString = trans_list[2].strip().split(self.sep)
-        if (len(trans_list) == 4):
-            pmuString = trans_list[3].strip().split(self.sep)
+        transactionUtility = utilitySum
+        itemsString = itemSet
+        utilityString = utilities
         items = []
         utilities = []
-        pmus = []
         for idx, item in enumerate(itemsString):
-            if (self.strToint).get(item) is None:
-                self.strToint[item] = self.cnt
-                self.intTostr[self.cnt] = item
+            if self.strToInt.get(item) is None:
+                self.strToInt[item] = self.cnt
+                self.intToStr[self.cnt] = item
                 self.cnt += 1
-            item_int = self.strToint.get(item)
+            item_int = self.strToInt.get(item)
             if item_int > self.maxItem:
                 self.maxItem = item_int
             items.append(item_int)
             utilities.append(int(utilityString[idx]))
-            if (len(trans_list) == 4):
-                pmus.append(int(pmuString[idx]))
-        return Transaction(items, utilities, transactionUtility, pmus)
+        return _Transaction(items, utilities, transactionUtility)
 
-    def getMaxItem(self):
+    def getMaxItem(self) -> int:
         """
         A method to return name of the largest item
+        :return; int
         """
         return self.maxItem
 
-    def getTransactions(self):
+    def getTransactions(self) -> list:
         """
         A method to return transactions from database
+        :return: list
         """
         return self.transactions
 
 
-class TKSHUIM(utilityPatterns):
+class RHUIM(_ab._utilityPatterns):
     """
-    :Description:
-       Top K Spatial High Utility ItemSet Mining (TKSHUIM) aims to discover Top-K Spatial High Utility Itemsets
-       (TKSHUIs) in a spatioTemporal database
-
-    :Reference:
-
-       P. Pallikila et al., "Discovering Top-k Spatial High Utility Itemsets in Very Large Quantitative Spatiotemporal 
-       databases," 2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 4925-4935, 
-       doi: 10.1109/BigData52589.2021.9671912.
+    :Description:   RHUIM algorithm helps us to mine Relative High Utility itemSets from transactional databases.
+    
+    :Reference:   R. U. Kiran, P. Pallikila, J. M. Luna, P. Fournier-Viger, M. Toyoda and P. K. Reddy,
+                 "Discovering Relative High Utility Itemsets in Very Large Transactional Databases Using Null-Invariant Measure,"
+                  2021 IEEE International Conference on Big Data (Big Data), Orlando, FL, USA, 2021, pp. 252-262,
+                  doi: 10.1109/BigData52589.2021.9672064.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of High Utility Spatial patterns
+                   Name of the Input file to mine complete set of Relative High Utility patterns
     :param  oFile: str :
-                   Name of the output file to store complete set of High Utility Spatial patterns
-    :param minUtil: int :
-                   Minimum utility threshold given by User
-    :param maxMemory: int :
-                   Maximum memory used by this program for running
-    :param candidateCount: int :
-                   Number of candidates to consider when calculating a high utility spatial pattern
-    :param nFile: str :
-                   Name of the input file to mine complete set of High Utility Spatial patterns
+                   Name of the output file to store complete set of Relative High Utility patterns
+    :param  minSup: float or int or str :
+                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
+    :param  minUtil: int :
+                   The minimum utility threshold.
 
     :Attributes:
 
         iFile : file
-            Name of the input file to mine complete set of frequent patterns
-        nFile : file
-            Name of the Neighbours file that contain neighbours of items
+            Name of the input file to mine complete set of patterns
         oFile : file
-            Name of the output file to store complete set of frequent patterns
+            Name of the output file to store complete set of patterns
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
-        startTime:float
+        startTime : float
             To record the start time of the mining process
-        endTime:float
+        endTime : float
             To record the completion time of the mining process
-        k : int
-            The user given k value
-        candidateCount: int
+        minUtil : int
+            The user given minUtil value
+        minUR : float
+            The user given minUR value
+        relativeHighUtilityItemSets : map
+            set of relative high utility itemSets
+        candidateCount : int
              Number of candidates 
-        utilityBinArrayLU: list
-             A map to hold the pmu values of the items in database
-        utilityBinArraySU: list
-            A map to hold the subtree utility values of the items is database
-        oldNamesToNewNames: list
+        utilityBinArrayLU : list
+             A map to hold the local utility values of the items in database
+        utilityBinArraySU : list
             A map to hold the subtree utility values of the items is database
-        newNamesToOldNames: list
-            A map to store the old name corresponding to new name
-        Neighbours : map
-            A dictionary to store the neighbours of a item
-        maxMemory: float
+        oldNamesToNewNames : list
+            A map which contains old names, new names of items as key value pairs
+        newNamesToOldNames : list
+            A map which contains new names, old names of items as key value pairs
+        maxMemory : float
             Maximum memory used by this program for running
-        itemsToKeep: list
-            keep only the promising items ie items having twu >= minUtil
-        itemsToExplore: list
-            keep items that subtreeUtility grater than minUtil
+        patternCount : int
+            Number of RHUI's
+        itemsToKeep : list
+            keep only the promising items i.e items that can extend other items to form RHUIs
+        itemsToExplore : list
+            list of items that needs to be explored
 
     :Methods:
 
         startMine()
                 Mining process will start from here
         getPatterns()
                 Complete set of patterns will be retrieved with this function
@@ -337,280 +367,209 @@
                 Complete set of patterns will be loaded in to a dataframe
         getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
                Total amount of runtime taken by the mining process will be retrieved from this function
-        calculateNeighbourIntersection(self, prefixLength)
-               A method to return common Neighbours of items
-        backtrackingEFIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
-               A method to mine the TKSHUIs Recursively
-        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep, neighbourhoodList)
-               A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
+        backTrackingRHUIM(transactionsOfP, itemsToKeep, itemsToExplore, prefixLength)
+               A method to mine the RHUIs Recursively
+        useUtilityBinArraysToCalculateUpperBounds(transactionsPe, j, itemsToKeep)
+               A method to calculate the sub-tree utility and local utility of all items that can extend itemSet P and e
         output(tempPosition, utility)
-               A method ave a high-utility itemSet to file or memory depending on what the user chose
+               A method to output a relative-high-utility itemSet to file or memory depending on what the user chose
         is_equal(transaction1, transaction2)
                A method to Check if two transaction are identical
-        intersection(lst1, lst2)
-               A method that return the intersection of 2 list
         useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(dataset)
-              Scan the initial database to calculate the subtree utility of each items using a utility-bin array
+              A method to calculate the sub tree utility values for single items
         sortDatabase(self, transactions)
-              A Method to sort transaction in the order of PMU
+              A Method to sort transaction
         sort_transaction(self, trans1, trans2)
-              A Method to sort transaction in the order of PMU
+              A Method to sort transaction
         useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset)
-             A method to scan the database using utility bin array to calculate the pmus                   
-
-    **Executing the code on terminal:**
-    -------------------------------------
+             A method to calculate local utility values for single itemSets
 
+    **Methods to execute code on terminal**
+    -------------------------------------------
     .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 TKSHUIM.py <inputFile> <outputFile> <Neighbours> <k> <sep>
+      (.venv) $ python3 RHUIM.py <inputFile> <outputFile> <minUtil> <sep>
+
+      Example usage:
+
+      (.venv) $ python3 RHUIM.py sampleTDB.txt output.txt 35 20
 
-      Example Usage:
 
-      (.venv) $ python3 TKSHUIM.py sampleTDB.txt output.txt sampleN.txt 35
+              .. note:: minSup will be considered in times of minSup and count of database transactions
 
-    .. note:: maxMemory will be considered as Maximum memory used by this program for running
 
 
-    **Sample run of importing the code:**
-    ----------------------------------------
+    **Importing this algorithm into a python program**
+    -----------------------------------------------------
     .. code-block:: python
-        
-            from PAMI.highUtilitySpatialPattern.topk import TKSHUIM as alg
 
-            obj=alg.TKSHUIM("input.txt","Neighbours.txt",35)
+            from PAMI.relativeHighUtilityPattern.basic import RHUIM as alg
+
+            obj=alg.RHUIM("input.txt", 35, 20)
 
-            obj.mine()
+            obj.startMine()
 
-            Patterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            obj.save("output")
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            memUSS = obj.getMemoryUSS()
+            obj.savePatterns(oFile)
+
+            Df = obj.getPatternsAsDataFrame()
+
+            memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
-    ---------------
-            The complete program was written by Pradeep Pallikila under the supervision of Professor Rage Uday Kiran.
+    -----------------
+             The complete program was written by  Pradeep Pallikila  under the supervision of Professor Rage Uday Kiran.
+
     """
-    candidateCount = 0
-    utilityBinArrayLU = {}
-    utilityBinArraySU = {}
-    oldNamesToNewNames = {}
-    newNamesToOldNames = {}
-    strToint = {}
-    intTostr = {}
-    Neighbours = {}
-    temp = [0] * 5000
-    maxMemory = 0
-    startTime = float()
-    endTime = float()
-    finalPatterns = {}
-    iFile = " "
-    oFile = " "
-    nFile = " "
-    sep = "\t"
-    minUtil = 0
-    memoryUSS = float()
-    memoryRSS = float()
-    heapList = []
-
-    def __init__(self, iFile, nFile, k, sep="\t"):
-        super().__init__(iFile, nFile, k, sep)
-
-    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
-    def startMine(self):
-        """
-        Main function of the program.
-        """
-        self.startTime = time.time()
-        self.finalPatterns = {}
-        self.dataset = Dataset(self.iFile, self.sep)
-        with open(self.nFile, 'r') as o:
-            lines = o.readlines()
-            for line in lines:
-                line = line.split("\n")[0]
-                line_split = line.split(self.sep)
-                item = self.dataset.strToint.get(line_split[0])
-                lst = []
-                for i in range(1, len(line_split)):
-                    lst.append(self.dataset.strToint.get(line_split[i]))
-                self.Neighbours[item] = lst
-        o.close()
-        InitialMemory = psutil.virtual_memory()[3]
-        self.useUtilityBinArrayToCalculateLocalUtilityFirstTime(self.dataset)
-        itemsToKeep = []
-        for key in self.utilityBinArrayLU.keys():
-            if self.utilityBinArrayLU[key] >= self.minUtil:
-                itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self.utilityBinArrayLU[x])
-        currentName = 1
-        for idx, item in enumerate(itemsToKeep):
-            self.oldNamesToNewNames[item] = currentName
-            self.newNamesToOldNames[currentName] = item
-            itemsToKeep[idx] = currentName
-            currentName += 1
-        for transaction in self.dataset.getTransactions():
-            transaction.removeUnpromisingItems(self.oldNamesToNewNames)
-        self.sortDatabase(self.dataset.getTransactions())
-        emptyTransactionCount = 0
-        for transaction in self.dataset.getTransactions():
-            if len(transaction.getItems()) == 0:
-                emptyTransactionCount += 1
-        self.dataset.transactions = self.dataset.transactions[emptyTransactionCount:]
-        self.useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self.dataset)
-        self.heapList = []
-        itemsToExplore = []
-        for item in itemsToKeep:
-            if self.utilityBinArraySU[item] >= self.minUtil:
-                itemsToExplore.append(item)
-        commonitems = []
-        for i in range(self.dataset.maxItem):
-            commonitems.append(i)
-        self.backtrackingEFIM(self.dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
-        finalMemory = psutil.virtual_memory()[3]
-        memory = (finalMemory - InitialMemory) / 10000
-        if memory > self.maxMemory:
-            self.maxMemory = memory
-        self.endTime = time.time()
-        process = psutil.Process(os.getpid())
-        self.memoryUSS = float()
-        self.memoryRSS = float()
-        self.memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
-        for item in self.heapList:
-            self.finalPatterns[item[1]] = item[0]
-        print('TOP-K mining process is completed by TKSHUIM')
-
-    def mine(self):
-        """
-        Main function of the program.
-        """
-        self.startTime = time.time()
-        self.finalPatterns = {}
-        self.dataset = Dataset(self.iFile, self.sep)
-        with open(self.nFile, 'r') as o:
-            lines = o.readlines()
-            for line in lines:
-                line = line.split("\n")[0]
-                line_split = line.split(self.sep)
-                item = self.dataset.strToint.get(line_split[0])
-                lst = []
-                for i in range(1, len(line_split)):
-                    lst.append(self.dataset.strToint.get(line_split[i]))
-                self.Neighbours[item] = lst
-        o.close()
-        InitialMemory = psutil.virtual_memory()[3]
-        self.useUtilityBinArrayToCalculateLocalUtilityFirstTime(self.dataset)
+
+    _relativeHighUtilityItemSets = []
+    _candidateCount = 0
+    _utilityBinArrayLU = {}
+    _utilityBinArraySU = {}
+    _oldNamesToNewNames = {}
+    _newNamesToOldNames = {}
+    _singleItemSetsUtilities = {}
+    _strToInt = {}
+    _intToStr = {}
+    _temp = [0]*5000
+    _patternCount = int()
+    _maxMemory = 0
+    _startTime = float()
+    _endTime = float()
+    _finalPatterns = {}
+    _iFile = " "
+    _oFile = " "
+    _nFile = " "
+    _lno = 0
+    _sep = "\t"
+    _minUtil = 0
+    _minUR = 0
+    _memoryUSS = float()
+    _memoryRSS = float()
+
+    def __init__(self, iFile: str, minUtil: int, minUR: float, sep: str="\t") -> None:
+        super().__init__(iFile, minUtil, minUR, sep)
+
+    def startMine(self) -> None:
+        """
+        Mining process will start from this function
+        :return: None
+        """
+        self._startTime = _ab._time.time()
+        self._dataset = _Dataset(self._iFile, self._sep)
+        self._finalPatterns = {}
+        self._useUtilityBinArrayToCalculateLocalUtilityFirstTime(self._dataset)
+        _minUtil = int(self._minUtil)
+        _minUR = float(self._minUR)
+        # print(minUR)
+        self._singleItemSetsUtilities = _ab._defaultdict(int)
         itemsToKeep = []
-        for key in self.utilityBinArrayLU.keys():
-            if self.utilityBinArrayLU[key] >= self.minUtil:
+        for key in self._utilityBinArrayLU.keys():
+            if self._utilityBinArrayLU[key] >= _minUtil:
                 itemsToKeep.append(key)
-        itemsToKeep = sorted(itemsToKeep, key=lambda x: self.utilityBinArrayLU[x])
+        itemsToKeep = sorted(itemsToKeep, key=lambda x: self._utilityBinArrayLU[x])
         currentName = 1
         for idx, item in enumerate(itemsToKeep):
-            self.oldNamesToNewNames[item] = currentName
-            self.newNamesToOldNames[currentName] = item
+            self._oldNamesToNewNames[item] = currentName
+            self._newNamesToOldNames[currentName] = item
             itemsToKeep[idx] = currentName
             currentName += 1
-        for transaction in self.dataset.getTransactions():
-            transaction.removeUnpromisingItems(self.oldNamesToNewNames)
-        self.sortDatabase(self.dataset.getTransactions())
+        for transaction in self._dataset.getTransactions():
+            transaction.removeUnpromisingItems(self._oldNamesToNewNames)
+        self.sortDatabase(self._dataset.getTransactions())
         emptyTransactionCount = 0
-        for transaction in self.dataset.getTransactions():
+        for transaction in self._dataset.getTransactions():
             if len(transaction.getItems()) == 0:
                 emptyTransactionCount += 1
-        self.dataset.transactions = self.dataset.transactions[emptyTransactionCount:]
-        self.useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self.dataset)
-        self.heapList = []
+        self._dataset.transactions = self._dataset.transactions[emptyTransactionCount:]
+        self._useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self._dataset)
         itemsToExplore = []
         for item in itemsToKeep:
-            if self.utilityBinArraySU[item] >= self.minUtil:
+            if self._utilityBinArraySU[item] >= _minUtil:
                 itemsToExplore.append(item)
-        commonitems = []
-        for i in range(self.dataset.maxItem):
-            commonitems.append(i)
-        self.backtrackingEFIM(self.dataset.getTransactions(), itemsToKeep, itemsToExplore, 0)
-        finalMemory = psutil.virtual_memory()[3]
-        memory = (finalMemory - InitialMemory) / 10000
-        if memory > self.maxMemory:
-            self.maxMemory = memory
-        self.endTime = time.time()
-        process = psutil.Process(os.getpid())
-        self.memoryUSS = float()
-        self.memoryRSS = float()
-        self.memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
-        for item in self.heapList:
-            self.finalPatterns[item[1]] = item[0]
-        print('TOP-K mining process is completed by TKSHUIM')
+        utilitySum = 0
+        self._backTrackingRHUIM(self._dataset.getTransactions(), itemsToKeep, itemsToExplore, 0, utilitySum)
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
+        self._memoryUSS = float()
+        self._memoryRSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
+        print("Relative High Utility patterns were generated successfully using RHUIM algorithm")
 
-    def backtrackingEFIM(self, transactionsOfP, itemsToKeep, itemsToExplore, prefixLength):
+    def _backTrackingRHUIM(self, transactionsOfP: list, itemsToKeep: list, itemsToExplore: list, prefixLength: int, utilitySumP: int) -> None:
         """
-        A method to mine the TKSHUIs Recursively
+        A method to mine the RHUIs Recursively
+
+        :Attributes:
 
         :param transactionsOfP: the list of transactions containing the current prefix P
         :type transactionsOfP: list
         :param itemsToKeep: the list of secondary items in the p-projected database
         :type itemsToKeep: list
         :param itemsToExplore: the list of primary items in the p-projected database
         :type itemsToExplore: list
         :param prefixLength: current prefixLength
         :type prefixLength: int
+        :param utilitySumP: a variable to hold sum of utilities of all items in P
+        :type utilitySumP int
+        :return: None
         """
-        self.candidateCount += len(itemsToExplore)
+        self._candidateCount += len(itemsToExplore)
         for idx, e in enumerate(itemsToExplore):
-            initialMemory = psutil.virtual_memory()[3]
             transactionsPe = []
             utilityPe = 0
-            if len(transactionsOfP) == 0:
-                break 
+            utilitySumPe = utilitySumP + self._singleItemSetsUtilities[e]
             previousTransaction = transactionsOfP[0]
             consecutiveMergeCount = 0
             for transaction in transactionsOfP:
                 items = transaction.getItems()
                 if e in items:
                     positionE = items.index(e)
                     if transaction.getLastPosition() == positionE:
                         utilityPe += transaction.getUtilities()[positionE] + transaction.prefixUtility
                     else:
                         projectedTransaction = transaction.projectTransaction(positionE)
                         utilityPe += projectedTransaction.prefixUtility
                         if previousTransaction == transactionsOfP[0]:
                             previousTransaction = projectedTransaction
-                        elif self.is_equal(projectedTransaction, previousTransaction):
+                        elif self._isEqual(projectedTransaction, previousTransaction):
                             if consecutiveMergeCount == 0:
                                 items = previousTransaction.items[previousTransaction.offset:]
                                 utilities = previousTransaction.utilities[previousTransaction.offset:]
                                 itemsCount = len(items)
                                 positionPrevious = 0
                                 positionProjection = projectedTransaction.offset
                                 while positionPrevious < itemsCount:
                                     utilities[positionPrevious] += projectedTransaction.utilities[positionProjection]
                                     positionPrevious += 1
                                     positionProjection += 1
                                 previousTransaction.prefixUtility += projectedTransaction.prefixUtility
                                 sumUtilities = previousTransaction.prefixUtility
-                                previousTransaction = Transaction(items, utilities, previousTransaction.transactionUtility + projectedTransaction.transactionUtility)
+                                previousTransaction = _Transaction(items, utilities, previousTransaction.transactionUtility + projectedTransaction.transactionUtility)
                                 previousTransaction.prefixUtility = sumUtilities
                             else:
                                 positionPrevious = 0
                                 positionProjected = projectedTransaction.offset
                                 itemsCount = len(previousTransaction.items)
                                 while positionPrevious < itemsCount:
                                     previousTransaction.utilities[positionPrevious] += projectedTransaction.utilities[
@@ -623,192 +582,160 @@
                         else:
                             transactionsPe.append(previousTransaction)
                             previousTransaction = projectedTransaction
                             consecutiveMergeCount = 0
                     transaction.offset = positionE
             if previousTransaction != transactionsOfP[0]:
                 transactionsPe.append(previousTransaction)
-            self.temp[prefixLength] = self.newNamesToOldNames[e]
-            if utilityPe >= self.minUtil:
-                self.output(prefixLength, utilityPe)
-            neighbourhoodList = self.calculateNeighbourIntersection(prefixLength)
-            self.useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep, neighbourhoodList)
+            self._temp[prefixLength] = self._newNamesToOldNames[e]
+            utility_ratio_pe = float(utilityPe / utilitySumPe)
+            if (utilityPe >= self._minUtil) and (utility_ratio_pe * 100 >= self._minUR):
+                self._output(prefixLength, utilityPe, utility_ratio_pe)
+            self._useUtilityBinArraysToCalculateUpperBounds(transactionsPe, idx, itemsToKeep)
             newItemsToKeep = []
             newItemsToExplore = []
             for l in range(idx + 1, len(itemsToKeep)):
                 itemK = itemsToKeep[l]
-                if self.utilityBinArraySU[itemK] >= self.minUtil:
-                    if itemK in neighbourhoodList:
-                        newItemsToExplore.append(itemK)
-                        newItemsToKeep.append(itemK)
-                elif self.utilityBinArrayLU[itemK] >= self.minUtil:
-                    if itemK in neighbourhoodList:
-                        newItemsToKeep.append(itemK)
-            self.backtrackingEFIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1)
-            finalMemory = psutil.virtual_memory()[3]
-            memory = (finalMemory - initialMemory) / 10000
-            if self.maxMemory < memory:
-                self.maxMemory = memory
+                utility_sum_pek = utilitySumPe + self._singleItemSetsUtilities[itemK]
+                subtree_utility_ratio = float(self._utilityBinArraySU[itemK] / utility_sum_pek)
+                local_utility_ratio = float(self._utilityBinArrayLU[itemK] / utility_sum_pek)
+                if self._utilityBinArraySU[itemK] >= self._minUtil and subtree_utility_ratio * 100 >= self._minUR:
+                    newItemsToExplore.append(itemK)
+                    newItemsToKeep.append(itemK)
+                elif self._utilityBinArrayLU[itemK] >= self._minUtil and local_utility_ratio * 100 >= self._minUR:
+                    newItemsToKeep.append(itemK)
+            self._backTrackingRHUIM(transactionsPe, newItemsToKeep, newItemsToExplore, prefixLength + 1, utilitySumPe)
 
-    def useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe, j, itemsToKeep, neighbourhoodList):
+    def _useUtilityBinArraysToCalculateUpperBounds(self, transactionsPe: list, j: int, itemsToKeep: list) -> None:
         """
-        A method to  calculate the sub-tree utility and local utility of all items that can extend itemSet P U {e}
+        A method to  calculate the subtree utility and local utility of all items that can extend itemSet P U {e}
+
+        :Attributes:
 
         :param transactionsPe: transactions the projected database for P U {e}
-        :type transactionsPe: list
-        :param j:the position of j in the list of promising items
+        :type transactionsPe: list or Dataset
+        :param j: the position of j in the list of promising items
         :type j:int
         :param itemsToKeep :the list of promising items
-        :type itemsToKeep: list
-        :param neighbourhoodList: list of neighbourhood elements
-        :type neighbourhoodList: list
+        :type itemsToKeep: list or Dataset
+        :return: None
         """
         for i in range(j + 1, len(itemsToKeep)):
             item = itemsToKeep[i]
-            self.utilityBinArrayLU[item] = 0
-            self.utilityBinArraySU[item] = 0
+            self._utilityBinArrayLU[item] = 0
+            self._utilityBinArraySU[item] = 0
         for transaction in transactionsPe:
-            length = len(transaction.getItems())
-            i = length - 1
+            sumRemainingUtility = 0
+            i = len(transaction.getItems()) - 1
             while i >= transaction.offset:
                 item = transaction.getItems()[i]
                 if item in itemsToKeep:
-                    remainingUtility = 0
-                    if self.newNamesToOldNames[item] in self.Neighbours:
-                        item_neighbours = self.Neighbours[self.newNamesToOldNames[item]]
-                        for k in range(i, length):
-                            transaction_item = transaction.getItems()[k]
-                            if self.newNamesToOldNames[transaction_item] in item_neighbours and transaction_item in neighbourhoodList:
-                                remainingUtility += transaction.getUtilities()[k]
-
-                    remainingUtility += transaction.getUtilities()[i]
-                    self.utilityBinArraySU[item] += remainingUtility + transaction.prefixUtility
-                    self.utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
+                    sumRemainingUtility += transaction.getUtilities()[i]
+                    self._utilityBinArraySU[item] += sumRemainingUtility + transaction.prefixUtility
+                    self._utilityBinArrayLU[item] += transaction.transactionUtility + transaction.prefixUtility
                 i -= 1
 
-    def calculateNeighbourIntersection(self, prefixLength):
+    def _output(self, tempPosition: int, utility: int, utilityRatio: float) -> None:
         """
-        A method to find common Neighbours
+         Method to print relative high utility itemSet
 
-        :param prefixLength: the prefix itemSet
-        :type prefixLength:int
-        """
-        intersectionList = self.Neighbours.get(self.temp[0])
-        for i in range(1, prefixLength+1):
-            intersectionList = self.intersection(self.Neighbours[self.temp[i]], intersectionList)
-        finalIntersectionList = []
-        if intersectionList is None:
-            return finalIntersectionList
-        for item in intersectionList:
-            if item in self.oldNamesToNewNames:
-                finalIntersectionList.append(self.oldNamesToNewNames[item])
-        return finalIntersectionList
-    
-    def output(self, tempPosition, utility):
-        """
-        A method save all high-utility itemSet to file or memory depending on what the user chose
+         :Attributes:
 
-        :param tempPosition: position of last item
-        :type tempPosition : int
-        :param utility: total utility of itemSet
-        :type utility: int
+         :param tempPosition: position of last item 
+         :type tempPosition : int 
+         :param utility: total utility of itemSet
+         :type utility: int
+         :param utilityRatio: utility ratio of an itemSet
+         :type utilityRatio: float
+         :return: None
         """
+        self._patternCount += 1
         s1 = str()
         for i in range(0, tempPosition+1):
-            s1 += self.dataset.intTostr.get((self.temp[i]))
+            s1 += self._dataset.intToStr.get((self._temp[i]))
             if i != tempPosition:
                 s1 += "\t"
-        self.additemset(s1, utility)
+        self._finalPatterns[s1] = [utility, utilityRatio]
 
-    def is_equal(self, transaction1, transaction2):
+    def _isEqual(self, transaction1: _Transaction, transaction2: _Transaction) -> bool:
         """
-        A method to Check if two transaction are identical
+         A method to Check if two transaction are identical
 
-        :param  transaction1: the first transaction.
-        :type  transaction1: Transaction
-        :param  transaction2:   the second transaction.
-        :type  transaction2: Transaction
-        :return : whether both are identical or not
-        :rtype: bool
-        """
+         :Attributes:
 
+         :param  transaction1: the first transaction.
+         :type  transaction1: Transaction
+         :param  transaction2:   The second transaction.
+         :type  transaction2: Transaction
+         :return : whether both are identical or not
+         :rtype: bool
+        """
         length1 = len(transaction1.items) - transaction1.offset
         length2 = len(transaction2.items) - transaction2.offset
         if length1 != length2:
             return False
         position1 = transaction1.offset
         position2 = transaction2.offset
         while position1 < len(transaction1.items):
             if transaction1.items[position1] != transaction2.items[position2]:
                 return False
             position1 += 1
             position2 += 1
         return True
-    
-    def intersection(self, lst1, lst2):
-        """
-        A method that return the intersection of 2 list
-
-        :param  lst1: items neighbour to item1
-        :type lst1: list
-        :param lst2: items neighbour to item2
-        :type lst2: list
-        :return :intersection of two lists
-        :rtype : list
-        """
-        temp = set(lst2)
-        lst3 = [value for value in lst1 if value in temp]
-        return lst3
 
-    def useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset):
+    def _useUtilityBinArrayToCalculateSubtreeUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
         Scan the initial database to calculate the subtree utility of each item using a utility-bin array
 
+        :Attributes:
+
         :param dataset: the transaction database
         :type dataset: Dataset
+        :return: None
         """
         for transaction in dataset.getTransactions():
-            items = transaction.getItems()
-            utilities = transaction.getUtilities()
-            for idx, item in enumerate(items):
-                if item not in self.utilityBinArraySU:
-                    self.utilityBinArraySU[item] = 0
-                if self.newNamesToOldNames[item] not in self.Neighbours:
-                    self.utilityBinArraySU[item] += utilities[idx]
-                    continue
-                i = idx + 1
-                sumSu = utilities[idx]
-                while i < len(items):
-                    if self.newNamesToOldNames[items[i]] in self.Neighbours[self.newNamesToOldNames[item]]:
-                        sumSu += utilities[i]
-                    i += 1
-                self.utilityBinArraySU[item] += sumSu
+            sumSU = 0
+            i = len(transaction.getItems()) - 1
+            while i >= 0:
+                item = transaction.getItems()[i]
+                currentUtility = transaction.getUtilities()[i]
+                sumSU += currentUtility
+                self._singleItemSetsUtilities[item] += currentUtility
+                if item in self._utilityBinArraySU.keys():
+                    self._utilityBinArraySU[item] += sumSU
+                else:
+                    self._utilityBinArraySU[item] = sumSU
+                i -= 1
 
-    def sortDatabase(self, transactions):
+    def sortDatabase(self, transactions: list) -> None:
         """
-        A Method to sort transaction in the order of PMU
+        A Method to sort transaction
+
+        :Attributes:
 
         :param transactions: transaction of items
-        :type transactions: Transaction
-        :return: sorted transaction
-        :rtype: Transaction
+        :type transactions: list
+        :return: sorted transactions.
+        :rtype: Transactions or list
         """
-        cmp_items = cmp_to_key(self.sort_transaction)
+        cmp_items = _ab._functools.cmp_to_key(self.sort_transaction)
         transactions.sort(key=cmp_items)
 
-    def sort_transaction(self, trans1, trans2):
+    def sort_transaction(self, trans1: _Transaction, trans2: _Transaction) -> int:
         """
-        A Method to sort transaction in the order of PMU
+        A Method to sort transaction
+
+        :Attributes:
 
-        :param trans1: the first transaction.
+        :param trans1: the first transaction .
         :type trans1: Transaction
         :param trans2:the second transaction.
         :type trans2: Transaction
         :return: sorted transaction.
-        :rtype: int
+        :rtype:   Transaction
         """
         trans1_items = trans1.getItems()
         trans2_items = trans2.getItems()
         pos1 = len(trans1_items) - 1
         pos2 = len(trans2_items) - 1
         if len(trans1_items) < len(trans2_items):
             while pos1 >= 0:
@@ -831,180 +758,121 @@
                 sub = trans2_items[pos2] - trans1_items[pos1]
                 if sub != 0:
                     return sub
                 pos1 -= 1
                 pos2 -= 1
             return 0
 
-    def useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset):
+    def _useUtilityBinArrayToCalculateLocalUtilityFirstTime(self, dataset: _Dataset) -> None:
         """
-        A method to scan the database using utility bin array to calculate the pmus
+        A method to calculate local utility of single itemSets
+
+        :Attributes:
 
         :param dataset: the transaction database.
         :type dataset: database
+        :return: None
+
         """
-        utilityMatrix = defaultdict(lambda: defaultdict(int))
         for transaction in dataset.getTransactions():
-            for idx, item in enumerate(transaction.getItems()):
-                pmu = transaction.getUtilities()[idx]
-                if item in self.Neighbours:
-                    neighbors = self.Neighbours[item]
-                    for idx, item in enumerate(transaction.getItems()):
-                        if item in neighbors:
-                            pmu += transaction.getUtilities()[idx]
-                if item in self.utilityBinArrayLU:
-                    # self.utilityBinArrayLU[item] += transaction.getPmus()[idx]
-                    self.utilityBinArrayLU[item] += pmu
+            for item in transaction.getItems():
+                if item in self._utilityBinArrayLU:
+                    self._utilityBinArrayLU[item] += transaction.transactionUtility
                 else:
-                    # self.utilityBinArrayLU[item] = transaction.getPmus()[idx]
-                    self.utilityBinArrayLU[item] = pmu
-                utilityMatrix[item][item] += transaction.getUtilities()[idx]
-                if item in self.Neighbours:
-                    neighbors = self.Neighbours[item]
-                    utility = transaction.getUtilities()[idx]
-                    for i, itemj in enumerate(transaction.getItems()):
-                        if (itemj != item) and (itemj in neighbors):
-                            utilityMatrix[item][itemj] += (utility + transaction.getUtilities()[i])
-
-        for item in utilityMatrix.keys():
-            for itemj in utilityMatrix[item].keys():
-                if itemj >= item:
-                    val = utilityMatrix[item][itemj]
-                    if val != 0 and val > self.minUtil:
-                        if itemj == item:
-                            itemset = str(item)
-                        else:
-                            itemset = str(item) + str(itemj)
-                        self.additemset(itemset, val)
-
-    def additemset(self, itemset, utility):
-        """
-        adds the itemset to the priority queue
+                    self._utilityBinArrayLU[item] = transaction.transactionUtility
 
-        :param itemset: the itemset to be added
-
-        :type itemset: str
-
-        :param utility: utility matrix for the itemset to be added
-
-        :type utility: numpy.array
-        """
-        heapq.heappush(self.heapList, (utility, itemset))
-        if len(self.heapList) > self.k:
-            while len(self.heapList) > self.k:
-                heapq.heappop(self.heapList)
-                if len(self.heapList) == 0:
-                    break
-            self.minUtil = heapq.nsmallest(1, self.heapList)[0][0]
-
-    def getPatternsAsDataFrame(self):
-        """
-        Storing final patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+        """Storing final patterns in a dataframe
 
         :return: returning patterns in a dataframe
         :rtype: pd.DataFrame
-        """
+            """
         dataFrame = {}
         data = []
-        for a, b in self.finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Utility'])
+        for a, b in self._finalPatterns.items():
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Utility', 'UtilityRatio'])
 
         return dataFrame
     
-    def getPatterns(self):
-        """
-        Function to send the set of patterns after completion of the mining process
+    def getPatterns(self) -> dict:
+        """ Function to send the set of patterns after completion of the mining process
 
         :return: returning patterns
         :rtype: dict
         """
-        return self.finalPatterns
+        return self._finalPatterns
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
-        Complete set of patterns will be loaded in to an output file
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
-        for x, y in self.finalPatterns.items():
-            patternsAndSupport = x.strip() + ":" + str(y)
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % patternsAndSupport)
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.memoryUSS
+        return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
-        """
-        return self.memoryRSS
+       """
+        return self._memoryRSS
+
+    def getRuntime(self) -> float:
+        """Calculating the total amount of runtime taken by the mining process
 
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
-        """
-        return self.endTime-self.startTime
+       """
+        return self._endTime-self._startTime
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
+        :return: None
         """
-        print("Top K Spatial  High Utility Patterns:", len(self.getPatterns()))
+        print("Total number of Relative Utility Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in seconds:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
-def main():
-    inputFile = 'mushroom_utility_spmf.txt'
-    neighborFile = 'mushroom_neighbourhood.txt' #Users can also specify this constraint between 0 to 1.
-    k = 1000
-    seperator = ' ' 
-    obj = TKSHUIM(iFile=inputFile, nFile=neighborFile, k=k,  sep=seperator)    #initialize
-    obj.startMine()
-    obj.mine()
-    obj.printResults()
-    print(obj.getPatterns())
 
 if __name__ == '__main__':
-    main()
-    # _ap = str()
-    # if len(sys.argv) == 5 or len(sys.argv) == 6:
-    #     if len(sys.argv) == 6:
-    #         _ap = TKSHUIM(sys.argv[1], sys.argv[3], int(sys.argv[4]), sys.argv[5])
-    #     if len(sys.argv) == 5:
-    #         _ap = TKSHUIM(sys.argv[1], sys.argv[3], int(sys.argv[4]))
-    #     _ap.startMine()
-    #     _ap.mine()
-    #     print("Top K Spatial  High Utility Patterns:", len(_ap.getPatterns()))
-    #     _ap.save(sys.argv[2])
-    #     print("Total Memory in USS:", _ap.getMemoryUSS())
-    #     print("Total Memory in RSS",  _ap.getMemoryRSS())
-    #     print("Total ExecutionTime in seconds:", _ap.getRuntime())
-    # else:
-    #     for i in [1000, 5000]:
-    #         _ap = TKSHUIM('/Users/Likhitha/Downloads/mushroom_main_2000.txt',
-    #                 '/Users/Likhitha/Downloads/mushroom_neighbors_2000.txt', i, ' ')
-    #         _ap.startMine()
-    #         _ap.mine()
-    #         print("Total number of Spatial High Utility Patterns:", len(_ap.getPatterns()))
-    #         print("Total Memory in USS:", _ap.getMemoryUSS())
-    #         print("Total Memory in RSS", _ap.getMemoryRSS())
-    #         print("Total ExecutionTime in seconds:", _ap.getRuntime())
-    #     print("Error! The number of input parameters do not match the total number of parameters provided")
-
+    _ap = str()
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:    #includes separator
+            _ap = RHUIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]), _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:    #takes "\t" as a separator
+            _ap = RHUIM(_ab._sys.argv[1], int(_ab._sys.argv[3]), float(_ab._sys.argv[4]))
+        _ap.startMine()
+        print("Total number of Relative High Utility Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+    else:
+        _ap = RHUIM('/Users/likhitha/Downloads/utility_datasets/Utility_T10I4D100K.csv', 150000, 0.6, '\t')
+        _ap.startMine()
+        print("Total number of Relative High Utility Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/highUtilitySpatialPattern/topk/abstract.py` & `pami-2024.5.1/PAMI/highUtilitySpatialPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py` & `pami-2024.5.1/PAMI/localPeriodicPattern/basic/LPPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -4,45 +4,47 @@
 # lengths where a pattern is continuously periodic, while the minDur (minimal duration) measure ensures that those
 # time-intervals have a minimum duration.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#     from PAMI.localPeriodicPattern.basic import LPPGrowth as alg
+#             from PAMI.localPeriodicPattern.basic import LPPGrowth as alg
 #
-#     obj = alg.LPPGrowth(iFile, maxPer, maxSoPer, minDur)
+#             obj = alg.LPPGrowth(iFile, maxPer, maxSoPer, minDur)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     localPeriodicPatterns = obj.getPatterns()
+#             localPeriodicPatterns = obj.getPatterns()
 #
-#     print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
+#             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print(f'Total memory in USS: {memUSS}')
+#             print(f'Total memory in USS: {memUSS}')
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print(f'Total memory in RSS: {memRSS}')
+#             print(f'Total memory in RSS: {memRSS}')
 #
-#     runtime = obj.getRuntime()
-#
-#     print(f'Total execution time in seconds: {runtime})
+#             runtime = obj.getRuntime()
 #
+#             print(f'Total execution time in seconds: {runtime})
 #
 
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -56,14 +58,15 @@
 
 """
 
 
 
 from PAMI.localPeriodicPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 class Node:
     """
     A class used to represent the node of localPeriodicPatternTree
 
     :Attributes:
 
@@ -90,16 +93,22 @@
         self.child = []
         self.nodeLink = None
         self.tidList = set()
 
     def getChild(self, item: int) -> 'Node':
         """
         This function is used to get child node from the parent node
-        :param item:
+
+        :param item: item of the parent node
+
+        :type item: int
+
         :return: if node have node of item, then return it. if node don't have return []
+
+        :rtype: Node
         """
         for child in self.child:
             if child.item == item:
                 return child
         return []
 
 
@@ -137,14 +146,15 @@
         """
         add transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
         :param tid: represents the timestamp of transaction
         :type tid: list or int
+        :return: None
         """
         current = self.root
         for item in transaction:
             child = current.getChild(item)
             if not child:
                 newNode = Node()
                 newNode.item = item
@@ -160,28 +170,30 @@
         """
         fix node link
 
         :param item: it represents item name of newNode
         :type item: string
         :param newNode: it represents node which is added
         :type newNode: Node
+        :return: None
         """
         if item in self.nodeLinks:
             lastNode = self.nodeLinks[item]
             lastNode.nodeLink = newNode
         self.nodeLinks[item] = newNode
         if item not in self.firstNodeLink:
             self.firstNodeLink[item] = newNode
 
     def deleteNode(self, item: int) -> None:
         """
         delete the node from tree
 
         :param item: it represents the item name of node
         :type item: str
+        :return: None
         """
         deleteNode = self.firstNodeLink[item]
         parentNode = deleteNode.parent
         parentNode.child.remove(deleteNode)
         parentNode.child += deleteNode.child
         parentNode.tidList |= deleteNode.tidList
         for child in deleteNode.child:
@@ -199,14 +211,15 @@
         """
         create prefix tree by path
 
         :param path: it represents path to root from prefix node
         :type path: list
         :param tidList: it represents tid of each item
         :type tidList: list
+        :return: None
         """
         currentNode = self.root
         for item in path:
             child = currentNode.getChild(item)
             if not child:
                 newNode = Node()
                 newNode.item = item
@@ -231,17 +244,17 @@
 
     :Reference:
 
         Fournier-Viger, P., Yang, P., Kiran, R. U., Ventura, S., Luna, J. M.. (2020). Mining Local Periodic Patterns in
         a Discrete Sequence. Information Sciences, Elsevier, to appear. [ppt] DOI: 10.1016/j.ins.2020.09.044
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of local periodic pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of local periodic patterns
     :param  minDur: str:
                    Minimal duration in seconds between consecutive periods of time-intervals where a pattern is continuously periodic.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  maxSoPer: float:
                    Controls the maximum number of time periods between consecutive periods of time-intervals where a pattern is continuously periodic.
 
@@ -295,15 +308,15 @@
                 Create LPPTree of local periodic item from input data.
             patternGrowth(tree, prefix, prefixPFList)
                 Execute pattern growth algorithm. It is important function in this program.
             calculatePTL(tsList)
                 Calculate PTL from input tsList as integer list.
             calculatePTLbit(tsList)
                 Calculate PTL from input tsList as bit vector.
-            startMine()
+            mine()
                 Mining process will start from here.
             getMemoryUSS()
                 Total amount of USS memory consumed by the mining process will be retrieved from this function.
             getMemoryRSS()
                 Total amount of RSS memory consumed by the mining process will be retrieved from this function.
             getRuntime()
                 Total amount of runtime taken by the mining process will be retrieved from this function.
@@ -311,31 +324,38 @@
                 return local periodic patterns and its PTL
             save(oFile)
                 Complete set of local periodic patterns will be loaded in to an output file.
             getPatternsAsDataFrame()
                 Complete set of local periodic patterns will be loaded in to a dataframe.
 
     **Executing the code on terminal:**
-    -------------------------------------
-            Format:
-                    >>> python3 LPPMGrowth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
+    ---------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 LPPMGrowth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
+
+      Example Usage:
 
-            Examples:
-                    >>> python3 LPPMGrowth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
+      (.venv) $ python3 LPPMGrowth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
+
+    .. note: minDur will be considered as time interval between two consecutive periods
 
 
     **Sample run of importing the code:**
     ----------------------------------------
     .. code-block:: python
 
             from PAMI.localPeriodicPattern.basic import LPPGrowth as alg
 
             obj = alg.LPPGrowth(iFile, maxPer, maxSoPer, minDur)
 
-            obj.startMine()
+            obj.mine()
 
             localPeriodicPatterns = obj.getPatterns()
 
             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 
             obj.save(oFile)
 
@@ -581,14 +601,15 @@
 
         :param tree: The root node of prefix tree.
         :type tree: Node or Tree
         :param prefix: Prefix item list.
         :type prefix: list
         :param prefixPFList: tsList of prefix patterns.
         :type prefixPFList: dict or list
+        :return: None
         """
         items = list(prefixPFList)
         if not prefix:
             items = reversed(items)
         for item in items:
             prefixCopy = prefix.copy()
             prefixCopy.append(item)
@@ -637,14 +658,15 @@
     def __calculatePTL(self, tsList: List[int]) -> set:
         """
         Calculate PTL from input tsList as integer list
 
         :param tsList: It is tsList which store time stamp as integer.
         :type tsList: list
         :return: PTL
+        :rtype: set
         """
         start = -1
         PTL = set()
         tsList = sorted(tsList)
         tsPre = tsList[0]
         soPer = ' '
         for ts in tsList[1:]:
@@ -670,14 +692,15 @@
     def __calculatePTLbit(self, tsList: List[int]) -> set:
         """
         Calculate PTL from input tsList as bit vector.
 
         :param tsList: It is tsList which store time stamp as bit vector.
         :type tsList: list
         :return: PTL
+        :rtype: set
         """
         tsList = list(bin(tsList))
         tsList = tsList[2:]
         start = -1
         currentTs = 1
         PTL = set()
         tsPre = ' '
@@ -716,32 +739,41 @@
         return PTL
 
     def __convert(self, value: Any) -> float:
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process start from here.
         """
+        self.mine()
+
+    def mine(self) -> None:
+        """
+        Mining process start from here.
+        """
         self._localPeriodicPatterns__startTime = _ab._time.time()
         self._localPeriodicPatterns__finalPatterns = {}
         self.__creatingItemSets()
         self._localPeriodicPatterns__maxPer = self.__convert(self._localPeriodicPatterns__maxPer)
         self._localPeriodicPatterns__maxSoPer = self.__convert(self._localPeriodicPatterns__maxSoPer)
         self._localPeriodicPatterns__minDur = self.__convert(self._localPeriodicPatterns__minDur)
         self.__createTSList()
@@ -752,42 +784,46 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._localPeriodicPatterns__memoryUSS = float()
         self._localPeriodicPatterns__memoryRSS = float()
         self._localPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._localPeriodicPatterns__memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__endTime - self._localPeriodicPatterns__startTime
 
     def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
-        """Storing final local periodic patterns in a dataframe
+        """
+        Storing final local periodic patterns in a dataframe
 
         :return: returning local periodic patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
@@ -801,29 +837,31 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of local periodic patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._localPeriodicPatterns__oFile = outFile
         writer = open(self._localPeriodicPatterns__oFile, 'w+')
         for x, y in self._localPeriodicPatterns__finalPatterns.items():
             pat = str()
             for i in x:
                 pat = pat + i + '\t'
             pat = pat + ":"
             for i in y:
                 pat = pat + str(i) + '\t'
             patternsAndPTL = pat.strip()
             writer.write("%s \n" % patternsAndPTL)
 
     def getPatterns(self) -> Dict:
-        """ Function to send the set of local periodic patterns after completion of the mining process
+        """
+        Function to send the set of local periodic patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._localPeriodicPatterns__finalPatterns
 
     def printResults(self) -> None:
@@ -840,14 +878,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = LPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = LPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Local Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py` & `pami-2024.5.1/PAMI/localPeriodicPattern/basic/LPPMBreadth.py`

 * *Files 6% similar despite different names*

```diff
@@ -5,43 +5,46 @@
 # time-intervals have a minimum duration.
 #
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.localPeriodicPattern.basic import LPPMBreadth as alg
+#             from PAMI.localPeriodicPattern.basic import LPPMBreadth as alg
 #
-#     obj = alg.LPPMBreadth(iFile, maxPer, maxSoPer, minDur)
+#             obj = alg.LPPMBreadth(iFile, maxPer, maxSoPer, minDur)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     localPeriodicPatterns = obj.getPatterns()
+#             localPeriodicPatterns = obj.getPatterns()
 #
-#     print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
+#             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print(f'Total memory in USS: {memUSS}')
+#             print(f'Total memory in USS: {memUSS}')
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print(f'Total memory in RSS: {memRSS}')
+#             print(f'Total memory in RSS: {memRSS}')
 #
-#     runtime = obj.getRuntime()
+#             runtime = obj.getRuntime()
 #
-#     print(f'Total execution time in seconds: {runtime})
+#             print(f'Total execution time in seconds: {runtime})
+#
+
+
 
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -54,14 +57,16 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.localPeriodicPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
+from deprecated import deprecated
+
 
 class LPPMBreadth(_ab._localPeriodicPatterns):
 
     """
     :Description:
 
         Local Periodic Patterns, which are patterns (sets of events) that have a periodic behavior in some non predefined
@@ -72,17 +77,17 @@
 
     :Reference:
 
         Fournier-Viger, P., Yang, P., Kiran, R. U., Ventura, S., Luna, J. M.. (2020). Mining Local Periodic Patterns in
         a Discrete Sequence. Information Sciences, Elsevier, to appear. [ppt] DOI: 10.1016/j.ins.2020.09.044
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of local periodic pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of local periodic patterns
     :param  minDur: str:
                    Minimal duration in seconds between consecutive periods of time-intervals where a pattern is continuously periodic.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  maxSoPer: float:
                    Controls the maximum number of time periods between consecutive periods of time-intervals where a pattern is continuously periodic.
 
@@ -123,15 +128,15 @@
             Create the tsList as bit vector from input data.
         generateLPP()
             Generate 1 length local periodic pattens by tsList and execute depth first search.
         calculatePTL(tsList)
             Calculate PTL from input tsList as bit vector
         LPPMBreathSearch(extensionOfP)
             Mining local periodic patterns using breadth first search.
-        startMine()
+        mine()
             Mining process will start from here.
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function.
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function.
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function.
@@ -139,29 +144,37 @@
             return local periodic patterns and its PTL
         save(oFile)
             Complete set of local periodic patterns will be loaded in to an output file.
         getPatternsAsDataFrame()
             Complete set of local periodic patterns will be loaded in to a dataframe.
 
     **Executing the code on terminal:**
-    ------------------------------------
-            Format:
-                >>> python3 LPPBreadth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
-            Examples:
-                >>> python3 LPPMBreadth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
+    --------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 LPPBreadth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
+
+      Example Usage:
+
+      (.venv) $ python3 LPPMBreadth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
+
+    .. note: minDur will be considered as time interval between two consecutive periods
 
     **Sample run of importing the code:**
     -------------------------------------
     .. code-block:: python
     
             from PAMI.localPeriodicPattern.basic import LPPMBreadth as alg
 
             obj = alg.LPPMBreadth(iFile, maxPer, maxSoPer, minDur)
 
-            obj.startMine()
+            obj.mine()
 
             localPeriodicPatterns = obj.getPatterns()
 
             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 
             obj.save(oFile)
 
@@ -345,14 +358,15 @@
     def __calculatePTL(self, tsList: int) -> Set[Tuple[int, int]]:
         """
         calculate PTL from tsList as bit vector.
 
         :param tsList: it is one item's tsList which is used bit vector.
         :type tsList: int
         :return: it is PTL of input item.
+        :rtype: set
         """
         tsList = list(bin(tsList))
         tsList = tsList[2:]
         start = -1
         currentTs = 1
         PTL = set()
         tsPre = ' '
@@ -437,32 +451,41 @@
         return w1map
 
     def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
+        :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process start from here.
         """
+        self.mine()
+
+    def mine(self) -> None:
+        """
+        Mining process start from here.
+        """
         self._localPeriodicPatterns__startTime = _ab._time.time()
         self.__creatingItemSets()
         self._localPeriodicPatterns__maxPer = self.__convert(self._localPeriodicPatterns__maxPer)
         self._localPeriodicPatterns__maxSoPer = self.__convert(self._localPeriodicPatterns__maxSoPer)
         self._localPeriodicPatterns__minDur = self.__convert(self._localPeriodicPatterns__minDur)
         self._localPeriodicPatterns__finalPatterns = {}
         self.__createTSList()
@@ -471,42 +494,46 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._localPeriodicPatterns__memoryUSS = float()
         self._localPeriodicPatterns__memoryRSS = float()
         self._localPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._localPeriodicPatterns__memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__endTime - self._localPeriodicPatterns__startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
-        """Storing final local periodic patterns in a dataframe
+        """
+        Storing final local periodic patterns in a dataframe
 
         :return: returning local periodic patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
@@ -515,33 +542,36 @@
             for i in a:
                 pat = pat + i + ' '
             data.append([pat, b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'PTL'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
-        """Complete set of local periodic patterns will be loaded in to an output file
+        """
+        Complete set of local periodic patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._localPeriodicPatterns__oFile = outFile
         writer = open(self._localPeriodicPatterns__oFile, 'w+')
         for x, y in self._localPeriodicPatterns__finalPatterns.items():
             pat = str()
             for i in x:
                 pat = pat + i + '\t'
             pat = pat + ":"
             for i in y:
                 pat = pat + str(i) + '\t'
             patternsAndPTL = pat.strip()
             writer.write("%s \n" % patternsAndPTL)
 
     def getPatterns(self) -> Dict[Union[Tuple[str, ...], str], Set[Tuple[int, int]]]:
-        """ Function to send the set of local periodic patterns after completion of the mining process
+        """
+        Function to send the set of local periodic patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._localPeriodicPatterns__finalPatterns
 
     def printResults(self) -> None:
@@ -558,14 +588,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = LPPMBreadth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = LPPMBreadth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Local Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py` & `pami-2024.5.1/PAMI/localPeriodicPattern/basic/LPPMDepth.py`

 * *Files 4% similar despite different names*

```diff
@@ -4,42 +4,46 @@
 # lengths where a pattern is continuously periodic, while the minDur (minimal duration) measure ensures that those
 # time-intervals have a minimum duration.
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#     from PAMI.localPeriodicPattern.basic import LPPMDepth as alg
+#             from PAMI.localPeriodicPattern.basic import LPPMDepth as alg
 #
-#     obj = alg.LPPMDepth(iFile, maxPer, maxSoPer, minDur)
+#             obj = alg.LPPMDepth(iFile, maxPer, maxSoPer, minDur)
 #
-#     obj.startMine()
+#             obj.mine()
 #
-#     localPeriodicPatterns = obj.getPatterns()
+#             localPeriodicPatterns = obj.getPatterns()
 #
-#     print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
+#             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 #
-#     obj.save(oFile)
+#             obj.save(oFile)
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print(f'Total memory in USS: {memUSS}')
+#             print(f'Total memory in USS: {memUSS}')
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print(f'Total memory in RSS: {memRSS}')
+#             print(f'Total memory in RSS: {memRSS}')
 #
-#     runtime = obj.getRuntime()
+#             runtime = obj.getRuntime()
 #
-#     print(f'Total execution time in seconds: {runtime})
+#             print(f'Total execution time in seconds: {runtime})
+#
+
+
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
 
      This program is distributed in the hope that it will be useful,
@@ -52,14 +56,16 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.localPeriodicPattern.basic import abstract as _ab
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import pandas as pd
+from deprecated import deprecated
+
 
 class LPPMDepth(_ab._localPeriodicPatterns):
 
     """
     :Description:
 
         Local Periodic Patterns, which are patterns (sets of events) that have a periodic behavior in some non predefined
@@ -70,17 +76,17 @@
 
     :Reference:
 
         Fournier-Viger, P., Yang, P., Kiran, R. U., Ventura, S., Luna, J. M.. (2020). Mining Local Periodic Patterns in
         a Discrete Sequence. Information Sciences, Elsevier, to appear. [ppt] DOI: 10.1016/j.ins.2020.09.044
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
+                   Name of the Input file to mine complete set of local periodic pattern's
     :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
+                   Name of the output file to store complete set of local periodic patterns
     :param  minDur: str:
                    Minimal duration in seconds between consecutive periods of time-intervals where a pattern is continuously periodic.
     :param  maxPer: float:
                    Controls the maximum number of transactions in which any two items within a pattern can reappear.
     :param  maxSoPer: float:
                    Controls the maximum number of time periods between consecutive periods of time-intervals where a pattern is continuously periodic.
 
@@ -120,15 +126,15 @@
             Create the TSlist as bit vector from input data.
         generateLPP()
             Generate 1 length local periodic pattens by TSlist and execute depth first search.
         calculatePTL(tsList)
             Calculate PTL from input tsList as bit vector
         LPPMDepthSearch(extensionOfP)
             Mining local periodic patterns using depth first search.
-        startMine()
+        mine()
             Mining process will start from here.
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function.
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function.
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function.
@@ -136,32 +142,38 @@
             return local periodic patterns and its PTL
         save(oFile)
             Complete set of local periodic patterns will be loaded in to an output file.
         getPatternsAsDataFrame()
             Complete set of local periodic patterns will be loaded in to a dataframe.
 
     **Executing the code on terminal:**
-    -------------------------------------
-            Format:
+    --------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 LPPMDepth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur>
+
+      Example Usage:
 
-                >>> python3 LPPMDepth.py <inputFile> <outputFile> <maxPer> <minSoPer> <minDur> <sep>
-            Examples:
+      (.venv) $ python3 LPPMDepth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
 
-                >>> python3 LPPMDepth.py sampleDB.txt patterns.txt 0.3 0.4 0.5
+    .. note: minDur will be considered as time interval between two consecutive periods
 
 
     **Sample run of importing the code:**
     ----------------------------------------
     .. code-block:: python
 
             from PAMI.localPeriodicPattern.basic import LPPMDepth as alg
 
             obj = alg.LPPMDepth(iFile, maxPer, maxSoPer, minDur)
 
-            obj.startMine()
+            obj.mine()
 
             localPeriodicPatterns = obj.getPatterns()
 
             print(f'Total number of local periodic patterns: {len(localPeriodicPatterns)}')
 
             obj.save(oFile)
 
@@ -329,19 +341,20 @@
                 self._localPeriodicPatterns__finalPatterns[item] = PTL[item]
         I = sorted(list(I))
         # I = set(I)
         self.__LPPMDepthSearch(I)
 
     def __calculatePTL(self, tsList: int) -> Set[Tuple[int, int]]:
         """
-         calculate PTL from tsList as bit vector.
+        calculate PTL from tsList as bit vector.
 
         :param tsList: it is one item's tsList which is used bit vector.
         :type tsList: int
         :return: it is PTL of input item.
+        :rtype: set
         """
         tsList = list(bin(tsList))
         tsList = tsList[2:]
         start = -1
         currentTs = 1
         PTL = set()
         tsPre = ' '
@@ -381,14 +394,15 @@
 
     def __LPPMDepthSearch(self, extensionsOfP: List[Union[Tuple[str, ...], str]]) -> None:
         """
         Mining n-length local periodic pattens from n-1-length patterns by depth first search.
 
         :param extensionsOfP: it is n-1 length patterns list.
         :type extensionsOfP: list
+        :return: None
         """
         for x in range(len(extensionsOfP)-1):
             extensionsOfPx = set()
             for y in range(x+1,len(extensionsOfP)):
                 tspxy = self.__tsList[extensionsOfP[x]] & self.__tsList[extensionsOfP[y]]
                 PTL = self.__calculatePTL(tspxy)
                 if len(PTL) > 0:
@@ -408,33 +422,41 @@
                 self.__LPPMDepthSearch(list(extensionsOfPx))
 
     def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
+        :type value: int or float or str
         :return: converted type
+        :rtype: int or float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
                 value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
-
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Mining process start from here. This function calls createTSlist and generateLPP.
         """
+        self.mine()
+
+    def mine(self) -> None:
+        """
+        Mining process start from here. This function calls createTSlist and generateLPP.
+        """
         self._localPeriodicPatterns__startTime = _ab._time.time()
         self._localPeriodicPatterns__finalPatterns = {}
         self.__creatingItemSets()
         self._localPeriodicPatterns__maxPer = self.__convert(self._localPeriodicPatterns__maxPer)
         self._localPeriodicPatterns__maxSoPer = self.__convert(self._localPeriodicPatterns__maxSoPer)
         self._localPeriodicPatterns__minDur = self.__convert(self._localPeriodicPatterns__minDur)
         self.__createTSlist()
@@ -443,33 +465,36 @@
         process = _ab._psutil.Process(_ab._os.getpid())
         self._localPeriodicPatterns__memoryRSS = float()
         self._localPeriodicPatterns__memoryUSS = float()
         self._localPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._localPeriodicPatterns__memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._localPeriodicPatterns__endTime - self._localPeriodicPatterns__startTime
 
@@ -493,29 +518,31 @@
 
     def save(self, outFile: str) -> None:
         """
         Complete set of local periodic patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._localPeriodicPatterns__oFile = outFile
         writer = open(self._localPeriodicPatterns__oFile, 'w+')
         for x, y in self._localPeriodicPatterns__finalPatterns.items():
             pat = str()
             for i in x:
                 pat = pat + i + '\t'
             pat = pat + ":"
             for i in y:
                 pat = pat + str(i) + '\t'
             patternsAndPTL = pat.strip()
             writer.write("%s \n" % patternsAndPTL)
 
     def getPatterns(self) -> Dict[Union[Tuple[str, ...], str], Set[Tuple[int, int]]]:
-        """ Function to send the set of local periodic patterns after completion of the mining process
+        """
+        Function to send the set of local periodic patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._localPeriodicPatterns__finalPatterns
 
     def printResults(self) -> None:
@@ -532,14 +559,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = LPPMDepth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]), _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = LPPMDepth(_ab._sys.argv[1], _ab._sys.argv[3], float(_ab._sys.argv[4]))
         _ap.startMine()
+        _ap.mine()
         print("Total number of Local Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/localPeriodicPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/localPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py` & `pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -546,40 +546,15 @@
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         main program to start the operation
         :return: none
 
         """
-        global _MIS
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self.__creatingItemSets()
-        self._getMISValues()
-        #MIS = self._MISValues
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            _MIS[y] = self._MISValues[x]
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Frequent patterns were generated successfully using basic algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
     def Mine(self) -> None:
         """
         main program to start the operation
         :return: none
 
         """
```

### Comparing `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py` & `pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/CFPGrowthPlus.py`

 * *Files 2% similar despite different names*

```diff
@@ -500,39 +500,15 @@
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         main program to start the operation
 
         """
-        global MIS
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        self.__creatingItemSets()
-        self._getMISValues()
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            MIS[y] = self._MISValues[x]
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
     def Mine(self):
         """
         main program to start the operation
 
         """
         global MIS
```

### Comparing `pami-2024.4.9.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/multipleMinimumSupportBasedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py` & `pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/GPFgrowth.py`

 * *Files 0% similar despite different names*

```diff
@@ -46,15 +46,15 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-import sys
+import deprecated
 from PAMI.partialPeriodicFrequentPattern.basic.abstract import *
 
 orderOfItem = {}
 
 class Node:
     """
     A class used to represent the node of frequentPatternTree
@@ -659,16 +659,15 @@
                             temp = [i.rstrip() for i in line.split(self._partialPeriodicPatterns__sep)]
                             temp = [x for x in temp if x]
                             self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-
-    def startMine(self):
+    def mine(self):
         self.__inputFile = self._partialPeriodicPatterns__iFile
         self._partialPeriodicPatterns__startTime = time.time()
         self._partialPeriodicPatterns__finalPatterns = {}
         self.__readDatabase()
         self._partialPeriodicPatterns__minSup = self.__convert(self._partialPeriodicPatterns__minSup)
         self._partialPeriodicPatterns__maxPer = self.__convert(self._partialPeriodicPatterns__maxPer)
         # self.minPR = self.convert(self.minPR)
@@ -765,23 +764,23 @@
 if __name__ == '__main__':
     ap = str()
     if len(sys.argv) == 6 or len(sys.argv) == 7:
         if len(sys.argv) == 7:
             ap = GPFgrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5], sys.argv[6])
         if len(sys.argv) == 6:
             ap = GPFgrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
-        ap.startMine()
+        ap.mine()
         print("Total number of Frequent Patterns:", len(ap.getPatterns()))
         ap.save(sys.argv[2])
         print("Total Memory in USS:", ap.getMemoryUSS())
         print("Total Memory in RSS", ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", ap.getRuntime())
     else:
         for i in [1000, 2000, 3000, 4000, 5000]:
             _ap = GPFgrowth('/Users/Likhitha/Downloads/temporal_T10I4D100K.csv', i, 500, 0.7, '\t')
-            _ap.startMine()
+            _ap.mine()
             print("Total number of Maximal Partial Periodic Patterns:", len(_ap.getPatterns()))
             _ap.save('/Users/Likhitha/Downloads/output.txt')
             print("Total Memory in USS:", _ap.getMemoryUSS())
             print("Total Memory in RSS", _ap.getMemoryRSS())
             print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py` & `pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/PPF_DFS.py`

 * *Files 1% similar despite different names*

```diff
@@ -49,15 +49,15 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
 from PAMI.partialPeriodicFrequentPattern.basic.abstract import *
-
+import deprecated
 
 class PPF_DFS(partialPeriodicPatterns):
     """
     :Description:   PPF_DFS is algorithm to mine the partial periodic frequent patterns.
 
     :References:    (Has to be added)
 
@@ -416,15 +416,16 @@
                 if len(y) >= self._partialPeriodicPatterns__minSup and val / (self._partialPeriodicPatterns__minSup + 1) >= self._partialPeriodicPatterns__minPR:
                     classItemsets.append(itemj)
                     classtidsets.append(y)
             newprefix = list(set(itemsetx)) + prefix
             self.__Generation(newprefix, classItemsets, classtidsets)
             self.__save(prefix, list(set(itemsetx)), tidsetx)
 
-    def startMine(self):
+
+    def mine(self):
         """
         Main program start with extracting the periodic frequent items from the database and
         performs prefix equivalence to form the combinations and generates closed periodic frequent patterns.
         """
         self.__path = self._partialPeriodicPatterns__iFile
         self._partialPeriodicPatterns__startTime = time.time()
         self.__creatingItemSets()
@@ -449,16 +450,14 @@
         self._partialPeriodicPatterns__endTime = time.time()
         self.__runTime = self._partialPeriodicPatterns__endTime - self._partialPeriodicPatterns__startTime
         process = psutil.Process(os.getpid())
         self._partialPeriodicPatterns__memoryUSS = float()
         self._partialPeriodicPatterns__memoryRSS = float()
         self._partialPeriodicPatterns__memoryUSS = process.memory_full_info().uss
         self._partialPeriodicPatterns__memoryRSS = process.memory_info().rss
-        # print("eclat Time taken:",temp)
-        # print("eclat Memory Space:",resource.getrusage(resource.RUSAGE_SELF).ru_maxrss)
 
     def getMemoryUSS(self):
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
@@ -538,24 +537,24 @@
 if __name__ == '__main__':
     ap = str()
     if len(sys.argv) == 6 or len(sys.argv) == 7:
         if len(sys.argv) == 7:
             ap = PPF_DFS(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5], sys.argv[6])
         if len(sys.argv) == 6:
             ap = PPF_DFS(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
-        ap.startMine()
+        ap.mine()
         print("Total number of Frequent Patterns:", len(ap.getPatterns()))
         ap.save(sys.argv[2])
         print("Total Memory in USS:", ap.getMemoryUSS())
         print("Total Memory in RSS", ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", ap.getRuntime())
     else:
         for i in [1000, 2000, 3000, 4000, 5000]:
             _ap = PPF_DFS('/Users/Likhitha/Downloads/temporal_T10I4D100K.csv', i, 500, 0.7, '\t')
-            _ap.startMine()
+            _ap.mine()
             print("Total number of Maximal Partial Periodic Patterns:", len(_ap.getPatterns()))
             _ap.save('/Users/Likhitha/Downloads/output.txt')
             print("Total Memory in USS:", _ap.getMemoryUSS())
             print("Total Memory in RSS", _ap.getMemoryRSS())
             print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/__init__.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/basic/GThreePGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -45,15 +45,15 @@
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from pandas.core.arrays import period
-
+import deprecated
 from PAMI.partialPeriodicPattern.basic import Gabstract as _abstract
 from typing import List, Dict, Tuple, Set, Union, Any, Generator
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 
 _minPS = float()
@@ -567,15 +567,16 @@
             if '%' in value:
                 value = value[:-1]
                 value = float(int(value)/100)
             else:
                 value = int(value)
         return value
 
-    def startMine(self) -> None:
+
+    def mine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
 
         """
         global _minPS, _period, _relativePS, _lno
         self._startTime = float()
         self._startTime = _abstract._time.time()
@@ -680,30 +681,30 @@
 if __name__ == "__main__":
     _ap = str()
     if len(_sys.argv) == 6 or len(_sys.argv) == 7:
         if len(_sys.argv) == 7:
             _ap = GThreePGrowth(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5], _sys.argv[6])
         if len(_sys.argv) == 6:
             _ap = GThreePGrowth(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
-        _ap.startMine()
+        _ap.mine()
         _Patterns = _ap.getPatterns()
         print("Total number of Partial Periodic Patterns:", len(_Patterns))
         _ap.save(_sys.argv[2])
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _memRSS)
         _run = _ap.getRuntime()
         print("Total ExecutionTime in ms:", _run)
     else:
         minPS = 0.001
         l = [0.2, 0.4, 0.6, 0.7, 0.8]
         for i in l:
             ap = GThreePGrowth('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_T10I4D100K.csv', minPS, 10000, i)
-            ap.startMine()
+            ap.mine()
             Patterns = ap.getPatterns()
             print("Total number of  Patterns:", len(Patterns))
             ap.save('/Users/Likhitha/Downloads/output')
             memUSS = ap.getMemoryUSS()
             print("Total Memory in USS:", memUSS)
             memRSS = ap.getMemoryRSS()
             print("Total Memory in RSS", memRSS)
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/Gabstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/basic/Gabstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/basic/PPPGrowth.py`

 * *Files 2% similar despite different names*

```diff
@@ -54,16 +54,14 @@
 
 
 from PAMI.partialPeriodicPattern.basic import abstract as _abstract
 from typing import List, Dict, Tuple, Set, Union, Any, Iterable, Generator
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
-
-from PAMI.partialPeriodicPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
 _minPS = float()
 _period = float()
 _lno = int()
 
@@ -580,44 +578,20 @@
 
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
         :return: None
-
         """
-        global _minPS, _period, _lno
-        self._startTime = _abstract._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minPS is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        generatedItems, pfList = self._partialPeriodicOneItem()
-        _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
-        updatedTransactions = self._updateTransactions(generatedItems)
-        for x, y in self._rank.items():
-            self._rankdup[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        Tree = self._buildTree(updatedTransactions, info)
-        patterns = Tree._generatePatterns([])
-        self._finalPatterns = {}
-        for i in patterns:
-            s = self._savePeriodic(i[0])
-            self._finalPatterns[s] = i[1]
-        self._endTime = _abstract._time.time()
-        process = _abstract._psutil.Process(_abstract._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Partial Periodic Patterns were generated successfully using 3PGrowth algorithm ")
 
-    def Mine(self) -> None:
+        self.mine()
+
+
+    def mine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree.
         :return: None
 
         """
         global _minPS, _period, _lno
         self._startTime = _abstract._time.time()
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/basic/PPP_ECLAT.py`

 * *Files 3% similar despite different names*

```diff
@@ -383,41 +383,15 @@
     def startMine(self) -> None:
         """
         Main program start with extracting the periodic frequent items from the database and
         performs prefix equivalence to form the combinations and generates partial-periodic patterns.
         :return: None
 
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        plist = self._creatingOneitemSets()
-        self._finalPatterns = {}
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetX = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                val = self._getPeriodicSupport(y1)
-                if val >= self._minPS:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetX)
-        print("Partial Periodic Patterns were generated successfully using 3PEclat algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
     def Mine(self) -> None:
         """
         Main program start with extracting the periodic frequent items from the database and
         performs prefix equivalence to form the combinations and generates partial-periodic patterns.
         :return: None
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/__init__.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/PPPClose.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,15 +1,17 @@
-
-
+#  CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
+#  It uses depth-first search.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-#             from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 #
-#             obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
+#             from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
+#
+#             obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
 #
 #             obj.startMine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
@@ -25,16 +27,16 @@
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
-#
-#
+
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -47,74 +49,61 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-
-
-import sys as _sys
-import validators as _validators
-from urllib.request import urlopen as _urlopen
-from PAMI.partialPeriodicPattern.closed import abstract as _abstract
-
-
-from PAMI.partialPeriodicPattern.basic import abstract as _ab
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-class PPPClose(_abstract._partialPeriodicPatterns):
+from PAMI.periodicFrequentPattern.closed import abstract as _ab
+
+class CPFPMiner(_ab._periodicFrequentPatterns):
     """
-    :Description:
+    About this algorithm
+    ====================
 
-    PPPClose algorithm is used to discover the closed partial periodic patterns in temporal databases.
-    It uses depth-first search.
+    :Description:   CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
+                    It uses depth-first search.
 
-    :Reference: R. Uday Kiran1 , J. N. Venkatesh2 , Philippe Fournier-Viger3 , Masashi Toyoda1 , P. Krishna Reddy2 and Masaru Kitsuregawa
-                 https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/799/PAKDD.pdf
+    :Reference:   P. Likhitha et al., "Discovering Closed Periodic-Frequent Patterns in Very Large Temporal Databases"
+                  2020 IEEE International Conference on Big Data (Big Data), 2020, https://ieeexplore.ieee.org/document/9378215
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of periodic frequent pattern's
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of frequent patterns
-    :param  period: float:
-                   Minimum partial periodic...
-    :param  periodicSupport: float:
-                   Minimum partial periodic...
+    :param  minSup: float:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: float:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
         oFile : str
             Name of the output file or path of the input file
-        periodicSupport: int or float or str
-            The user can specify periodicSupport either in count or proportion of database size.
-            If the program detects the data type of periodicSupport is integer, then it treats periodicSupport is expressed in count.
+        minSup: int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
-            Example: periodicSupport=10 will be treated as integer, while periodicSupport=10.0 will be treated as float
-        period: int or float or str
-            The user can specify period either in count or proportion of database size.
-            If the program detects the data type of period is integer, then it treats period is expressed in count.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer: int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
-            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
@@ -126,94 +115,103 @@
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
 
-    **Executing the code on terminal:**
-    -------------------------------------
-    .. code-block:: console
-
-
-       Format:
-
-       (.venv) $ python3 PPPClose.py <inputFile> <outputFile> <periodicSupport> <period>
-
-       Examples:
-
-       (.venv) $ python3 PPPClose.py sampleTDB.txt patterns.txt 0.3 0.4
-
-
-    **Sample run of the imported code:**
-    --------------------------------------
-    .. code-block:: python
-
-            from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 
-            obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
+    Execution methods
+    =================
 
-            obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+    **Terminal command**
 
-            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 
-            obj.save("patterns")
-
-            Df = obj.getPatternsAsDataFrame()
+    .. code-block:: console
 
-            memUSS = obj.getMemoryUSS()
+       Format:
 
-            print("Total Memory in USS:", memUSS)
+       (.venv) $  python3 CPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer>
 
-            memRSS = obj.getMemoryRSS()
+       Example:
 
-            print("Total Memory in RSS", memRSS)
+       (.venv) $ python3 CPFPMiner.py sampleTDB.txt patterns.txt 0.3 0.4
 
-            run = obj.getRuntime()
+     .. note:: minSup will be considered in percentage of database transactions
+        
+        
+    **Calling from a python program**
 
-            print("Total ExecutionTime in seconds:", run)
+    .. code-block:: python
 
+                    from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
+        
+                    obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
+        
+                    obj.startMine()
+        
+                    periodicFrequentPatterns = obj.getPatterns()
+        
+                    print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+        
+                    obj.save("patterns")
+        
+                    Df = obj.getPatternsAsDataFrame()
+        
+                    memUSS = obj.getMemoryUSS()
+        
+                    print("Total Memory in USS:", memUSS)
+        
+                    memRSS = obj.getMemoryRSS()
+        
+                    print("Total Memory in RSS", memRSS)
+        
+                    run = obj.getRuntime()
+        
+                    print("Total ExecutionTime in seconds:", run)
+        
     **Credits:**
-    --------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-
+    ------------------
+    The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
     """
 
-    _periodicSupport = float()
-    _period = float()
+    _minSup = float()
+    _maxPer = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
-    _Database = []
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _transaction = []
     _hashing = {}
     _mapSupport = {}
     _itemSetCount = 0
     _maxItemId = 0
     _tableSize = 10000
     _tidList = {}
     _lno = 0
 
+    def __init__(self, iFile, minSup, maxPer, sep='\t'):
+        super().__init__(iFile, minSup, maxPer, sep)
+        self._finalPatterns = {}
+    
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
@@ -225,83 +223,77 @@
             if '.' in value:
                 value = float(value)
                 value = (self._lno * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingItemSets(self):
+    def _scanDatabase(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
+        To scan the database and extracts the 1-length periodic-frequent items
+
+        :return:   Returns the 1-length periodic-frequent items
         """
-        self._Database = []
-        if isinstance(self._iFile, _abstract._pd.DataFrame):
-            timeStamp, data = [], []
+        Database = []
+        if isinstance(self._iFile, _ab._pd.DataFrame):
+            ts, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
-                timeStamp = self._iFile['TS'].tolist()
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
-                tr = [timeStamp[i]]
+                tr = [ts[i][0]]
                 tr = tr + data[i]
-                self._Database.append(tr)
-            self._lno = len(self._Database)
+                Database.append(tr)
+
         if isinstance(self._iFile, str):
-            if _validators.url(self._iFile):
-                data = _urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    self._lno += 1
+                    line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            self._lno += 1
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-
-    def _OneLengthPartialItems(self):
-        """
-        To scan the database and extracts the 1-length periodic-frequent items
-
-        :return: Returns the 1-length periodic-frequent items
-        """
-        self._mapSupport = {}
         self._tidList = {}
-        self._period = self._convert(self._period)
-        for line in self._Database:
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
+        self._mapSupport = {}
+        for line in Database:
+            self._lno += 1
+            s = line
+            n = int(s[0])
+            for i in range(1, len(s)):
+                si = s[i]
                 if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, 0, n]
+                    self._mapSupport[si] = [1, abs(0 - n), n]
                     self._tidList[si] = [n]
                 else:
                     self._mapSupport[si][0] += 1
-                    period = abs(n - self._mapSupport[si][2])
-                    if period <= self._period:
-                        self._mapSupport[si][1] += 1
+                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
                     self._mapSupport[si][2] = n
                     self._tidList[si].append(n)
         for x, y in self._mapSupport.items():
-            period = abs(self._lno - self._mapSupport[x][2])
-            if period <= self._period:
-                self._mapSupport[x][1] += 1
-        self._periodicSupport = self._convert(self._periodicSupport)
-        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items() if v[1] >= self._periodicSupport}
+            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(self._lno - self._mapSupport[x][2]))
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if
+                           v[0] >= self._minSup and v[1] <= self._maxPer}
         periodicFrequentItems = {}
         self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
         for x, y in self._tidList.items():
             t1 = 0
             for i in y:
                 t1 += i
             periodicFrequentItems[x] = t1
@@ -323,91 +315,101 @@
         return hashcode % self._tableSize
 
     def _contains(self, itemSet, val, hashcode):
         """
         To check if the key(hashcode) is in dictionary(hashing) variable
 
         :param itemSet: generated periodic-frequent itemSet
-        :param val: support and period of itemSet
+        :param val: support and periodicity of itemSet
         :param hashcode: the key generated in calculate() method for every itemSet
 
         :return: true if itemSet with same support present in dictionary(hashing) or else returns false
         """
         if self._hashing.get(hashcode) is None:
             return False
         for i in self._hashing[hashcode]:
             itemSetX = i
-            if val == self._hashing[hashcode][itemSetX] and set(itemSetX).issuperset(itemSet):
+            if val[0] == self._hashing[hashcode][itemSetX][0] and set(itemSetX).issuperset(itemSet):
                 return True
         return False
 
-    def _getPeriodicSupport(self, timeStamps):
+    def _getPeriodAndSupport(self, timeStamps):
         """
-        Calculates the period and support of timeStamps
+        Calculates the periodicity and support of timeStamps
 
-        :param: timeStamps: timeStamps of itemSet
-        :return: period and support
+        :param timeStamps: timeStamps of itemSet
+        :return: periodicity and support
         """
         timeStamps.sort()
+        cur = 0
+        per = 0
         sup = 0
-        for j in range(len(timeStamps) - 1):
-            per = abs(timeStamps[j + 1] - timeStamps[j])
-            if per <= self._period:
-                sup += 1
-        return sup
+        for j in range(len(timeStamps)):
+            per = max(per, timeStamps[j] - cur)
+            if per > self._maxPer:
+                return [0, 0]
+            cur = timeStamps[j]
+            sup += 1
+        per = max(per, self._lno - cur)
+        return [sup, per]
 
     def _save(self, prefix, suffix, tidSetX):
         """
         Saves the generated pattern which satisfies the closed property
+        Parameters:
+        -----------
+            prefix: the prefix part of itemSet
+            suffix: the suffix part of itemSet
+            tidSetX: the timeStamps of the generated itemSet
+
+        Returns:
+        --------
+            saves the closed periodic-frequent pattern
 
-        :param prefix: the prefix part of itemSet
-        :param suffix: the suffix part of itemSet
-        :param tidSetX: the timeStamps of the generated itemSet
-        :return: saves the closed periodic-frequent pattern
         """
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
         prefix = list(set(prefix))
         prefix.sort()
-        val = self._getPeriodicSupport(tidSetX)
-        if val >= self._periodicSupport:
+        val = self._getPeriodAndSupport(tidSetX)
+        if val[0] >= self._minSup and val[1] <= self._maxPer:
             hashcode = self._calculate(tidSetX)
             if self._contains(prefix, val, hashcode) is False:
                 self._itemSetCount += 1
                 sample = str()
                 for i in prefix:
-                    sample = sample + i + "\t"
+                    sample = sample + i + " "
                 self._finalPatterns[sample] = val
             if hashcode not in self._hashing:
                 self._hashing[hashcode] = {tuple(prefix): val}
             else:
                 self._hashing[hashcode][tuple(prefix)] = val
 
     def _processEquivalenceClass(self, prefix, itemSets, tidSets):
         """
-
+        identifies and saves closed periodic patterns of length more than 2 in a dataset, by processing equivalence classes of item sets that satisfy a minimum support condition.
         :param prefix: Prefix class of an itemSet
-        :param itemSets: suffix items in periodicFrequentItems that satisfies the periodicSupport condition
+        :param itemSets: suffix items in periodicFrequentItems that satisfies the minSup condition
         :param tidSets: timeStamps of items in itemSets respectively
-        :return: closed periodic patterns with length more than 2
+        :return:  closed periodic patterns with length more than 2
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidList = tidSets[0]
             self._save(prefix, [i], tidList)
             return
         if len(itemSets) == 2:
             itemI = itemSets[0]
             tidSetI = tidSets[0]
             itemJ = itemSets[1]
             tidSetJ = tidSets[1]
             y1 = list(set(tidSetI).intersection(tidSetJ))
-            if len(y1) >= self._periodicSupport:
+            if len(y1) >= self._minSup:
                 suffix = []
                 suffix += [itemI, itemJ]
                 suffix = list(set(suffix))
                 self._save(prefix, suffix, y1)
             if len(y1) != len(tidSetI):
                 self._save(prefix, [itemI], tidSetI)
             if len(y1) != len(tidSetJ):
@@ -423,15 +425,15 @@
             itemSetX = [itemX]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 if itemJ is None:
                     continue
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) < self._periodicSupport:
+                if len(y) < self._minSup:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
                     itemSets.insert(j, None)
                     tidSets.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
                     itemSetX.append(itemJ)
@@ -449,108 +451,109 @@
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from here
         """
-        self._startTime = _abstract._time.time()
-        self._creatingItemSets()
-        self._hashing = {}
+        self._startTime = _ab._time.time()
         self._finalPatterns = {}
-        periodicFrequentItems = self._OneLengthPartialItems()
+        self._hashing = {}
+        periodicFrequentItems = self._scanDatabase()
         for i in range(len(periodicFrequentItems)):
             itemX = periodicFrequentItems[i]
             if itemX is None:
                 continue
             tidSetX = self._tidList[itemX]
             itemSetX = [itemX]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(periodicFrequentItems)):
                 itemJ = periodicFrequentItems[j]
                 if itemJ is None:
                     continue
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._periodicSupport:
+                if len(y1) < self._minSup:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
                     periodicFrequentItems.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
                     itemSetX.append(itemJ)
                 elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
                     periodicFrequentItems.insert(j, None)
                     itemSets.append(itemJ)
                     tidSets.append(y1)
                 else:
+
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             if len(itemSets) > 0:
                 self._processEquivalenceClass(itemSetX, itemSets, tidSets)
             self._save([], itemSetX, tidSetX)
-        self._endTime = _abstract._time.time()
-        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
+        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
 
     def Mine(self):
         """
         Mining process will start from here
         """
-        self._startTime = _abstract._time.time()
-        self._creatingItemSets()
-        self._hashing = {}
+        self._startTime = _ab._time.time()
         self._finalPatterns = {}
-        periodicFrequentItems = self._OneLengthPartialItems()
+        self._hashing = {}
+        periodicFrequentItems = self._scanDatabase()
         for i in range(len(periodicFrequentItems)):
             itemX = periodicFrequentItems[i]
             if itemX is None:
                 continue
             tidSetX = self._tidList[itemX]
             itemSetX = [itemX]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(periodicFrequentItems)):
                 itemJ = periodicFrequentItems[j]
                 if itemJ is None:
                     continue
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._periodicSupport:
+                if len(y1) < self._minSup:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
                     periodicFrequentItems.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
                     itemSetX.append(itemJ)
                 elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
                     periodicFrequentItems.insert(j, None)
                     itemSets.append(itemJ)
                     tidSets.append(y1)
                 else:
+
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             if len(itemSets) > 0:
                 self._processEquivalenceClass(itemSetX, itemSets, tidSets)
             self._save([], itemSetX, tidSetX)
-        self._endTime = _abstract._time.time()
-        process = _abstract._psutil.Process(_abstract._os.getpid())
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
+        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
 
     def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
@@ -578,54 +581,57 @@
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
+            data.append([a, b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
     def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+        """
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        print("Total number of  Closed Partial Periodic Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Closed Periodic Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-
+        print("Total ExecutionTime in ms:", self.getRuntime())
+        
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
-        if len(_sys.argv) == 6:
-            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
-        if len(_sys.argv) == 5:
-            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        print("Total number of  Patterns:", len(_ap.getPatterns()))
-        _ap.save(_sys.argv[2])
+        print("Total number of Closed Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/closed/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/Max3PGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -52,14 +52,15 @@
 
 """
 
 import sys as _sys
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 from PAMI.partialPeriodicPattern.maximal import abstract as _abstract
+import deprecated
 
 global maximalTree
 _periodicSupport = float()
 _period = float()
 _lno = int()
 
 
@@ -679,15 +680,15 @@
         :return: pattern with original item names
         """
         t1 = []
         for i in itemSet:
             t1.append(self._pfList[i])
         return t1
 
-    def startMine(self):
+    def mine(self):
         """
         Mining process will start from this function
         """
 
         global _periodicSupport, _period, _lno
         self._startTime = _abstract._time.time()
         if self._iFile is None:
@@ -803,23 +804,23 @@
 if __name__ == "__main__":
     _ap = str()
     if len(_sys.argv) == 5 or len(_sys.argv) == 6:
         if len(_sys.argv) == 6:
             _ap = Max3PGrowth(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
         if len(_sys.argv) == 5:
             _ap = Max3PGrowth(_sys.argv[1], _sys.argv[3], _sys.argv[4])
-        _ap.startMine()
+        _ap.mine()
         print("Total number of Maximal Partial Periodic Patterns:", len(_ap.getPatterns()))
         _ap.save(_sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         for i in [100, 200, 300, 400, 500]:
             _ap = Max3PGrowth('/Users/Likhitha/Downloads/temporal_T10I4D100K.csv', i, 5000, '\t')
-            _ap.startMine()
+            _ap.mine()
             print("Total number of Maximal Partial Periodic Patterns:", len(_ap.getPatterns()))
             _ap.save('/Users/Likhitha/Downloads/output.txt')
             print("Total Memory in USS:", _ap.getMemoryUSS())
             print("Total Memory in RSS", _ap.getMemoryRSS())
             print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/__init__.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/maximal/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/__init__.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/pyspark/parallel3PGrowth.py`

 * *Files 8% similar despite different names*

```diff
@@ -51,16 +51,14 @@
 """
 
 from PAMI.partialPeriodicPattern.pyspark import abstract as _ab
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
 from pyspark import SparkContext, SparkConf
-
-from PAMI.partialPeriodicPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
 _periodicSupport = float()
 _period = float()
 _lno = int()
 
@@ -480,73 +478,17 @@
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree.
         """
         
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minPS is None:
-            raise Exception("Please enter the Minimum Period-Support")
-            
-        self._period = self._convert(self._period)
-        self._minPS = self._convert(self._minPS)
-        minPS = self._minPS
-        period = self._period
-
-        
-        APP_NAME = "4PGrowth"
-        conf = SparkConf().setAppName(APP_NAME)
-        sc  = SparkContext(conf=conf).getOrCreate()
-
-        self._startTime = _ab._time.time()
-        
-        data = sc.textFile(self._iFile,self.numPartitions).map(lambda x: [y for y in x.strip().split(self._sep)])
-        # self.numPartitions = data.getNumPartitions()
-        # numPartitions = 50
-        freqItems,RecItems = self.getFrequentItems(data)
-        # print(RecItems)
-
-        trans = self.getFrequentItemsets(data,freqItems,self._period,self._minPS, dict(RecItems))
-        a = trans.collect()
-        
-        # print(type(a))
-        for k,v in a:
-            string = "\t".join(k)
-            # print(string,":",v)
-            self._finalPatterns[string] = v
-
-        # print(self._finalPatterns)
-        #     print(k,":",v)
-        # trans.saveAsTextFile('temp')
-        self._endTime = _ab._time.time()
-        sc.stop()
-        
-        # self._creatingItemSets()
-        # generatedItems, pfList = self._partialPeriodicOneItem()
-        # _minPS, _period, _lno = self._minPS, self._period, len(self._Database)
-        # updatedTransactions = self._updateTransactions(generatedItems)
-        # for x, y in self._rank.items():
-        #     self._rankdup[y] = x
-        # info = {self._rank[k]: v for k, v in generatedItems.items()}
-        # Tree = self._buildTree(updatedTransactions, info)
-        # patterns = Tree._generatePatterns([])
-        # self._finalPatterns = {}
-        # for i in patterns:
-        #     s = self._savePeriodic(i[0])
-        #     self._finalPatterns[s] = i[1]
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Partial Periodic Patterns were generated successfully using 4PGrowth algorithm ")
+        self.mine()
 
-    def Mine(self):
+    def mine(self):
         """
         Main method where the patterns are mined by constructing tree.
         """
 
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minPS is None:
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/topk/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/topk/k3PMiner.py`

 * *Files 10% similar despite different names*

```diff
@@ -52,16 +52,14 @@
 
 """
 
 from PAMI.partialPeriodicPattern.topk import abstract as _abstract
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 import sys as _sys
-
-from PAMI.partialPeriodicPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
 class k3PMiner(_abstract.partialPeriodicPatterns):
     """
     :Description:   k3PMiner is and algorithm to discover top - k partial periodic patterns in a temporal  database.
 
@@ -398,45 +396,17 @@
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Main function of the program
 
         """
-        self._startTime = _abstract._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y1)
-                if val > self._minimum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("TopK partial periodic patterns were generated successfully")
-        self._endTime = _abstract._time.time()
-        process = _abstract._psutil.Process(_abstract._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self):
+    def mine(self):
             """
             Main function of the program
 
             """
             self._startTime = _abstract._time.time()
             if self._iFile is None:
                 raise Exception("Please enter the file path or file name:")
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py` & `pami-2024.5.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/PPGrowth.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,12 +1,10 @@
 # PPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#
 #
 #     from PAMI.periodicFrequentPattern.basic import PPGrowth as alg
 #
 #     obj = alg.PPGrowth(iFile, minSup, maxPer)
 #
 #     obj.startMine()
 #
@@ -23,14 +21,16 @@
 #     print("Total Memory in USS:", memUSS)
 #
 #     memRSS = obj.getMemoryRSS()
 #
 #     print("Total Memory in RSS", memRSS)
 #
 #     run = obj.getRuntime()
+#
+#     print("Total ExecutionTime in seconds:", run)
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
@@ -40,15 +40,15 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
+     
 
 """
 
 import pandas as pd
 from deprecated import deprecated
 from PAMI.partialPeriodicPatternInMultipleTimeSeries import abstract as _ab
 
@@ -308,14 +308,17 @@
                 for q in conditionalTree.generatePatterns(pattern):
                     yield q
             self.removeNode(i)
 
 
 class PPGrowth(_ab._partialPeriodicPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description:   PPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
 
     :Reference:   C. Saideep, R. Uday Kiran, K. Zettsu, P. Fournier-Viger, M. Kitsuregawa and P. Krishna Reddy,
                  "Discovering Periodic Patterns in Irregular Time Series," 2019 International Conference on Data Mining Workshops (ICDMW), 2019,
                   pp. 1020-1028, doi: 10.1109/ICDMW.2019.00147.
 
     :param  iFile: str :
@@ -389,19 +392,23 @@
         updateDatabases()
             Update the database by removing aperiodic items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
 
-    **Executing the code on terminal:**
-    -------------------------------------
-    .. code-block:: console
+    Execution methods
+    =================
+
+
+    **Terminal command**
 
 
+    .. code-block:: console
+
        Format:
 
        (.venv) $ python3 PPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
 
        Examples:
 
        (.venv) $  python3 PPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
@@ -433,15 +440,17 @@
 
         run = obj.getRuntime()
 
         print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+
+
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
     _startTime = float()
     _endTime = float()
     _periodicSupport = str()
     _period = float()
     _finalPatterns = {}
@@ -491,15 +500,14 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-
     def _periodicFrequentOneItem(self):
         """
         Calculates the support of each item in the database and assign ranks to the items
         by decreasing support and returns the frequent items list
 
         :returns: return the one-length periodic frequent patterns
         """
```

### Comparing `pami-2024.4.9.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py` & `pami-2024.5.1/PAMI/partialPeriodicPatternInMultipleTimeSeries/abstract.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,7 +1,22 @@
+#  Copyright (C)  2021 Rage Uday Kiran
+#
+#      This program is free software: you can redistribute it and/or modify
+#      it under the terms of the GNU General Public License as published by
+#      the Free Software Foundation, either version 3 of the License, or
+#      (at your option) any later version.
+#
+#      This program is distributed in the hope that it will be useful,
+#      but WITHOUT ANY WARRANTY; without even the implied warranty of
+#      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+#      GNU General Public License for more details.
+#
+#      You should have received a copy of the GNU General Public License
+#      along with this program.  If not, see <https://www.gnu.org/licenses/>.
+
 from abc import ABC as _ABC, abstractmethod as _abstractmethod
 import time as _time
 import math as _math
 import csv as _csv
 import pandas as _pd
 from collections import defaultdict as _defaultdict
 from itertools import combinations as _combinations
@@ -11,14 +26,17 @@
 import sys as _sys
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 
 
 class _partialPeriodicPatterns(_ABC):
     """
+    About this algorithm
+    ====================
+
     :Description:   This abstract base class defines the variables and methods that every frequent pattern mining algorithm must
                     employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
@@ -79,14 +97,15 @@
     def iFile(self):
         """Variable to store the input file path/file name"""
         pass
     @abstractmethod
     def periodicSupport(self):
         """Variable to store the user-specified minimum support value"""
         pass
+    @abstractmethod
     def period(self):
         """Variable to store the user specified maximum periodicity value"""
         pass
     @abstractmethod
     def sep(self):
         """Variable to store the input file path/file name"""
         pass
@@ -158,10 +177,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-        """ To print all the results of execution"""
+        """ To print the results of execution"""
 
         pass
```

### Comparing `pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py` & `pami-2024.5.1/PAMI/periodicCorrelatedPattern/basic/EPCPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -642,57 +642,62 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Correlated Periodic-Frequent patterns were generated successfully using EPCPGrowth algorithm ")
 
     def getMemoryUSS(self) -> float:
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
-        """Calculating the total amount of runtime taken by the mining process
+        """
+        Calculating the total amount of runtime taken by the mining process
 
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
-        """Storing final periodic-frequent patterns in a dataframe
+        """
+        Storing final periodic-frequent patterns in a dataframe
 
         :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1], b[2], b[3]])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity', 'allConf', 'maxPerAllConf'])
         return dataFrame
 
     def save(self, outFile: str) -> None:
-        """Complete set of periodic-frequent patterns will be loaded in to an output file
+        """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
```

### Comparing `pami-2024.4.9.1/PAMI/periodicCorrelatedPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/periodicCorrelatedPattern/basic/abstract.py`

 * *Files 2% similar despite different names*

```diff
@@ -26,14 +26,17 @@
 import sys as _sys
 import validators as _validators
 from urllib.request import urlopen as _urlopen
 
 
 class _periodicCorrelatedPatterns(_ABC):
     """
+    About this algorithm
+    ====================
+
     :Description:   This abstract base class defines the variables and methods that every periodic-frequent pattern mining algorithm must
         employ in PAMI
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
```

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFECLAT.py`

 * *Files 8% similar despite different names*

```diff
@@ -53,14 +53,15 @@
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
+import numpy as np
 
 from PAMI.periodicFrequentPattern.basic import abstract as _ab
 
 
 class PFECLAT(_ab._periodicFrequentPatterns):
     """
     :Description:   PFECLAT is the fundamental approach to mine the periodic-frequent patterns.
@@ -211,27 +212,14 @@
     _tidSet = set()
     _finalPatterns = {}
     _startTime = None
     _endTime = None
     _memoryUSS = float()
     _memoryRSS = float()
 
-    def _getPeriodic(self, tids: set) -> int:
-        tidList = list(tids)
-        tidList.sort()
-        tidList.append(self._dbSize)
-        cur = 0
-        per = 0
-        for tid in tidList:
-            per = max(per, tid - cur)
-            if per > self._maxPer:  # early stopping
-                break
-            cur = tid
-        return per
-
     def _convert(self, value) -> float:
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
@@ -243,135 +231,143 @@
             if '.' in value:
                 value = float(value)
                 value = (self._dbSize * value)
             else:
                 value = int(value)
         return value
 
-    def _creatingOneItemSets(self) -> list:
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
-        :return: list
+            Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
-        plist = []
-        Database = []
+        self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
-                Database.append(tr)
+                self._Database.append(tr)
+
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            Database.append(temp)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-        tid = 0
-        itemsets = {}  # {key: item, value: list of tids}
-        periodicHelper = {}  # {key: item, value: [period, last_tid]}
-        for line in Database:
-            tid = int(line[0])
-            self._tidSet.add(tid)
-            for item in line[1:]:
-                if item in itemsets:
-                    itemsets[item].add(tid)
-                    periodicHelper[item][0] = max(periodicHelper[item][0],
-                                                  abs(tid - periodicHelper[item][1]))  # update current max period
-                    periodicHelper[item][1] = tid  # update the last tid
-                else:
-                    itemsets[item] = {tid}
-                    periodicHelper[item] = [abs(0 - tid), tid]  # initialize helper
-
-        # finish all items' period
-        self._dbSize = len(Database)
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        del Database
-        for item, _ in periodicHelper.items():
-            periodicHelper[item][0] = max(periodicHelper[item][0],
-                                          abs(self._dbSize - periodicHelper[item][1]))  # tid of the last transaction
-        candidates = []
-        for item, tids in itemsets.items():
-            per = periodicHelper[item][0]
-            sup = len(tids)
-            if sup >= self._minSup and per <= self._maxPer:
-                candidates.append(item)
-                self._finalPatterns[item] = [sup, per, tids]
-        return candidates
-    
-    def _generateEclat(self, candidates: list) -> None:
-
-        newCandidates = []
-        for i in range(0, len(candidates)):
-            prefixItem = candidates[i]
-            prefixItemSet = prefixItem.split()
-            for j in range(i + 1, len(candidates)):
-                item = candidates[j]
-                itemSet = item.split()
-                if prefixItemSet[:-1] == itemSet[:-1] and prefixItemSet[-1] != itemSet[-1]:
-                    _value = self._finalPatterns[item][2].intersection(self._finalPatterns[prefixItem][2])
-                    sup = len(_value)
-                    per = self._getPeriodic(_value)
-                    if sup >= self._minSup and per <= self._maxPer:
-                        newItem = prefixItem + "\t" + itemSet[-1]
-                        self._finalPatterns[newItem] = [sup, per, _value]
-                        newCandidates.append(newItem)
-
-        if len(newCandidates) > 0:
-            self._generateEclat(newCandidates)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self) -> None:
         """
         Mining process will start from this function
         :return: None
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        frequentSets = self._creatingOneItemSets()
-        self._generateEclat(frequentSets)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+        self.Mine()
+        # self._startTime = _ab._time.time()
+        # self._finalPatterns = {}
+        # frequentSets = self._creatingOneItemSets()
+        # self._generateEclat(frequentSets)
+        # self._endTime = _ab._time.time()
+        # process = _ab._psutil.Process(_ab._os.getpid())
+        # self._memoryRSS = float()
+        # self._memoryUSS = float()
+        # self._memoryUSS = process.memory_full_info().uss
+        # self._memoryRSS = process.memory_info().rss
+        # print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
+
+    def _getMaxPer(self, arr, maxTS):
+        arr = np.append(list(arr), [0, maxTS])
+        arr = np.sort(arr)
+        arr = np.diff(arr)
+
+        return np.max(arr)
 
     def Mine(self) -> None:
         """
         Mining process will start from this function
         :return: None
         """
         self._startTime = _ab._time.time()
         self._finalPatterns = {}
-        frequentSets = self._creatingOneItemSets()
-        self._generateEclat(frequentSets)
+        frequentSets = self._creatingItemSets()
+
+        items = {}
+        maxTS = 0
+        for line in self._Database:
+            index = int(line[0])
+            maxTS = max(maxTS, index)
+            for item in line[1:]:
+                if tuple([item]) not in items:
+                    items[tuple([item])] = set()
+                items[tuple([item])].add(index)
+
+        self._dbSize = maxTS
+
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        minSup = self._minSup
+        maxPer = self._maxPer
+
+
+        items = {k: v for k, v in items.items() if len(v) >= minSup}
+        items = {k: v for k, v in sorted(items.items(), key = lambda x: len(x[1]), reverse = True)}
+
+        keys = []
+        for item in list(items.keys()):
+            per = self._getMaxPer(items[item], maxTS)
+            if per <= maxPer:
+                keys.append(item)
+                self._finalPatterns[item] = [len(items[item]), per, set(items[item])]
+
+        while keys:
+            newKeys = []
+            for i in range(len(keys)):
+                for j in range(i + 1, len(keys)):
+                    if keys[i][:-1] == keys[j][:-1] and keys[i][-1] != keys[j][-1]:
+                        # print(keys[i], keys[j])
+                        newKey = tuple(keys[i] + (keys[j][-1],))
+                        intersect = items[keys[i]].intersection(items[keys[j]])
+                        per = self._getMaxPer(intersect, maxTS)
+                        sup = len(intersect)
+                        if sup >= minSup and per <= maxPer:
+                            items[newKey] = intersect
+                            newKeys.append(newKey)
+                            self._finalPatterns[newKey] = [sup, per, set(intersect)]
+                    else:
+                        break
+            keys = newKeys
+
+        newPattern = {}
+        for k, v in self._finalPatterns.items():
+            newPattern["\t".join([str(x) for x in k])] = v
+
+        self._finalPatterns = newPattern
+
+        # self._generateEclat(frequentSets)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
         self._memoryUSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Periodic-Frequent patterns were generated successfully using PFECLAT algorithm ")
```

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/_PFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PFPMC.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFPMC.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PSGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/__init__.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -153,10 +153,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-        """ To print results of the execution."""
+        """ To print the results of the execution."""
 
         pass
```

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/CPFPMiner.py` & `pami-2024.5.1/PAMI/partialPeriodicPattern/closed/PPPClose.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,17 +1,15 @@
-#  CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
-#  It uses depth-first search.
-#
+
+
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             from PAMI.partialPeriodicPattern.closed import PPPClose as alg
 #
-#             from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
-#
-#             obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
+#             obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
 #
 #             obj.startMine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
 #
@@ -27,16 +25,16 @@
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
-
-
+#
+#
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
@@ -49,59 +47,71 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
+
+
+import sys as _sys
+import validators as _validators
+from urllib.request import urlopen as _urlopen
+from PAMI.partialPeriodicPattern.closed import abstract as _abstract
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.periodicFrequentPattern.closed import abstract as _ab
-
+class PPPClose(_abstract._partialPeriodicPatterns):
+    """
+    :Description:
 
-class CPFPMiner(_ab._periodicFrequentPatterns):
-    """ 
-    :Description:   CPFPMiner algorithm is used to discover the closed periodic frequent patterns in temporal databases.
-                    It uses depth-first search.
+    PPPClose algorithm is used to discover the closed partial periodic patterns in temporal databases.
+    It uses depth-first search.
 
-    :Reference:   P. Likhitha et al., "Discovering Closed Periodic-Frequent Patterns in Very Large Temporal Databases"
-                  2020 IEEE International Conference on Big Data (Big Data), 2020, https://ieeexplore.ieee.org/document/9378215
+    :Reference: R. Uday Kiran1 , J. N. Venkatesh2 , Philippe Fournier-Viger3 , Masashi Toyoda1 , P. Krishna Reddy2 and Masaru Kitsuregawa
+                 https://www.tkl.iis.u-tokyo.ac.jp/new/uploads/publication_file/file/799/PAKDD.pdf
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of periodic frequent pattern's
     :param  oFile: str :
                    Name of the output file to store complete set of periodic frequent pattern's
-    :param  minSup: float:
-                   Controls the minimum number of transactions in which every item must appear in a database.
-    :param  maxPer: float:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  period: float:
+                   Minimum partial periodic...
+    :param  periodicSupport: float:
+                   Minimum partial periodic...
 
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : str
             Input file name or path of the input file
         oFile : str
             Name of the output file or path of the input file
-        minSup: int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+        periodicSupport: int or float or str
+            The user can specify periodicSupport either in count or proportion of database size.
+            If the program detects the data type of periodicSupport is integer, then it treats periodicSupport is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer: int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Example: periodicSupport=10 will be treated as integer, while periodicSupport=10.0 will be treated as float
+        period: int or float or str
+            The user can specify period either in count or proportion of database size.
+            If the program detects the data type of period is integer, then it treats period is expressed in count.
             Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+            Example: period=10 will be treated as integer, while period=10.0 will be treated as float
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
         startTime:float
             To record the start time of the mining process
         endTime:float
             To record the completion time of the mining process
         finalPatterns: dict
             Storing the complete set of patterns in a dictionary variable
@@ -113,99 +123,94 @@
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to an output file
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
 
-    **Methods to execute code on terminal**
-    --------------------------------------------
+    **Executing the code on terminal:**
+    -------------------------------------
     .. code-block:: console
 
 
        Format:
 
-       (.venv) $  python3 CPFPMiner.py <inputFile> <outputFile> <minSup> <maxPer>
+       (.venv) $ python3 PPPClose.py <inputFile> <outputFile> <periodicSupport> <period>
 
-       Example:
+       Examples:
 
-       (.venv) $ python3 CPFPMiner.py sampleTDB.txt patterns.txt 0.3 0.4
+       (.venv) $ python3 PPPClose.py sampleTDB.txt patterns.txt 0.3 0.4
 
-        
-               .. note:: minSup will be considered in percentage of database transactions
-        
-        
-    **Importing this algorithm into a python program**
-    -------------------------------------------------------
+
+    **Sample run of the imported code:**
+    --------------------------------------
     .. code-block:: python
-        
-                    from PAMI.periodicFrequentPattern.closed import CPFPMiner as alg
-        
-                    obj = alg.CPFPMiner("../basic/sampleTDB.txt", "2", "6")
-        
-                    obj.startMine()
-        
-                    periodicFrequentPatterns = obj.getPatterns()
-        
-                    print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
-        
-                    obj.save("patterns")
-        
-                    Df = obj.getPatternsAsDataFrame()
-        
-                    memUSS = obj.getMemoryUSS()
-        
-                    print("Total Memory in USS:", memUSS)
-        
-                    memRSS = obj.getMemoryRSS()
-        
-                    print("Total Memory in RSS", memRSS)
-        
-                    run = obj.getRuntime()
-        
-                    print("Total ExecutionTime in seconds:", run)
-        
+
+            from PAMI.partialPeriodicPattern.closed import PPPClose as alg
+
+            obj = alg.PPPClose("../basic/sampleTDB.txt", "2", "6")
+
+            obj.startMine()
+
+            periodicFrequentPatterns = obj.getPatterns()
+
+            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+
+            obj.save("patterns")
+
+            Df = obj.getPatternsAsDataFrame()
+
+            memUSS = obj.getMemoryUSS()
+
+            print("Total Memory in USS:", memUSS)
+
+            memRSS = obj.getMemoryRSS()
+
+            print("Total Memory in RSS", memRSS)
+
+            run = obj.getRuntime()
+
+            print("Total ExecutionTime in seconds:", run)
+
     **Credits:**
-    ------------------
-    The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
+    --------------
+        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+
     """
 
-    _minSup = float()
-    _maxPer = float()
+    _periodicSupport = float()
+    _period = float()
     _startTime = float()
     _endTime = float()
     _finalPatterns = {}
+    _Database = []
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _transaction = []
     _hashing = {}
     _mapSupport = {}
     _itemSetCount = 0
     _maxItemId = 0
     _tableSize = 10000
     _tidList = {}
     _lno = 0
 
-    def __init__(self, iFile, minSup, maxPer, sep='\t'):
-        super().__init__(iFile, minSup, maxPer, sep)
-        self._finalPatterns = {}
-    
     def _convert(self, value):
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
@@ -217,76 +222,83 @@
             if '.' in value:
                 value = float(value)
                 value = (self._lno * value)
             else:
                 value = int(value)
         return value
 
-    def _scanDatabase(self):
+    def _creatingItemSets(self):
         """
-        To scan the database and extracts the 1-length periodic-frequent items
-        :return:   Returns the 1-length periodic-frequent items
+        Storing the complete transactions of the database/input file in a database variable
         """
-        Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            ts, data = [], []
+        self._Database = []
+        if isinstance(self._iFile, _abstract._pd.DataFrame):
+            timeStamp, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
+                timeStamp = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
-                tr = [ts[i][0]]
+                tr = [timeStamp[i]]
                 tr = tr + data[i]
-                Database.append(tr)
-
+                self._Database.append(tr)
+            self._lno = len(self._Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _validators.url(self._iFile):
+                data = _urlopen(self._iFile)
                 for line in data:
-                    line.strip()
+                    self._lno += 1
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    Database.append(temp)
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line.strip()
+                            self._lno += 1
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            Database.append(temp)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-        self._tidList = {}
+
+    def _OneLengthPartialItems(self):
+        """
+        To scan the database and extracts the 1-length periodic-frequent items
+
+        :return: Returns the 1-length periodic-frequent items
+        """
         self._mapSupport = {}
-        for line in Database:
-            self._lno += 1
-            s = line
-            n = int(s[0])
-            for i in range(1, len(s)):
-                si = s[i]
+        self._tidList = {}
+        self._period = self._convert(self._period)
+        for line in self._Database:
+            n = int(line[0])
+            for i in range(1, len(line)):
+                si = line[i]
                 if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
+                    self._mapSupport[si] = [1, 0, n]
                     self._tidList[si] = [n]
                 else:
                     self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
+                    period = abs(n - self._mapSupport[si][2])
+                    if period <= self._period:
+                        self._mapSupport[si][1] += 1
                     self._mapSupport[si][2] = n
                     self._tidList[si].append(n)
         for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(self._lno - self._mapSupport[x][2]))
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if
-                           v[0] >= self._minSup and v[1] <= self._maxPer}
+            period = abs(self._lno - self._mapSupport[x][2])
+            if period <= self._period:
+                self._mapSupport[x][1] += 1
+        self._periodicSupport = self._convert(self._periodicSupport)
+        self._mapSupport = {k: v[1] for k, v in self._mapSupport.items() if v[1] >= self._periodicSupport}
         periodicFrequentItems = {}
         self._tidList = {k: v for k, v in self._tidList.items() if k in self._mapSupport}
         for x, y in self._tidList.items():
             t1 = 0
             for i in y:
                 t1 += i
             periodicFrequentItems[x] = t1
@@ -308,101 +320,91 @@
         return hashcode % self._tableSize
 
     def _contains(self, itemSet, val, hashcode):
         """
         To check if the key(hashcode) is in dictionary(hashing) variable
 
         :param itemSet: generated periodic-frequent itemSet
-        :param val: support and periodicity of itemSet
+        :param val: support and period of itemSet
         :param hashcode: the key generated in calculate() method for every itemSet
 
         :return: true if itemSet with same support present in dictionary(hashing) or else returns false
         """
         if self._hashing.get(hashcode) is None:
             return False
         for i in self._hashing[hashcode]:
             itemSetX = i
-            if val[0] == self._hashing[hashcode][itemSetX][0] and set(itemSetX).issuperset(itemSet):
+            if val == self._hashing[hashcode][itemSetX] and set(itemSetX).issuperset(itemSet):
                 return True
         return False
 
-    def _getPeriodAndSupport(self, timeStamps):
+    def _getPeriodicSupport(self, timeStamps):
         """
-        Calculates the periodicity and support of timeStamps
+        Calculates the period and support of timeStamps
 
-        :param timeStamps: timeStamps of itemSet
-        :return: periodicity and support
+        :param: timeStamps: timeStamps of itemSet
+        :return: period and support
         """
         timeStamps.sort()
-        cur = 0
-        per = 0
         sup = 0
-        for j in range(len(timeStamps)):
-            per = max(per, timeStamps[j] - cur)
-            if per > self._maxPer:
-                return [0, 0]
-            cur = timeStamps[j]
-            sup += 1
-        per = max(per, self._lno - cur)
-        return [sup, per]
+        for j in range(len(timeStamps) - 1):
+            per = abs(timeStamps[j + 1] - timeStamps[j])
+            if per <= self._period:
+                sup += 1
+        return sup
 
     def _save(self, prefix, suffix, tidSetX):
         """
         Saves the generated pattern which satisfies the closed property
-        Parameters:
-        -----------
-            prefix: the prefix part of itemSet
-            suffix: the suffix part of itemSet
-            tidSetX: the timeStamps of the generated itemSet
-
-        Returns:
-        --------
-            saves the closed periodic-frequent pattern
 
+        :param prefix: the prefix part of itemSet
+        :param suffix: the suffix part of itemSet
+        :param tidSetX: the timeStamps of the generated itemSet
+        :return: saves the closed periodic-frequent pattern
         """
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
         prefix = list(set(prefix))
         prefix.sort()
-        val = self._getPeriodAndSupport(tidSetX)
-        if val[0] >= self._minSup and val[1] <= self._maxPer:
+        val = self._getPeriodicSupport(tidSetX)
+        if val >= self._periodicSupport:
             hashcode = self._calculate(tidSetX)
             if self._contains(prefix, val, hashcode) is False:
                 self._itemSetCount += 1
                 sample = str()
                 for i in prefix:
-                    sample = sample + i + " "
+                    sample = sample + i + "\t"
                 self._finalPatterns[sample] = val
             if hashcode not in self._hashing:
                 self._hashing[hashcode] = {tuple(prefix): val}
             else:
                 self._hashing[hashcode][tuple(prefix)] = val
 
     def _processEquivalenceClass(self, prefix, itemSets, tidSets):
         """
 
         :param prefix: Prefix class of an itemSet
-        :param itemSets: suffix items in periodicFrequentItems that satisfies the minSup condition
+        :param itemSets: suffix items in periodicFrequentItems that satisfies the periodicSupport condition
         :param tidSets: timeStamps of items in itemSets respectively
-        :return:  closed periodic patterns with length more than 2
+        :return: closed periodic patterns with length more than 2
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidList = tidSets[0]
             self._save(prefix, [i], tidList)
             return
         if len(itemSets) == 2:
             itemI = itemSets[0]
             tidSetI = tidSets[0]
             itemJ = itemSets[1]
             tidSetJ = tidSets[1]
             y1 = list(set(tidSetI).intersection(tidSetJ))
-            if len(y1) >= self._minSup:
+            if len(y1) >= self._periodicSupport:
                 suffix = []
                 suffix += [itemI, itemJ]
                 suffix = list(set(suffix))
                 self._save(prefix, suffix, y1)
             if len(y1) != len(tidSetI):
                 self._save(prefix, [itemI], tidSetI)
             if len(y1) != len(tidSetJ):
@@ -418,15 +420,15 @@
             itemSetX = [itemX]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 if itemJ is None:
                     continue
                 tidSetJ = tidSets[j]
                 y = list(set(tidSetX).intersection(tidSetJ))
-                if len(y) < self._minSup:
+                if len(y) < self._periodicSupport:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y) == len(tidSetX):
                     itemSets.insert(j, None)
                     tidSets.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y) == len(tidSetX):
                     itemSetX.append(itemJ)
@@ -444,109 +446,66 @@
             self._save(prefix, list(set(itemSetX)), tidSetX)
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
-        self._hashing = {}
-        periodicFrequentItems = self._scanDatabase()
-        for i in range(len(periodicFrequentItems)):
-            itemX = periodicFrequentItems[i]
-            if itemX is None:
-                continue
-            tidSetX = self._tidList[itemX]
-            itemSetX = [itemX]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(periodicFrequentItems)):
-                itemJ = periodicFrequentItems[j]
-                if itemJ is None:
-                    continue
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._minSup:
-                    continue
-                if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
-                    periodicFrequentItems.insert(j, None)
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
-                    itemSetX.append(itemJ)
-                elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
-                    periodicFrequentItems.insert(j, None)
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-                else:
-
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            if len(itemSets) > 0:
-                self._processEquivalenceClass(itemSetX, itemSets, tidSets)
-            self._save([], itemSetX, tidSetX)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
+        self.mine()
 
-    def Mine(self):
+    def mine(self):
         """
         Mining process will start from here
         """
-        self._startTime = _ab._time.time()
-        self._finalPatterns = {}
+        self._startTime = _abstract._time.time()
+        self._creatingItemSets()
         self._hashing = {}
-        periodicFrequentItems = self._scanDatabase()
+        self._finalPatterns = {}
+        periodicFrequentItems = self._OneLengthPartialItems()
         for i in range(len(periodicFrequentItems)):
             itemX = periodicFrequentItems[i]
             if itemX is None:
                 continue
             tidSetX = self._tidList[itemX]
             itemSetX = [itemX]
             itemSets = []
             tidSets = []
             for j in range(i + 1, len(periodicFrequentItems)):
                 itemJ = periodicFrequentItems[j]
                 if itemJ is None:
                     continue
                 tidSetJ = self._tidList[itemJ]
                 y1 = list(set(tidSetX).intersection(tidSetJ))
-                if len(y1) < self._minSup:
+                if len(y1) < self._periodicSupport:
                     continue
                 if len(tidSetX) == len(tidSetJ) and len(y1) is len(tidSetX):
                     periodicFrequentItems.insert(j, None)
                     itemSetX.append(itemJ)
                 elif len(tidSetX) < len(tidSetJ) and len(y1) is len(tidSetX):
                     itemSetX.append(itemJ)
                 elif len(tidSetX) > len(tidSetJ) and len(y1) is len(tidSetJ):
                     periodicFrequentItems.insert(j, None)
                     itemSets.append(itemJ)
                     tidSets.append(y1)
                 else:
-
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             if len(itemSets) > 0:
                 self._processEquivalenceClass(itemSetX, itemSets, tidSets)
             self._save([], itemSetX, tidSetX)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
+        self._endTime = _abstract._time.time()
+        process = _abstract._psutil.Process(_abstract._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Closed periodic frequent patterns were generated successfully using CPFPMiner algorithm ")
+        print("Closed periodic frequent patterns were generated successfully using PPPClose algorithm ")
 
     def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
@@ -574,57 +533,54 @@
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a.replace('\t', ' '), b])
+            dataFrame = _abstract._pd.DataFrame(data, columns=['Patterns', 'periodicSupport'])
         return dataFrame
 
     def save(self, outFile):
-        """
-        Complete set of frequent patterns will be loaded in to an output file
+        """Complete set of frequent patterns will be loaded in to a output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
+        :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
+            s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """ Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        """
-        This function is used to print the results
-        """
-        print("Total number of Closed Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Closed Partial Periodic Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
-        
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
-        if len(_ab._sys.argv) == 5:
-            _ap = CPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+    if len(_sys.argv) == 5 or len(_sys.argv) == 6:
+        if len(_sys.argv) == 6:
+            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4], _sys.argv[5])
+        if len(_sys.argv) == 5:
+            _ap = PPPClose(_sys.argv[1], _sys.argv[3], _sys.argv[4])
         _ap.startMine()
-        print("Total number of Closed Periodic-Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
+        print("Total number of  Patterns:", len(_ap.getPatterns()))
+        _ap.save(_sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/__init__.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/closed/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/closed/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/closed/abstract.py`

 * *Files 1% similar despite different names*

```diff
@@ -153,10 +153,10 @@
     def getRuntime(self):
         """Total amount of runtime taken by the program will be retrieved from this function"""
 
         pass
 
     @_abstractmethod
     def printResults(self):
-        """ TO print the results of execution """
+        """ To print the results of execution """
 
         pass
```

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/cuGPFMiner.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/cuda/gPFMinerBit.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/MaxPFGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/__init__.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/maximal/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/maximal/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/pyspark/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/pyspark/parallelPFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/TopkPFP.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,357 +1,438 @@
+# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+#
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
+#             from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+#
+#             iFile = 'sampleDB.txt'
 #
-#             import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.TopkPFPGrowth(iFile, k, maxPer,oFile)
+#             obj = alg.UVEclat(iFile, minSup)
 #
-#             obj.startMine()
+#             obj.mine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
-
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
-import pandas as pd
-from deprecated import deprecated
+import operator as _operator
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+import deprecated
+
+_minSup = float()
+_finalPatterns = {}
 
 
-class TopkPFPGrowth(_ab._periodicFrequentPatterns):
+class _Item:
     """
-    :Description:   Top - K is and algorithm to discover top periodic frequent patterns in a temporal database.
+    A class used to represent the item with probability in transaction of dataset
 
-    :Reference:   Komate Amphawan, Philippe Lenca, Athasit Surarerks: "Mining Top-K Periodic-Frequent Pattern from Transactional Databases without Support Threshold"
-                  International Conference on Advances in Information Technology: https://link.springer.com/chapter/10.1007/978-3-642-10392-6_3
+    :Attributes:
+
+        item : int or word
+          Represents the name of the item
 
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
-    :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-    :param  maxPer: str:
-                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
 
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    def __init__(self, item, probability):
+        self.item = item
+        self.probability = probability
+
+
+class UVEclat(_ab._frequentPatterns):
+    """
+    About this algorithm
+    ====================
+
+    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
+
+    :Reference:  Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
+                 SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983–984,
+                 https://doi.org/10.1145/1982185.1982399
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top frequent patterns
+        iFile : file
+            Name of the Input file or path of the input file
+
+        oFile : file
+            Name of the output file or path of the output file
+
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
 
-    :Methods:
+        startTime:float
+            To record the start time of the mining process
+
+        endTime:float
+            To record the completion time of the mining process
+
+        Database : list
+            To store the transactions of a database in list
+
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
 
-        startMine()
+        lno : int
+            To represent the total no of transaction
+
+        tree : class
+            To represent the Tree class
+
+        itemSetCount : int
+            To represents the total no of patterns
+
+        finalPatterns : dict
+            To store the complete patterns
+
+    :Methods:
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        save(oFile)
+        storePatternsInFile(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsAsDataFrame()
+        getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
         frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
+            Extracts the one-length frequent patterns from database
+
+    Execution methods
+    =================
 
-    **Executing the code on terminal:**
-    -------------------------------------
-   .. code-block:: console
 
+    **Terminal command**
 
-       Format:
 
-       (.venv) $ python3 TopkPFP.py <inputFile> <outputFile> <k> <maxPer>
+    .. code-block:: console
 
-       Examples:
+      Format:
 
-       (.venv) $ python3 TopkPFP.py sampleDB.txt patterns.txt 10 3
+      (.venv) $ python3 uveclat.py <inputFile> <outputFile> <minSup>
 
+      Example Usage:
 
-    **Sample run of the importing code:**
-    ---------------------------------------
+      (.venv) $ python3 uveclat.py sampleDB.txt patterns.txt 3
+
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+
+
+    **Calling from a python program**
+    ---------------------------------------------------
     .. code-block:: python
 
-            import PAMI.periodicFrequentPattern.topk.TopkPFPGrowth as alg
+            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.TopkPFPGrowth(iFile, k, maxPer)
+            obj = alg.UVEclat(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(periodicFrequentPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+            memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    --------------
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
 
+             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
-
     _startTime = float()
     _endTime = float()
-    _k = int()
-    _maxPer = " "
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _tidList = {}
-    _lno = int()
-    _minimum = int()
-    _mapSupport = {}
+    _rank = {}
 
     def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
-
+        Scans the dataset
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            data, ts = [], []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
-            if 'TS' in i:
-                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                data = self._iFile['Transactions'].tolist()
-            if 'Patterns' in i:
-                data = self._iFile['Patterns'].tolist()
-            for i in range(len(data)):
-                tr = [ts[i][0]]
-                tr = tr + data[i]
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
                 self._Database.append(tr)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._Database.append(temp)
+                            tr = []
+                            for i in temp:
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _convert(self, value):
+    def _frequentOneItem(self):
+        """
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        """
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[str(j.item)] = j.probability
+                    self._tidList[str(j.item)] = {k: j.probability}
+                else:
+                    mapSupport[str(j.item)] += j.probability
+                    self._tidList[str(j.item)].update({k: j.probability})
+        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
+        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
+        return list(plist.keys())
+
+    @staticmethod
+    def _check(i, x):
+        """
+        To check the presence of item or pattern in transaction
+
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain self.Database
+        :type i : list
+        """
+
+        # This method taken a transaction as input and returns the tree
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
+
+    @staticmethod
+    def _convert(value):
         """
-        To convert the given user specified value
+        To convert the type of user specified minSup value
 
-        :param value: user specified value
-        :return: converted value
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
+    def _removeFalsePositives(self):
         """
+        To remove the false positive patterns generated in frequent patterns
 
-        self._mapSupport = {}
-        self._tidList = {}
-        n = 0
-        for line in self._Database:
-            self._lno += 1
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
+        :return: patterns with accurate probability
+        """
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
                 else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        self._maxPer = self._convert(self._maxPer)
-        self._k = self._convert(self._k)
-        self._mapSupport = {k: [v[0], v[1]] for k, v in self._mapSupport.items() if v[1] <= self._maxPer}
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._finalPatterns = {}
-        #print(len(plist))
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = [self._mapSupport[i][0], self._mapSupport[i][1]]
-        self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-    def _getSupportAndPeriod(self, timeStamps):
-        """To calculate the periodicity and support
-
-        :param timeStamps: Timestamps of an item set
-        :return: support, periodicity
-        """
-
-        global lno
-        timeStamps.sort()
-        cur = 0
-        per = list()
-        sup = 0
-        for j in range(len(timeStamps)):
-            per.append(timeStamps[j] - cur)
-            cur = timeStamps[j]
-            sup += 1
-        per.append(self._lno - cur)
-        if len(per) == 0:
-            return [0, 0]
-        return [sup, max(per)]
+                    s = 1
+                    check = self._check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
+
+    @staticmethod
+    def _Intersection(tidSetx, tidSetY):
+        """
+        This function is used to find the intersection
+
+        :param tidSetx: the timestamp of a patterns
+        :type tidSetx: dict
+        :param tidSetY: the timestamp of a patterns
+        :type tidSetY: dict
+        """
+        tids = []
+        support = []
+        tidDict = {}
+        for x, y in tidSetx.items():
+            for x1, y1 in tidSetY.items():
+                if x == x1:
+                    tids.append(x)
+                    support.append(y * y1)
+                    tidDict.update({x: y * y1})
+        return tidDict
+
+    def _calculateExpSup(self, tidList):
+        """
+        This function is used to calculate support of tidList
+
+        :param tidList: timestamp of a list.
+        :type tidList: List
+        """
+        return sum(tidList.values())
 
     def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
+        """
+        Saves the patterns that satisfy the periodic frequent property.
 
         :param prefix: the prefix of a pattern
         :type prefix: list
         :param suffix: the suffix of a patterns
         :type suffix: list
         :param tidSetI: the timestamp of a patterns
-        :type tidSetI: list
+        :type tidSetI: dict
         """
 
+        global _finalPatterns
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
-        val = self._getSupportAndPeriod(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + " "
-        if len(self._finalPatterns) < self._k:
-            if val[0] >= self._minimum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in
-                                  sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1][0]):
-                if val[0] > y[0]:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[x] = y
-                    self._finalPatterns = {k: v for k, v in
-                                          sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                    self._minimum = min([self._finalPatterns[i][0] for i in self._finalPatterns.keys()])
-                    return
+        val = self._calculateExpSup(tidSetI)
+        _finalPatterns[tuple(prefix)] = val
 
     def _Generation(self, prefix, itemSets, tidSets):
         """
         Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
 
         :param prefix:  main equivalence prefix
         :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity
-                            and frequent with their timestamps
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
         :type itemSets: list
         :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
         """
         if len(itemSets) == 1:
             i = itemSets[0]
             tidI = tidSets[0]
@@ -364,169 +445,147 @@
             tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
             for j in range(i + 1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                y = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y) >= self._minSup:
                     classItemSets.append(itemJ)
                     classTidSets.append(y)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
             self._save(prefix, list(set(itemSetX)), tidSetI)
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+    def mine(self):
         """
-        Main function of the program
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
+        global _minSup
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        _plist = self._frequentOneItem()
-        for i in range(len(_plist)):
-            itemI = _plist[i]
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
             tidSetI = self._tidList[itemI]
             itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemJ = _plist[j]
+            for j in range(i+1, len(plist)):
+                itemJ = plist[j]
                 tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y1)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
+                y1 = self._Intersection(tidSetI, tidSetJ)
+                if self._calculateExpSup(y1) >= self._minSup:
                     itemSets.append(itemJ)
                     tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
-        print("TopK Periodic Frequent patterns were generated successfully")
+            self._save(None, itemSetX, tidSetI)
+        self._removeFalsePositives()
+        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
         self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryRSS = float()
         self._memoryUSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
-    def Mine(self):
-        """
-        Main function of the program
+    def getMemoryUSS(self):
         """
-        self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        _plist = self._frequentOneItem()
-        for i in range(len(_plist)):
-            itemI = _plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(_plist)):
-                itemJ = _plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                val = self._getSupportAndPeriod(y1)
-                if val[0] >= self._minimum and val[1] <= self._maxPer:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("TopK Periodic Frequent patterns were generated successfully")
-        self._endTime = _ab._time.time()
-        _process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = _process.memory_full_info().uss
-        self._memoryRSS = _process.memory_info().rss
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        """
+
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        """
+
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
-        return dataFrame
+            data.append([a.replace('\t', ' '), b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
+    def save(self, oFile):
+        """
 
-        :param outFile: name of the output file
-        :type outFile: file
+        Complete set of frequent patterns will be loaded in to an output file
+
+        :param oFile: name of the output file
+        :type oFile: csv file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = oFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            patternsAndSupport = x.replace(' ', '\t') + ":" + f'{y[0]}:{y[1]}'
-            writer.write("%s \n" % patternsAndSupport)
+            s1 = x.strip() + ":" + str(y)
+            writer.write("%s \n" % s1)
 
     def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+        """
+
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self):
-        print("Top K Periodic Frequent Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
-        if len(_ab._sys.argv) == 6:
-            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = TopkPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        _ap.startMine()
-        print("Top K Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+        _ap.mine()
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/topk/TopkPFP/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/periodicFrequentPattern/topk/kPFPMiner/kPFPMiner.py` & `pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py`

 * *Files 12% similar despite different names*

```diff
@@ -1,24 +1,28 @@
+# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
+# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
+# greater than maximum liability.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
-
-#             import PAMI.periodicFrequentPattern.kPFPMiner as alg
 #
-#             obj = alg.kPFPMiner(iFile, k)
+#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#
+#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
 #
 #             obj.startMine()
 #
-#             periodicFrequentPatterns = obj.getPatterns()
+#             Patterns = obj.getPatterns()
 #
-#             print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#             obj.save(oFile)
+#             obj.save("patterns")
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -45,164 +49,205 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
-
-from PAMI.periodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.periodicFrequentPattern.topk.kPFPMiner import abstract as _ab
-
+from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
 
-class kPFPMiner(_ab._periodicFrequentPatterns):
+class SPPEclat(_ab._stablePeriodicFrequentPatterns):
     """
-    :Description:   Top - K is and algorithm to discover top periodic-frequent patterns in a temporal database.
+    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
+                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
+                    greater than maximum lability.
 
-    :Reference:   Likhitha, P., Ravikumar, P., Kiran, R.U., Watanobe, Y. (2022).
-                  Discovering Top-k Periodic-Frequent Patterns in Very Large Temporal Databases. Big Data Analytics.
-                 BDA 2022. Lecture Notes in Computer Science, vol 13773. Springer, Cham. https://doi.org/10.1007/978-3-031-24094-2_14
+    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
+                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of periodic frequent pattern's
+                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
     :param  oFile: str :
-                   Name of the output file to store complete set of periodic frequent pattern's
-
+                   Name of the output file to store complete set of stable periodic Frequent Pattern.
+    :param  minSup: float or int or str :
+                    The user can specify minSup either in count or proportion of database size.
+                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                    Otherwise, it will be treated as float.
+                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+    :param  itemSup: int or float :
+                    Frequency of an item
+    :param maxLa: float :
+                  minimum loss of a pattern
     :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
 
     :Attributes:
 
-        iFile : str
-            Input file name or path of the input file
-        k: int
-            User specified counte of top-k periodic frequent patterns
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        maxLa : int or float or str
+            The user can specify maxLa either in count or proportion of database size.
+            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : str
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        finalPatterns: dict
-            Storing the complete set of patterns in a dictionary variable
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            it represents the total no of transactions
+        tree : class
+            it represents the Tree class
+        itemSetCount : int
+            it represents the total no of patterns
+        finalPatterns : dict
+            it represents to store the patterns
+        tidList : dict
+            stores the timestamps of an item
 
     :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        savePatterns(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
-        frequentOneItem()
-            Generates one frequent patterns
-        eclatGeneration(candidateList)
-            It will generate the combinations of frequent items
-        generateFrequentPatterns(tidList)
-            It will generate the combinations of frequent items from a list of items
+            Scan the database and store the items with their timestamps which are periodic frequent
+        calculateLa()
+            Calculates the support and period for a list of timestamps.
+        Generation()
+            Used to implement prefix class equivalence method to generate the periodic patterns recursively
 
-    **Executing the code on terminal:**
-    ------------------------------------------
+
+
+    **Methods to execute code on terminal**
+    -----------------------------------------
     .. code-block:: console
 
 
        Format:
 
+       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
 
-       (.venv) $ python3 kPFPMiner.py <inputFile> <outputFile> <k>
+       Example usage:
 
-       Examples :
+       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
 
-       (.venv) $  python3 kPFPMiner.py sampleDB.txt patterns.txt 10
 
+               .. note:: constraints will be considered in percentage of database transactions
 
-    **Sample run of the importing code:
-    --------------------------------------
-    .. code-block:: python
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    ... code-block:: python
 
-            import PAMI.periodicFrequentPattern.kPFPMiner as alg
+                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
 
-            obj = alg.kPFPMiner(iFile, k)
+                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
 
-            obj.startMine()
+                    obj.startMine()
 
-            periodicFrequentPatterns = obj.getPatterns()
+                    Patterns = obj.getPatterns()
 
-            print("Total number of top-k Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 
-            obj.save(oFile)
+                    obj.save("patterns")
 
-            Df = obj.getPatternInDataFrame()
+                    Df = obj.getPatternsAsDataFrame()
 
-            memUSS = obj.getMemoryUSS()
+                    memUSS = obj.getMemoryUSS()
 
-            print("Total Memory in USS:", memUSS)
+                    print("Total Memory in USS:", memUSS)
 
-            memRSS = obj.getMemoryRSS()
+                    memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", memRSS)
+                    print("Total Memory in RSS", memRSS)
 
-            run = obj.getRuntime()
+                    run = obj.getRuntime()
 
-            print("Total ExecutionTime in seconds:", run)
+                    print("Total ExecutionTime in seconds:", run)
 
     **Credits:**
     --------------
-            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
 
-    """
-
-    _startTime = float()
-    _endTime = float()
-    _k = int()
-    _finalPatterns = {}
+       """
     _iFile = " "
     _oFile = " "
+    _minSup = str()
+    _maxPer = str()
+    _maxLa = float()
     _sep = " "
+    _SPPList = {}
+    _itemList = []
+    _last = int()
+    _finalPatterns = {}
+    _tsList = {}
+    _startTime = float()
+    _endTime = float()
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _tidList = {}
-    lno = int()
-    _maximum = int()
 
-    def _creatingItemSets(self):
+    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
+        self._iFile = inputFile
+        self._minSup = minSup
+        self._maxPer = maxPer
+        self._maxLa = maxLa
+        self._sep = sep
+
+    def _creatingItemsets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
-
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-
-            # print(self.Database)
+            if 'Patterns' in i:
+                self._Database = self._iFile['Patterns'].tolist()
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
@@ -215,132 +260,14 @@
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
-                    
-    def getPer_Sup(self, tids):
-        tids.sort()
-        cur=0
-        per=list()
-        sup=0
-        #print(tids)
-        for i in range(len(tids)-1):
-            j = i + 1
-            #if tids[j] - cur <= periodicity:
-                #return [0,0]
-            per.append(tids[j] - cur)
-            cur = tids[j]
-        per.append(self.lno - cur)
-        return max(per)
-
-    def _frequentOneItem(self):
-        """
-        Generating one frequent patterns
-        """
-        self._mapSupport = {}
-        self._tidList = {}
-        n = 0
-        for line in self._Database:
-            self.lno += 1
-            n = int(line[0])
-            for i in range(1, len(line)):
-                si = line[i]
-                if self._mapSupport.get(si) is None:
-                    self._mapSupport[si] = [1, abs(0 - n), n]
-                    self._tidList[si] = [n]
-                else:
-                    self._mapSupport[si][0] += 1
-                    self._mapSupport[si][1] = max(self._mapSupport[si][1], abs(n - self._mapSupport[si][2]))
-                    self._mapSupport[si][2] = n
-                    self._tidList[si].append(n)
-        for x, y in self._mapSupport.items():
-            self._mapSupport[x][1] = max(self._mapSupport[x][1], abs(n - self._mapSupport[x][2]))
-        plist = [key for key, value in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        for i in plist:
-            if len(self._finalPatterns) >= self._k:
-                break
-            else:
-                self._finalPatterns[i] = self._mapSupport[i][1]
-        self._maximum = max([self._finalPatterns[i] for i in self._finalPatterns.keys()])
-        plist = list(self._finalPatterns.keys())
-        return plist
-
-
-    def _save(self, prefix, suffix, tidSetI):
-        """Saves the patterns that satisfy the periodic frequent property.
-
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: list
-        """
-
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self.getPer_Sup(tidSetI)
-        sample = str()
-        for i in prefix:
-            sample = sample + i + " "
-        if len(self._finalPatterns) < self._k:
-            if val < self._maximum:
-                self._finalPatterns[sample] = val
-                self._finalPatterns = {k: v for k, v in sorted(self._finalPatterns.items(), key=lambda item: item[1], reverse=True)}
-                self._maximum = max([i for i in self._finalPatterns.values()])
-        else:
-            for x, y in sorted(self._finalPatterns.items(), key=lambda x: x[1], reverse=True):
-                if val < y:
-                    del self._finalPatterns[x]
-                    self._finalPatterns[sample] = val
-                    self._finalPatterns = {k: v for k, v in
-                                              sorted(self._finalPatterns.items(), key=lambda item: item[1],
-                                                     reverse=True)}
-                    self._maximum = max([i for i in self._finalPatterns.values()])
-                    return
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y) <= self._maximum:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
 
     def _convert(self, value):
         """
         to convert the type of user specified minSup value
 
         :param value: user specified minSup value
         :return: converted type
@@ -348,137 +275,193 @@
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = ((len(self._Database)) * value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    def _createSPPList(self):
+        """
+        to convert the single length stable periodic patterns
+        """
+        tidLast = {}
+        la = {}
+        self._SPPList = {}
+        self._tsList = {}
+        for transaction in self._Database:
+            ts = int(transaction[0])
+            for item in transaction[1:]:
+                if item not in self._SPPList:
+                    la[item] = max(0, ts - self._maxPer)
+                    self._SPPList[item] = [1, la[item]]
+                    self._tsList[item] = [ts]
+                else:
+                    s = self._SPPList[item][0] + 1
+                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
+                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
+                    self._tsList[item].append(ts)
+                tidLast[item] = ts
+            self._last = ts
+        for item in self._SPPList:
+            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
+            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
+        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        self._Generation(list(self._SPPList), set())
+
+    def _Generation(self, GPPFList, CP):
+        """
+        To generate the patterns using depth-first search
+        """
+        for i in range(len(GPPFList)):
+            item = GPPFList[i]
+            CP1 = CP | {item}
+            if CP != set():
+                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
+            la = self._calculateLa(self._tsList['\t'.join(CP1)])
+            support = len(self._tsList['\t'.join(CP1)])
+            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
+                #CP = CP1
+                self._finalPatterns['\t'.join(CP1)] = [support, la]
+                if i+1 < len(GPPFList):
+                    self._Generation(GPPFList[i+1:], CP1)
+
+    def _calculateLa(self, tsList):
+        """
+        To calculate the liability of a patterns based on its timestamps
+        """
+        previous = 0
+        la = 0
+        tsList = sorted(tsList)
+        laList = []
+        for ts in tsList:
+            la = max(0, la + ts - previous - self._maxPer)
+            laList.append(la)
+            previous = ts
+            
+        la = max(0, la + self._last - previous - self._maxPer)
+        laList.append(la)
+        maxla = max(laList)
+        return maxla
+
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Main function of the program
+        Method to start the mining of patterns
+        """
+        self.mine()
 
+    def mine(self):
+        """
+        Method to start the mining of patterns
         """
         self._startTime = _ab._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._k is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._k = self._convert(self._k)
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i + 1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = list(set(tidSetI).intersection(tidSetJ))
-                if self.getPer_Sup(y1) <= self._maximum:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("kPFPMiner has successfully generated top-k frequent patterns")
+        self._creatingItemsets()
+        self._minSup = self._convert(self._minSup)
+        self._maxPer = self._convert(self._maxPer)
+        self._maxLa = self._convert(self._maxLa)
+        self._finalPatterns = {}
+        #print(self._minSup, self._maxPer, self._maxLa)
+        self._createSPPList()
         self._endTime = _ab._time.time()
         self._memoryUSS = float()
         self._memoryRSS = float()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
+        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-        :return: returning USS memory consumed by the mining process
+    def getRuntime(self):
+        """
+        Calculating the total amount of runtime taken by the mining process
+
+        :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
+        return self._endTime - self._startTime
 
-        return self._memoryUSS
+    def getPatterns(self):
+        """
+        Function to return the set of stable periodic-frequent patterns after completion of the mining process
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning stable periodic-frequent patterns
+        :rtype: dict
+        """
+        return self._finalPatterns
 
-        :return: returning RSS memory consumed by the mining process
+    def getMemoryUSS(self):
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
-
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+        return self._memoryUSS
 
-        :return: returning total amount of runtime taken by the mining process
-        :rtype: float
+    def save(self, outFile):
         """
+        Complete set of periodic-frequent patterns will be loaded in to an output file
 
-        return self._endTime - self._startTime
+        :param outFile: name of the output file
+        :type outFile: csv file
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
     def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+        """
+        Storing final periodic-frequent patterns in a dataframe
 
-        :return: returning frequent patterns in a dataframe
+        :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'periodicity'])
+            data.append([a.replace('\t', ' '), b[0], b[1]])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to a output file
-
-        :param outFile: name of the output file
-
-        :type outFile: file
+    def getMemoryRSS(self):
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            patternsAndSupport = x + ":" + str(y)
-            writer.write("%s \n" % patternsAndSupport)
-
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
-
-        :return: returning frequent patterns
-        :rtype: dict
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        :return: returning RSS memory consumed by the mining process
+        :rtype: float
         """
-        return self._finalPatterns
+
+        return self._memoryRSS
 
     def printResults(self):
-        print("Total number of  Top-k Periodic Frequent Patterns:", len(self.getPatterns()))
+        """
+        This function is used to print the results
+        """
+        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
-
+        print("Total ExecutionTime in ms:", self.getRuntime())
 
-if __name__ == "__main__":
+if __name__ == '__main__':
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = kPFPMiner(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of top-k periodic frequent patterns:", len(_Patterns))
+        _ap.mine()
+        print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
-
```

### Comparing `pami-2024.4.9.1/PAMI/recurringPattern/basic/RPGrowth.py` & `pami-2024.5.1/PAMI/recurringPattern/basic/RPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/recurringPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/recurringPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py` & `pami-2024.5.1/PAMI/relativeFrequentPattern/basic/RSFPGrowth.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/relativeFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/relativeFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/relativeHighUtilityPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/relativeHighUtilityPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPADE.py` & `pami-2024.5.1/PAMI/sequentialPatternMining/basic/SPADE.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/SPAM.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/TUFP.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,27 +1,29 @@
-# SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
-# This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
-#  This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
+# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
+#             from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+#
+#             iFile = 'sampleDB.txt'
 #
-#             import PAMI.sequentialPatternMining.basic.SPAM as alg
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj = alg.SPAM(iFile, minSup)
+#             obj = alg.TUFP(iFile, minSup)
 #
 #             obj.startMine()
 #
-#             sequentialPatternMining = obj.getPatterns()
+#             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
@@ -29,495 +31,544 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-
+from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Union
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.sequentialPatternMining.basic import abstract as _ab
-_ab._sys.setrecursionlimit(10000)
+_minSup = float()
+_finalPatterns = {}
+
 
-class SPAM(_ab._sequentialPatterns):
+class _Item:
     """
-    :Description:    SPAM is one of the fundamental algorithm to discover sequential frequent patterns in a transactional database.
-                     This program employs SPAM property (or downward closure property) to  reduce the search space effectively.
-                     This algorithm employs breadth-first search technique  to find the complete set of frequent patterns in a sequential database.
-
-    :Reference:   J. Ayres, J. Gehrke, T.Yiu, and J. Flannick. Sequential Pattern Mining Using Bitmaps. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Edmonton, Alberta, Canada, July 2002.
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of  Sequential frequent patterns
-    :param  oFile: str :
-                   Name of the output file to store complete set of  Sequential frequent patterns
-    :param  minSup: float or int or str :
-                    minSup measure constraints the minimum number of transactions in a database where a pattern must appear
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-            iFile : str
-                Input file name or path of the input file
-            oFile : str
-                Name of the output file or the path of output file
-            minSup : float or int or str
-                The user can specify minSup either in count or proportion of database size.
-                If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                Otherwise, it will be treated as float.
-                Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-            sep : str
-                This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-                However, the users can override their default separator.
-            startTime : float
-                To record the start time of the mining process
-            endTime : float
-                To record the completion time of the mining process
-            finalPatterns : dict
-                Storing the complete set of patterns in a dictionary variable
-            memoryUSS : float
-                To store the total amount of USS memory consumed by the program
-            memoryRSS : float
-                To store the total amount of RSS memory consumed by the program
-            Database : list
-                To store the sequences of a database in list
-            _idDatabase : dict
-                To store the sequences of a database by bit map
-            _maxSeqLen:
-                the maximum length of subsequence in sequence.
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
+
+    def __init__(self, item, probability) -> None:
+        self.item = item
+        self.probability = probability
+
+
+class TUFP(_ab._frequentPatterns):
+    """
+    About this algorithm
+    ====================
+
+    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+
+    :Reference:  Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+                 Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+
+    :Attributes:
+
+        iFile : file
+            Name of the Input file or path of the input file
+
+        oFile : file
+            Name of the output file or path of the output file
+
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+
+        startTime : float
+            To record the start time of the mining process
+
+        endTime : float
+            To record the completion time of the mining process
+
+        Database : list
+            To store the transactions of a database in list
+
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+
+        lno : int
+            To represent the total no of transaction
+
+        tree : class
+            To represents the Tree class
+
+        itemSetCount : int
+            To represents the total no of patterns
+
+        finalPatterns : dict
+            To store the complete patterns
 
     :Methods:
+        mine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        storePatternsInFile(oFile)
+            Complete set of frequent patterns will be loaded in to a output file
+        getPatternsInDataFrame()
+            Complete set of frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        frequentOneItem()
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+        startMine()
+            Mining process will start from this function
+
+    Execution methods
+    =================
 
-            _creatingItemSets():
-                Storing the complete sequences of the database/input file in a database variable
-            _convert(value):
-                To convert the user specified minSup value
-            make2BitDatabase():
-                To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
-            DfsPruning(items,sStep,iStep):
-                the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
-            Sstep(s):
-                To convert bit to ssteo bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
-            startMine()
-                Mining process will start from here
-            getPatterns()
-                Complete set of patterns will be retrieved with this function
-            savePatterns(oFile)
-                Complete set of frequent patterns will be loaded in to a output file
-            getPatternsAsDataFrame()
-                Complete set of frequent patterns will be loaded in to a dataframe
-            getMemoryUSS()
-                Total amount of USS memory consumed by the mining process will be retrieved from this function
-            getMemoryRSS()
-                Total amount of RSS memory consumed by the mining process will be retrieved from this function
-            getRuntime()
-                Total amount of runtime taken by the mining process will be retrieved from this function
-            candidateToFrequent(candidateList)
-                Generates frequent patterns from the candidate patterns
-            frequentToCandidate(frequentList, length)
-                Generates candidate patterns from the frequent patterns
+
+    **Terminal command**
 
 
-    **Executing the code on terminal**:
-    ----------------------------------------
     .. code-block:: console
 
+      Format:
+
+      (.venv) $ python3 TUFP.py <inputFile> <outputFile> <minSup>
 
-       Format:
+      Example Usage:
 
-       (.venv) $ python3 SPAM.py <inputFile> <outputFile> <minSup> (<separator>)
+      (.venv) $ python3 TUFP.py sampleDB.txt patterns.txt 0.6
 
-       Examples usage:
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
-       (.venv) $ python3 SPAM.py sampleDB.txt patterns.txt 10.0
 
+    **Calling from a python program**
 
-               .. note:: minSup will be considered in times of minSup and count of database transactions
+    .. code-block:: python
 
-    **Sample run of the importing code**:
-    -------------------------------------
-            import PAMI.sequentialPatternMining.basic.SPAM as alg
+            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 
-            obj = alg.SPAM(iFile, minSup)
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            obj = alg.TUFP(iFile, minSup)
 
             obj.startMine()
 
-            sequentialPatternMining = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.savePatterns(oFile)
+            obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits**:
-    ------------
-            The complete program was written by Shota Suzuki  under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
+
+
+            The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
 
-    _minSup = float()
     _startTime = float()
     _endTime = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _idDatabase={}
-    _maxSeqLen=0
-    def _creatingItemSets(self):
+    _cupList = {}
+    _topk = {}
+    _minimum = 9999
+
+
+
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete sequences of the database/input file in a database variable
+        Scans the dataset
         """
         self._Database = []
-
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            temp = []
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                temp = self._iFile['Transactions'].tolist()
-            if "tid" in i:
-                temp2=self._iFile[''].tolist()
-            addList=[]
-            addList.append(temp[0])
-            for k in range(len(temp)-1):
-                if temp2[k]==temp[k+1]:
-                    addList.append(temp[k+1])
-                else:
-                    self._Database.append(addList)
-                    addList=[]
-                    addList.append(temp[k+1])
-            self._Database.append(addList)
+                self._Database = self._iFile['Transactions'].tolist()
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+
+            # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    temp.pop()
+                    tr = []
+                    for i in temp:
+                        i1 = i.index('(')
+                        i2 = i.index(')')
+                        item = i[0:i1]
+                        probability = float(i[i1 + 1:i2])
+                        product = _Item(item, probability)
+                        tr.append(product)
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split('-1')]
-                            temp = [x for x in temp if x ]
-                            temp.pop()
-
-                            seq = []
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            tr = []
                             for i in temp:
-                                k = -2
-                                if len(i)>1:
-                                    seq.append(list(sorted(set(i.split()))))
+                                i1 = i.index('(')
+                                i2 = i.index(')')
+                                item = i[0:i1]
+                                probability = float(i[i1 + 1:i2])
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._Database.append(tr)
+                except IOError:
+                    print("File Not Found")
 
-                                else:
-                                    seq.append(i)
+    def _frequentOneItem(self) -> List[str]:
+        """
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
 
-                            self._Database.append(seq)
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        """
 
-                except IOError:
-                    print("File Not Found")
-                    quit()
+        mapSupport = {}
+        k = 0
+        for i in self._Database:
+            k += 1
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = j.probability
+                    self._cupList[j.item] = {k:j.probability}
+                else:
+                    mapSupport[j.item] += j.probability
+                    self._cupList[j.item].update({k: j.probability})
+        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        k = 0
+        for x, in plist:
+            k +=1
+            if k >= self._minSup:
+                break
+            self._finalPatterns[x] = mapSupport[x]
+        self._minimum = min(list(self._finalPatterns.values()))
+        return plist
 
-    def _convert(self, value):
+    @staticmethod
+    def _convert(value: Union[int, float, str]) -> Union[int, float]:
         """
-        To convert the user specified minSup value
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
-        :return: converted type
+        :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = float(value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-
-    def make2BitDatabase(self):
+    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
         """
-        To make 1 length frequent patterns by breadth-first search technique   and update Database to sequential database
+        Saves the patterns that satisfy the periodic frequent property.
+
+        :param prefix: the prefix of a pattern
+        :type prefix: list
+        :param suffix: the suffix of a patterns
+        :type suffix: list
+        :param tidSetI: the timestamp of a patterns
+        :type tidSetI: dict
+        """
+
+        if prefix is None:
+            prefix = suffix
+        else:
+            prefix = prefix + suffix
+        val = sum(tidSetI.values())
+        # print(prefix, val)
+        if len(self._finalPatterns) <= self._minSup:
+            sample = str()
+            for i in prefix:
+                sample = sample + i + " "
+            self._finalPatterns[sample] = val
+        if len(self._finalPatterns) == self._minSup:
+            if val > self._minimum:
+                sample = str()
+                for i in prefix:
+                    sample = sample + i + " "
+                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
+                del self._finalPatterns[index]
+                self._finalPatterns[sample] = val
+                self._minimum = min(list(self._finalPatterns.values()))
+        # print(self.finalPatterns, self.minimum, self.minSup)
+
+    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
+        """
+        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
+        :param prefix:  main equivalence prefix
+        :type prefix: periodic-frequent item or pattern
+        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
+        :type itemSets: list
+        :param tidSets: timestamps of the items in the argument itemSets
+        :type tidSets: list
+        """
+        if len(itemSets) == 1:
+            i = itemSets[0]
+            tidI = tidSets[0]
+            self._save(prefix, [i], tidI)
+            return
+        for i in range(0, len(itemSets)):
+            itemI = itemSets[i]
+            if itemI is None:
+                continue
+            tidSetI = tidSets[i]
+            classItemSets = []
+            classTidSets = []
+            itemSetX = [itemI]
+            for j in range(i + 1, len(itemSets)):
+                itemJ = itemSets[j]
+                tidSetJ = tidSets[j]
+                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                sum2 = sum(list(y.values()))
+                # print(prefix, itemJ, y, sum2)
+                # if sum2 >= self.minimum:
+                self._save(prefix, [itemJ], y)
+                classItemSets.append(itemJ)
+                classTidSets.append(y)
+            # print(itemI, tidSetI, classItemSets)
+            newPrefix = list(set(itemSetX)) + prefix
+            self._Generation(newPrefix, classItemSets, classTidSets)
+            # self.save(prefix, list(set(itemSetX)), tidSetI)
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self._maxSeqLen=max([len(i) for i in self._Database])
-        lineNumber=0
-        idDatabase={}
-        for line in self._Database:
-            seqNumber=1
-            for seq in line:
-
-                for data in seq:
-                    if data in idDatabase:
-                        while lineNumber+1!=len(idDatabase[data]):
-                            idDatabase[data].append(0)
-                        idDatabase[data][lineNumber]+=int(2**(self._maxSeqLen-seqNumber))
-
-                    else:
-                        idDatabase[data]=[]
-                        while lineNumber+1!=len(idDatabase[data]):
-                            idDatabase[data].append(0)
-                        idDatabase[data][lineNumber]+=(int(2 ** (self._maxSeqLen-seqNumber)))
-
-                seqNumber+=1
-            lineNumber+=1
-        for key,val in idDatabase.items():
-
-            sup=self.countSup(val)
-            while lineNumber+1!=len(idDatabase[key]):
-                            idDatabase[key].append(0)
-            if sup>=self._minSup:
-                self._finalPatterns[str(key)+self._sep+"-2"]=sup
-                self._idDatabase[str(key)]=val
-
-    def DfsPruning(self,items,sStep,iStep):
-        """
-        the main algorithm of spam. This can search sstep and istep items and find next patterns, its sstep, and its istep. And call this function again by using them. Recursion until there are no more items available for exploration.
-
-        :Attributes:
-
-        items : str
-            The pattrens I got before
-        sStep : list
-            Items presumed to have "sstep" relationship with "items".(sstep is What appears later like a-b and a-c)
-        iStep : list
-            Items presumed to have "istep" relationship with "items"(istep is What appears in same time like ab and ac)
-
-        """
-        Snext=[]
-        Inext=[]
-        ns = self.Sstep(self._idDatabase[items])
-        for i in sStep:
-            nnext=[]
-            for k in  range(len(self._idDatabase[items])):
-                nandi=ns[k] & self._idDatabase[i][k]
-                nnext.append(nandi)
-
-
-            sup=self.countSup(nnext)
-            if sup>=self._minSup:
-                key=items+self._sep+"-1"+self._sep+i
-                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
-                self._idDatabase[key]=nnext
-                Snext.append(i)
-
-        for i in Snext:
-            key = items+self._sep+"-1"+self._sep+i
-            self.DfsPruning(key,Snext,[k for k in Snext if self._Database.index(i)<self._Database.index(k)])
-        for i in iStep:
-            nnext = []
-
-            for k in range(len(self._idDatabase[items])):
-                nandi = self._idDatabase[items][k] & self._idDatabase[i][k]
-                nnext.append(nandi)
-            sup=self.countSup(nnext)
-            if sup>=self._minSup:
-                key=items+self._sep+str(i)
-                self._finalPatterns[key+self._sep+"-1"+self._sep+"-2"]=sup
-                self._idDatabase[key]=nnext
-                Inext.append(i)
-        for i in Inext:
-            key = items +self._sep +str(i)
-            self.DfsPruning(key,Snext,[k for k in Inext if self._Database.index(i)<self._Database.index(k)])
-
-    def Sstep(self,s):
-        """
-        To convert bit to Sstep bit.The first time you get 1, you set it to 0 and subsequent ones to 1.(like 010101=>001111, 00001001=>00000111)
-
-
-        :param s:list
-            to store each bit sequence
-        :return:
-            nextS:list to store the bit sequence converted by sstep
-
-        """
-        nextS=[]
-        for bins in s:
-            binS=str(bin(bins))
-
-
-            LenNum=2
-            for i in range(len(binS)-2):
-                if binS[LenNum] == "1":
-
-                    binS = binS[:LenNum] + "0" + binS[LenNum + 1:]
-                    while len(binS)-1!=LenNum:
-                        LenNum += 1
-                        binS = binS[:LenNum] + "1" + binS[LenNum + 1:]
-                    break
-                LenNum+=1
-            nextS.append(int(binS, 0))
-
-
-        return nextS
-
-    def countSup(self,n):
-        """
-        count support
-
-        :param n:list
-                to store each bit sequence
-        :return:
-            count: int support of this list
-        """
-        count=0
-        for i in n:
-            if "1" in str(bin(i)):
-                count+=1
-        return count
+        self.mine()
 
-    def startMine(self):
+    def mine(self) -> None:
         """
-        Frequent pattern mining process will start from here
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        self._Database = []
+        global _minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
-        self.make2BitDatabase()
-        self._Database = [i for i in self._idDatabase.keys()]
-        for i in self._Database:
-            x=[]
-            for j in self._Database:
-                if self._Database.index(i)<self._Database.index(j):
-                    x.append(j)
-
-            self.DfsPruning(i,self._Database,x)
+        _minSup = self._minSup
+        plist = self._frequentOneItem()
+        for i in range(len(plist)):
+            itemI = plist[i]
+            tidSetI = self._cupList[itemI]
+            itemSetX = [itemI]
+            itemSets = []
+            tidSets = []
+            for j in range(i + 1, len(plist)):
+                itemJ = plist[j]
+                tidSetJ = self._cupList[itemJ]
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                self._save(itemSetX, [itemJ], y1)
+                itemSets.append(itemJ)
+                tidSets.append(y1)
+            self._Generation(itemSetX, itemSets, tidSets)
+        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Frequent patterns were generated successfully using Apriori algorithm ")
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def getMemoryUSS(self) -> float:
+        """
+
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """
+
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """
+
+        Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+        """
+
+        Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataFrame = {}
+        dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a, b])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataFrame
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataframe
+
+    def save(self, outFile: str) -> None:
+        """
+
+        Complete set of frequent patterns will be loaded in to an output file
 
-    def save(self, outFile):
-        """Complete set of frequent patterns will be loaded in to an output file
         :param outFile: name of the output file
         :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
+    def getPatterns(self) -> Dict[str, float]:
+        """
+
+        Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:", self.getRuntime())
 
+    if __name__ == "__main__":
+        _ap = str()
+        if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+            if len(_ab._sys.argv) == 5:
+                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            if len(_ab._sys.argv) == 4:
+                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap.startMine()
+            _ap.mine()
+            _Patterns = _ap.getPatterns()
+            print("Total number of Patterns:", len(_Patterns))
+            _ap.save(_ab._sys.argv[2])
+            _memUSS = _ap.getMemoryUSS()
+            print("Total Memory in USS:", _memUSS)
+            _memRSS = _ap.getMemoryRSS()
+            print("Total Memory in RSS", _memRSS)
+            _run = _ap.getRuntime()
+            print("Total ExecutionTime in ms:", _run)
+        else:
+            '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+            ap.startMine()
+            Patterns = ap.getPatterns()
+            print("Total number of Patterns:", len(Patterns))
+            ap.save("patterns.txt")
+            memUSS = ap.getMemoryUSS()
+            print("Total Memory in USS:", memUSS)
+            memRSS = ap.getMemoryRSS()
+            print("Total Memory in RSS", memRSS)
+            run = ap.getRuntime()
+            print("Total ExecutionTime in ms:", run)'''
+            print("Error! The number of input parameters do not match the total number of parameters provided")
 
-if __name__ == "__main__":
-    _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = SPAM(_ab._sys.argv[1], _ab._sys.argv[3])
-        _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(_Patterns))
-        _ap.savePatterns(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
-    else:
-
-        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/abstract.py` & `pami-2024.5.1/PAMI/sequentialPatternMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/sequentialPatternMining/basic/prefixSpan.py` & `pami-2024.5.1/PAMI/sequentialPatternMining/basic/prefixSpan.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/sequentialPatternMining/closed/abstract.py` & `pami-2024.5.1/PAMI/sequentialPatternMining/closed/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPEclat.py` & `pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,26 +1,22 @@
-# Stable periodic pattern mining aims to discover all interesting patterns in a temporal database using three constraints minimum support,
-# maximum period and maximum liability, that have support no less than the user-specified minimum support  constraint and liability no
-# greater than maximum liability.
-#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
+#             from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowthDump as alg
 #
-#             obj = alg.SPPEclat("../basic/sampleTDB.txt", 5, 3, 3)
+#             obj = alg.SPPGrowthDump(iFile, minSup, maxPer, maxLa)
 #
 #             obj.startMine()
 #
 #             Patterns = obj.getPatterns()
 #
 #             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
 #
-#             obj.save("patterns")
+#             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -49,208 +45,275 @@
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
+
+from PAMI.stableperiodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
-from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
+from urllib.request import urlopen
+import validators
+import pandas as pd
+import resource
+import time
+import sys
+import os
+import psutil
 
-class SPPEclat(_ab._stablePeriodicFrequentPatterns):
-    """
-    :Description:   Stable periodic pattern mining aims to dicover all interesting patterns in a temporal database using three contraints minimum support,
-                    maximum period and maximum lability, that have support no less than the user-specified minimum support  constraint and lability no
-                    greater than maximum lability.
-
-    :Reference:   Fournier-Viger, P., Yang, P., Lin, J. C.-W., Kiran, U. (2019). Discovering Stable Periodic-Frequent Patterns in Transactional Data. Proc.
-                  32nd Intern. Conf. on Industrial, Engineering and Other Applications of Applied Intelligent Systems (IEA AIE 2019), Springer LNAI, pp. 230-244
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of stable periodic Frequent Pattern.
-    :param  oFile: str :
-                   Name of the output file to store complete set of stable periodic Frequent Pattern.
-    :param  minSup: float or int or str :
-                    The user can specify minSup either in count or proportion of database size.
-                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                    Otherwise, it will be treated as float.
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-    :param  itemSup: int or float :
-                    Frequency of an item
-    :param maxLa: float :
-                  minimum loss of a pattern
-    :param  sep: str :
-                 This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-
-
-    :Attributes:
-
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : int or float or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        maxPer : int or float or str
-            The user can specify maxPer either in count or proportion of database size.
-            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
-        maxLa : int or float or str
-            The user can specify maxLa either in count or proportion of database size.
-            If the program detects the data type of maxLa is integer, then it treats maxLa is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: maxLa=10 will be treated as integer, while maxLa=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            it represents the total no of transactions
-        tree : class
-            it represents the Tree class
-        itemSetCount : int
-            it represents the total no of patterns
-        finalPatterns : dict
-            it represents to store the patterns
-        tidList : dict
-            stores the timestamps of an item
-
-    :Methods:
-
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        save(oFile)
-            Complete set of periodic-frequent patterns will be loaded in to an output file
-        getPatternsAsDataFrame()
-            Complete set of periodic-frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scan the database and store the items with their timestamps which are periodic frequent
-        calculateLa()
-            Calculates the support and period for a list of timestamps.
-        Generation()
-            Used to implement prefix class equivalence method to generate the periodic patterns recursively
-
-
-
-    **Methods to execute code on terminal**
-    -----------------------------------------
-    .. code-block:: console
-
-
-       Format:
-
-       (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer> <maxLa>
-
-       Example usage:
-
-       (.venv) $ python3 basic.py sampleDB.txt patterns.txt 10.0 4.0 2.0
-
-
-               .. note:: constraints will be considered in percentage of database transactions
-
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------
-    ... code-block:: python
-
-                    from PAMI.stablePeriodicFrequentPattern.basic import basic as alg
-
-                    obj = alg.PFPECLAT("../basic/sampleTDB.txt", 5, 3, 3)
-
-                    obj.startMine()
-
-                    Patterns = obj.getPatterns()
-
-                    print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
-
-                    obj.save("patterns")
-
-                    Df = obj.getPatternsAsDataFrame()
-
-                    memUSS = obj.getMemoryUSS()
-
-                    print("Total Memory in USS:", memUSS)
-
-                    memRSS = obj.getMemoryRSS()
-
-                    print("Total Memory in RSS", memRSS)
-
-                    run = obj.getRuntime()
-
-                    print("Total ExecutionTime in seconds:", run)
-
-    **Credits:**
-    --------------
-             The complete program was written by  P.Likhitha under the supervision of Professor Rage Uday Kiran.
+_minSup = int()
+_maxPer = int()
+_maxLa = int()
+_last = int()
 
-       """
-    _iFile = " "
-    _oFile = " "
+
+class _Node:
+
+    def __init__(self, item, children):
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        """
+
+        self.item = item
+        self.children = children
+        self.parent = None
+        self.timeStamps = []
+
+    def addChild(self, node):
+        """
+        To add the children to a node
+
+        :param node: parent node in the tree
+        """
+
+        self.children[node.item] = node
+        node.parent = self
+
+class _Tree:
+    def __init__(self):
+        self.root = _Node(None, {})
+        self.summaries = {}
+        self.info = {}
+
+    def addTransaction(self, transaction, tid):
+        """
+        Adding a transaction into tree
+
+        :param transaction: To represent the complete database
+        :type transaction: list
+        :param tid: To represent the timestamp of a database
+        :type tid: list
+        :return: pfp-growth tree
+        """
+
+        currentNode = self.root
+        for i in range(len(transaction)):
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
+                currentNode.addChild(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
+                else:
+                    self.summaries[transaction[i]] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i]]
+        currentNode.timeStamps = currentNode.timeStamps + tid
+
+    def getConditionalPatterns(self, alpha):
+        """
+        Generates all the conditional patterns of a respective node
+
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        """
+        finalPatterns = []
+        finalSets = []
+        for i in self.summaries[alpha]:
+            set1 = i.timeStamps
+            set2 = []
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
+                i = i.parent
+            if len(set2) > 0:
+                set2.reverse()
+                finalPatterns.append(set2)
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
+        return finalPatterns, finalSets, info
+
+    @staticmethod
+    def generateTimeStamps(node):
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
+
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
+        """
+
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getTimeStamps(self, alpha):
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
+        """
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
+
+    @staticmethod
+    def getSupportAndPeriod(timeStamps):
+        """
+        To calculate the periodicity and support
+
+        :param timeStamps: Timestamps of an item set
+        :return: support, periodicity
+        """
+        global _maxPer, _last
+        previous = 0
+        la = 0
+        tsList = sorted(timeStamps)
+        for ts in tsList:
+            la = max(0, la + ts - previous - _maxPer)
+            previous = ts
+        la = max(0, la + _last - previous - _maxPer)
+        return len(timeStamps), la
+
+    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps):
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        """
+
+        global _maxPer, _minSup, _maxLa
+        pat = []
+        timeStamps = []
+        data1 = {}
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
+                if j in data1:
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                else:
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
+        for m in data1:
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _minSup and v[1] <= _maxLa}
+        count = 0
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+            if len(trans) > 0:
+                pat.append(trans)
+                timeStamps.append(conditionalTimeStamps[count])
+            count += 1
+        return pat, timeStamps, updatedDictionary
+
+    def generatePatterns(self, prefix):
+        """
+        Generates the patterns
+
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
+        """
+
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+            pattern = prefix[:]
+            pattern.append(i)
+            yield pattern, self.info[i]
+            patterns, timeStamps, info = self.getConditionalPatterns(i)
+            conditionalTree = _Tree()
+            conditionalTree.info = info.copy()
+            for pat in range(len(patterns)):
+                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+            if len(patterns) > 0:
+                for q in conditionalTree.generatePatterns(pattern):
+                    yield q
+            self.removeNode(i)
+
+class SPPGrowth():
+    _startTime = float()
+    _endTime = float()
     _minSup = str()
-    _maxPer = str()
+    _maxPer = float()
     _maxLa = float()
-    _sep = " "
-    _SPPList = {}
-    _itemList = []
-    _last = int()
     _finalPatterns = {}
-    _tsList = {}
-    _startTime = float()
-    _endTime = float()
+    _iFile = " "
+    _oFile = " "
+    _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
+    _rank = {}
+    _rankedUp = {}
+    _lno = 0
+    SPPList = {}
 
     def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
         self._iFile = inputFile
         self._minSup = minSup
         self._maxPer = maxPer
         self._maxLa = maxLa
         self._sep = sep
 
-    def _creatingItemsets(self):
+    def _creatingItemSets(self):
         """
         Storing the complete transactions of the database/input file in a database variable
         """
         self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
+        if isinstance(self._iFile, pd.DataFrame):
+            data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
+            if 'TS' in i:
+                ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'Patterns' in i:
-                self._Database = self._iFile['Patterns'].tolist()
+                data = self._iFile['Transactions'].tolist()
+            for i in range(len(data)):
+                tr = [ts[i][0]]
+                tr = tr + data[i]
+                self._Database.append(tr)
+
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if validators.url(self._iFile):
+                data = urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
@@ -261,220 +324,253 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _convert(self, value):
-        """
-        to convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-        :return: converted type
+    def _periodicFrequentOneItem(self):
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
+        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
 
-    def _createSPPList(self):
-        """
-        to convert the single length stable periodic patterns
+        :returns: return the one-length periodic frequent patterns
         """
+        global _last
         tidLast = {}
         la = {}
-        self._SPPList = {}
-        self._tsList = {}
         for transaction in self._Database:
             ts = int(transaction[0])
             for item in transaction[1:]:
-                if item not in self._SPPList:
+                if item not in self.SPPList:
                     la[item] = max(0, ts - self._maxPer)
-                    self._SPPList[item] = [1, la[item]]
-                    self._tsList[item] = [ts]
+                    self.SPPList[item] = [1, la[item]]
                 else:
-                    s = self._SPPList[item][0] + 1
+                    s = self.SPPList[item][0] + 1
                     la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self._SPPList[item] = [s, max(la[item], self._SPPList[item][1])]
-                    self._tsList[item].append(ts)
+                    self.SPPList[item] = [s, max(la[item], self.SPPList[item][1])]
                 tidLast[item] = ts
-            self._last = ts
-        for item in self._SPPList:
-            la[item] = max(0, la[item] + self._last - tidLast[item] - self._maxPer)
-            self._SPPList[item][1] = max(la[item], self._SPPList[item][1])
-        self._SPPList = {k: v for k, v in self._SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self._SPPList = {k: v for k, v in sorted(self._SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        self._Generation(list(self._SPPList), set())
-
-    def _Generation(self, GPPFList, CP):
-        """
-        To generate the patterns using depth-first search
-        """
-        for i in range(len(GPPFList)):
-            item = GPPFList[i]
-            CP1 = CP | {item}
-            if CP != set():
-                self._tsList['\t'.join(CP1)] = list(set(self._tsList['\t'.join(CP)]) & set(self._tsList[item]))
-            la = self._calculateLa(self._tsList['\t'.join(CP1)])
-            support = len(self._tsList['\t'.join(CP1)])
-            if la <= self._maxLa and len(self._tsList['\t'.join(CP1)]) >= self._minSup:
-                #CP = CP1
-                self._finalPatterns['\t'.join(CP1)] = [support, la]
-                if i+1 < len(GPPFList):
-                    self._Generation(GPPFList[i+1:], CP1)
+            _last = ts
+        for item in self.SPPList:
+            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
+            self.SPPList[item][1] = max(la[item], self.SPPList[item][1])
+        self.SPPList = {k: v for k, v in self.SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
+        self.SPPList = {k: v for k, v in sorted(self.SPPList.items(), key=lambda x: x[1][0], reverse=True)}
+        data = self.SPPList
+        pfList = [k for k, v in sorted(data.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
+        #print(len(pfList))
+        return data, pfList
+
+    def _updateDatabases(self, dict1):
+        """
+        Remove the items which are not frequent from database and updates the database with rank of items
+
+        :param dict1: frequent items with support
+        :type dict1: dictionary
+        :return: Sorted and updated transactions
+        """
+        list1 = []
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
+                if tr[i] in dict1:
+                    list2.append(self._rank[tr[i]])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort()
+                list2[1:] = basket[0:]
+                list1.append(list2)
+        return list1
+
+    @staticmethod
+    def _buildTree(data, info):
+        """
+        It takes the database and support of each item and construct the main tree by setting root node as a null
+
+        :param data: it represents the one Database in database
+        :type data: list
+        :param info: it represents the support of each item
+        :type info: dictionary
+        :return: returns root node of tree
+        """
+
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            set1 = [data[i][0]]
+            rootNode.addTransaction(data[i][1:], set1)
+        return rootNode
+
+    def _savePeriodic(self, itemSet):
+        """
+        To convert the ranks of items in to their original item names
+
+        :param itemSet: frequent pattern.
+        :return: frequent pattern with original item names
+        """
+        t1 = str()
+        for i in itemSet:
+            t1 = t1 + self._rankedUp[i] + " "
+        return t1
 
-    def _calculateLa(self, tsList):
+    def _convert(self, value):
         """
-        To calculate the liability of a patterns based on its timestamps
+        To convert the given user specified value
+
+        :param value: user specified value
+        :return: converted value
         """
-        previous = 0
-        la = 0
-        tsList = sorted(tsList)
-        laList = []
-        for ts in tsList:
-            la = max(0, la + ts - previous - self._maxPer)
-            laList.append(la)
-            previous = ts
-            
-        la = max(0, la + self._last - previous - self._maxPer)
-        laList.append(la)
-        maxla = max(laList)
-        return maxla
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
-        Method to start the mining of patterns
+        Mining process will start from this function
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._maxLa = self._convert(self._maxLa)
-        self._finalPatterns = {}
-        #print(self._minSup, self._maxPer, self._maxLa)
-        self._createSPPList()
-        self._endTime = _ab._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
 
-    def Mine(self):
+        self.mine()
+
+    def mine(self):
         """
-        Method to start the mining of patterns
+        Mining process will start from this function
         """
-        self._startTime = _ab._time.time()
-        self._creatingItemsets()
+
+        global _minSup, _maxPer, _lno, _maxLa
+        self._startTime = time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         self._maxPer = self._convert(self._maxPer)
         self._maxLa = self._convert(self._maxLa)
+        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
+        print(_minSup, _maxPer, _maxLa)
+        if self._minSup > len(self._Database):
+            raise Exception("Please enter the minSup in range between 0 to 1")
+        generatedItems, pfList = self._periodicFrequentOneItem()
+        updatedDatabases = self._updateDatabases(generatedItems)
+        for x, y in self._rank.items():
+            self._rankedUp[y] = x
+        info = {self._rank[k]: v for k, v in generatedItems.items()}
+        Tree = self._buildTree(updatedDatabases, info)
+        patterns = Tree.generatePatterns([])
         self._finalPatterns = {}
-        #print(self._minSup, self._maxPer, self._maxLa)
-        self._createSPPList()
-        self._endTime = _ab._time.time()
+        for i in patterns:
+            sample = self._savePeriodic(i[0])
+            self._finalPatterns[sample] = i[1]
+        self._endTime = time.time()
+        process = psutil.Process(os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
-        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using basic algorithm ")
+        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
 
 
-    def getRuntime(self):
-        """
-        Calculating the total amount of runtime taken by the mining process
+    def getMemoryUSS(self):
+        """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
-        :return: returning total amount of runtime taken by the mining process
+        :return: returning USS memory consumed by the mining process
         :rtype: float
         """
-        return self._endTime - self._startTime
 
-    def getPatterns(self):
-        """
-        Function to return the set of stable periodic-frequent patterns after completion of the mining process
+        return self._memoryUSS
 
-        :return: returning stable periodic-frequent patterns
-        :rtype: dict
-        """
-        return self._finalPatterns
+    def getMemoryRSS(self):
+        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
-    def getMemoryUSS(self):
-        """
-        Total amount of USS memory consumed by the mining process will be retrieved from this function
-        :return: returning USS memory consumed by the mining process
+        :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self._memoryRSS
 
-    def save(self, outFile):
-        """
-        Complete set of periodic-frequent patterns will be loaded in to an output file
+    def getRuntime(self):
+        """Calculating the total amount of runtime taken by the mining process
 
-        :param outFile: name of the output file
-        :type outFile: csv file
+        :return: returning total amount of runtime taken by the mining process
+        :rtype: float
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
-            writer.write("%s \n" % s1)
+
+        return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
-        """
-        Storing final periodic-frequent patterns in a dataframe
+        """Storing final periodic-frequent patterns in a dataframe
 
         :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b[0], b[1]])
-            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            data.append([a, b[0], b[1]])
+            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def getMemoryRSS(self):
+    def save(self, outFile):
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        :return: returning RSS memory consumed by the mining process
-        :rtype: float
+        Complete set of periodic-frequent patterns will be loaded in to an output file
+
+        :param outFile: name of the output file
+        :type outFile: csv file
         """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            writer.write("%s \n" % s1)
 
-        return self._memoryRSS
+    def getPatterns(self):
+        """ Function to send the set of periodic-frequent patterns after completion of the mining process
 
-    def printResults(self):
-        """
-        This function is used to print the results
+        :return: returning periodic-frequent patterns
+        :rtype: dict
         """
-        print("Total number of Stable Periodic  Patterns:", len(self.getPatterns()))
-        print("Total Memory in USS:", self.getMemoryUSS())
-        print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        return self._finalPatterns
 
-if __name__ == '__main__':
+
+if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
-        if len(_ab._sys.argv) == 7:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
-        if len(_ab._sys.argv) == 6:
-            _ap = SPPEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+    if len(sys.argv) == 5 or len(sys.argv) == 6:
+        if len(sys.argv) == 6:
+            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
+        if len(sys.argv) == 5:
+            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4])
         _ap.startMine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
-        _ap.save(_ab._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        _ap.mine()
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save(sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
     else:
+        '''ap = topk('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_retail.csv', 0.001, 0.005, 0.004)
+        #ap = topk('/Users/likhitha/Downloads/contextPrefixSpan.txt', 3, 6, 2, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Frequent Patterns:", len(Patterns))
+        ap.save('/Users/Likhitha/Downloads/output')
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

### Comparing `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py` & `pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -53,14 +53,16 @@
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
 
 from PAMI.stablePeriodicFrequentPattern.basic import abstract as _ab
+from deprecated import deprecated
+
 
 _minSup = int()
 _maxPer = int()
 _maxLa = int()
 _last = int()
 
 
@@ -577,19 +579,27 @@
             if '.' in value:
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
+    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
     def startMine(self):
         """
         Mining process will start from this function
         """
 
+        self.mine()
+
+    def mine(self):
+        """
+        Mining process will start from this function
+        """
+
         global _minSup, _maxPer, _lno, _maxLa
         self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
@@ -700,14 +710,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
         if len(_ab._sys.argv) == 7:
             _ap = SPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         if len(_ab._sys.argv) == 6:
             _ap = SPPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/SPPGrowthDump.py` & `pami-2024.5.1/PAMI/periodicFrequentPattern/basic/PFPGrowth.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,20 +1,22 @@
+# PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
+#
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
-#             from PAMI.stablePeriodicFrequentPattern.basic import SPPGrowthDump as alg
+#             from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
 #
-#             obj = alg.SPPGrowthDump(iFile, minSup, maxPer, maxLa)
+#             obj = alg.PFPGrowth(iFile, minSup, maxPer)
 #
 #             obj.startMine()
 #
-#             Patterns = obj.getPatterns()
+#             periodicFrequentPatterns = obj.getPatterns()
 #
-#             print("Total number of Stable Periodic Frequent Patterns:", len(Patterns))
+#             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
@@ -46,274 +48,209 @@
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
 
 """
 
-from PAMI.stableperiodicFrequentPattern.basic import abstract as _ab
-import pandas as pd
-from deprecated import deprecated
+from PAMI.periodicFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
 
-from urllib.request import urlopen
-import validators
 import pandas as pd
-import resource
-import time
-import sys
-import os
-import psutil
-
-_minSup = int()
-_maxPer = int()
-_maxLa = int()
-_last = int()
-
-
-class _Node:
-
-    def __init__(self, item, children):
-        """
-        Initializing the Node class
-
-        :param item: Storing the item of a node
-        :type item: int or None
-        :param children: To maintain the children of a node
-        :type children: dict
-        """
-
-        self.item = item
-        self.children = children
-        self.parent = None
-        self.timeStamps = []
-
-    def addChild(self, node):
-        """
-        To add the children to a node
-
-        :param node: parent node in the tree
-        """
-
-        self.children[node.item] = node
-        node.parent = self
-
-class _Tree:
-    def __init__(self):
-        self.root = _Node(None, {})
-        self.summaries = {}
-        self.info = {}
-
-    def addTransaction(self, transaction, tid):
-        """
-        Adding a transaction into tree
-
-        :param transaction: To represent the complete database
-        :type transaction: list
-        :param tid: To represent the timestamp of a database
-        :type tid: list
-        :return: pfp-growth tree
-        """
-
-        currentNode = self.root
-        for i in range(len(transaction)):
-            if transaction[i] not in currentNode.children:
-                newNode = _Node(transaction[i], {})
-                currentNode.addChild(newNode)
-                if transaction[i] in self.summaries:
-                    self.summaries[transaction[i]].append(newNode)
-                else:
-                    self.summaries[transaction[i]] = [newNode]
-                currentNode = newNode
-            else:
-                currentNode = currentNode.children[transaction[i]]
-        currentNode.timeStamps = currentNode.timeStamps + tid
-
-    def getConditionalPatterns(self, alpha):
-        """
-        Generates all the conditional patterns of a respective node
-
-        :param alpha: To represent a Node in the tree
-        :type alpha: Node
-        :return: A tuple consisting of finalPatterns, conditional pattern base and information
-        """
-        finalPatterns = []
-        finalSets = []
-        for i in self.summaries[alpha]:
-            set1 = i.timeStamps
-            set2 = []
-            while i.parent.item is not None:
-                set2.append(i.parent.item)
-                i = i.parent
-            if len(set2) > 0:
-                set2.reverse()
-                finalPatterns.append(set2)
-                finalSets.append(set1)
-        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets)
-        return finalPatterns, finalSets, info
-
-    @staticmethod
-    def generateTimeStamps(node):
-        """
-        To get the timestamps of a node
-
-        :param node: A node in the tree
-        :return: Timestamps of a node
-        """
-
-        finalTimeStamps = node.timeStamps
-        return finalTimeStamps
-
-    def removeNode(self, nodeValue):
-        """
-        Removing the node from tree
+from deprecated import deprecated
+import numpy as np
 
-        :param nodeValue: To represent a node in the tree
-        :type nodeValue: node
-        :return: Tree with their nodes updated with timestamps
-        """
+_maxPer = float()
+_minSup = float()
+_lno = int()
 
-        for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
-            del i.parent.children[nodeValue]
 
-    def getTimeStamps(self, alpha):
-        """
-        To get all the timestamps of the nodes which share same item name
+class _Node(object):
+    """
+    A class used to represent the node of frequentPatternTree
 
-        :param alpha: Node in a tree
-        :return: Timestamps of a  node
-        """
-        temporary = []
-        for i in self.summaries[alpha]:
-            temporary += i.timeStamps
-        return temporary
+    :Attributes:
 
-    @staticmethod
-    def getSupportAndPeriod(timeStamps):
-        """
-        To calculate the periodicity and support
-
-        :param timeStamps: Timestamps of an item set
-        :return: support, periodicity
-        """
-        global _maxPer, _last
-        previous = 0
-        la = 0
-        tsList = sorted(timeStamps)
-        for ts in tsList:
-            la = max(0, la + ts - previous - _maxPer)
-            previous = ts
-        la = max(0, la + _last - previous - _maxPer)
-        return len(timeStamps), la
+        item : int or None
+            Storing item of a node
+        timeStamps : list
+            To maintain the timestamps of a database at the end of the branch
+        parent : node
+            To maintain the parent of every node
+        children : list
+            To maintain the children of a node
 
-    def conditionalDatabases(self, conditionalPatterns, conditionalTimeStamps):
-        """
-        It generates the conditional patterns with periodic-frequent items
+    :Methods:
 
-        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
-        :type conditionalPatterns: list
-        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
-        :type conditionalTimeStamps: list
-        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        addChild(itemName)
+            Storing the children to their respective parent nodes
         """
 
-        global _maxPer, _minSup, _maxLa
-        pat = []
-        timeStamps = []
-        data1 = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
-                if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
-                else:
-                    data1[j] = conditionalTimeStamps[i]
-        updatedDictionary = {}
-        for m in data1:
-            updatedDictionary[m] = self.getSupportAndPeriod(data1[m])
-        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _minSup and v[1] <= _maxLa}
-        count = 0
-        for p in conditionalPatterns:
-            p1 = [v for v in p if v in updatedDictionary]
-            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
-            if len(trans) > 0:
-                pat.append(trans)
-                timeStamps.append(conditionalTimeStamps[count])
-            count += 1
-        return pat, timeStamps, updatedDictionary
-
-    def generatePatterns(self, prefix):
-        """
-        Generates the patterns
-
-        :param prefix: Forms the combination of items
-        :type prefix: list
-        :returns: yields patterns with their support and periodicity
-        """
-
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
-            pattern = prefix[:]
-            pattern.append(i)
-            yield pattern, self.info[i]
-            patterns, timeStamps, info = self.getConditionalPatterns(i)
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
-            if len(patterns) > 0:
-                for q in conditionalTree.generatePatterns(pattern):
-                    yield q
-            self.removeNode(i)
-
-class SPPGrowth():
+    def __init__(self, item, locations, parent=None):
+        self.item = item
+        self.locations = locations
+        self.parent = parent
+        self.children = {}
+
+    def addChild(self, item, locations):
+        if item not in self.children:
+            self.children[item] = _Node(item, locations, self)
+        else:
+            self.children[item].locations = locations + self.children[item].locations
+            
+        return self.children[item]
+
+    def traverse(self):
+        transaction = []
+        locs = self.locations
+        node = self.parent
+        while node.parent is not None:
+            transaction.append(node.item)
+            node = node.parent
+        return transaction[::-1], locs
+
+    def traverse(self):
+        transaction = []
+        locs = self.locations
+        node = self.parent
+        while node.parent is not None:
+            transaction.append(node.item)
+            node = node.parent
+        return transaction[::-1], locs
+
+class PFPGrowth(_ab._periodicFrequentPatterns):
+    """
+    :Description:   PFPGrowth is one of the fundamental algorithm to discover periodic-frequent patterns in a transactional database.
+
+    :Reference:   Syed Khairuzzaman Tanbeer, Chowdhury Farhan, Byeong-Soo Jeong, and Young-Koo Lee, "Discovering Periodic-Frequent
+                   Patterns in Transactional Databases", PAKDD 2009, https://doi.org/10.1007/978-3-642-01307-2_24
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of periodic frequent pattern's
+    :param  oFile: str :
+                   Name of the output file to store complete set of periodic frequent pattern's
+    :param  minSup: str:
+                   Controls the minimum number of transactions in which every item must appear in a database.
+    :param  maxPer: float:
+                   Controls the maximum number of transactions in which any two items within a pattern can reappear.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
+    :Attributes:
+
+        iFile : file
+            Name of the Input file or path of the input file
+        oFile : file
+            Name of the output file or path of the output file
+        minSup : int or float or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+        maxPer : int or float or str
+            The user can specify maxPer either in count or proportion of database size.
+            If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+            Otherwise, it will be treated as float.
+            Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+        sep : str
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+            However, the users can override their default separator.
+        memoryUSS : float
+            To store the total amount of USS memory consumed by the program
+        memoryRSS : float
+            To store the total amount of RSS memory consumed by the program
+        startTime:float
+            To record the start time of the mining process
+        endTime:float
+            To record the completion time of the mining process
+        Database : list
+            To store the transactions of a database in list
+        mapSupport : Dictionary
+            To maintain the information of item and their frequency
+        lno : int
+            To represent the total no of transaction
+        tree : class
+            To represents the Tree class
+        itemSetCount : int
+            To represents the total no of patterns
+        finalPatterns : dict
+            To store the complete patterns
+
+    :Methods:
+
+        startMine()
+            Mining process will start from here
+        getPatterns()
+            Complete set of patterns will be retrieved with this function
+        save(oFile)
+            Complete set of periodic-frequent patterns will be loaded in to a output file
+        getPatternsAsDataFrame()
+            Complete set of periodic-frequent patterns will be loaded in to a dataframe
+        getMemoryUSS()
+            Total amount of USS memory consumed by the mining process will be retrieved from this function
+        getMemoryRSS()
+            Total amount of RSS memory consumed by the mining process will be retrieved from this function
+        getRuntime()
+            Total amount of runtime taken by the mining process will be retrieved from this function
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
+        PeriodicFrequentOneItem()
+            Extracts the one-periodic-frequent patterns from database
+        updateDatabases()
+            Update the database by removing aperiodic items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+
+
+
+    **Credits:**
+    --------------
+             The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    """
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _maxPer = float()
-    _maxLa = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
     _rankedUp = {}
     _lno = 0
-    SPPList = {}
 
-    def __init__(self, inputFile, minSup, maxPer, maxLa, sep='\t'):
-        self._iFile = inputFile
-        self._minSup = minSup
-        self._maxPer = maxPer
-        self._maxLa = maxLa
-        self._sep = sep
-
-    def _creatingItemSets(self):
+    def _creatingItemSets(self) -> None:
         """
-        Storing the complete transactions of the database/input file in a database variable
+            Storing the complete transactions of the database/input file in a database variable
+        :return: None
         """
         self._Database = []
-        if isinstance(self._iFile, pd.DataFrame):
+        if isinstance(self._iFile, _ab._pd.DataFrame):
             data, ts = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'TS' in i:
                 ts = self._iFile['TS'].tolist()
             if 'Transactions' in i:
                 data = self._iFile['Transactions'].tolist()
             for i in range(len(data)):
                 tr = [ts[i][0]]
                 tr = tr + data[i]
                 self._Database.append(tr)
 
         if isinstance(self._iFile, str):
-            if validators.url(self._iFile):
-                data = urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
                     self._Database.append(temp)
             else:
@@ -324,99 +261,15 @@
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
                             self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _periodicFrequentOneItem(self):
-        """
-        Calculates the support of each item in the database and assign ranks to the items by decreasing support and returns the frequent items list
-
-        :returns: return the one-length periodic frequent patterns
-        """
-        global _last
-        tidLast = {}
-        la = {}
-        for transaction in self._Database:
-            ts = int(transaction[0])
-            for item in transaction[1:]:
-                if item not in self.SPPList:
-                    la[item] = max(0, ts - self._maxPer)
-                    self.SPPList[item] = [1, la[item]]
-                else:
-                    s = self.SPPList[item][0] + 1
-                    la[item] = max(0, la[item] + ts - tidLast.get(item) - self._maxPer)
-                    self.SPPList[item] = [s, max(la[item], self.SPPList[item][1])]
-                tidLast[item] = ts
-            _last = ts
-        for item in self.SPPList:
-            la[item] = max(0, la[item] + _last - tidLast[item] - self._maxPer)
-            self.SPPList[item][1] = max(la[item], self.SPPList[item][1])
-        self.SPPList = {k: v for k, v in self.SPPList.items() if v[0] >= self._minSup and v[1] <= self._maxLa}
-        self.SPPList = {k: v for k, v in sorted(self.SPPList.items(), key=lambda x: x[1][0], reverse=True)}
-        data = self.SPPList
-        pfList = [k for k, v in sorted(data.items(), key=lambda x: (x[1][0], x[0]), reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(pfList)])
-        #print(len(pfList))
-        return data, pfList
-
-    def _updateDatabases(self, dict1):
-        """
-        Remove the items which are not frequent from database and updates the database with rank of items
-
-        :param dict1: frequent items with support
-        :type dict1: dictionary
-        :return: Sorted and updated transactions
-        """
-        list1 = []
-        for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
-                if tr[i] in dict1:
-                    list2.append(self._rank[tr[i]])
-            if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort()
-                list2[1:] = basket[0:]
-                list1.append(list2)
-        return list1
-
-    @staticmethod
-    def _buildTree(data, info):
-        """
-        It takes the database and support of each item and construct the main tree by setting root node as a null
-
-        :param data: it represents the one Database in database
-        :type data: list
-        :param info: it represents the support of each item
-        :type info: dictionary
-        :return: returns root node of tree
-        """
-
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(data)):
-            set1 = [data[i][0]]
-            rootNode.addTransaction(data[i][1:], set1)
-        return rootNode
-
-    def _savePeriodic(self, itemSet):
-        """
-        To convert the ranks of items in to their original item names
-
-        :param itemSet: frequent pattern.
-        :return: frequent pattern with original item names
-        """
-        t1 = str()
-        for i in itemSet:
-            t1 = t1 + self._rankedUp[i] + " "
-        return t1
-
-    def _convert(self, value):
+    def _convert(self, value) -> int:
         """
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
         if type(value) is int:
@@ -428,179 +281,307 @@
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self):
+    def startMine(self) -> None:
         """
         Mining process will start from this function
+        :return: None
         """
 
-        global _minSup, _maxPer, _lno, _maxLa
-        self._startTime = time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._maxLa = self._convert(self._maxLa)
-        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
-        print(_minSup, _maxPer, _maxLa)
-        if self._minSup > len(self._Database):
-            raise Exception("Please enter the minSup in range between 0 to 1")
-        generatedItems, pfList = self._periodicFrequentOneItem()
-        updatedDatabases = self._updateDatabases(generatedItems)
-        for x, y in self._rank.items():
-            self._rankedUp[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        Tree = self._buildTree(updatedDatabases, info)
-        patterns = Tree.generatePatterns([])
-        self._finalPatterns = {}
-        for i in patterns:
-            sample = self._savePeriodic(i[0])
-            self._finalPatterns[sample] = i[1]
-        self._endTime = time.time()
-        process = psutil.Process(os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
+        self.Mine()
+
+    def _getMaxPer(self, arr, maxTS):
+        arr = np.append(arr, [0, maxTS])
+        arr = np.sort(arr)
+        arr = np.diff(arr)
+
+        return np.max(arr)
+
+    def _construct(self, items, data, minSup, maxPer, maxTS, patterns):
+
+        # maxPerItems = {k: self.getMaxPer(v, maxTS) for k, v in items.items() if len(v) >= minSup}
+
+        items = {k: v for k, v in items.items() if len(v) >= minSup and self._getMaxPer(v, maxTS) <= maxPer}
+
+        #tested ok
+        for item, ts in items.items():
+            # pat = "\t".join(item)
+            # self.patCount += 1
+            # patterns[pat] = (len(ts), self.getMaxPer(ts, maxTS))
+            patterns[tuple([item])] = [len(ts), self._getMaxPer(ts, maxTS)]
+
+        root = _Node([], None, None)
+        itemNodes = {}
+        for line in data:
+            currNode = root
+            index = int(line[0])
+            line = line[1:]
+            line = sorted([item for item in line if item in items], key = lambda x: len(items[x]), reverse = True)
+            for item in line:
+                currNode = currNode.addChild(item, [index])   # heavy
+                if item in itemNodes:
+                    itemNodes[item].add(currNode)
+                else:
+                    itemNodes[item] = set([currNode])
+
+        return root, itemNodes
+
+
+    def _recursive(self, root, itemNode, minSup, maxPer, patterns, maxTS):
+
+        for item in itemNode:
+            newRoot = _Node(root.item + [item], None, None)
 
-    def Mine(self):
+            itemLocs = {}
+            transactions = {}
+            for node in itemNode[item]:
+                transaction, locs = node.traverse()
+                if len(transaction) < 1:
+                    continue
+                # transactions.append((transaction, locs))
+                if tuple(transaction) in transactions:
+                    transactions[tuple(transaction)].extend(locs)
+                else:
+                    transactions[tuple(transaction)] = locs
+
+                for item in transaction:
+                    if item in itemLocs:
+                        itemLocs[item] += locs
+                    else:
+                        itemLocs[item] = list(locs)
+
+            # Precompute getMaxPer results for itemLocs
+            maxPerResults = {item: self._getMaxPer(itemLocs[item], maxTS) for item in itemLocs if len(itemLocs[item]) >= minSup}
+
+            # Filter itemLocs based on minSup and maxPer
+            itemLocs = {k: len(v) for k, v in itemLocs.items() if k in maxPerResults and maxPerResults[k] <= maxPer}
+
+            # Iterate over filtered itemLocs
+            for item in itemLocs:
+                # pat = "\t".join([str(x) for x in newRoot.item + [item]])
+                # self.patCount += 1
+                # patterns[pat] = [itemLocs[item], maxPerResults[item]]
+                patterns[tuple(newRoot.item + [item])] = [itemLocs[item], maxPerResults[item]]
+            
+            if not itemLocs:
+                continue
+
+            newItemNodes = {}
+
+            for transaction, locs in transactions.items():
+                transaction = sorted([item for item in transaction if item in itemLocs], key = lambda x: itemLocs[x], reverse = True)
+                if len(transaction) < 1:
+                    continue
+                currNode = newRoot
+                for item in transaction:
+                    currNode = currNode.addChild(item, locs)
+                    if item in newItemNodes:
+                        newItemNodes[item].add(currNode)
+                    else:
+                        newItemNodes[item] = set([currNode])
+
+            self._recursive(newRoot, newItemNodes, minSup, maxPer, patterns, _lno)
+
+    def Mine(self) -> None:
         """
         Mining process will start from this function
+        :return: None
         """
 
-        global _minSup, _maxPer, _lno, _maxLa
-        self._startTime = time.time()
+        global _minSup, _maxPer, _lno
+        self._startTime = _ab._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
         if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
+        if self._maxPer is None:
+            raise Exception("Please enter the Maximum Periodicity")
+        if self._sep is None:
+            raise Exception("Default separator is tab space, please enter the separator if you have different separator in the input file")
+
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         self._maxPer = self._convert(self._maxPer)
-        self._maxLa = self._convert(self._maxLa)
-        _minSup, _maxPer, _maxLa, _lno = self._minSup, self._maxPer, self._maxLa, len(self._Database)
-        print(_minSup, _maxPer, _maxLa)
+        #tested ok
+        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
         if self._minSup > len(self._Database):
             raise Exception("Please enter the minSup in range between 0 to 1")
-        generatedItems, pfList = self._periodicFrequentOneItem()
-        updatedDatabases = self._updateDatabases(generatedItems)
-        for x, y in self._rank.items():
-            self._rankedUp[y] = x
-        info = {self._rank[k]: v for k, v in generatedItems.items()}
-        Tree = self._buildTree(updatedDatabases, info)
-        patterns = Tree.generatePatterns([])
-        self._finalPatterns = {}
-        for i in patterns:
-            sample = self._savePeriodic(i[0])
-            self._finalPatterns[sample] = i[1]
-        self._endTime = time.time()
-        process = psutil.Process(os.getpid())
+        
+
+        items = {}
+
+        # tested ok
+        for line in self._Database:
+            index = int(line[0])
+            for item in line[1:]:
+                if item not in items:
+                    items[item] = []
+                items[item].append(index)
+
+        root, itemNodes = self._construct(items, self._Database, _minSup, _maxPer, _lno, self._finalPatterns)
+
+        self._recursive(root, itemNodes, _minSup, _maxPer, self._finalPatterns, _lno)
+
+    
+
+        newPattern = {}
+        for k, v in self._finalPatterns.items():
+            newPattern["\t".join([str(x) for x in k])] = v
+
+        self._finalPatterns = newPattern
+
+
+
+
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
-        print("Stable Periodic Frequent patterns were generated successfully using topk algorithm ")
+        print("Periodic Frequent patterns were generated successfully using PFPGrowth algorithm ")
 
-
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """Calculating the total amount of runtime taken by the mining process
 
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final periodic-frequent patterns in a dataframe
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+        """
+        Storing final periodic-frequent patterns in a dataframe
 
         :return: returning periodic-frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a, b[0], b[1]])
-            dataFrame = pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataFrame
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of periodic-frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
+        :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y[0]) + ":" + str(y[1])
+            #s1 = x.replace(' ', '\t').strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
-        """ Function to send the set of periodic-frequent patterns after completion of the mining process
+    def getPatterns(self) -> Dict[str, Tuple[int, int]]:
+        """
+        Function to send the set of periodic-frequent patterns after completion of the mining process
 
         :return: returning periodic-frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
+    def printResults(self) -> None:
+        """
+        This function is used to print the results
+        :return: None
+        """
+        print("Total number of Periodic Frequent Patterns:", len(self.getPatterns()))
+        print("Total Memory in USS:", self.getMemoryUSS())
+        print("Total Memory in RSS", self.getMemoryRSS())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(sys.argv) == 5 or len(sys.argv) == 6:
-        if len(sys.argv) == 6:
-            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4], sys.argv[5])
-        if len(sys.argv) == 5:
-            _ap = SPPGrowth(sys.argv[1], sys.argv[3], sys.argv[4])
+    if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
+        if len(_ab._sys.argv) == 6:
+            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
+        if len(_ab._sys.argv) == 5:
+            _ap = PFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save(sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        print("Total number of Periodic-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        '''ap = topk('https://www.u-aizu.ac.jp/~udayrage/datasets/temporalDatabases/temporal_retail.csv', 0.001, 0.005, 0.004)
-        #ap = topk('/Users/likhitha/Downloads/contextPrefixSpan.txt', 3, 6, 2, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Frequent Patterns:", len(Patterns))
-        ap.save('/Users/Likhitha/Downloads/output')
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
 
+
+
+    """
+    **Methods to execute code on terminal**
+    --------------------------------------------
+    .. code-block:: console
+
+      Format:
+            
+      (.venv) $ python3 PFPGrowth.py <inputFile> <outputFile> <minSup> <maxPer>
+
+      Example:
+      
+      (.venv) $ python3 PFPGrowth.py sampleTDB.txt patterns.txt 0.3 0.4
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+    **Importing this algorithm into a python program**
+    ---------------------------------------------------
+    .. code-block:: python
+
+                from PAMI.periodicFrequentPattern.basic import PFPGrowth as alg
+
+                obj = alg.PFPGrowth(iFile, minSup, maxPer)
+
+                obj.startMine()
+
+                periodicFrequentPatterns = obj.getPatterns()
+
+                print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
+
+                obj.save(oFile)
+
+                Df = obj.getPatternsAsDataFrame()
+
+                memUSS = obj.getMemoryUSS()
+
+                print("Total Memory in USS:", memUSS)
+
+                memRSS = obj.getMemoryRSS()
+
+                print("Total Memory in RSS", memRSS)
+
+                run = obj.getRuntime()
+
+                print("Total ExecutionTime in seconds:", run)
+    """
```

### Comparing `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py` & `pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/topK/TSPIN.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py` & `pami-2024.5.1/PAMI/stablePeriodicFrequentPattern/topK/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/abstract.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/dfsCode.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/dfsCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/edge.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/extendedEdge.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/frequentSubgraph.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/graph.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/graph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/gspan.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/gspan.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/basic/vertex.py` & `pami-2024.5.1/PAMI/subgraphMining/basic/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSCode.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/DFSCode.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/DFSThread.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/DFSThread.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/abstract.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/edge.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/edge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/extendedEdge.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/extendedEdge.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/frequentSubgraph.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/frequentSubgraph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/graph.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/graph.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/sparseTriangularMatrix.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/tkg.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/tkg.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/subgraphMining/topK/vertex.py` & `pami-2024.5.1/PAMI/subgraphMining/topK/vertex.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py` & `pami-2024.5.1/PAMI/uncertainFaultTolerantFrequentPattern/VBFTMine.py`

 * *Files 15% similar despite different names*

```diff
@@ -1,18 +1,26 @@
 # VBFTMine is one of the fundamental algorithm to discover fault-tolerant frequent patterns in an uncertain transactional database based on bitset representation.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#
 #
 #             import PAMI.uncertainFaultTolerantFrequentPattern.basic.VBFTMine as alg
 #
+#             iFile = 'sampleDB.txt'
+#
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             itemSup = 2  # can also be specified between 0 and 1
+#
+#             minLength = 3 # can also be specified between 0 and 1
+#
+#             faultTolerance = 2 # can also be specified between 0 and 1
+#
 #             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             faultTolerantFrequentPattern = obj.getPatterns()
 #
 #             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 #
 #             obj.save(oFile)
 #
@@ -52,32 +60,34 @@
 from deprecated import deprecated
 
 import numpy as _np
 from PAMI.faultTolerantFrequentPattern.basic import abstract as _ab
 
 class VBFTMine(_ab._faultTolerantFrequentPatterns):
     """
+    About this algorithm
+    ====================
     
     :Description:  VBFTMine is one of the fundamental algorithm to discover fault tolerant frequent patterns in an uncertain transactional database based on
                    bitset representation.
                    This program employs apriori property (or downward closure property) to  reduce the search space effectively.
 
     :Reference:   Koh, JL., Yo, PW. (2005). An Efficient Approach for Mining Fault-Tolerant Frequent Patterns Based on Bit Vector Representations.
-                  In: Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
+            In:   Zhou, L., Ooi, B.C., Meng, X. (eds) Database Systems for Advanced Applications. DASFAA 2005. Lecture Notes in Computer Science,
                   vol 3453. Springer, Berlin, Heidelberg. https://doi.org/10.1007/11408079_51
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of uncertain Fault Tolerant FrequentFrequent Patterns
     :param  oFile: str :
                    Name of the output file to store complete set of uncertain Fault Tolerant FrequentFrequent Patterns
     :param  minSup: float or int or str :
-                    The user can specify minSup either in count or proportion of database size.
-                    If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-                    Otherwise, it will be treated as float.
-                    Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+                   The user can specify minSup either in count or proportion of database size.
+                   If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+                   Otherwise, it will be treated as float.
+                   Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
     :param  itemSup: int or float :
                     Frequency of an item
     :param minLength: int
                     minimum length of a pattern
     :param faultTolerance: int :
                     The ability of a pattern mining algorithm to handle errors or inconsistencies in the data without completely failing or producing incorrect results.
     :param  sep: str :
@@ -100,58 +110,77 @@
         memoryRSS : float
           To store the total amount of RSS memory consumed by the program
 
         Database : list
           To store the transactions of a database in list
 
 
-    **Executing the code on terminal**:
-    ------------------------------------
-    .. code-block:: console
+    Execution methods
+    =================
 
+    **Terminal command**
+
+
+    .. code-block:: console
 
        Format:
 
        (.venv) $ python3 VBFTMine.py <inputFile> <outputFile> <minSup> <itemSup> <minLength> <faultTolerance>
 
        Examples usage:
 
        (.venv) $ python3 VBFTMine.py sampleDB.txt patterns.txt 10.0 3.0 3 1
 
+    .. note:: minSup will be considered in times of minSup and count of database transactions
 
-               .. note:: minSup will be considered in times of minSup and count of database transactions
 
+    **Calling from a python program**
 
-    **Sample run of the importing code**:
-    --------------------------------------------
     .. code-block:: python
     
             import PAMI.faultTolerantFrequentPattern.basic.VBFTMine as alg
 
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            itemSup = 2  # can also be specified between 0 and 1
+
+            minLength = 3 # can also be specified between 0 and 1
+
+            faultTolerance = 2 # can also be specified between 0 and 1
+
             obj = alg.VBFTMine(iFile, minSup, itemSup, minLength, faultTolerance)
 
-            obj.startMine()
+            obj.mine()
 
             faultTolerantFrequentPattern = obj.getPatterns()
 
             print("Total number of Fault Tolerant Frequent Patterns:", len(faultTolerantFrequentPattern))
 
             obj.save(oFile)
 
             Df = obj.getPatternInDataFrame()
 
-            print("Total Memory in USS:", obj.getMemoryUSS())
+            memUSS = obj.getMemoryUSS()
+
+            print("Total Memory in USS:", memUSS)
+
+            memRSS = obj.getMemoryRSS()
 
-            print("Total Memory in RSS", obj.getMemoryRSS())
+            print("Total Memory in RSS", memRSS)
 
-            print("Total ExecutionTime in seconds:", obj.getRuntime())
+            run = obj.getRuntime()
 
-    **Credits**:
-    ------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+            print("Total ExecutionTime in seconds:", run)
+
+    Credits
+    =======
+
+           The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
     """
 
     _minSup = float()
     _itemSup = float()
     _minLength = int()
     _faultTolerance = int()
@@ -222,32 +251,62 @@
                 value = float(value)
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     def _Count(self, tids):
+        """
+        Count the occurrences of 1s in the given list of transaction IDs.
+
+        :param tids: List of transaction IDs.
+        :type tids: List[int]
+        :return: Count of occurrences of 1s in the list.
+        :rtype: int
+        """
         count = 0
         for i in tids:
             if i == 1:
                 count += 1
         return count
 
     def _save(self, prefix, suffix, tidsetx):
+        """
+        Save the pattern with its support count if it meets the fault tolerance criteria.
+
+        :param prefix: Prefix part of the pattern.
+        :type prefix: list
+        :param suffix: Suffix part of the pattern.
+        :type suffix: list
+        :param tidsetx: Transaction IDs associated with the pattern.
+        :type tidsetx: list
+        :return: None
+        """
         if (prefix == None):
             prefix = suffix
         else:
             prefix = prefix + suffix
         prefix = list(set(prefix))
         prefix.sort()
         val = self._Count(tidsetx)
         if len(prefix) > self._faultTolerance:
             self._finalPatterns[tuple(prefix)] = val
 
     def _processEquivalenceClass(self, prefix, itemsets, tidsets):
+        """
+        Process the equivalence class to generate frequent patterns.
+
+        :param prefix: Prefix part of the pattern.
+        :type prefix: list.
+        :param itemsets: List of itemsets in the equivalence class.
+        :type itemsets: list.
+        :param tidsets: List of transaction IDs associated with each itemset.
+        :type tidsets: list
+        :return: None
+        """
         if (len(itemsets) == 1):
             i = itemsets[0]
             tidi = tidsets[0]
             self._save(prefix, [i], tidi)
             return
         for i in range(len(itemsets)):
             itemx = itemsets[i]
@@ -287,53 +346,23 @@
                     Vector[j] = [count]
         for x, y in Vector.items():
             v = self._Count(y)
             if v >= self._itemSup:
                 items.append(x)
         return Vector, items
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Frequent pattern mining process will start from here
         """
-        self._Database = []
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._itemSup = self._convert(self._itemSup)
-        self._minLength = int(self._minLength)
-        self._faultTolerance = int(self._faultTolerance)
-        Vector, plist = self._oneLengthFrequentItems()
-        for i in range(len(plist)):
-            itemx = plist[i]
-            tidsetx = Vector[itemx]
-            itemsetx = [itemx]
-            itemsets = []
-            tidsets = []
-            for j in range(i + 1, len(plist)):
-                itemj = plist[j]
-                tidsetj = Vector[itemj]
-                y1 = list(_np.array(tidsetx) | _np.array(tidsetj))
-                total = self._Count(y1)
-                if total >= self._minSup:
-                    itemsets.append(itemj)
-                    tidsets.append(y1)
-            if (len(itemsets) > 0):
-                self._processEquivalenceClass(itemsetx, itemsets, tidsets)
-            self._save(None, itemsetx, tidsetx)
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
-        print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
+        self.mine()
 
-    def Mine(self):
+    def mine(self):
         """
         Frequent pattern mining process will start from here
         """
         self._Database = []
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
@@ -364,44 +393,48 @@
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
         print("Fault-Tolerant Frequent patterns were generated successfully using VBFTMine algorithm ")
 
     def getMemoryUSS(self):
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataFrame = {}
@@ -413,14 +446,15 @@
             data.append([s, b])
             dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         # dataFrame = dataFrame.replace(r'\r+|\n+|\t+',' ', regex=True)
         return dataFrame
 
     def save(self, outFile):
         """
+
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
@@ -429,14 +463,15 @@
             for i in x:
                 s = s + i + '\t'
             s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
@@ -455,14 +490,15 @@
     if len(_ab._sys.argv) == 7 or len(_ab._sys.argv) == 8:
         if len(_ab._sys.argv) == 8:
             _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3],  _ab._sys.argv[4],
                             _ab._sys.argv[5], _ab._sys.argv[6], _ab._sys.argv[7],)
         if len(_ab._sys.argv) == 7:
             _ap = VBFTMine(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         _ap = VBFTMine('/Users/Likhitha/Downloads/fault/sample4.txt', 5, 3, 2, 1, ' ')
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py` & `pami-2024.5.1/PAMI/uncertainFaultTolerantFrequentPattern/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/CUFPTree.py`

 * *Files 6% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 # CUFPTree is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using CUFP-Tree
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#
 #
 #             from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
 #
 #             obj = alg.CUFPTree(iFile, minSup,oFile,sep)
 #
-#             obj.startMine()
+#             iFile = 'sampleDB.txt'
+#
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -28,15 +30,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -44,16 +45,14 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 import pandas as pd
 from deprecated import deprecated
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 from typing import List, Tuple
@@ -68,14 +67,15 @@
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
         item : int or word
             Represents the name of the item
+
         probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability) -> None:
         self.item = item
         self.probability = probability
@@ -85,18 +85,21 @@
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
             storing item of a node
+
         probability : int
             To maintain the expected support of node
+
         parent : node
             To maintain the parent of every node
+
         children : list
             To maintain the children of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
@@ -105,30 +108,41 @@
     def __init__(self, item, children) -> None:
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
 
     def addChild(self, node) -> None:
+        """
+        This method adds a child node to the current node in the frequent pattern tree. It updates the children
+        dictionary of the current node with the new child node and sets the parent of the child node to the current node.
+
+        :param node: The child node to be added.
+        :type node: _Node
+        :return: None
+        """
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
             Represents the root node of the tree
+
         summaries : dictionary
             storing the nodes with same item name
+
         info : dictionary
             stores the support of items
+
     :Methods:
 
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
@@ -146,14 +160,15 @@
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
     def addTransaction(self, transaction) -> None:
         """
         adding transaction into tree
+
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         :return: None
         """
 
         currentNode = self.root
         for i in range(len(transaction)):
@@ -185,14 +200,15 @@
                     currentNode.probability += transaction[i].probability
                 else:
                     currentNode.probability += max(lp) * transaction[i].probability
 
     def addConditionalPattern(self, transaction, sup) -> None:
         """
         constructing conditional tree from prefixPaths
+
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
         :return: None
         """
 
@@ -211,14 +227,15 @@
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
 
     def conditionalPatterns(self, alpha) -> Tuple[List, List, dict]:
         """
         generates all the conditional patterns of respective node
+
         :param alpha : it represents the Node in tree
         :type alpha : _Node
         :return: Tuple
         """
 
         # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
@@ -308,19 +325,21 @@
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 class CUFPTree(_ab._frequentPatterns):
   
     """
+    About this algorithm
+    ====================
+
     :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using CUFP-Tree.
 
-    :Reference:
-        Chun-Wei Lin Tzung-PeiHong, 'new mining approach for uncertain databases using CUFP trees',
-        Expert Systems with Applications, Volume 39, Issue 4, March 2012, Pages 4084-4093, https://doi.org/10.1016/j.eswa.2011.09.087
+    :Reference: Chun-Wei Lin Tzung-PeiHong, 'new mining approach for uncertain databases using CUFP trees',
+                Expert Systems with Applications, Volume 39, Issue 4, March 2012, Pages 4084-4093, https://doi.org/10.1016/j.eswa.2011.09.087
     
     :param  iFile: str :
                    Name of the Input file to mine complete set of Uncertain Frequent Patterns
     :param  oFile: str :
                    Name of the output file to store complete set of Uncertain frequent patterns
     :param  minSup: int or float or str :
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
@@ -328,48 +347,61 @@
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
+
         oFile : file
             Name of the output file or path of the output file
+
         minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         startTime:float
             To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
             To represent the total no of transaction
+
         tree : class
             To represents the Tree class
+
         itemSetCount : int
             To represents the total no of patterns
+
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -388,41 +420,46 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
 
-    **Methods to execute code on terminal**
-    --------------------------------------------
+    Execution methods
+    =================
 
-    .. code-block:: console
 
+    **Terminal command**
+
+
+    .. code-block:: console
 
        Format:
 
        (.venv) $ python3 CUFPTree.py <inputFile> <outputFile> <minSup>
 
        Example Usage:
 
        (.venv) $ python3 CUFPTree.py sampleTDB.txt patterns.txt 3
 
+    .. note:: minSup  will be considered in support count or frequency
 
+    **Calling from a python program**
 
-               .. note:: minSup  will be considered in support count or frequency
-
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.uncertainFrequentPattern.basic import CUFPTree as alg
 
-            obj = alg.CUFPTree(iFile, minSup)v
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            obj = alg.CUFPTree(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
@@ -436,19 +473,21 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    --------------
+    Credits
+    =======
+
+
+            The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    """
 
-        The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-"""
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
@@ -459,14 +498,15 @@
 
     def __init__(self, iFile, minSup, sep='\t') -> None:
         super().__init__(iFile, minSup, sep)
 
     def _creatingItemSets(self) -> None:
         """
         Scans the uncertain transactional dataset
+
         :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
@@ -543,15 +583,15 @@
     def _buildTree(data, info) -> '_Tree':
         """
         It takes the self.Database and support of each item and construct the main tree with setting root node as null
 
         :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
-        :type info : dictionary
+        :type info : dict
         :return: Dictionary
         """
 
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
@@ -617,15 +657,14 @@
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self) -> None:
         """
-
         To remove the false positive patterns generated in frequent patterns.
 
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
@@ -646,43 +685,28 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
+
         :return: None
         """
-        global minSup
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-        self._finalPatterns = {}
-        mapSupport, plist = self._frequentOneItem()
-        self.Database1 = self._updateTransactions(mapSupport)
-        info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(self.Database1, info)
-        Tree1.generatePatterns([])
-        self._removeFalsePositives()
-        print("Uncertain Frequent patterns were successfully generated using CUFPTree algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self.memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self) -> None:
+    def mine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
+
         :return: None
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         minSup = self._minSup
@@ -700,44 +724,48 @@
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
@@ -745,39 +773,40 @@
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> dict:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
-        :return: None
         """
         print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
@@ -785,14 +814,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = CUFPTree(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/PUFGrowth.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,119 +1,143 @@
 # PUFGrowth is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import puf as alg
+#             from PAMI.uncertainFrequentPattern.basic import puf as alg
 #
-#     obj = alg.PUFGrowth(iFile, minSup)
+#             iFile = 'sampleDB.txt'
 #
-#     obj.startMine()
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj = alg.PUFGrowth(iFile, minSup)
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             obj.mine()
 #
-#     obj.save(oFile)
+#             frequentPatterns = obj.getPatterns()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     memUSS = obj.getMemoryUSS()
+#             obj.save(oFile)
 #
-#     print("Total Memory in USS:", memUSS)
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memRSS = obj.getMemoryRSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     run = obj.getRuntime()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 """
 
-
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 from typing import List, Tuple
+from deprecated import deprecated
 
 _minSup = str()
 _ab._sys.setrecursionlimit(20000)
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
+
     :Attributes:
-    item : int or word
-        Represents the name of the item
-    probability : float
-        Represent the existential probability(likelihood presence) of an item
+
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
+
     :Attributes:
-    item : int
-        storing item of a node
-    probability : int
-        To maintain the expected support of node
-    parent : node
-        To maintain the parent of every node
-    children : list
-        To maintain the children of node
+
+        item : int
+          storing item of a node
+
+        probability : int
+          To maintain the expected support of node
+
+        parent : node
+          To maintain the parent of every node
+
+        children : list
+          To maintain the children of node
+
     :Methods:
         addChild(itemName)
             storing the children to their respective parent nodes
     """
 
     def __init__(self, item, children) -> None:
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
 
     def addChild(self, node) -> None:
         """
-        This function is used to add a child
+        This method adds a child node to the current node in the frequent pattern tree. It updates the children
+        dictionary of the current node with the new child node and sets the parent of the child node to the current node.
+
+        :param node: The child node to be added.
+        :type node: _Node
+        :return: None
         """
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
+
     Attributes:
-    root : Node
-        Represents the root node of the tree
-    summaries : dictionary
-        storing the nodes with same item name
-    info : dictionary
-        stores the support of items
+
+        root : Node
+          Represents the root node of the tree
+
+        summaries : dictionary
+          storing the nodes with same item name
+
+        info : dictionary
+          stores the support of items
+
     :Methods:
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
@@ -130,14 +154,15 @@
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
     def addTransaction(self, transaction) -> None:
         """
         Adding transaction into tree
+
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         """
 
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i].item not in currentNode.children:
@@ -168,14 +193,15 @@
                     currentNode.probability += transaction[i].probability
                 else:
                     currentNode.probability += max(lp) * transaction[i].probability
 
     def addConditionalPattern(self, transaction, sup) -> None:
         """
         Constructing conditional tree from prefixPaths
+
         :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
         """
 
         # This method takes transaction, support and constructs the conditional tree
@@ -193,14 +219,15 @@
             else:
                 currentNode = currentNode.children[transaction[i]]
                 currentNode.probability += sup
 
     def conditionalPatterns(self, alpha) -> Tuple[List, List, dict]:
         """
         Generates all the conditional patterns of respective node
+
         :param alpha : it represents the Node in tree
         :type alpha : _Node
         """
         # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
         sup = []
         for i in self.summaries[alpha]:
@@ -215,24 +242,26 @@
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
         return finalPatterns, support, info
 
     def removeNode(self, nodeValue) -> None:
         """
         Removing the node from tree
+
         :param nodeValue : it represents the node in tree
         :type nodeValue : node
         """
 
         for i in self.summaries[nodeValue]:
             del i.parent.children[nodeValue]
 
     def conditionalTransactions(self, condPatterns, support) -> Tuple[List, List, dict]:
         """
         It generates the conditional patterns with frequent items
+
         :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
         :support : the support of conditional pattern in tree
         :support : int
         """
         global minSup
         pat = []
@@ -255,14 +284,15 @@
                 sup.append(support[count])
                 count += 1
         return pat, sup, updatedDict
 
     def generatePatterns(self, prefix) -> None:
         """
         Generates the patterns
+
         :param prefix : forms the combination of items
         :type prefix : list
         """
 
         global _finalPatterns, minSup
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
@@ -280,53 +310,72 @@
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
 class PUFGrowth(_ab._frequentPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
-    :Reference:
-        Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
-        Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
+
+    :Reference:  Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
+                 Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
+
     :Attributes:
+
         iFile : file
             Name of the Input file or path of the input file
+
         oFile : file
             Name of the output file or path of the output file
+
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         startTime : float
             To record the start time of the mining process
+
         endTime : float
             To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
             To represent the total no of transaction
+
         tree : class
             To represents the Tree class
+
         itemSetCount : int
             To represents the total no of patterns
+
         finalPatterns : dict
             To store the complete patterns
+
     :Methods:
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -344,41 +393,75 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                    >>> python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
-            Example:
-                     >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
-            .. note:: minSup  will be considered in support count or frequency
-    **Importing this algorithm into a python program**
-    -----------------------------------------------------
+
+    Execution methods
+    =================
+
+
+    **Terminal command**
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 PUFGrowth.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+
+
+    **Calling from a python program**
+
     .. code-block:: python
+
             from PAMI.uncertainFrequentPattern.basic import puf as alg
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
             obj = alg.PUFGrowth(iFile, minSup)
-            obj.startMine()
+
+            obj.startmine()
+
             frequentPatterns = obj.getPatterns()
+
             print("Total number of Frequent Patterns:", len(frequentPatterns))
+
             obj.save(oFile)
+
             Df = obj.getPatternsAsDataFrame()
+
             memUSS = obj.getmemoryUSS()
+
             print("Total Memory in USS:", memUSS)
+
             memRSS = obj.getMemoryRSS()
+
             print("Total Memory in RSS", memRSS)
+
             run = obj.getRuntime()
+
             print("Total ExecutionTime in seconds:", run)
-    **Credits:**
-    --------------------
+
+    Credits
+    =======
+
+
              The complete program was written by  P.Likhitha  under the supervision of Professor Rage Uday Kiran.
-"""
+    """
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
@@ -447,14 +530,15 @@
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
     def _frequentOneItem(self) -> Tuple[dict, List]:
         """
         Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
         mapSupport = {}
         for i in self._Database:
             for j in i:
@@ -466,30 +550,32 @@
         plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
     @staticmethod
     def _buildTree(data, info) -> '_Tree':
         """
-        it takes the self.Database and support of each item and construct the main tree with setting root node as null
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+
         :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
         """
 
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
     def _updateTransactions(self, dict1) -> List:
         """
-        remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+
         :param dict1 : frequent items with support
         :type dict1 : dictionary
         """
 
         list1 = []
         for tr in self._Database:
             list2 = []
@@ -503,14 +589,15 @@
                 list1.append(list2)
         return list1
 
     @staticmethod
     def _check(i, x) -> int:
         """
         To check the presence of item or pattern in transaction
+
         :param x: it represents the pattern
         :type x : list
         :param i : represents the uncertain self.Database
         :type i : list
         """
 
         # This method taken a transaction as input and returns the tree
@@ -522,14 +609,15 @@
             if k == 0:
                 return 0
         return 1
 
     def _convert(self, value) -> float:
         """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
@@ -539,14 +627,15 @@
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self) -> None:
         """
         To remove the false positive patterns generated in frequent patterns
+
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
                 if len(x) == 1:
@@ -565,18 +654,25 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
+        self.mine()
+
+    def mine(self) -> None:
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         minSup = self._minSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
@@ -591,68 +687,80 @@
         self._memoryUSS = float()
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> dict:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
@@ -668,14 +776,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = PUFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Uncertain Frequent Patterns:", _ap.getPatterns())
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TUFP.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/TubeP.py`

 * *Files 9% similar despite different names*

```diff
@@ -1,118 +1,147 @@
 # TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+#             from PAMI.uncertainFrequentPattern.basic import TUFP as alg
 #
-#     obj = alg.TUFP(iFile, minSup)
+#             iFile = 'sampleDB.txt'
 #
-#     obj.startMine()
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj = alg.TUFP(iFile, minSup)
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             obj.mine()
 #
-#     obj.save(oFile)
+#             frequentPatterns = obj.getPatterns()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     memUSS = obj.getMemoryUSS()
+#             obj.save(oFile)
 #
-#     print("Total Memory in USS:", memUSS)
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memRSS = obj.getMemoryRSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     run = obj.getRuntime()
+#             memRSS = obj.getMemoryRSS()
+#
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
+
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 """
 
 
 from PAMI.uncertainFrequentPattern.basic import abstract as _ab
 from typing import List, Dict, Union
 import pandas as pd
+from deprecated import deprecated
 
 _minSup = float()
 _finalPatterns = {}
 
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
+
     :Attributes:
+
         item : int or word
-            Represents the name of the item
+          Represents the name of the item
+
         probability : float
-            Represent the existential probability(likelihood presence) of an item
+          Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability) -> None:
         self.item = item
         self.probability = probability
 
 
 class TUFP(_ab._frequentPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
-    :Reference:
-        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
-        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+
+    :Reference:  Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
+                 Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
+
     :Attributes:
+
         iFile : file
             Name of the Input file or path of the input file
+
         oFile : file
             Name of the output file or path of the output file
+
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         startTime : float
             To record the start time of the mining process
+
         endTime : float
             To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
             To represent the total no of transaction
+
         tree : class
             To represents the Tree class
+
         itemSetCount : int
             To represents the total no of patterns
+
         finalPatterns : dict
             To store the complete patterns
+
     :Methods:
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         storePatternsInFile(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsInDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
@@ -130,40 +159,74 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
-                      .. note:: minSup  will be considered in support count or frequency
-    **Importing this algorithm into a python program**
-    ------------------------------------------------------
+
+    Execution methods
+    =================
+
+
+    **Terminal command**
+
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 TUFP.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 TUFP.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+
+
+    **Calling from a python program**
+
     .. code-block:: python
+
             from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
             obj = alg.TUFP(iFile, minSup)
-            obj.startMine()
+
+            obj.mine()
+
             frequentPatterns = obj.getPatterns()
+
             print("Total number of Frequent Patterns:", len(frequentPatterns))
+
             obj.save(oFile)
+
             Df = obj.getPatternsAsDataFrame()
+
             memUSS = obj.getmemoryUSS()
+
             print("Total Memory in USS:", memUSS)
+
             memRSS = obj.getMemoryRSS()
+
             print("Total Memory in RSS", memRSS)
+
             run = obj.getRuntime()
+
             print("Total ExecutionTime in seconds:", run)
-    **Credits:**
-    ---------------
-    The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+
+    Credits
+    =======
+
+            The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
     """
 
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
@@ -231,15 +294,16 @@
                                 tr.append(product)
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
     def _frequentOneItem(self) -> List[str]:
         """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
         :param self.Database : it represents the one self.Database in database
         :type self.Database : list
         """
 
         mapSupport = {}
         k = 0
         for i in self._Database:
@@ -261,14 +325,15 @@
         self._minimum = min(list(self._finalPatterns.values()))
         return plist
 
     @staticmethod
     def _convert(value: Union[int, float, str]) -> Union[int, float]:
         """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = float(value)
@@ -278,47 +343,50 @@
             else:
                 value = int(value)
         return value
 
     def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
         """
         Saves the patterns that satisfy the periodic frequent property.
+
         :param prefix: the prefix of a pattern
         :type prefix: list
         :param suffix: the suffix of a patterns
         :type suffix: list
         :param tidSetI: the timestamp of a patterns
         :type tidSetI: dict
         """
 
         if prefix is None:
             prefix = suffix
         else:
             prefix = prefix + suffix
         val = sum(tidSetI.values())
-        # print(prefix, val)
+        #print(prefix, val)
         if len(self._finalPatterns) <= self._minSup:
             sample = str()
             for i in prefix:
                 sample = sample + i + " "
             self._finalPatterns[sample] = val
         if len(self._finalPatterns) == self._minSup:
             if val > self._minimum:
                 sample = str()
                 for i in prefix:
                     sample = sample + i + " "
                 index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
                 del self._finalPatterns[index]
                 self._finalPatterns[sample] = val
                 self._minimum = min(list(self._finalPatterns.values()))
-        # print(self.finalPatterns, self.minimum, self.minSup)
+        #print(self.finalPatterns, self.minimum, self.minSup)
+
 
     def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
         """
         Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
+
         :param prefix:  main equivalence prefix
         :type prefix: periodic-frequent item or pattern
         :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
         :type itemSets: list
         :param tidSets: timestamps of the items in the argument itemSets
         :type tidSets: list
         """
@@ -331,155 +399,176 @@
             itemI = itemSets[i]
             if itemI is None:
                 continue
             tidSetI = tidSets[i]
             classItemSets = []
             classTidSets = []
             itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
+            for j in range(i+1, len(itemSets)):
                 itemJ = itemSets[j]
                 tidSetJ = tidSets[j]
                 y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
                 sum2 = sum(list(y.values()))
-                # print(prefix, itemJ, y, sum2)
-                # if sum2 >= self.minimum:
+                #print(prefix, itemJ, y, sum2)
+                #if sum2 >= self.minimum:
                 self._save(prefix, [itemJ], y)
                 classItemSets.append(itemJ)
                 classTidSets.append(y)
-            # print(itemI, tidSetI, classItemSets)
+            #print(itemI, tidSetI, classItemSets)
             newPrefix = list(set(itemSetX)) + prefix
             self._Generation(newPrefix, classItemSets, classTidSets)
-            # self.save(prefix, list(set(itemSetX)), tidSetI)
+            #self.save(prefix, list(set(itemSetX)), tidSetI)
 
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
+        self.mine()
+
+
+    def mine(self) -> None:
+        """
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        """
         global _minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
         _minSup = self._minSup
         plist = self._frequentOneItem()
         for i in range(len(plist)):
             itemI = plist[i]
             tidSetI = self._cupList[itemI]
             itemSetX = [itemI]
             itemSets = []
             tidSets = []
-            for j in range(i + 1, len(plist)):
+            for j in range(i+1, len(plist)):
                 itemJ = plist[j]
                 tidSetJ = self._cupList[itemJ]
-                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
+                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
                 self._save(itemSetX, [itemJ], y1)
                 itemSets.append(itemJ)
                 tidSets.append(y1)
             self._Generation(itemSetX, itemSets, tidSets)
         print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a, b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
         :type outFile: file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> Dict[str, float]:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
         print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:", self.getRuntime())
+        print("Total ExecutionTime in ms:",  self.getRuntime())
 
-    if __name__ == "__main__":
-        _ap = str()
-        if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-            if len(_ab._sys.argv) == 5:
-                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-            if len(_ab._sys.argv) == 4:
-                _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
-            _ap.startMine()
-            _Patterns = _ap.getPatterns()
-            print("Total number of Patterns:", len(_Patterns))
-            _ap.save(_ab._sys.argv[2])
-            _memUSS = _ap.getMemoryUSS()
-            print("Total Memory in USS:", _memUSS)
-            _memRSS = _ap.getMemoryRSS()
-            print("Total Memory in RSS", _memRSS)
-            _run = _ap.getRuntime()
-            print("Total ExecutionTime in ms:", _run)
-        else:
-            '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
-            ap.startMine()
-            Patterns = ap.getPatterns()
-            print("Total number of Patterns:", len(Patterns))
-            ap.save("patterns.txt")
-            memUSS = ap.getMemoryUSS()
-            print("Total Memory in USS:", memUSS)
-            memRSS = ap.getMemoryRSS()
-            print("Total Memory in RSS", memRSS)
-            run = ap.getRuntime()
-            print("Total ExecutionTime in ms:", run)'''
-            print("Error! The number of input parameters do not match the total number of parameters provided")
+if __name__ == "__main__":
+    _ap = str()
+    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
+        if len(_ab._sys.argv) == 5:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+        if len(_ab._sys.argv) == 4:
+            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+        _ap.startMine()
+        _ap.mine()
+        _Patterns = _ap.getPatterns()
+        print("Total number of Patterns:", len(_Patterns))
+        _ap.save(_ab._sys.argv[2])
+        _memUSS = _ap.getMemoryUSS()
+        print("Total Memory in USS:", _memUSS)
+        _memRSS = _ap.getMemoryRSS()
+        print("Total Memory in RSS", _memRSS)
+        _run = _ap.getRuntime()
+        print("Total ExecutionTime in ms:", _run)
+    else:
+        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
+        ap.startMine()
+        Patterns = ap.getPatterns()
+        print("Total number of Patterns:", len(Patterns))
+        ap.save("patterns.txt")
+        memUSS = ap.getMemoryUSS()
+        print("Total Memory in USS:", memUSS)
+        memRSS = ap.getMemoryRSS()
+        print("Total Memory in RSS", memRSS)
+        run = ap.getRuntime()
+        print("Total ExecutionTime in ms:", run)'''
+        print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeP.py` & `pami-2024.5.1/PAMI/frequentPattern/basic/FPGrowth.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,486 +1,538 @@
-# TUFP is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
+# FPGrowth is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It  employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TUFP as alg
+#             from PAMI.frequentPattern.basic import FPGrowth as alg
 #
-#     obj = alg.TUFP(iFile, minSup)
+#             iFile = 'sampleDB.txt'
 #
-#     obj.startMine()
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj = alg.FPGrowth(iFile, minSup)
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             obj.mine()
 #
-#     obj.save(oFile)
+#             frequentPatterns = obj.getPatterns()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#     memUSS = obj.getMemoryUSS()
+#             obj.save(oFile)
 #
-#     print("Total Memory in USS:", memUSS)
+#             Df = obj.getPatternInDataFrame()
 #
-#     memRSS = obj.getMemoryRSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     run = obj.getRuntime()
+#             memRSS = obj.getMemoryRSS()
+#
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
-#     print("Total ExecutionTime in seconds:", run)
+
 
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
 """
 
+from PAMI.frequentPattern.basic import abstract as _fp
+from typing import List, Dict, Tuple, Any
+from deprecated import deprecated
+from itertools import combinations
+from collections import Counter
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Union
-import pandas as pd
-
-_minSup = float()
-_finalPatterns = {}
+_minSup = str()
+_fp._sys.setrecursionlimit(20000)
 
 
-class _Item:
+class _Node:
     """
-    A class used to represent the item with probability in transaction of dataset
-    :Attributes:
-        item : int or word
-            Represents the name of the item
-        probability : float
-            Represent the existential probability(likelihood presence) of an item
+    A class used to represent the node of frequentPatternTree
+
+    :**Attributes**:    - **itemId** (*int*) -- *storing item of a node.*
+                        - **counter** (*int*) -- *To maintain the support of node.*
+                        - **parent** (*node*) -- *To maintain the parent of node.*
+                        - **children** (*list*) -- *To maintain the children of node.*
+
+    :**Methods**:   - **addChild(node)** -- *Updates the nodes children list and parent for the given node.*
     """
 
-    def __init__(self, item, probability) -> None:
+    def __init__(self, item, count, parent) -> None:
         self.item = item
-        self.probability = probability
+        self.count = count
+        self.parent = parent
+        self.children = {}
+
+    def addChild(self, item, count = 1) -> Any:
+        """
 
+        Adds a child node to the current node with the specified item and count.
 
-class TUFP(_ab._frequentPatterns):
+        :param item: The item associated with the child node.
+        :type item: List
+        :param count: The count or support of the item. Default is 1.
+        :type count: int
+        :return: The child node added.
+        :rtype: List
+        """
+        if item not in self.children:
+            self.children[item] = _Node(item, count, self)
+        else:
+            self.children[item].count += count
+        return self.children[item]
+    
+    def traverse(self) -> Tuple[List[int], int]:
+        """
+        Traversing the tree to get the transaction
+
+        :return: transaction and count of each item in transaction
+        :rtype: Tuple, List and int
+        """
+        transaction = []
+        count = self.count
+        node = self.parent
+        while node.parent is not None:
+            transaction.append(node.item)
+            node = node.parent
+        return transaction[::-1], count
+
+
+class FPGrowth(_fp._frequentPatterns):
     """
-    :Description: It is one of the fundamental algorithm to discover top-k frequent patterns in a uncertain transactional database using CUP-Lists.
-    :Reference:
-        Tuong Le, Bay Vo, Van-Nam Huynh, Ngoc Thanh Nguyen, Sung Wook Baik 5, "Mining top-k frequent patterns from uncertain databases",
-        Springer Science+Business Media, LLC, part of Springer Nature 2020, https://doi.org/10.1007/s10489-019-01622-1
-    :Attributes:
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
-        memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
-            To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            To represent the total no of transaction
-        tree : class
-            To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
-        finalPatterns : dict
-            To store the complete patterns
-    :Methods:
-        startMine()
-            Mining process will start from here
-        getPatterns()
-            Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
-            Complete set of frequent patterns will be loaded in to a dataframe
-        getMemoryUSS()
-            Total amount of USS memory consumed by the mining process will be retrieved from this function
-        getMemoryRSS()
-            Total amount of RSS memory consumed by the mining process will be retrieved from this function
-        getRuntime()
-            Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-        updateTransactions()
-            Update the transactions by removing non-frequent items and sort the Database by item decreased support
-        buildTree()
-            After updating the Database, remaining items will be added into the tree by setting root node as null
-        convert()
-            to convert the user specified value
-        startMine()
-            Mining process will start from this function
-    **Methods to execute code on terminal**
-    -----------------------------------------
-            Format:
-                      >>> python3 TUFP.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 TUFP.py sampleTDB.txt patterns.txt 0.6
-                      .. note:: minSup  will be considered in support count or frequency
-    **Importing this algorithm into a python program**
-    ------------------------------------------------------
+    About this algorithm
+    ====================
+
+    :**Description**:   FPGrowth is one of the fundamental algorithm to discover frequent patterns in a transactional database. It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
+
+    :**Reference**:  Han, J., Pei, J., Yin, Y. et al. Mining Frequent Patterns without Candidate Generation: A Frequent-Pattern
+                     Tree Approach. Data  Mining and Knowledge Discovery 8, 53–87 (2004). https://doi.org/10.1023
+
+    :**Parameters**:    - **iFile** (*str or URL or dataFrame*) -- *Name of the Input file to mine complete set of frequent patterns.*
+                        - **oFile** (*str*) -- *Name of the output file to store complete set of frequent patterns.*
+                        - **minSup** (*int or float or str*) -- *The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.*
+                        - **sep** (*str*) -- *This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.*
+
+    :**Attributes**:    - **startTime** (*float*) -- *To record the start time of the mining process.*
+                        - **endTime** (*float*) -- *To record the completion time of the mining process.*
+                        - **finalPatterns** (*dict*) -- *Storing the complete set of patterns in a dictionary variable.*
+                        - **memoryUSS** (*float*) -- *To store the total amount of USS memory consumed by the program.*
+                        - **memoryRSS** (*float*) -- *To store the total amount of RSS memory consumed by the program.*
+                        - **Database** (*list*) -- *To store the transactions of a database in list.*
+                        - **mapSupport** (*Dictionary*) -- *To maintain the information of item and their frequency.*
+                        - **tree** (*class*) --  *it represents the Tree class.*
+
+
+    Execution methods
+    =================
+
+    **Terminal command**
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FPGrowth.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 FPGrowth.py sampleDB.txt patterns.txt 10.0
+
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
+
+
+    **Calling from a python program**
+
     .. code-block:: python
-            from PAMI.uncertainFrequentPattern.basic import TUFP as alg
-            obj = alg.TUFP(iFile, minSup)
-            obj.startMine()
+
+            from PAMI.frequentPattern.basic import FPGrowth as alg
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            obj = alg.FPGrowth(iFile, minSup)
+
+            obj.mine()
+
             frequentPatterns = obj.getPatterns()
+
             print("Total number of Frequent Patterns:", len(frequentPatterns))
-            obj.save(oFile)
-            Df = obj.getPatternsAsDataFrame()
-            memUSS = obj.getmemoryUSS()
+
+            obj.savePatterns(oFile)
+
+            Df = obj.getPatternInDataFrame()
+
+            memUSS = obj.getMemoryUSS()
+
             print("Total Memory in USS:", memUSS)
+
             memRSS = obj.getMemoryRSS()
+
             print("Total Memory in RSS", memRSS)
+
             run = obj.getRuntime()
+
             print("Total ExecutionTime in seconds:", run)
-    **Credits:**
-    ---------------
-             The complete program was written by   P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+
+
+    Credits:
+    ========
+
+    The complete program was written by P. Likhitha and revised by Tarun Sreepada under the supervision of Professor Rage Uday Kiran.
+
     """
 
-    _startTime = float()
-    _endTime = float()
+    __startTime = float()
+    __endTime = float()
     _minSup = str()
-    _finalPatterns = {}
+    __finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
-    _memoryUSS = float()
-    _memoryRSS = float()
-    _Database = []
-    _cupList = {}
-    _topk = {}
-    _minimum = 9999
-
-    def _creatingItemSets(self) -> None:
-        """
-        Scans the dataset
-        """
-        self._Database = []
-        if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
+    __memoryUSS = float()
+    __memoryRSS = float()
+    __Database = []
+    __mapSupport = {}
+    __lno = 0
+    __rank = {}
+    __rankDup = {}
+
+    def __init__(self, iFile, minSup, sep='\t') -> None:
+        super().__init__(iFile, minSup, sep)
+
+    def __creatingItemSets(self) -> None:
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        self.__Database = []
+        if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
+                self.__Database = self._iFile['Transactions'].tolist()
 
-            # print(self.Database)
+            #print(self.Database)
         if isinstance(self._iFile, str):
-            if _ab._validators.url(self._iFile):
-                data = _ab._urlopen(self._iFile)
+            if _fp._validators.url(self._iFile):
+                data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
+                            line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self) -> List[str]:
+    def __convert(self, value) -> float:
         """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        :param self.Database : it represents the one self.Database in database
-        :type self.Database : list
-        """
-
-        mapSupport = {}
-        k = 0
-        for i in self._Database:
-            k += 1
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[j.item] = j.probability
-                    self._cupList[j.item] = {k:j.probability}
-                else:
-                    mapSupport[j.item] += j.probability
-                    self._cupList[j.item].update({k: j.probability})
-        plist = [k for k,v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        k = 0
-        for x, in plist:
-            k +=1
-            if k >= self._minSup:
-                break
-            self._finalPatterns[x] = mapSupport[x]
-        self._minimum = min(list(self._finalPatterns.values()))
-        return plist
 
-    @staticmethod
-    def _convert(value: Union[int, float, str]) -> Union[int, float]:
-        """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
-        :return: converted type minSup value
+        :return: converted type
+        :rtype: float
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = float(value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
-
-    def _save(self, prefix: List[str], suffix: List[str], tidSetI: Dict[int, float]) -> None:
+    
+    def _construct(self, items, data, minSup):
         """
-        Saves the patterns that satisfy the periodic frequent property.
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
+        Constructs the FP-tree from the given transactions.
+
+        :param items: A dictionary containing item frequencies.
+        :type items: Dict
+        :param data: A list of transactions.
+        :type data: List
+        :param minSup: The minimum support threshold.
+        :type minSup: int
+        :return: The root node of the constructed FP-tree and a dictionary containing information about nodes associated with each item.
+        :rtype: Tuple[_Node, Dict]
+        """
+
+        items = {k: v for k, v in items.items() if v >= minSup}
+
+        root = _Node([], 0, None)
+        itemNodes = {}
+        for line in data:
+            currNode = root
+            line = sorted([item for item in line if item in items], key = lambda x: items[x], reverse = True)
+            for item in line:
+                currNode = currNode.addChild(item)
+                if item in itemNodes:
+                    itemNodes[item][0].add(currNode)
+                    itemNodes[item][1] += 1
+                else:
+                    itemNodes[item] = [set([currNode]), 1]
+
+        return root, itemNodes
+
+    def _all_combinations(self, arr):
         """
 
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = sum(tidSetI.values())
-        #print(prefix, val)
-        if len(self._finalPatterns) <= self._minSup:
-            sample = str()
-            for i in prefix:
-                sample = sample + i + " "
-            self._finalPatterns[sample] = val
-        if len(self._finalPatterns) == self._minSup:
-            if val > self._minimum:
-                sample = str()
-                for i in prefix:
-                    sample = sample + i + " "
-                index = list(self._finalPatterns.keys())[list(self._finalPatterns.values()).index(self._minimum)]
-                del self._finalPatterns[index]
-                self._finalPatterns[sample] = val
-                self._minimum = min(list(self._finalPatterns.values()))
-        #print(self.finalPatterns, self.minimum, self.minSup)
-
-
-    def _Generation(self, prefix: List[str], itemSets: List[str], tidSets: List[Dict[int, float]]) -> None:
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(0, len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
+        Generates all possible combinations of items from a given transaction.
+
+        :param arr: A list of items in a transaction.
+        :type arr: List
+        :return: A list containing all possible combinations of items.
+        :rtype: List
+        """
+
+        all_combinations_list = []
+        for r in range(1, len(arr) + 1):
+            all_combinations_list.extend(combinations(arr, r))
+        return all_combinations_list
+    
+    def _recursive(self, root, itemNode, minSup, patterns):
+        """
+
+         Recursively explores the FP-tree to generate frequent patterns.
+
+         :param root: The root node of the current subtree.
+         :type root: _Node
+         :param itemNode: A dictionary containing information about the nodes associated with each item.
+         :type itemNode: Dict
+         :param minSup: The minimum support threshold.
+         :type minSup: int
+         :param patterns: A dictionary to store the generated frequent patterns.
+         :type patterns: Dict
+        """
+        itemNode = {k: v for k, v in sorted(itemNode.items(), key = lambda x: x[1][1])}
+
+        for item in itemNode:
+            if itemNode[item][1] < self._minSup:
+                break 
+
+            newRoot = _Node(root.item + [item], 0, None)
+            # pat = "\t".join([str(i) for i in newRoot.item])
+            # self.__finalPatterns[pat] = itemNode[item][1]
+            self._finalPatterns[tuple(newRoot.item)] = itemNode[item][1]
+            newItemNode = {}
+
+            if len(itemNode[item][0]) == 1:
+                transaction, count = itemNode[item][0].pop().traverse()
+                if len(transaction) == 0:
+                    continue
+                combination = self._all_combinations(transaction)
+                for comb in combination:
+                    # pat = "\t".join([str(i) for i in comb])
+                    # pat = pat + "\t" + "\t".join([str(i) for i in newRoot.item])
+                    # self.__finalPatterns[pat] = count
+                    self._finalPatterns[tuple(list(comb) + newRoot.item)] = count
+                pass
+
+
+            itemCount = {}
+            transactions = {}
+            for node in itemNode[item][0]:
+                transaction, count = node.traverse()
+                if len(transaction) == 0:
+                    continue
+                if tuple(transaction) in transactions:
+                    transactions[tuple(transaction)] += count
+                else:
+                    transactions[tuple(transaction)] = count
+
+
+                for item in transaction:
+                    if item in itemCount:
+                        itemCount[item] += count
+                    else:
+                        itemCount[item] = count
+
+
+            # remove items that are below minSup
+            itemCount = {k: v for k, v in itemCount.items() if v >= minSup}
+            if len(itemCount) == 0:
                 continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i+1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = {key: tidSetJ[key] * tidSetI.get(key, 0) for key in tidSetJ.keys()}
-                sum2 = sum(list(y.values()))
-                #print(prefix, itemJ, y, sum2)
-                #if sum2 >= self.minimum:
-                self._save(prefix, [itemJ], y)
-                classItemSets.append(itemJ)
-                classTidSets.append(y)
-            #print(itemI, tidSetI, classItemSets)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            #self.save(prefix, list(set(itemSetX)), tidSetI)
 
-    def startMine(self) -> None:
+            for transaction, count in transactions.items():
+                transaction = sorted([item for item in transaction if item in itemCount], key = lambda x: itemCount[x], reverse = True)
+                currNode = newRoot
+                for item in transaction:
+                    currNode = currNode.addChild(item, count)
+                    if item in newItemNode:
+                        newItemNode[item][0].add(currNode)
+                        newItemNode[item][1] += count
+                    else:
+                        newItemNode[item] = [set([currNode]), count]
+
+            if len(newItemNode) < 1:
+                continue
+
+            # mine(newRoot, newItemNode, minSup, patterns)
+            self._recursive(newRoot, newItemNode, minSup, patterns)
+
+
+    def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        Main program to start the operation
         """
         global _minSup
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
+        self.__startTime = _fp._time.time()
+        if self._iFile is None:
+            raise Exception("Please enter the file path or file name:")
+        if self._minSup is None:
+            raise Exception("Please enter the Minimum Support")
+        self.__creatingItemSets()
+        self._minSup = self.__convert(self._minSup)
         _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._cupList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._cupList[itemJ]
-                y1 = {key: tidSetJ[key] * tidSetI.get(key, 0)  for key in tidSetJ.keys()}
-                self._save(itemSetX, [itemJ], y1)
-                itemSets.append(itemJ)
-                tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-        print("Top-K Frequent patterns were generated from uncertain databases successfully using TUFP algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+
+        itemCount = Counter()
+        for line in self.__Database:
+            itemCount.update(line)
+
+        root, itemNode = self._construct(itemCount, self.__Database, self._minSup)
+        self._recursive(root, itemNode, self._minSup, self.__finalPatterns)
+        
+        print("Frequent patterns were generated successfully using frequentPatternGrowth algorithm")
+        self.__endTime = _fp._time.time()
+        self.__memoryUSS = float()
+        self.__memoryRSS = float()
+        process = _fp._psutil.Process(_fp._os.getpid())
+        self.__memoryUSS = process.memory_full_info().uss
+        self.__memoryRSS = process.memory_info().rss
+
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self):
+        """
+        Starting the mining process
+        """
+        self.mine()
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryUSS
+        return self.__memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self._memoryRSS
+        return self.__memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self._endTime - self._startTime
+        return self.__endTime - self.__startTime
+    
 
-    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
-        data = []
-        for a, b in self._finalPatterns.items():
-            data.append([a, b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataframe
+        # dataframe = {}
+        # data = []
+        # for a, b in self.__finalPatterns.items():
+        #     data.append([a.replace('\t', ' '), b])
+        #     dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        dataFrame = _fp._pd.DataFrame(list(self._finalPatterns.items()), columns=['Patterns', 'Support'])
+        return dataFrame
 
     def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
+
         :param outFile: name of the output file
-        :type outFile: file
+        :type outFile: csvfile
+        :return: None
         """
-        self.oFile = outFile
-        writer = open(self.oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x + ":" + str(y)
-            writer.write("%s \n" % s1)
+        with open(outFile, 'w') as f:
+            for x, y in self._finalPatterns.items():
+                x = self._sep.join(x)
+                f.write(f"{x} : {y}\n")
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self) -> Dict[str, int]:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
-
+    
     def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in ms:", self.getRuntime())
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
-        if len(_ab._sys.argv) == 5:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
-        if len(_ab._sys.argv) == 4:
-            _ap = TUFP(_ab._sys.argv[1], _ab._sys.argv[3])
+    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
+        if len(_fp._sys.argv) == 5:
+            _ap = FPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
+        if len(_fp._sys.argv) == 4:
+            _ap = FPGrowth(_fp._sys.argv[1], _fp._sys.argv[3])
         _ap.startMine()
-        _Patterns = _ap.getPatterns()
-        print("Total number of Patterns:", len(_Patterns))
-        _ap.save(_ab._sys.argv[2])
-        _memUSS = _ap.getMemoryUSS()
-        print("Total Memory in USS:", _memUSS)
-        _memRSS = _ap.getMemoryRSS()
-        print("Total Memory in RSS", _memRSS)
-        _run = _ap.getRuntime()
-        print("Total ExecutionTime in ms:", _run)
+        _ap.mine()
+        print("Total number of Frequent Patterns:", len( _ap.getPatterns()))
+        _ap.save(_fp._sys.argv[2])
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
-        '''ap = TUFP("/home/apiiit-rkv/Desktop/uncertain/tubeSample", 10, ' ')
-        ap.startMine()
-        Patterns = ap.getPatterns()
-        print("Total number of Patterns:", len(Patterns))
-        ap.save("patterns.txt")
-        memUSS = ap.getMemoryUSS()
-        print("Total Memory in USS:", memUSS)
-        memRSS = ap.getMemoryRSS()
-        print("Total Memory in RSS", memRSS)
-        run = ap.getRuntime()
-        print("Total ExecutionTime in ms:", run)'''
         print("Error! The number of input parameters do not match the total number of parameters provided")
-
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/TubeS.py` & `pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/basic/WUFIM.py`

 * *Files 21% similar despite different names*

```diff
@@ -1,381 +1,402 @@
-# TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
+# WUFIM is one of the algorithm to discover weighted frequent patterns in an uncertain transactional database using PUF-Tree.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
-#     from PAMI.uncertainFrequentPattern.basic import TubeS as alg
+#             from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
 #
-#     obj = alg.TubeS(iFile, minSup)
+#             iFile = 'sampleDB.txt'
 #
-#     obj.startMine()
+#             minSup = 10
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj = alg.basic(iFile, wFile, minSup, sep)
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             obj.mine()
 #
-#     obj.save(oFile)
+#             Patterns = obj.getPatterns()
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             print("Total number of  Patterns:", len(Patterns))
 #
-#     memUSS = obj.getMemoryUSS()
+#             obj.save(oFile)
 #
-#     print("Total Memory in USS:", memUSS)
+#             Df = obj.getPatternsAsDataFrame()
 #
-#     memRSS = obj.getMemoryRSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     run = obj.getRuntime()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total Memory in RSS", memRSS)
+#
+#             run = obj.getRuntime()
+#
+#             print("Total ExecutionTime in seconds:", run)
 #
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+
      You should have received a copy of the GNU General Public License
-     along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
+     along with this program.  If not, see `<https://www.gnu.org/licenses/>`_.    
 """
 
-from PAMI.uncertainFrequentPattern.basic import abstract as _fp
-
-_minSup = float()
-_fp._sys.setrecursionlimit(20000)
+from PAMI.weightedUncertainFrequentPattern.basic import abstract as _ab
+import pandas as pd
+from deprecated import deprecated
+
+_expSup = str()
+_expWSup = str()
+_weights = {}
 _finalPatterns = {}
-
-
+_ab._sys.setrecursionlimit(20000)
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
-    :Attributes
+
+    :Attributes:
+
         item : int or word
-            Represents the name of the item
+          Represents the name of the item
+
         probability : float
-            Represent the existential probability(likelihood presence) of an item
+          Represent the existential probability(likelihood presence) of an item
     """
 
-    def __init__(self, item, probability):
+    def __init__(self, item: int, probability: float) -> None:
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
+
     :Attributes:
+
         item : int
-            storing item of a node
+          storing item of a node
+
         probability : int
-            To maintain the expected support of node
+          To maintain the expected support of node
+
         parent : node
-            To maintain the parent of every node
+          To maintain the parent of every node
+
         children : list
-            To maintain the children of node
+          To maintain the children of node
+
     :Methods:
+
         addChild(itemName)
-             storing the children to their respective parent nodes
+            storing the children to their respective parent nodes
     """
 
-    def __init__(self, item, children):
+    def __init__(self, item, children: list) -> None:
         self.item = item
         self.probability = 1
-        self.secondProbability = 1
         self.children = children
         self.parent = None
 
-    def addChild(self, node):
+    def addChild(self, node) -> None:
         """
-        This function is used to add child
+        This method is used to add a child node to the current node in the frequent pattern tree.
+
+        :param node:The node to be added as a child
+        :type node:_Node
+        :return: None
         """
         self.children[node.item] = node
         node.parent = self
 
 
-def Second(transaction, i):
-    """
-    To calculate the second probability of a node in transaction
-    :param transaction: transaction in a database
-    :param i: index of item in transaction
-    :return: second probability of a node
-    """
-    temp = []
-    for j in range(0, i):
-        temp.append(transaction[j].probability)
-    l1 = max(temp)
-    temp.remove(l1)
-    l2 = max(temp)
-    return l2 * l2
-
-
-def printTree(root):
-    """
-    To print the tree with root node through recursion
-    :param root: root node of  tree
-    :return: details of tree
-    """
-    for x, y in root.children.items():
-        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
-        printTree(y)
-
-
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
+
     :Attributes:
+
         root : Node
             Represents the root node of the tree
+
         summaries : dictionary
             storing the nodes with same item name
+
         info : dictionary
             stores the support of items
+
     :Methods:
+
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
-        addConditionalTransaction(prefixPaths, supportOfItems)
+        addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
             generates the conditional patterns from tree for specific node
         conditionalTransactions(prefixPaths,Support)
-            takes the prefixPath of a node and support at child of the path and extract the frequent items from
-            prefixPaths and generates prefixPaths with items which are frequent
-        removeNode(Node)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from prefixPaths and generates prefixPaths with items which are frequent
+        remove(Node)
             removes the node from tree once after generating all the patterns respective to the node
-        generate_patterns(Node)
+        generatePatterns(Node)
             starts from the root node of the tree and mines the frequent patterns
-            """
 
-    def __init__(self):
+    """
+
+    def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction):
+    def addTransaction(self, transaction) -> None:
         """
-        adding transaction into tree
-        :param transaction : it represents the one transactions in database
-        :type transaction : list
+        Adding transaction into tree
+
+        :param transaction: it represents the one self.Database in database
+        :type transaction: list
+        :return: None
         """
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
             if transaction[i].item not in currentNode.children:
                 newNode = _Node(transaction[i].item, {})
-                newNode.k = k
-                if k >= 3:
-                    newNode.secondProbability = Second(transaction, i)
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
-                    newNode.probability = round(transaction[i].probability, 2)
+                if len(lp) == 0:
+                    newNode.probability = transaction[i].probability
                 else:
-                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
+                    newNode.probability = max(lp) * transaction[i].probability
                 currentNode.addChild(newNode)
                 if transaction[i].item in self.summaries:
                     self.summaries[transaction[i].item].append(newNode)
                 else:
                     self.summaries[transaction[i].item] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i].item]
-                if k >= 3:
-                    currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
-                currentNode.k = k
                 l1 = i - 1
-                temp = []
+                lp = []
                 while l1 >= 0:
-                    temp.append(transaction[l1].probability)
+                    lp.append(transaction[l1].probability)
                     l1 -= 1
-                if len(temp) == 0:
-                    currentNode.probability += round(transaction[i].probability, 2)
+                if len(lp) == 0:
+                    currentNode.probability += transaction[i].probability
                 else:
-                    nn = max(temp) * transaction[i].probability
-                    currentNode.probability += round(nn, 2)
+                    currentNode.probability += max(lp) * transaction[i].probability
 
-    def addConditionalTransaction(self, transaction, sup, second):
+    def addConditionalPattern(self, transaction, sup) -> None:
         """
         constructing conditional tree from prefixPaths
-        :param transaction : it represents the one transactions in database
+
+        :param transaction : it represents the one self.Database in database
         :type transaction : list
         :param sup : support of prefixPath taken at last child of the path
         :type sup : int
-        :param second: second probability of the leaf node
-        :type second: float
+        :return: None
         """
+        # This method takes transaction, support and constructs the conditional tree
         currentNode = self.root
-        k = 0
         for i in range(len(transaction)):
-            k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
-                newNode.k = k
-                newNode.secondProbability = second
                 newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-                currentNode.k = k
-                currentNode.secondProbability = max(currentNode.secondProbability, second)
                 currentNode.probability += sup
 
-    def conditionalPatterns(self, alpha):
+    def conditionalPatterns(self, alpha) -> tuple:
         """
         generates all the conditional patterns of respective node
+
         :param alpha : it represents the Node in tree
         :type alpha : _Node
+        :return: tuple
         """
+        # This method generates conditional patterns of node by traversing the tree
         finalPatterns = []
         sup = []
-        second = []
         for i in self.summaries[alpha]:
             s = i.probability
-            s1 = i.secondProbability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                second.append(s1)
                 sup.append(s)
         finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
-        return finalPatterns, support, info, second
+        return finalPatterns, support, info
+
+    def removeNode(self, nodeValue) -> None:
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        :return: None
+        """
+
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
 
-    def conditionalTransactions(self, condPatterns, support):
+    def conditionalTransactions(self, condPatterns, support) -> tuple:
         """
         It generates the conditional patterns with frequent items
-        :param condPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+
+        :param condPatterns : conditionalPatterns generated from conditionalPattern method for respective node
         :type condPatterns : list
-        :param support : the support of conditional pattern in tree
-        :type support : list
+        :support : the support of conditional pattern in tree
+        :support : int
+        :return: tuple
         """
-        global _minSup
+        global _expSup, _expWSup
         pat = []
         sup = []
-        data1 = {}
+        count = {}
         for i in range(len(condPatterns)):
             for j in condPatterns[i]:
-                if j in data1:
-                    data1[j] += support[i]
+                if j in count:
+                    count[j] += support[i]
                 else:
-                    data1[j] = support[i]
+                    count[j] = support[i]
         updatedDict = {}
-        updatedDict = {k: v for k, v in data1.items() if v >= _minSup}
+        updatedDict = {k: v for k, v in count.items() if v >= _expSup}
         count = 0
         for p in condPatterns:
             p1 = [v for v in p if v in updatedDict]
-            trans = sorted(p1, key=lambda x: (updatedDict.get(x)), reverse=True)
+            trans = sorted(p1, key=lambda x: updatedDict[x], reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
                 sup.append(support[count])
-            count += 1
+                count += 1
         return pat, sup, updatedDict
 
-    def removeNode(self, nodeValue):
+    def generatePatterns(self, prefix) -> None:
         """
-        removing the node from tree
-        :param nodeValue : it represents the node in tree
-        :type nodeValue : node
-        """
-        for i in self.summaries[nodeValue]:
-            del i.parent.children[nodeValue]
+        Generates the patterns
 
-    def generatePatterns(self, prefix):
-        """
-        generates the patterns
         :param prefix : forms the combination of items
         :type prefix : list
+        :return: None
         """
-        global _finalPatterns, _minSup
+
+        global _finalPatterns, _expSup, _expWSup, _weights
         for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
-            s = 0
-            for x in self.summaries[i]:
-                #if x.k <= 2:
-                    #s += x.probability
-                #elif x.k >= 3:
-                    #n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    #s += n
-                if len(pattern) <= 2:
-                    s += x.probability
-                elif len(pattern) >= 3:
-                    n = x.probability * pow(x.secondProbability, (x.k - 2))
-                    s += n
-            _finalPatterns[tuple(pattern)] = self.info[i]
-            if s >= _minSup:
-                patterns, support, info, second = self.conditionalPatterns(i)
+            weight = 0
+            for k in pattern:
+                weight = weight + _weights[k]
+            weight = weight/len(pattern)
+            if self.info.get(i) >= _expSup and self.info.get(i) * weight >= _expWSup:
+                _finalPatterns[tuple(pattern)] = self.info.get(i)
+                patterns, support, info = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addConditionalTransaction(patterns[pat], support[pat], second[pat])
+                    conditionalTree.addConditionalPattern(patterns[pat], support[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
-
-class TubeS(_fp._frequentPatterns):
+class WUFIM(_ab._weightedFrequentPatterns):
     """
-    :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
-    :Reference:
-        Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
-        In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893–898. https://doi.org/10.1109/ICDM.2014.146
+    About this algorithm
+    ====================
+
+    :Description: It is one of the algorithm to discover weighted frequent patterns in a uncertain transactional database using PUF-Tree.
+
+    :Reference: Efficient Mining of Weighted Frequent Itemsets in Uncertain Databases.
+           In : Machine Learning and Data Mining in Pattern Recognition book Chun-Wei Jerry Lin, Wensheng Gan, Philippe Fournier Viger, Tzung-Pei Hong
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of Weighted Uncertain Periodic Frequent Patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of Weighted  Uncertain Periodic Frequent Patterns
+    :param  minSup: str:
+                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  wFile: str :
+                    This is a weighted file.
+
+
     :Attributes:
+
         iFile : file
-            Name of the Input file or path of the input file
+          Name of the Input file or path of the input file
+
+        wFile : file
+          Name of the Input file or path of the input file
+
         oFile : file
-            Name of the output file or path of the output file
+          Name of the output file or path of the output file
+
         minSup : float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+          The user can specify minSup either in count or proportion of database size.
+          If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+          Otherwise, it will be treated as float.
+          Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
+          This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
+          However, the users can override their default separator.
+
         memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+          To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
-            To store the total amount of RSS memory consumed by the program
-        startTime : float
-            To record the start time of the mining process
-        endTime : float
-            To record the completion time of the mining process
+          To store the total amount of RSS memory consumed by the program
+
+        startTime:float
+          To record the start time of the mining process
+
+        endTime:float
+          To record the completion time of the mining process
+
         Database : list
-            To store the transactions of a database in list
+          To store the transactions of a database in list
+
         mapSupport : Dictionary
-            To maintain the information of item and their frequency
+          To maintain the information of item and their frequency
+
         lno : int
-            To represent the total no of transaction
+          To represent the total no of transaction
+
         tree : class
-            To represents the Tree class
+          To represents the Tree class
+
         itemSetCount : int
-            To represents the total no of patterns
+          To represents the total no of patterns
+
         finalPatterns : dict
-            To store the complete patterns
+          To store the complete patterns
+
     :Methods:
+
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
@@ -392,330 +413,467 @@
             Extracts the one-length frequent patterns from database
         updateTransactions()
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
-    **Methods to execute code on terminal**
-    --------------------------------------------
-            Format:
-                      >>> python3 TubeS.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 TubeS.py sampleTDB.txt patterns.txt 3
-                    .. note:: minSup  will be considered in support count or frequency
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------
+        startMine()
+            Mining process will start from this function
+
+    Execution methods
+    =================
+
+
+    **Terminal command**
+
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup>
+
+      Example Usage:
+
+      (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 3
+
+    .. note:: minSup  will be considered in support count or frequency
+
+    **Calling from a python program**
+
     .. code-block:: python
-            from PAMI.uncertainFrequentPattern.basic import TubeS as alg
-            obj = alg.TubeS(iFile, minSup)
-            obj.startMine()
-            frequentPatterns = obj.getPatterns()
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+
+            from PAMI.weightedUncertainFrequentPattern.basic import basic as alg
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            obj = alg.basic(iFile, wFile, expSup, expWSup)
+
+            obj.mine()
+
+            Patterns = obj.getPatterns()
+
+            print("Total number of  Patterns:", len(Patterns))
+
             obj.save(oFile)
+
             Df = obj.getPatternsAsDataFrame()
+
             memUSS = obj.getMemoryUSS()
+
             print("Total Memory in USS:", memUSS)
+
             memRSS = obj.getMemoryRSS()
+
             print("Total Memory in RSS", memRSS)
+
             run = obj.getRuntime()
+
             print("Total ExecutionTime in seconds:", run)
-    **Credits:**
-    ---------------
-    The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
-"""
+
+   """
     _startTime = float()
     _endTime = float()
-    _minSup = float()
+    _minSup = str()
     _finalPatterns = {}
     _iFile = " "
+    _wFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
     _rank = {}
-    _lno = 0
-    def __init__(self, iFile, minSup, sep='\t'):
-        super().__init__(iFile, minSup, sep)
-    def _creatingItemSets(self):
+    _expSup = float()
+    _expWSup = float()
+
+    def __init__(self, iFile, wFile, expSup, expWSup, sep='\t') -> None:
+        super().__init__(iFile, wFile, expSup, expWSup, sep)
+
+    def _creatingItemSets(self) -> None:
         """
-        Scans the databases and stores the transactions into Database variable
+        Scans the uncertain transactional dataset
+
+        :return: None
         """
         self._Database = []
-        if isinstance(self._iFile, _fp._pd.DataFrame):
+        if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
             if 'uncertain' in i:
                 uncertain = self._iFile['uncertain'].tolist()
             for k in range(len(data)):
                 tr = []
                 for j in range(len(data[k])):
                     product = _Item(data[k][j], uncertain[k][j])
                     tr.append(product)
                 self._Database.append(tr)
-                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
-            if _fp._validators.url(self._iFile):
-                data = _fp._urlopen(self._iFile)
+            if _ab._validators.url(self._iFile):
+                data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line = line.strip()
                     line = line.decode("utf-8")
-                    temp1 = line.split(':')
-                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
-                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
+                    line = line.strip()
+                    line = [i for i in line.split(':')]
+                    temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                    temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                    temp1 = [x for x in temp1 if x]
+                    temp2 = [x for x in temp2 if x]
                     tr = []
-                    for i in range(len(temp)):
-                        item = temp[i]
-                        probability = uncertain[i]
+                    for i in range(len(temp1)):
+                        item = temp1[i]
+                        probability = float(temp2[i])
                         product = _Item(item, probability)
                         tr.append(product)
-                    self._lno += 1
-                    self._Database.append(temp)
+                    self._Database.append(tr)
             else:
                 try:
                     with open(self._iFile, 'r') as f:
                         for line in f:
-                            temp1 = line.strip()
-                            temp1 = temp1.split(':')
-                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
-                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
+                            line = line.strip()
+                            line = [i for i in line.split(':')]
+                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
+                            temp2 = [i.rstrip() for i in line[1].split(self._sep)]
+                            temp1 = [x for x in temp1 if x]
+                            temp2 = [x for x in temp2 if x]
                             tr = []
-                            for i in range(len(temp)):
-                                item = temp[i]
-                                probability = uncertain[i]
+                            for i in range(len(temp1)):
+                                item = temp1[i]
+                                probability = float(temp2[i])
                                 product = _Item(item, probability)
                                 tr.append(product)
-                            self._lno += 1
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
-    def _frequentOneItem(self):
+    def _scanningWeights(self) -> None:
         """
-        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+        Scans the uncertain transactional dataset
+
+        :return: None
         """
-        global _minSup
+        self._weights = {}
+        if isinstance(self._wFile, _ab._pd.DataFrame):
+            weights, data = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                data = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for k in range(len(data)):
+                self._weights[data[k]] = int(float(weights[k]))
+
+            # print(self.Database)
+        if isinstance(self._wFile, str):
+            if _ab._validators.url(self._wFile):
+                data = _ab._urlopen(self._wFile)
+                for line in data:
+                    line.strip()
+                    line = line.decode("utf-8")
+                    temp = [i.rstrip() for i in line.split(self._sep)]
+                    temp = [x for x in temp if x]
+                    self._weights[temp[0]] = int(float(temp[1]))
+            else:
+                try:
+                    with open(self._wFile, 'r') as f:
+                        for line in f:
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            self._weights[temp[0]] = float(temp[1])
+                except IOError:
+                    print("File Not Found")
+
+    def _frequentOneItem(self) -> tuple:
+        """
+        Takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
+
+        :param self.Database : it represents the one self.Database in database
+        :type self.Database : list
+        :return: tuple
+        """
+
         mapSupport = {}
         for i in self._Database:
             for j in i:
                 if j.item not in mapSupport:
-                    mapSupport[j.item] = round(j.probability, 2)
+                    if self._weights.get(j.item) is not None:
+                        mapSupport[j.item] = [j.probability, self._weights[j.item]]
                 else:
-                    mapSupport[j.item] += round(j.probability, 2)
-        mapSupport = {k: round(v, 2) for k, v in mapSupport.items() if v >= self._minSup}
+                    mapSupport[j.item][0] += j.probability
+        mapSupport = {k: v[0] for k, v in mapSupport.items() if v[0] >= self._expSup and v[0] * v[1] >= self._expWSup}
         plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        self.rank = dict([(index, item) for (item, index) in enumerate(plist)])
         return mapSupport, plist
 
-    def _buildTree(self, data, info):
+    @staticmethod
+    def _buildTree(data, info) -> _Tree:
         """
-        it takes the transactions and support of each item and construct the main tree with setting root node as null
-        :param data : it represents the one transactions in database
+        It takes the self.Database and support of each item and construct the main tree with setting root node as null
+
+        :param data : it represents the one self.Database in database
         :type data : list
         :param info : it represents the support of each item
         :type info : dictionary
+        :return: tree
         """
+
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(data)):
             rootNode.addTransaction(data[i])
         return rootNode
 
-    def updateTransactions(self, dict1):
+    def _updateTransactions(self, dict1) -> list:
         """
-        remove the items which are not frequent from transactions and updates the transactions with rank of items
+        Remove the items which are not frequent from self.Database and updates the self.Database with rank of items
+
         :param dict1 : frequent items with support
         :type dict1 : dictionary
+        :return: list
         """
         list1 = []
         for tr in self._Database:
             list2 = []
             for i in range(0, len(tr)):
                 if tr[i].item in dict1:
                     list2.append(tr[i])
-            if (len(list2) >= 2):
+            if len(list2) >= 2:
                 basket = list2
-                basket.sort(key=lambda val: self._rank[val.item])
+                basket.sort(key=lambda val: self.rank[val.item])
                 list2 = basket
                 list1.append(list2)
         return list1
 
-    def _Check(self, i, x):
+    @staticmethod
+    def _check(i, x) -> int:
         """
         To check the presence of item or pattern in transaction
+
         :param x: it represents the pattern
         :type x : list
-        :param i : represents the uncertain transactions
+        :param i : represents the uncertain self.Database
         :type i : list
+        :return: integer number
         """
+
+        # This method taken a transaction as input and returns the tree
         for m in x:
             k = 0
             for n in i:
                 if m == n.item:
                     k += 1
             if k == 0:
                 return 0
         return 1
 
-    def _convert(self, value):
+    def _convert(self, value) -> float:
         """
         To convert the type of user specified minSup value
+
         :param value: user specified minSup value
         :return: converted type minSup value
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
             value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def _removeFalsePositives(self):
+    def _removeFalsePositives(self) -> None:
         """
-        To remove the false positive patterns generated in frequent patterns
+        To remove the false positive patterns generated in frequent patterns.
+
         :return: patterns with accurate probability
         """
         global _finalPatterns
         periods = {}
         for i in self._Database:
             for x, y in _finalPatterns.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
                     s = 1
-                    check = self._Check(i, x)
+                    check = self._check(i, x)
                     if check == 1:
                         for j in i:
                             if j.item in x:
                                 s *= j.probability
                         if x in periods:
                             periods[x] += s
                         else:
                             periods[x] = s
         for x, y in periods.items():
-            if y >= self._minSup:
+            weight = 0
+            for i in x:
+                weight += self._weights[i]
+            weight = weight / len(x)
+            if weight * y >= self._expWSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    def startMine(self):
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
+        """
+        startMine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns.
+        """
+        self.mine()
+
+    def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        mine() method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patternS
         """
-        global _minSup
-        self._startTime = _fp._time.time()
+        global _expSup, _expWSup, _weights, _finalPatterns
+        self._startTime = _ab._time.time()
+        self._Database, self._weights = [], {}
         self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
+        self._scanningWeights()
+        _weights = self._weights
+        self._expSup = float(self._expSup)
+        self._expWSup = float(self._expWSup)
+        _expSup = self._expSup
+        _expWSup = self._expWSup
         self._finalPatterns = {}
         mapSupport, plist = self._frequentOneItem()
-        transactions1 = self.updateTransactions(mapSupport)
+        self.Database1 = self._updateTransactions(mapSupport)
         info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(transactions1, info)
+        Tree1 = self._buildTree(self.Database1, info)
         Tree1.generatePatterns([])
         self._removeFalsePositives()
-        print("Uncertain Frequent patterns were generated successfully using TubeS algorithm")
-        self._endTime = _fp._time.time()
-        process = _fp._psutil.Process(_fp._os.getpid())
+        print("Weighted Frequent patterns were generated  successfully using basic algorithm")
+        self._endTime = _ab._time.time()
+        process = _ab._psutil.Process(_ab._os.getpid())
         self._memoryUSS = float()
-        self._memoryRSS = float()
+        self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
+    def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
+    def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
+        return self.memoryRSS
 
-        return self._memoryRSS
-
-    def getRuntime(self):
+    def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
+
         Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
-
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
-            data.append([a.replace('\t', ' '), b])
-            dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
+            s = str()
+            for i in a:
+                s = s + i + " "
+            data.append([s, b])
+            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile):
+    def save(self, outFile: str) -> None:
         """
         Complete set of frequent patterns will be loaded in to an output file
-        :param outFile: name of the output file
-        :type outFile: file
+
+        :param outFile: Specify name of the output file
+        :type outFile: csv file
+        :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
+            s = str()
+            for i in x:
+                s = s + i + "\t"
+            s1 = s.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self):
+    def getPatterns(self) -> dict:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
+
         :return: returning frequent patterns
         :rtype: dict
         """
-        return len(self._finalPatterns)
+        return self._finalPatterns
 
-    def printResults(self):
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
-        if len(_fp._sys.argv) == 5:
-            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
-        if len(_fp._sys.argv) == 4:
-            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
+    if len(_ab._sys.argv) == 6 or len(_ab._sys.argv) == 7:
+        if len(_ab._sys.argv) == 7:
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5], _ab._sys.argv[6])
+        if len(_ab._sys.argv) == 6:
+            _ap = WUFIM(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         _ap.startMine()
-        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save(_fp._sys.argv[2])
+        _ap.mine()
+        print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
+        for k in [120, 140, 160, 180, 200]:
+            _ap = WUFIM('/Users/likhitha/Downloads/uncertainTransaction_T10I4D200K.csv', '/Users/likhitha/Downloads/T10_weights.txt',
+                        k, 500, '\t')
+            _ap.startMine()
+            print("Total number of Weighted Uncertain Frequent Patterns:", len(_ap.getPatterns()))
+            _ap.save('/Users/likhitha/Downloads/WUFIM_output.txt')
+            print("Total Memory in USS:", _ap.getMemoryUSS())
+            print("Total Memory in RSS", _ap.getMemoryRSS())
+            print("Total ExecutionTime in ms:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/UFGrowth.py`

 * *Files 1% similar despite different names*

```diff
@@ -64,14 +64,15 @@
 
     def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
 class _Node(object):
+
     """
     A class used to represent the node of frequentPatternTree
     :Attributes:
         item : int
             storing item of a node
         probability : int
             To maintain the expected support of node
@@ -235,17 +236,19 @@
                         self.fixNodeLinks(pathItem.itemid, newNode)
         return q
 
 
 class UFGrowth(_ab._frequentPatterns):
     """
     :Description: It is one of the fundamental algorithm to discover frequent patterns in a uncertain transactional database using PUF-Tree.
+
     :Reference:
         Carson Kai-Sang Leung, Syed Khairuzzaman Tanbeer, "PUF-Tree: A Compact Tree Structure for Frequent Pattern Mining of Uncertain Data",
         Pacific-Asia Conference on Knowledge Discovery and Data Mining(PAKDD 2013), https://link.springer.com/chapter/10.1007/978-3-642-37453-1_2
+
     :Attributes:
         iFile : file
             Name of the Input file or path of the input file
         oFile : file
             Name of the output file or path of the output file
         minSup : float or int or str
             The user can specify minSup either in count or proportion of database size.
@@ -271,14 +274,15 @@
             To represent the total no of transaction
         tree : class
             To represents the Tree class
         itemSetCount : int
             To represents the total no of patterns
         finalPatterns : dict
             To store the complete patterns
+
     :Methods:
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
@@ -298,37 +302,54 @@
             Update the transactions by removing non-frequent items and sort the Database by item decreased support
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
+
     **Methods to execute code on terminal**
     ----------------------------------------
             Format:
                       >>>  python3 PUFGrowth.py <inputFile> <outputFile> <minSup>
             Example:
                       >>>  python3 PUFGrowth.py sampleTDB.txt patterns.txt 3
+
                       .. note:: minSup  will be considered in support count or frequency
+
     **Importing this algorithm into a python program**
     --------------------------------------------------------
     .. code-block:: python
+
             from PAMI.uncertainFrequentPattern.basic import UFGrowth as alg
+
             obj = alg.UFGrowth(iFile, minSup)
+
             obj.startMine()
+
             frequentPatterns = obj.getPatterns()
+
             print("Total number of Frequent Patterns:", len(frequentPatterns))
+
             obj.save(oFile)
+
             Df = obj.getPatternsAsDataFrame()
+
             memUSS = obj.getmemoryUSS()
+
             print("Total Memory in USS:", memUSS)
+
             memRSS = obj.getMemoryRSS()
+
             print("Total Memory in RSS", memRSS)
+
             run = obj.getRuntime()
+
             print("Total ExecutionTime in seconds:", run)
+
     **Credits:**
     -----------------
              The complete program was written by P.Likhitha under the supervision of Professor Rage Uday Kiran.
     """
     _startTime = float()
     _endTime = float()
     _minSup = str()
@@ -523,15 +544,15 @@
         if type(value) is str:
             if '.' in value:
                 value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def startMine(self):
+    def mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
@@ -632,15 +653,15 @@
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
             _ap = UFGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
             _ap = UFGrowth(_ab._sys.argv[1], _ab._sys.argv[3])
-        _ap.startMine()
+        _ap.mine()
         print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/UVECLAT.py` & `pami-2024.5.1/PAMI/fuzzyFrequentPattern/basic/FFIMiner.py`

 * *Files 14% similar despite different names*

```diff
@@ -1,509 +1,660 @@
-# UVEclat is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
-#
-# **Importing this algorithm into a python program**
-# --------------------------------------------------------
+# Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
 #
+# to its huge search space.we are using efficient pruning techniques to reduce the search space.
 #
-#     from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
+# **Importing this algorithm into a python program**
+# ---------------------------------------------------------
 #
-#     obj = alg.UVEclat(iFile, minSup)
+#             from PAMI.fuzzyFrequentPattern import FFIMiner as alg
 #
-#     obj.startMine()
+#             obj = alg.FFIMiner("input.txt", 2)
 #
-#     frequentPatterns = obj.getPatterns()
+#             obj.mine()
 #
-#     print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             fuzzyFrequentPattern = obj.getPatterns()
 #
-#     obj.save(oFile)
+#             print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
 #
-#     Df = obj.getPatternsAsDataFrame()
+#             obj.save("outputFile")
 #
-#     memUSS = obj.getMemoryUSS()
+#             memUSS = obj.getMemoryUSS()
 #
-#     print("Total Memory in USS:", memUSS)
+#             print("Total Memory in USS:", memUSS)
 #
-#     memRSS = obj.getMemoryRSS()
+#             memRSS = obj.getMemoryRSS()
 #
-#     print("Total Memory in RSS", memRSS)
+#             print("Total Memory in RSS", memRSS)
 #
-#     run = obj.getRuntime()
+#             run = obj.getRuntime()
 #
-#     print("Total ExecutionTime in seconds:", run)
+#             print("Total ExecutionTime in seconds:", run)
 #
 
+
+
+
 __copyright__ = """
- Copyright (C)  2021 Rage Uday Kiran
+Copyright (C)  2021 Rage Uday Kiran
+
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
+
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
+
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
      Copyright (C)  2021 Rage Uday Kiran
+
 """
 
-import operator as _operator
-from PAMI.uncertainFrequentPattern.basic import abstract as _ab
+from PAMI.fuzzyFrequentPattern.basic import abstract as _ab
+from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from deprecated import deprecated
 
 
-_minSup = float()
-_finalPatterns = {}
+class _FFList:
+    """
+    A class represent a Fuzzy List of an element
 
+    :Attributes:
+
+        item: int
+            the item name
+
+        sumIUtil: float
+            the sum of utilities of a fuzzy item in database
+
+        sumRUtil: float
+            the sum of resting values of a fuzzy item in database
+
+        elements: list
+            a list of elements contain tid,Utility and resting values of element in each transaction
 
-class _Item:
+    :Methods:
+
+        addElement(element)
+            Method to add an element to this fuzzy list and update the sums at the same time.
+
+        printElement(e)
+            Method to print elements
     """
-    A class used to represent the item with probability in transaction of dataset
+
+    def __init__(self, itemName: int) -> None:
+        self.item = itemName
+        self.sumIUtil = 0.0
+        self.sumRUtil = 0.0
+        self.elements = []
+
+    def addElement(self, element) -> None:
+        """
+        A Method that add a new element to FFList
+
+        :param element: an element to be added to FFList
+        :type element: Element
+        :return: None
+        """
+        self.sumIUtil += element.iUtils
+        self.sumRUtil += element.rUtils
+        self.elements.append(element)
+
+    def printElement(self) -> None:
+        """
+        A method to print elements
+        """
+        for ele in self.elements:
+            print(ele.tid, ele.iUtils, ele.rUtils)
+
+
+class _Element:
+    """
+    A class represents an Element of a fuzzy list
+
     :Attributes:
-        item : int or word
-            Represents the name of the item
-        probability : float
-            Represent the existential probability(likelihood presence) of an item
+
+        tid : int
+            keep tact of transaction id
+
+        iUtils: float
+            the utility of a fuzzy item in the transaction
+
+        rUtils : float
+            the  resting value of a fuzzy item in the transaction
+    """
+
+    def __init__(self, tid: int, iUtil: float, rUtil: float) -> None:
+        self.tid = tid
+        self.iUtils = iUtil
+        self.rUtils = rUtil
+
+
+class _Pair:
+    """
+    A class to store item and it's quantity together
     """
 
-    def __init__(self, item, probability):
-        self.item = item
-        self.probability = probability
+    def __init__(self) -> None:
+        self.item = 0
+        self.quantity = 0
 
 
-class UVEclat(_ab._frequentPatterns):
+class FFIMiner(_ab._fuzzyFrequentPattenrs):
     """
-    :Description: It is one of the fundamental algorithm to discover frequent patterns in an uncertain transactional database using PUF-Tree.
-    :Reference:
-    Carson Kai-Sang Leung, Lijing Sun: "Equivalence class transformation based mining of frequent itemsets from uncertain data",
-    SAC '11: Proceedings of the 2011 ACM Symposium on Applied ComputingMarch, 2011, Pages 983–984,
-    https://doi.org/10.1145/1982185.1982399
+    :Description:   Fuzzy Frequent  Pattern-Miner is desired to find all  frequent fuzzy patterns which is on-trivial and challenging problem
+                    to its huge search space.we are using efficient pruning techniques to reduce the search space.
+
+    :Reference:   Lin, Chun-Wei & Li, Ting & Fournier Viger, Philippe & Hong, Tzung-Pei. (2015).
+                  A fast Algorithm for mining fuzzy frequent itemsets. Journal of Intelligent & Fuzzy Systems. 29.
+                  2373-2379. 10.3233/IFS-151936.
+                  https://www.researchgate.net/publication/286510908_A_fast_Algorithm_for_mining_fuzzy_frequent_itemSets
+
+    :param  iFile: str :
+                   Name of the Input file to mine complete set of frequent patterns
+    :param  oFile: str :
+                   Name of the output file to store complete set of frequent patterns
+    :param  minSup: int or float or str :
+                   The user can specify minSup either in count or proportion of database size. If the program detects the data type of minSup is integer, then it treats minSup is expressed in count. Otherwise, it will be treated as float.
+    :param maxPer: float :
+                   The user can specify maxPer in count or proportion of database size. If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
+    :param fuzFile: str :
+                    The user can specify fuzFile.
+    :param  sep: str :
+                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+
+
     :Attributes:
-        iFile : file
-            Name of the Input file or path of the input file
-        oFile : file
-            Name of the output file or path of the output file
-        minSup : float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        sep : str
-            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
-            However, the users can override their default separator.
-        memoryUSS : float
-            To store the total amount of USS memory consumed by the program
+
+        iFile : string
+            Name of the input file to mine complete set of fuzzy  frequent patterns
+
+        fmFile : string
+            Name of the fuzzy membership file to mine complete set of fuzzy  frequent patterns
+
+        oFile : string
+            Name of the oFile file to store complete set of fuzzy  frequent patterns
+
+        minSup : float
+            The user given minimum support
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         startTime:float
             To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
-        Database : list
-            To store the transactions of a database in list
-        mapSupport : Dictionary
-            To maintain the information of item and their frequency
-        lno : int
-            To represent the total no of transaction
-        tree : class
-            To represents the Tree class
-        itemSetCount : int
-            To represents the total no of patterns
-        finalPatterns : dict
-            To store the complete patterns
+
+        itemsCnt: int
+            To record the number of fuzzy spatial itemSets generated
+
+        mapItemSum: map
+            To keep track of sum of Fuzzy Values of items
+
+        joinsCnt: int
+            To keep track of the number of ffi-list that was constructed
+
+        BufferSize: int
+            represent the size of Buffer
+
+        itemSetBuffer list
+            to keep track of items in buffer
+
     :Methods:
-        startMine()
+
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
-        storePatternsInFile(oFile)
+        save(oFile)
             Complete set of frequent patterns will be loaded in to a output file
-        getPatternsInDataFrame()
+        getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets(fileName)
-            Scans the dataset and stores in a list format
-        frequentOneItem()
-            Extracts the one-length frequent patterns from database
-    **Methods to execute code on terminal**
+        convert(value)
+            To convert the given user specified value
+        compareItems(o1, o2)
+            A Function that sort all ffi-list in ascending order of Support
+        FSFIMining(prefix, prefixLen, FSFIM, minSup)
+            Method generate ffi from prefix
+        construct(px, py)
+            A function to construct Fuzzy itemSet from 2 fuzzy itemSets
+        findElementWithTID(uList, tid)
+            To find element with same tid as given
+        WriteOut(prefix, prefixLen, item, sumIUtil)
+            To Store the patten
+
+
+    **Executing the code on terminal :**
+    ------------------------------------------
+
+    .. code-block:: console
+
+      Format:
+
+      (.venv) $ python3 FFIMiner.py <inputFile> <outputFile> <minSup> <separator>
+
+      Example Usage:
+
+      (.venv) $ python3  FFIMiner.py sampleTDB.txt output.txt 6
+
+    .. note:: minSup will be considered in percentage of database transactions
+
+
+    **Sample run of importing the code:**
     ------------------------------------------
-            Format:
-                      >>> python3 uveclat.py <inputFile> <outputFile> <minSup>
-            Example:
-                      >>>  python3 uveclat.py sampleTDB.txt patterns.txt 3
-                      .. note:: minSup  will be considered in support count or frequency
-    **Importing this algorithm into a python program**
-    ---------------------------------------------------
     .. code-block:: python
-            from PAMI.uncertainFrequentPattern.basic import UVECLAT as alg
-            obj = alg.UVEclat(iFile, minSup)
-            obj.startMine()
-            frequentPatterns = obj.getPatterns()
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
-            obj.save(oFile)
-            Df = obj.getPatternsAsDataFrame()
-            memUSS = obj.getmemoryUSS()
+
+            from PAMI.fuzzyFrequentPattern import FFIMiner as alg
+
+            obj = alg.FFIMiner("input.txt", 2)
+
+            obj.mine()
+
+            fuzzyFrequentPattern = obj.getPatterns()
+
+            print("Total number of Fuzzy Frequent Patterns:", len(fuzzyFrequentPattern))
+
+            obj.save("outputFile")
+
+            memUSS = obj.getMemoryUSS()
+
             print("Total Memory in USS:", memUSS)
+
             memRSS = obj.getMemoryRSS()
+
             print("Total Memory in RSS", memRSS)
+
             run = obj.getRuntime()
+
             print("Total ExecutionTime in seconds:", run)
+
+
     **Credits:**
     ---------------
-         The complete program was written by   P.Likhitha    under the supervision of Professor Rage Uday Kiran.
+            The complete program was written by B.Sai Chitra under the supervision of Professor Rage Uday Kiran.
+
     """
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
+    _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
-    _sep = " "
+    _fuzFile = " "
     _memoryUSS = float()
     _memoryRSS = float()
-    _Database = []
-    _tidList = {}
-    _rank = {}
+    _sep = "\t"
+
+    def __init__(self, iFile: str, minSup: float, sep: str="\t") -> None:
+        super().__init__(iFile, minSup, sep)
+        self._startTime = 0
+        self._endTime = 0
+        self._itemsCnt = 0
+        self._mapItemSum = {}
+        self._joinsCnt = 0
+        self._BufferSize = 200
+        self._itemSetBuffer = []
+        self._transactions = []
+        self._fuzzyValues = []
+        self._finalPatterns = {}
+        self._dbLen = 0
+
+    def _compareItems(self, o1: _FFList, o2: _FFList) -> int:
+        """
+        A Function that sort all ffi-list in ascending order of Support
+
+        :param o1: First FFI-list
+        :type o1: _FFList
+        :param o2: Second FFI-list
+        :type o1: _FFList
+        :return: Comparision Value
+        :rtype: int
+        """
+        compare = self._mapItemSum[o1.item] - self._mapItemSum[o2.item]
+        if compare == 0:
+            if o1.item < o2.item:
+                return -1
+            elif o1.item > o2.item:
+                return 1
+            else:
+                return 0
+        else:
+            return compare
 
-    def _creatingItemSets(self):
+    def _convert(self, value) -> Union[int, float]:
         """
-        Scans the dataset
+        To convert the given user specified value
+
+        :param value: user specified value
+        :type value: int or float or str
+        :return: converted value
+        :rtype: int or float
         """
-        self._Database = []
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (self._dbLen * value)
+        if type(value) is str:
+            if '.' in value:
+                value = float(value)
+                value = (self._dbLen * value)
+            else:
+                value = int(value)
+        return value
+
+    def _creatingItemsets(self) -> None:
+        """
+        Storing the complete transactions of the database/input file in a database variable
+        """
+        self._transactions, self._fuzzyValues, self._Database = [], [], []
         if isinstance(self._iFile, _ab._pd.DataFrame):
-            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
-            if 'uncertain' in i:
-                uncertain = self._iFile['uncertain'].tolist()
-            for k in range(len(data)):
-                tr = []
-                for j in range(len(data[k])):
-                    product = _Item(data[k][j], uncertain[k][j])
-                    tr.append(product)
-                self._Database.append(tr)
-
+                self._transactions = self._iFile['Transactions'].tolist()
+            if 'fuzzyValues' in i:
+                self._fuzzyValues = self._iFile['fuzzyValues'].tolist()
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _ab._validators.url(self._iFile):
                 data = _ab._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    tr = []
-                    for i in temp:
-                        i1 = i.index('(')
-                        i2 = i.index(')')
-                        item = i[0:i1]
-                        probability = float(i[i1 + 1:i2])
-                        product = _Item(item, probability)
-                        tr.append(product)
-                    self._Database.append(temp)
+                    line = line.split("\n")[0]
+                    parts = line.split(":")
+                    parts[0] = parts[0].strip()
+                    parts[1] = parts[1].strip()
+                    items = parts[0].split(self._sep)
+                    quantities = parts[1].split(self._sep)
+                    self._transactions.append([x for x in items])
+                    self._fuzzyValues.append([float(x) for x in quantities])
             else:
                 try:
-                    with open(self._iFile, 'r') as f:
+                    with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            tr = []
-                            for i in temp:
-                                i1 = i.index('(')
-                                i2 = i.index(')')
-                                item = i[0:i1]
-                                probability = float(i[i1 + 1:i2])
-                                product = _Item(item, probability)
-                                tr.append(product)
-                            self._Database.append(tr)
+                            line = line.split("\n")[0]
+                            parts = line.split(":")
+                            parts[0] = parts[0].strip()
+                            parts[1] = parts[1].strip()
+                            items = parts[0].split(self._sep)
+                            quantities = parts[1].split(self._sep)
+                            self._transactions.append([x for x in items])
+                            self._fuzzyValues.append([float(x) for x in quantities])
                 except IOError:
                     print("File Not Found")
+                    quit()
 
-    def _frequentOneItem(self):
-        """
-        takes the self.Database and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        """
-
-        mapSupport = {}
-        k = 0
-        for i in self._Database:
-            k += 1
-            for j in i:
-                if j.item not in mapSupport:
-                    mapSupport[str(j.item)] = j.probability
-                    self._tidList[str(j.item)] = {k: j.probability}
-                else:
-                    mapSupport[str(j.item)] += j.probability
-                    self._tidList[str(j.item)].update({k: j.probability})
-        mapSupport = {k: v for k, v in mapSupport.items() if v >= self._minSup}
-        plist = dict( sorted(mapSupport.items(), key=_operator.itemgetter(1),reverse=True))
-        return list(plist.keys())
-
-    @staticmethod
-    def _check(i, x):
-        """
-        To check the presence of item or pattern in transaction
-        :param x: it represents the pattern
-        :type x : list
-        :param i : represents the uncertain self.Database
-        :type i : list
-        """
-
-        # This method taken a transaction as input and returns the tree
-        for m in x:
-            k = 0
-            for n in i:
-                if m == n.item:
-                    k += 1
-            if k == 0:
-                return 0
-        return 1
-
-    @staticmethod
-    def _convert(value):
-        """
-        To convert the type of user specified minSup value
-        :param value: user specified minSup value
-        :return: converted type minSup value
-        """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = float(value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-            else:
-                value = int(value)
-        return value
-
-    def _removeFalsePositives(self):
-        """
-        To remove the false positive patterns generated in frequent patterns
-        :return: patterns with accurate probability
-        """
-        global _finalPatterns
-        periods = {}
-        for i in self._Database:
-            for x, y in _finalPatterns.items():
-                if len(x) == 1:
-                    periods[x] = y
-                else:
-                    s = 1
-                    check = self._check(i, x)
-                    if check == 1:
-                        for j in i:
-                            if j.item in x:
-                                s *= j.probability
-                        if x in periods:
-                            periods[x] += s
-                        else:
-                            periods[x] = s
-        for x, y in periods.items():
-            if y >= self._minSup:
-                sample = str()
-                for i in x:
-                    sample = sample + i + "\t"
-                self._finalPatterns[sample] = y
-
-    @staticmethod
-    def _Intersection(tidSetx, tidSetY):
-        """
-        This function is used to find the intersection
-        :param tidSetx: the timestamp of a patterns
-        :type tidSetx: dict
-        :param tidSetY: the timestamp of a patterns
-        :type tidSetY: dict
-        """
-        tids = []
-        support = []
-        tidDict = {}
-        for x, y in tidSetx.items():
-            for x1, y1 in tidSetY.items():
-                if x == x1:
-                    tids.append(x)
-                    support.append(y * y1)
-                    tidDict.update({x: y * y1})
-        return tidDict
-
-    def _calculateExpSup(self, tidList):
-        """
-        This function is used to calculate support of tidList
-        :param tidList: timestamp of a list.
-        :type tidList: List
-        """
-        return sum(tidList.values())
-
-    def _save(self, prefix, suffix, tidSetI):
+    @deprecated("It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
+    def startMine(self) -> None:
         """
-        Saves the patterns that satisfy the periodic frequent property.
-        :param prefix: the prefix of a pattern
-        :type prefix: list
-        :param suffix: the suffix of a patterns
-        :type suffix: list
-        :param tidSetI: the timestamp of a patterns
-        :type tidSetI: dict
+        fuzzy-Frequent pattern mining process will start from here
         """
+        self.mine()
 
-        global _finalPatterns
-        if prefix is None:
-            prefix = suffix
-        else:
-            prefix = prefix + suffix
-        val = self._calculateExpSup(tidSetI)
-        _finalPatterns[tuple(prefix)] = val
-
-    def _Generation(self, prefix, itemSets, tidSets):
-        """
-        Equivalence class is followed  and checks for the patterns generated for periodic-frequent patterns.
-        :param prefix:  main equivalence prefix
-        :type prefix: periodic-frequent item or pattern
-        :param itemSets: patterns which are items combined with prefix and satisfying the periodicity and frequent with their timestamps
-        :type itemSets: list
-        :param tidSets: timestamps of the items in the argument itemSets
-        :type tidSets: list
-        """
-        if len(itemSets) == 1:
-            i = itemSets[0]
-            tidI = tidSets[0]
-            self._save(prefix, [i], tidI)
-            return
-        for i in range(len(itemSets)):
-            itemI = itemSets[i]
-            if itemI is None:
-                continue
-            tidSetI = tidSets[i]
-            classItemSets = []
-            classTidSets = []
-            itemSetX = [itemI]
-            for j in range(i + 1, len(itemSets)):
-                itemJ = itemSets[j]
-                tidSetJ = tidSets[j]
-                y = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y) >= self._minSup:
-                    classItemSets.append(itemJ)
-                    classTidSets.append(y)
-            newPrefix = list(set(itemSetX)) + prefix
-            self._Generation(newPrefix, classItemSets, classTidSets)
-            self._save(prefix, list(set(itemSetX)), tidSetI)
-
-    def startMine(self):
+    def mine(self) -> None:
         """
-        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
+        fuzzy-Frequent pattern mining process will start from here
         """
-        global _minSup
         self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        _minSup = self._minSup
-        plist = self._frequentOneItem()
-        for i in range(len(plist)):
-            itemI = plist[i]
-            tidSetI = self._tidList[itemI]
-            itemSetX = [itemI]
-            itemSets = []
-            tidSets = []
-            for j in range(i+1, len(plist)):
-                itemJ = plist[j]
-                tidSetJ = self._tidList[itemJ]
-                y1 = self._Intersection(tidSetI, tidSetJ)
-                if self._calculateExpSup(y1) >= self._minSup:
-                    itemSets.append(itemJ)
-                    tidSets.append(y1)
-            self._Generation(itemSetX, itemSets, tidSets)
-            self._save(None, itemSetX, tidSetI)
-        self._removeFalsePositives()
-        print("Frequent patterns were generated from uncertain databases successfully using PUF algorithm")
+        self._creatingItemsets()
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            self._dbLen += 1
+            for i in range(0, len(items)):
+                item = items[i]
+                if item in self._mapItemSum:
+                    self._mapItemSum[item] += quantities[i]
+                else:
+                    self._mapItemSum[item] = quantities[i]
+        listOfffilist = []
+        mapItemsToFFLIST = {}
+        #self._minSup = self._convert(self._minSup)
+        # minSup = self.minSup
+        for item1 in self._mapItemSum.keys():
+            item = item1
+            if self._mapItemSum[item] >= self._minSup:
+                fuList = _FFList(item)
+                mapItemsToFFLIST[item] = fuList
+                listOfffilist.append(fuList)
+        listOfffilist.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+        tid = 0
+        for line in range(len(self._transactions)):
+            items = self._transactions[line]
+            quantities = self._fuzzyValues[line]
+            revisedTransaction = []
+            for i in range(0, len(items)):
+                pair = _Pair()
+                pair.item = items[i]
+                pair.quantity = quantities[i]
+                item = pair.item
+                if self._mapItemSum[item] >= self._minSup:
+                    if pair.quantity > 0:
+                        revisedTransaction.append(pair)
+            revisedTransaction.sort(key=_ab._functools.cmp_to_key(self._compareItems))
+            for i in range(len(revisedTransaction) - 1, -1, -1):
+                pair = revisedTransaction[i]
+                remainUtil = 0
+                for j in range(len(revisedTransaction) - 1, i, -1):
+                    remainUtil += revisedTransaction[j].quantity
+                remainingUtility = remainUtil
+                if mapItemsToFFLIST.get(pair.item) is not None:
+                    FFListOfItem = mapItemsToFFLIST[pair.item]
+                    element = _Element(tid, pair.quantity, remainingUtility)
+                    FFListOfItem.addElement(element)
+            tid += 1
+        self._FFIMining(self._itemSetBuffer, 0, listOfffilist, self._minSup)
         self._endTime = _ab._time.time()
         process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self):
-        """Total amount of USS memory consumed by the mining process will be retrieved from this function
+    def _FFIMining(self, prefix, prefixLen, FSFIM, minSup):
+        """
+        Generates ffi from prefix
+
+        :param prefix: the prefix patterns of ffi
+        :type prefix: len
+        :param prefixLen: the length of prefix
+        :type prefixLen: int
+        :param FSFIM: the Fuzzy list of prefix itemSets
+        :type FSFIM: list
+        :param minSup: the minimum support of
+        :type minSup: int or float
+        """
+        for i in range(0, len(FSFIM)):
+            X = FSFIM[i]
+            if X.sumIUtil >= minSup:
+                self._WriteOut(prefix, prefixLen, X.item, X.sumIUtil)
+            if X.sumRUtil >= minSup:
+                exULs = []
+                for j in range(i + 1, len(FSFIM)):
+                    Y = FSFIM[j]
+                    exULs.append(self._construct(X, Y))
+                    self._joinsCnt += 1
+                self._itemSetBuffer.insert(prefixLen, X.item)
+                self._FFIMining(self._itemSetBuffer, prefixLen + 1, exULs, minSup)
+
+    def getMemoryUSS(self) -> float:
+        """
+        Total amount of USS memory consumed by the mining process will be retrieved from this function
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self):
-        """Total amount of RSS memory consumed by the mining process will be retrieved from this function
+    def getMemoryRSS(self) -> float:
+        """
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
-
         return self._memoryRSS
 
-    def getRuntime(self):
-        """Calculating the total amount of runtime taken by the mining process
+    def getRuntime(self) -> float:
+        """
+        Calculating the total amount of runtime taken by the mining process
+
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
-
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self):
-        """Storing final frequent patterns in a dataframe
+    def _construct(self, px, py) -> _FFList:
+        """
+        A function to construct a new Fuzzy itemSet from 2 fuzzy itemSets
+
+        :param px:the itemSet px
+        :type px:ffi-List
+        :param py:itemSet py
+        :type py:ffi-List
+        :return :the itemSet of pxy(px and py)
+        :rtype :ffi-List
+        """
+        pxyUL = _FFList(py.item)
+        for ex in px.elements:
+            ey = self._findElementWithTID(py, ex.tid)
+            if ey is None:
+                continue
+            eXY = _Element(ex.tid, min([ex.iUtils, ey.iUtils], key=lambda x: float(x)), ey.rUtils)
+            pxyUL.addElement(eXY)
+        return pxyUL
+
+    def _findElementWithTID(self, uList, tid) -> _Element:
+        """
+        To find element with same tid as given
+
+        :param uList: fuzzyList
+        :type uList: ffi-List
+        :param tid: transaction id
+        :type tid: int
+        :return: element  tid as given
+        :rtype: element if exit or None
+        """
+        List = uList.elements
+        first = 0
+        last = len(List) - 1
+        while first <= last:
+            mid = (first + last) >> 1
+            if List[mid].tid < tid:
+                first = mid + 1
+            elif List[mid].tid > tid:
+                last = mid - 1
+            else:
+                return List[mid]
+        return None
+
+    def _WriteOut(self, prefix: list, prefixLen: int, item: int, sumIUtil: float) -> None:
+        """
+        To Store the patten
+
+        :param prefix: prefix of itemSet
+        :type prefix: list
+        :param prefixLen: length of prefix
+        :type prefixLen: int
+        :param item: the last item
+        :type item: int
+        :param sumIUtil: sum of utility of itemSet
+        :type sumIUtil: float
+        :return: None
+        """
+        self._itemsCnt += 1
+        res = ""
+        for i in range(0, prefixLen):
+            res += str(prefix[i])  + "\t"
+        res += str(item)
+        res1 = str(sumIUtil)
+        self._finalPatterns[res] = res1
+
+    def getPatternsAsDataFrame(self) -> _ab._pd.DataFrame:
+        """
+        Storing final frequent patterns in a dataframe
+
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
-        dataframe = {}
+        dataFrame = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
-            dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
-        return dataframe
+            dataFrame = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
+        return dataFrame
 
-    def save(self, oFile):
-        """Complete set of frequent patterns will be loaded in to an output file
-        :param oFile: name of the output file
-        :type oFile: csv file
+    def getPatterns(self) -> dict:
         """
-        self.oFile = oFile
-        writer = open(self.oFile, 'w+')
-        for x, y in self._finalPatterns.items():
-            s1 = x.strip() + ":" + str(y)
-            writer.write("%s \n" % s1)
+        Function to send the set of frequent patterns after completion of the mining process
 
-    def getPatterns(self):
-        """ Function to send the set of frequent patterns after completion of the mining process
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
-    def printResults(self):
+    def save(self, outFile) -> dict:
+        """
+        Complete set of frequent patterns will be loaded in to an output file
+
+        :param outFile: name of the output file
+        :type outFile: csv file
+        :return: dictionary of frequent patterns
+        :rtype: dict
+        """
+        self._oFile = outFile
+        writer = open(self._oFile, 'w+')
+        for x, y in self._finalPatterns.items():
+            patternsAndSupport = x.strip() + ":" + str(y)
+            writer.write("%s \n" % patternsAndSupport)
+
+    def printResults(self) -> None:
         """
         This function is used to print the results
         """
-        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of Fuzzy Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
-        print("Total ExecutionTime in ms:",  self.getRuntime())
+        print("Total ExecutionTime in seconds:", self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
     if len(_ab._sys.argv) == 4 or len(_ab._sys.argv) == 5:
         if len(_ab._sys.argv) == 5:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
+            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         if len(_ab._sys.argv) == 4:
-            _ap = UVEclat(_ab._sys.argv[1], _ab._sys.argv[3])
+            _ap = FFIMiner(_ab._sys.argv[1], _ab._sys.argv[3])
         _ap.startMine()
-        print("Total number of Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
     else:
+        _ap = FFIMiner('sample.txt', 1, ' ')
+        _ap.startMine()
+        _ap.mine()
+        print("Total number of Fuzzy-Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.save('output.txt')
+        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in RSS", _ap.getMemoryRSS())
+        print("Total ExecutionTime in seconds:", _ap.getRuntime())
         print("Error! The number of input parameters do not match the total number of parameters provided")
+
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py` & `pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/GFPGrowth.py`

 * *Files 7% similar despite different names*

```diff
@@ -1,18 +1,20 @@
 # GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#
 #
 #             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 #
+#             iFile = 'sampleDB.txt'
+#
+#             minSup = 10  # can also be specified between 0 and 1
+#
 #             obj = alg.GFPGrowth(iFile, nFile, minSup,sep, oFile)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             Patterns = obj.getPatterns()
 #
 #             print("Total number of  Patterns:", len(Patterns))
 #
 #             obj.save(oFile)
 #
@@ -44,17 +46,16 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
+
 from PAMI.uncertainGeoreferencedFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
 _minSup = str()
 _neighbourList = {}
 _ab._sys.setrecursionlimit(20000)
@@ -65,14 +66,15 @@
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
         item : int or word
             Represents the name of the item
+
         probability : float
             Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability):
         self.item = item
         self.probability = probability
@@ -82,18 +84,21 @@
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
             storing item of a node
+
         probability : int
             To maintain the expected support of node
+
         parent : node
             To maintain the parent of every node
+
         children : list
             To maintain the children of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
@@ -102,30 +107,41 @@
     def __init__(self, item, children):
         self.item = item
         self.probability = 1
         self.children = children
         self.parent = None
 
     def addChild(self, node):
+        """
+        This method adds a child node to the current node in the frequent pattern tree. It updates the children
+        dictionary of the current node with the new child node and sets the parent of the child node to the current node.
+
+        :param node: The child node to be added.
+        :type node: _Node
+        :return: None
+        """
         self.children[node.item] = node
         node.parent = self
 
 
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
             Represents the root node of the tree
+
         summaries : dictionary
             storing the nodes with same item name
+
         info : dictionary
             stores the support of items
+
     :Methods:
 
         addTransaction(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalPattern(prefixPaths, supportOfItems)
             construct the conditional tree for prefix paths
         conditionalPatterns(Node)
@@ -310,63 +326,79 @@
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
 class GFPGrowth(_ab._frequentPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description: GFPGrowth algorithm is used to discover geo-referenced frequent patterns in a uncertain transactional database using GFP-Tree.
 
-    :Reference:
-         Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
-         "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
-         Transactional Databases".  PAKDD 2023.
-         https://doi.org/10.1007/978-3-031-33380-4_3
+    :Reference:  Palla Likhitha,Pamalla Veena, Rage, Uday Kiran, Koji Zettsu (2023).
+                 "Discovering Geo-referenced Frequent Patterns in Uncertain Geo-referenced
+                 Transactional Databases".  PAKDD 2023.
+                 https://doi.org/10.1007/978-3-031-33380-4_3
+
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of uncertain Geo referenced Frequent Patterns
     :param  oFile: str :
                    Name of the output file to store complete set of Uncertain Geo referenced frequent patterns
     :param  minSup: str:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
 
     :Attributes:
 
         iFile : file
             Name of the Input file or path of the input file
+
         oFile : file
             Name of the output file or path of the output file
+
         minSup: float or int or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         startTime:float
             To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
             To represent the total no of transaction
+
         tree : class
             To represents the Tree class
+
         itemSetCount : int
             To represents the total no of patterns
+
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
@@ -391,40 +423,46 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         startMine()
             Mining process will start from this function
 
-    **Executing the code on terminal**:
-    ------------------------------------------
+    Execution methods
+    =================
 
-    .. code-block:: console
 
+    **Terminal command**
+
+
+    .. code-block:: console
 
        Format:
 
        (.venv) $ python3 GFPGrowth.py <inputFile> <neighborFile> <outputFile> <minSup>
 
        Examples usage:
 
        (.venv) $ python3 GFPGrowth.py sampleTDB.txt sampleNeighbor.txt patterns.txt 3
 
-
-               .. note:: minSup  will be considered in support count or frequency
+    .. note:: minSup  will be considered in support count or frequency
     
-    **Sample run of importing the code**:
-    ----------------------------------------
+    **Calling from a python program**:
+
      .. code-block:: python
 
             from PAMI.uncertainGeoreferencedFrequentPattern.basic import GFPGrowth as alg
 
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
             obj = alg.GFPGrowth(iFile, nFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
             Patterns = obj.getPatterns()
 
             print("Total number of  Patterns:", len(Patterns))
 
             obj.save(oFile)
 
@@ -438,18 +476,23 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
         
-    **Credits**:
-    -------------
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
-        """
+    Credits
+    =======
+
+
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.\n
+
+    """
+
+
     _startTime = float()
     _endTime = float()
     _minSup = str()
     _finalPatterns = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
@@ -690,42 +733,23 @@
         for x, y in periods.items():
             if y >= self._minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global minSup
-        global minSup
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._creatingNeighbours()
-        #self._minSup = self._convert(self._minSup)
-        minSup = self._minSup
-        self._finalPatterns = {}
-        mapSupport, plist = self._frequentOneItem()
-        self.Database1 = self._updateTransactions(mapSupport)
-        info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(self.Database1, info)
-        Tree1.generatePatterns([])
-        self._removeFalsePositives()
-        print("Geo-Referenced Frequent patterns were generated from uncertain databases successfully using GFP algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self.memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self.memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self):
+    def mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         global minSup
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._creatingNeighbours()
@@ -744,44 +768,48 @@
         self._memoryUSS = float()
         self.memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self.memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.memoryRSS
 
     def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
@@ -789,35 +817,40 @@
         for a, b in self._finalPatterns.items():
             data.append([a, b])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile):
         """
+
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
     
     def printResults(self):
+        """
+        This function is used to print the result
+        """
         print("Total number of Patterns:", len(self.getPatterns()))
         self.save("patterns.txt")
         memUSS = self.getMemoryUSS()
         print("Total Memory in USS:", memUSS)
         memRSS = self.getMemoryRSS()
         print("Total Memory in RSS", memRSS)
         run = self.getRuntime()
@@ -828,14 +861,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = GFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         _Patterns = _ap.getPatterns()
         print("Total number of Patterns:", len(_Patterns))
         _ap.save(_ab._sys.argv[2])
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
         print("Total Memory in RSS", _memRSS)
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/uncertainGeoreferencedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py` & `pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/__init__.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py` & `pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowth.py`

 * *Files 6% similar despite different names*

```diff
@@ -2,14 +2,20 @@
 #
 # **Importing this algorithm into a python program**
 # --------------------------------------------------------
 #
 #
 #             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 #
+#             iFile = 'sampleDB.txt'
+#
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             maxPer = 2   # can also be specified between 0 and 1
+#
 #             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 #
 #             obj.mine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
@@ -28,15 +34,14 @@
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
 
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -44,24 +49,21 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 
-from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from typing import List, Dict, Tuple, Union
 
 _minSup = float()
 __maxPer = float()
 __first = int()
 _last = int()
 __lno = int()
 #rank = {}
@@ -71,14 +73,15 @@
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
         item: int or word
             Represents the name of the item
+
         probability: float
             Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item: str, probability: float) -> None:
         self.item = item
         self.probability = probability
@@ -88,20 +91,24 @@
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item: int
             storing item of a node
+
         probability: int
             To maintain the expected support of node
+
         parent: node
             To maintain the parent of every node
+
         children: list
             To maintain the children of node
+
         timeStamps: list
             To maintain the timeStamps of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
@@ -140,16 +147,18 @@
 class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
         root : Node
             Represents the root node of the tree
+
         summaries : dictionary
             storing the nodes with same item name
+
         info : dictionary
             stores the support of items
 
     :Methods:
         addTransactions(transaction)
             creating transaction as a branch in frequentPatternTree
         addConditionalTransaction(prefixPaths, supportOfItems)
@@ -275,14 +284,24 @@
         :return: None
         """
         for i in self.summaries[nodeValue]:
             i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
             del i.parent.children[nodeValue]
 
     def getPeriodAndSupport(self, s: float, timeStamps: List[int]) -> List[float]:
+        """
+        Calculates the period and support of an item based on the given support value and list of timestamps.
+
+        :param s: The support value.
+        :type s: float
+        :param timeStamps: A list of timestamps.
+        :type timeStamps: List[int]
+        :return: A list containing the support and period of the item.
+        :rtype: List[float]
+        """
         global _lno, _maxPer
         timeStamps.sort()
         cur = 0
         per = 0
         sup = s
         for j in range(len(timeStamps)):
             per = max(per, timeStamps[j] - cur)
@@ -358,75 +377,90 @@
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
 
 class UPFPGrowth(_ab._periodicFrequentPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description: Basic is  to discover periodic-frequent patterns in a uncertain temporal database.
 
-    :Reference:
-            Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).
-            Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases. In:
-            Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
-            ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
-            https://doi.org/10.1007/978-3-030-92307-5_83
+    :Reference:  Uday Kiran, R., Likhitha, P., Dao, MS., Zettsu, K., Zhang, J. (2021).Discovering Periodic-Frequent Patterns in Uncertain Temporal Databases.
+            In:  Mantoro, T., Lee, M., Ayu, M.A., Wong, K.W., Hidayanto, A.N. (eds) Neural Information Processing.
+                 ICONIP 2021. Communications in Computer and Information Science, vol 1516. Springer, Cham.
+                 https://doi.org/10.1007/978-3-030-92307-5_83
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
     :param  oFile: str :
                    Name of the output file to store complete set of Uncertain Periodic Frequent patterns
     :param  minSup: float:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
     :param  maxper: float :
                    where maxPer represents the maximum periodicity threshold value specified by the user.
 
 
     :Attributes:
+
         iFile : file
             Name of the Input file or path of the input file
+
         oFile : file
             Name of the output file or path of output file
+
         minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         maxPer: int or float or str
             The user can specify maxPer either in count or proportion of database size.
             If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+
         sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS: float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS: float
             To store the total amount of RSS memory consumed by the program
+
         startTime: float
             To record the start time of the mining process
+
         endTime: float
             To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         _lno : int
             To represent the total no of transaction
+
         tree : class
             To represents the Tree class
+
         finalPatterns : dict
             To store the complete patterns
 
     :Methods:
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
             Complete set of periodic-frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of periodic-frequent patterns will be loaded in to a dataframe
@@ -445,40 +479,49 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             To convert the user specified value
         removeFalsePositives()
             To remove the false positives in generated patterns
 
-    **Executing the code on terminal**:
-    --------------------------------------------
-    .. code-block:: console
+    Execution methods
+    =================
+
+
+    **Terminal command**
 
 
+    .. code-block:: console
+
        Format:
 
        (.venv) $ python3 basic.py <inputFile> <outputFile> <minSup> <maxPer>
 
        Example Usage:
 
        (.venv) $ python3 basic.py sampleTDB.txt patterns.txt 0.3 4
 
+    .. note:: minSup and maxPer will be considered in support count or frequency
 
-               .. note:: minSup and maxPer will be considered in support count or frequency
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowth as alg
 
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            maxPer = 2   # can also be specified between 0 and 1
+
             obj = alg.UPFPGrowth(iFile, minSup, maxPer)
 
-            obj.startMine()
+            obj.mine()
 
             periodicFrequentPatterns = obj.getPatterns()
 
             print("Total number of Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
@@ -492,20 +535,21 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits**:
-    -------------
 
-        The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
 
-"""
+
+            The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    """
     _rank = {}
     _startTime = float()
     _endTime = float()
     _minSup = float()
     _maxPer = float()
     _finalPatterns = {}
     _iFile = " "
@@ -516,14 +560,15 @@
     _Database = []
     _lno = 0
     _periodic = {}
 
     def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
         :return: None
         """
         self._Database = []
         if isinstance(self._iFile, _ab._pd.DataFrame):
             uncertain, data, ts = [], [], []
             if self._iFile.empty:
                 print("its empty..")
@@ -584,16 +629,16 @@
                             self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
 
     def _periodicFrequentOneItem(self) -> Tuple[Dict, List]:
         """
         Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
-        :return: Tuple
 
+        :return: Tuple
         """
         mapSupport = {}
         for i in self._Database:
             n = i[0]
             for j in i[1:]:
                 if j.item not in mapSupport:
                     mapSupport[j.item] = [round(j.probability, 3), abs(0 - n), n]
@@ -704,16 +749,20 @@
             else:
                 value = int(value)
 
         return value
 
     def _removeFalsePositives(self) -> None:
         """
+        Removes the false positive patterns from the generated patterns.
+
+        This method iterates through the database to identify false positive patterns and removes them from the
+        generated patterns.
 
-        :return: Removes the false positive patterns in generated patterns
+        :return: None
         """
         periods = {}
         for i in self._Database:
             for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
                 else:
@@ -730,47 +779,30 @@
         for x, y in periods.items():
             if y[0] >= _minSup:
                 sample = str()
                 for i in x:
                     sample = sample + i + "\t"
                 self._finalPatterns[sample] = y
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+          "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns
         by counting the original support of a patterns.
+
         :return: None
         """
-        global _lno, _maxPer, _minSup, _first, _last, periodic
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._finalPatterns = {}
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, self._lno
-        mapSupport, plist = self._periodicFrequentOneItem()
-        updatedTrans = self._updateTransactions(mapSupport)
-        info = {k: v for k, v in mapSupport.items()}
-        Tree1 = self._buildTree(updatedTrans, info)
-        self._periodic = {}
-        Tree1.generatePatterns([], self._periodic)
-        self._removeFalsePositives()
-        print("Periodic frequent patterns were generated successfully using UPFP algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self) -> None:
+    def mine(self) -> None:
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns
         by counting the original support of a patterns.
+
         :return: None
         """
         global _lno, _maxPer, _minSup, _first, _last, periodic
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._finalPatterns = {}
         self._minSup = self._convert(self._minSup)
@@ -789,44 +821,48 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self) -> '_ab._pd.DataFrame':
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
@@ -834,28 +870,30 @@
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
+
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self.oFile = outFile
         writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y[0]) + ":" + str(y[1])
             writer.write("%s \n" % s1)
 
     def getPatterns(self) -> Dict[str, List[float]]:
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
@@ -873,14 +911,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = UPFPGrowth(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         print("Total number of Uncertain Periodic-Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_ab._sys.argv[2])
         print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py` & `pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/UPFPGrowthPlus.py`

 * *Files 18% similar despite different names*

```diff
@@ -1,18 +1,22 @@
 # UPFPGrowthPlus is used to discover periodic-frequent patterns in an uncertain temporal database.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
-#
 #
 #             from PAMI.uncertainPeriodicFrequentPattern.basic import UPFPGrowthPlus as alg
 #
+#             iFile = 'sampleDB.txt'
+#
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             maxPer = 3   # can also be specified between 0 and 1
+#
 #             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 #
-#             obj.startMine()
+#             obj.mine()
 #
 #             periodicFrequentPatterns = obj.getPatterns()
 #
 #             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 #
 #             obj.save(oFile)
 #
@@ -43,16 +47,14 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
 
 from PAMI.uncertainPeriodicFrequentPattern.basic import abstract as _ab
 import pandas as pd
 from deprecated import deprecated
 
@@ -67,31 +69,30 @@
 
 class _Item:
     """
     A class used to represent the item with probability in transaction of dataset
 
     :Attributes:
 
-    item : int or string
-        Represents the name of the item
-    probability : float
-        Represent the existential probability(likelihood presence) of an item
+        item : int or string
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
     """
 
     def __init__(self, item, probability):
         self.item = item
         self.probability = probability
 
 
 def printTree(root):
     """
     To print the tree with nodes with item name, probability, timestamps, and second probability respectively.
 
-    Attributes:
-
     :param root: Node
     :return: print all Tree with nodes with items, probability, parent item, timestamps, second probability respectively.
     """
     for x, y in root.children.items():
         print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
         printTree(y)
 
@@ -100,18 +101,21 @@
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         item : int
             storing item of a node
+
         probability : int
             To maintain the expected support of node
+
         parent : node
             To maintain the parent of every node
+
         children : list
             To maintain the children of node
 
     :Methods:
 
         addChild(itemName)
             storing the children to their respective parent nodes
@@ -141,16 +145,18 @@
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     Attributes:
 
         root: Node
             Represents the root node of the tree
+
         summaries: dictionary
             storing the nodes with same item name
+
         info: dictionary
             stores the support of items
 
 
     :Methods:
 
         addTransaction(transaction)
@@ -395,21 +401,24 @@
                     conditionalTree.addConditionalPatterns(patterns[pat], TimeStamps[pat], support[pat], probability[pat])
                 if len(patterns) > 0:
                     conditionalTree.generatePatterns(pattern, periodic)
             self.removeNode(i)
 
 class UPFPGrowthPlus(_ab._periodicFrequentPatterns):
     """
+    About this algorithm
+    ====================
+
     :Description: Basic Plus is  to discover periodic-frequent patterns in a uncertain temporal database.
 
-    :Reference:
-          Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023). 
-          UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases. 
-          ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
-          https://doi.org/10.1007/978-981-99-1642-9_16
+    :Reference:  Palla Likhitha, Rage Veena,Rage Uday Kiran, Koji Zettsu, Masashi Toyoda, Philippe Fournier-Viger, (2023).
+                 UPFP-growth++: An Efficient Algorithm to Find Periodic-Frequent Patterns in Uncertain Temporal Databases.
+                 ICONIP 2022. Communications in Computer and Information Science, vol 1792. Springer, Singapore.
+                 https://doi.org/10.1007/978-981-99-1642-9_16
+
 
     :param  iFile: str :
                    Name of the Input file to mine complete set of Uncertain Periodic Frequent Patterns
     :param  oFile: str :
                    Name of the output file to store complete set of Uncertain Periodic Frequent patterns
     :param  minSup: str:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
@@ -419,47 +428,61 @@
                    where maxPer represents the maximum periodicity threshold value specified by the user.
 
 
     :Attributes:
 
         iFile: file
             Name of the Input file or path of input file
+
         oFile: file
             Name of the output file or path of output file
+
         minSup: int or float or str
             The user can specify minSup either in count or proportion of database size.
             If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
             Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         maxPer: int or float or str
             The user can specify maxPer either in count or proportion of database size.
             If the program detects the data type of maxPer is integer, then it treats maxPer is expressed in count.
             Otherwise, it will be treated as float.
             Example: maxPer=10 will be treated as integer, while maxPer=10.0 will be treated as float
+
         sep: str
             This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
+
         memoryUSS: float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS: float
             To store the total amount of RSS memory consumed by the program
+
         startTime: float
             To record the start time of the mining process
+
         endTime: float
             To record the completion time of the mining process
+
         Database: list
             To store the transactions of a database in list
+
         mapSupport: Dictionary
             To maintain the information of item and their frequency
+
         lno: int
             To represent the total no of transaction
+
         tree: class
             To represents the Tree class
+
         itemSetCount: int
             To represents the total no of patterns
+
         finalPatterns: dict
             To store the complete patterns
 
     :Methods:
 
         startMine()
             Mining process will start from here
@@ -482,41 +505,49 @@
         buildTree()
             After updating the Database, remaining items will be added into the tree by setting root node as null
         convert()
             to convert the user specified value
         PeriodicFrequentOneItems()
             To extract the one-length periodic-frequent items
 
-    **Executing the code on terminal**:
-    --------------------------------------------
+    Execution methods
+    =================
 
-    .. code-block:: console
 
+    **Terminal command**
+
+
+    .. code-block:: console
 
        Format:
 
        (.venv) $ python3 UPFPGrowthPlus.py <inputFile> <outputFile> <minSup> <maxPer>
 
        Examples Usage:
 
        (.venv) $ python3 UPFPGrowthPlus.py sampleTDB.txt patterns.txt 0.3 4
 
+    .. note:: minSup and maxPer will be considered in support count or frequency
 
-               .. note:: minSup and maxPer will be considered in support count or frequency
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    -----------------------------------------------------------------
     .. code-block:: python
 
             from PAMI.uncertainPeriodicFrequentPattern import UPFPGrowthPlus as alg
 
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
+
+            maxPer = 2   # can also be specified between 0 and 1
+
             obj = alg.UPFPGrowthPlus(iFile, minSup, maxPer)
 
-            obj.startMine()
+            obj.mine()
 
             periodicFrequentPatterns = obj.getPatterns()
 
             print("Total number of uncertain Periodic Frequent Patterns:", len(periodicFrequentPatterns))
 
             obj.save(oFile)
 
@@ -699,15 +730,14 @@
                     k += 1
             if k == 0:
                 return 0
         return 1
 
     def _convert(self, value):
         """
-
         To convert the given user specified value
 
         :param value: user specified value
         :return: converted value
         """
         if type(value) is int:
             value = int(value)
@@ -719,14 +749,15 @@
             else:
                 value = int(value)
         return value
 
     def _removeFalsePositives(self):
         """
         To remove false positives in generated patterns
+
         :return: original patterns
         """
         periods = {}
         for i in self._Database:
             for x, y in self._periodic.items():
                 if len(x) == 1:
                     periods[x] = y
@@ -747,42 +778,23 @@
                 count += 1
                 sample = str()
                 for i in x:
                     sample = sample + i + " "
                 self._finalPatterns[sample] = y
         #print("Total false patterns generated:", len(self._periodic) - count)
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+         "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global _minSup, _maxPer, _first, _last, _lno
-        self._startTime = _ab._time.time()
-        self._creatingItemSets()
-        self._minSup = self._convert(self._minSup)
-        self._maxPer = self._convert(self._maxPer)
-        self._finalPatterns = {}
-        _minSup, _maxPer, _lno = self._minSup, self._maxPer, len(self._Database)
-        mapSupport, plist = self._PeriodicFrequentOneItems()
-        updatedTrans = self._updateTransactions(mapSupport)
-        info = {k: v for k, v in mapSupport.items()}
-        root = self._buildTree(updatedTrans, info)
-        self._periodic = {}
-        root.generatePatterns([], self._periodic)
-        self._removeFalsePositives()
-        print("Periodic Frequent patterns were generated successfully using UPFP-Growth++ algorithm")
-        self._endTime = _ab._time.time()
-        process = _ab._psutil.Process(_ab._os.getpid())
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self):
+    def mine(self):
         """
         Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
         global _minSup, _maxPer, _first, _last, _lno
         self._startTime = _ab._time.time()
         self._creatingItemSets()
         self._minSup = self._convert(self._minSup)
@@ -802,42 +814,47 @@
         self._memoryUSS = float()
         self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self):
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function.
+
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
     def getMemoryRSS(self):
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
     def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
         return self._endTime - self._startTime
 
     def getPatternsAsDataFrame(self):
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
@@ -845,27 +862,29 @@
         for a, b in self._finalPatterns.items():
             data.append([a, b[0], b[1]])
             dataframe = _ab._pd.DataFrame(data, columns=['Patterns', 'Support', 'Periodicity'])
         return dataframe
 
     def save(self, outFile):
         """
+
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x + ":" + str(y)
             writer.write("%s \n" % s1)
 
     def getPatterns(self):
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self._finalPatterns
 
@@ -883,14 +902,15 @@
     _ap = str()
     if len(_ab._sys.argv) == 5 or len(_ab._sys.argv) == 6:
         if len(_ab._sys.argv) == 6:
             _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4], _ab._sys.argv[5])
         if len(_ab._sys.argv) == 5:
             _ap = UPFPGrowthPlus(_ab._sys.argv[1], _ab._sys.argv[3], _ab._sys.argv[4])
         _ap.startMine()
+        _ap.mine()
         _Patterns = _ap.getPatterns()
         print("Total number of Patterns:", len(_Patterns))
         _ap.savePatterns(_ab._sys.argv[2])
         # print(ap.getPatternsAsDataFrame())
         _memUSS = _ap.getMemoryUSS()
         print("Total Memory in USS:", _memUSS)
         _memRSS = _ap.getMemoryRSS()
```

### Comparing `pami-2024.4.9.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/uncertainPeriodicFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/SWFPGrowth.py` & `pami-2024.5.1/PAMI/weightedFrequentPattern/basic/WFIM.py`

 * *Files 10% similar despite different names*

```diff
@@ -1,23 +1,28 @@
-# SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
+# WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+# It stores the database in compressed fp-tree decreasing the memory usage and extracts the
+# patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# -------------------------------------------------------
 #
-#             from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
+#             from PAMI.weightFrequentPattern.basic import basic as alg
 #
-#             obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, sep)
+#             iFile = 'sampleDB.txt'
 #
-#             obj.startMine()
+#             minSup = 10  # can also be specified between 0 and 1
+#
+#             obj = alg.basic(iFile, wFile, minSup, minWeight)
+#
+#             obj.mine()
 #
 #             frequentPatterns = obj.getPatterns()
 #
 #             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
-#             obj.save(oFile)
+#             obj.savePatterns(oFile)
 #
 #             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
@@ -26,17 +31,14 @@
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
-
-
-
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -44,396 +46,337 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.weightedFrequentNeighbourhoodPattern.basic import abstract as _fp
+from PAMI.weightedFrequentPattern.basic import abstract as _fp
+from typing import List, Dict, Tuple, Union, Generator
 import pandas as pd
 from deprecated import deprecated
-from typing import List, Dict, Tuple, Union, Iterable
 
-_minWS = str()
-_weights = {}
-_rank = {}
-_neighbourList = {}
 
+_minSup = str()
+_minWeight = int()
+_miniWeight = int()
+_maxWeight = int()
+_weights = {}
 _fp._sys.setrecursionlimit(20000)
 
 
-class _WeightedItem:
-    """
-    A class used to represent the weight of the item
-
-    :Attributes:
-
-        item: str
-            storing item of the frequent pattern
-        weight: float
-            stores the weight of the item
-
-    """
-    def __init__(self, item: str, weight: float) -> None:
-        self.item = item
-        self.weight = weight
-
-
 class _Node:
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         itemId: int
             storing item of a node
+
         counter: int
             To maintain the support of node
+
         parent: node
             To maintain the parent of node
+
         children: list
             To maintain the children of node
 
     :Methods:
 
         addChild(node)
             Updates the nodes children list and parent for the given node
-
     """
 
-    def __init__(self, item: str, children: Dict[str, '_Node']) -> None:
+    def __init__(self, item: str, children: list) -> None:
         self.itemId = item
         self.counter = 1
-        self.weight = 0
         self.parent = None
         self.children = children
 
     def addChild(self, node: '_Node') -> None:
         """
         Retrieving the child from the tree
 
-        :param node: Children node.
-        :type node: Node
+        :param node: Children node
+        :type node: _Node
         :return: Updates the children nodes and parent nodes
-        :return: None
-
         """
         self.children[node.itemId] = node
         node.parent = self
 
 
 class _Tree:
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
             The first node of the tree set to Null.
+
         summaries : dictionary
             Stores the nodes itemId which shares same itemId
+
         info : dictionary
             frequency of items in the transactions
 
     :Methods:
 
         addTransaction(transaction, freq)
             adding items of  transactions into the tree as nodes and freq is the count of nodes
         getFinalConditionalPatterns(node)
             getting the conditional patterns from fp-tree for a node
         getConditionalPatterns(patterns, frequencies)
-            sort the patterns by removing the items with lower minWS
+            sort the patterns by removing the items with lower minSup
         generatePatterns(prefix)
             generating the patterns from fp-tree
     """
 
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction: List[_WeightedItem], count: int) -> None:
+    def addTransaction(self, transaction: List[str], count: int) -> None:
         """
         Adding transaction into tree
 
         :param transaction: it represents the one transaction in database
         :type transaction: list
         :param count: frequency of item
         :type count: int
         :return: None
         """
-
-        # This method takes transaction as input and returns the tree
-        global _neighbourList, _rank
-        currentNode = self.root
-        for i in range(len(transaction)):
-            wei = 0
-            l1 = i
-            while l1 >= 0:
-                wei += transaction[l1].weight
-                l1 -= 1
-            if transaction[i].item not in currentNode.children:
-                newNode = _Node(transaction[i].item, {})
-                newNode.freq = count
-                newNode.weight = wei
-                currentNode.addChild(newNode)
-                if _rank[transaction[i].item] in self.summaries:
-                    self.summaries[_rank[transaction[i].item]].append(newNode)
-                else:
-                    self.summaries[_rank[transaction[i].item]] = [newNode]
-                currentNode = newNode
-            else:
-                currentNode = currentNode.children[transaction[i].item]
-                currentNode.freq += count
-                currentNode.weight += wei
-
-    def addConditionalPattern(self, transaction: List[_WeightedItem], count: int) -> None:
-        """
-        Adding transaction into tree
-
-        :param transaction: it represents the one transaction in database
-        :type transaction: list
-        :param count: frequency of item
-        :type count: int
-        :return : None
-        """
         # This method takes transaction as input and returns the tree
-        global _neighbourList, _rank
         currentNode = self.root
         for i in range(len(transaction)):
-            wei = 0
-            l1 = i
-            while l1 >= 0:
-                wei += transaction[l1].weight
-                l1 -= 1
-            if transaction[i].itemId not in currentNode.children:
-                newNode = _Node(transaction[i].itemId, {})
+            if transaction[i] not in currentNode.children:
+                newNode = _Node(transaction[i], {})
                 newNode.freq = count
-                newNode.weight = wei
                 currentNode.addChild(newNode)
-                if _rank[transaction[i].itemId] in self.summaries:
-                    self.summaries[_rank[transaction[i].itemId]].append(newNode)
+                if transaction[i] in self.summaries:
+                    self.summaries[transaction[i]].append(newNode)
                 else:
-                    self.summaries[_rank[transaction[i].itemId]] = [newNode]
+                    self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
-                currentNode = currentNode.children[transaction[i].itemId]
+                currentNode = currentNode.children[transaction[i]]
                 currentNode.freq += count
-                currentNode.weight += wei
-
-    def printTree(self, root: _Node) -> None:
-        """
-        To print the details of tree
-
-        :param root: root node of the tree
-        :return: details of tree
-        """
-        if len(root.children) == 0:
-            return
-        else:
-            for x, y in root.children.items():
-                #print(y.itemId, y.parent.itemId, y.freq, y.weight)
-                self.printTree(y)
-
 
-    def getFinalConditionalPatterns(self, alpha: int) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+    def getFinalConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
         """
         Generates the conditional patterns for a node
 
         :param alpha: node to generate conditional patterns
         :return: returns conditional patterns, frequency of each item in conditional patterns
-
         """
         finalPatterns = []
         finalFreq = []
-        global _neighbourList
         for i in self.summaries[alpha]:
-            set1 = i.weight
+            set1 = i.freq
             set2 = []
             while i.parent.itemId is not None:
-                if i.parent.itemId in _neighbourList[i.itemId]:
-                    set2.append(i.parent)
+                set2.append(i.parent.itemId)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
                 finalFreq.append(set1)
         finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
         return finalPatterns, finalFreq, info
 
     @staticmethod
-    def getConditionalTransactions(ConditionalPatterns: List[List[_Node]], conditionalFreq: List[float]) -> Tuple[List[List[_Node]], List[float], Dict[int, float]]:
+    def getConditionalTransactions(ConditionalPatterns: List[List[str]], conditionalFreq: List[int]) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
         """
         To calculate the frequency of items in conditional patterns and sorting the patterns
 
         :param ConditionalPatterns: paths of a node
         :param conditionalFreq: frequency of each item in the path
         :return: conditional patterns and frequency of each item in transactions
         """
-        global _rank
+        global _minSup, _miniWeight
         pat = []
         freq = []
         data1 = {}
         for i in range(len(ConditionalPatterns)):
             for j in ConditionalPatterns[i]:
-                if j.itemId in data1:
-                    data1[j.itemId] += conditionalFreq[i]
+                if j in data1:
+                    data1[j] += conditionalFreq[i]
                 else:
-                    data1[j.itemId] = conditionalFreq[i]
-        up_dict = {k: v for k, v in data1.items() if v >= _minWS}
+                    data1[j] = conditionalFreq[i]
+        up_dict = {k: v for k, v in data1.items() if v >= _minSup and v * _miniWeight > _minSup}
         count = 0
         for p in ConditionalPatterns:
-            p1 = [v for v in p if v.itemId in up_dict]
-            trans = sorted(p1, key=lambda x: (up_dict.get(x)), reverse=True)
+            p1 = [v for v in p if v in up_dict]
+            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
                 freq.append(conditionalFreq[count])
             count += 1
-        up_dict = {_rank[k]: v for k, v in up_dict.items()}
         return pat, freq, up_dict
 
-    def generatePatterns(self, prefix: List[int]) -> Iterable[Tuple[List[int], float]]:
+    def generatePatterns(self, prefix: List[str]) -> Generator[Tuple[List[str], int], None, None]:
         """
         To generate the frequent patterns
 
         :param prefix: an empty list
         :return: Frequent patterns that are extracted from fp-tree
-
         """
-        global _minWS
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
+        global _miniWeight, _maxWeight, _minWeight, _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
             pattern = prefix[:]
             pattern.append(i)
             yield pattern, self.info[i]
             patterns, freq, info = self.getFinalConditionalPatterns(i)
             conditionalTree = _Tree()
             conditionalTree.info = info.copy()
             for pat in range(len(patterns)):
-                conditionalTree.addConditionalPattern(patterns[pat], freq[pat])
+                conditionalTree.addTransaction(patterns[pat], freq[pat])
             if len(patterns) > 0:
                 for q in conditionalTree.generatePatterns(pattern):
                     yield q
 
 
-class SWFPGrowth(_fp._weightedFrequentSpatialPatterns):
+class WFIM(_fp._weightedFrequentPatterns):
     """
-    :Description: SWFPGrowth is an algorithm to mine the weighted spatial frequent patterns in spatiotemporal databases.
+    About this algorithm
+    ====================
 
-    :Reference:
-        R. Uday Kiran, P. P. C. Reddy, K. Zettsu, M. Toyoda, M. Kitsuregawa and P. Krishna Reddy,
-        "Discovering Spatial Weighted Frequent Itemsets in Spatiotemporal Databases," 2019 International
-        Conference on Data Mining Workshops (ICDMW), 2019, pp. 987-996, doi: 10.1109/ICDMW.2019.00143.
+    :Description: * WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
+                  * It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs employs downward closure property to  reduce the search space effectively.
+
+    :Reference:  U. Yun and J. J. Leggett, “Wfim: weighted frequent itemset mining with a weight range and a minimum weight,”
+           In:   Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM, 2005, pp. 636–640.
+                 https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.76
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of weighted Frequent Neighbourhood Patterns.
+                   Name of the Input file to mine complete set of weighted Frequent Patterns.
     :param  oFile: str :
-                   Name of the output file to store complete set of weighted Frequent Neighbourhood Patterns.
-    :param  minSup: int or str or float:
+                   Name of the output file to store complete set of weighted Frequent Patterns.
+    :param  minSup: str or int or float:
                    minimum support thresholds were tuned to find the appropriate ranges in the limited memory
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  maxper: floot :
-                   where maxPer represents the maximum periodicity threshold value specified by the user.
 
 
-    :Attributes:
+    :Attributes :
 
         iFile : file
             Input file name or path of the input file
-        minWS: float or int or str
-            The user can specify minWS either in count or proportion of database size.
-            If the program detects the data type of minWS is integer, then it treats minWS is expressed in count.
+
+        minSup: float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minWS=10 will be treated as integer, while minWS=10.0 will be treated as float
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         minWeight: float or int or str
             The user can specify minWeight either in count or proportion of database size.
             If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
             Otherwise, it will be treated as float.
             Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
+
         oFile : file
             Name of the output file or the path of the output file
+
         startTime:float
             To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
             it represents the total no of transactions
+
         tree : class
             it represents the Tree class
+
         finalPatterns : dict
             it represents to store the patterns
 
     :Methods :
 
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to an output file
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
             Scans the dataset or dataframes and stores in list format
         frequentOneItem()
             Extracts the one-frequent patterns from transactions
 
-    **Methods to execute code on terminal**
-    -------------------------------------------
-    .. code-block:: console
+    Execution methods
+    =================
+
+
+    **Terminal command**
 
 
+    .. code-block:: console
+
        Format:
 
-       (.venv) $ python3 SWFPGrowth.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+       (.venv) $ python3 basic.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
 
-       Example usage :
+       Example Usage:
 
-       (.venv) $ python3 SWFPGrowth.py sampleDB.txt weightFile.txt patterns.txt 10  2
+       (.venv) $ python3 basic.py sampleDB.txt weightSample.txt patterns.txt 10.0 3.4
 
+    .. note:: minSup and maxPer will be considered in support count or frequency
 
-               .. note:: minSup will be considered in support count or frequency
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.weightFrequentNeighbourhoodPattern.basic import SWFPGrowth as alg
+            from PAMI.weightFrequentPattern.basic import basic as alg
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.SWFPGrowth(iFile, wFile, nFile, minSup, minWeight, seperator)
+            obj = alg.basic(iFile, wFile, minSup, minWeight)
 
-            obj.startMine()
+            obj.mine()
 
             frequentPatterns = obj.getPatterns()
 
             print("Total number of Frequent Patterns:", len(frequentPatterns))
 
-            obj.save(oFile)
+            obj.savePatterns(oFile)
 
             Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getmemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
@@ -441,384 +384,350 @@
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    --------------
-    The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
 
-        """
+
+             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    """
 
     __startTime = float()
     __endTime = float()
-    _Weights = {}
-    _minWS = str()
+    _minSup = str()
     __finalPatterns = {}
-    _neighbourList = {}
     _iFile = " "
     _oFile = " "
     _sep = " "
     __memoryUSS = float()
     __memoryRSS = float()
     __Database = []
     __mapSupport = {}
     __lno = 0
     __tree = _Tree()
     __rank = {}
     __rankDup = {}
 
-    def __init__(self, iFile: Union[str, _fp._pd.DataFrame], nFile: Union[str, _fp._pd.DataFrame], minWS: Union[int, float, str], sep='\t') -> None:
-        super().__init__(iFile, nFile, minWS, sep)
+    def __init__(self, iFile: str, wFile: str, minSup: str, minWeight: int, sep: str='\t') -> None:
+        super().__init__(iFile, wFile, minSup, minWeight, sep)
 
     def __creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
         :return: None
         """
-        self._Database = []
+        self.__Database = []
         if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self._Database = self._iFile['Transactions'].tolist()
+                self.__Database = self._iFile['Transactions'].tolist()
+
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _fp._validators.url(self._iFile):
                 data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._Database.append(temp)
+                    self.__Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
-                            line = line.strip()
-                            line = line.split(':')
-                            temp1 = [i.rstrip() for i in line[0].split(self._sep)]
-                            temp2 = [int(i.strip()) for i in line[1].split(self._sep)]
-                            tr = []
-                            for i in range(len(temp1)):
-                                we = _WeightedItem(temp1[i], temp2[i])
-                                tr.append(we)
-                            self._Database.append(tr)
+                            line.strip()
+                            temp = [i.rstrip() for i in line.split(self._sep)]
+                            temp = [x for x in temp if x]
+                            # print(len(temp))
+                            self.__Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _scanNeighbours(self) -> None:
-        self._neighbourList = {}
-        if isinstance(self._nFile, _fp._pd.DataFrame):
-            data, items = [], []
-            if self._nFile.empty:
+    def _scanningWeights(self) -> None:
+        """
+        Storing the weights of the variables in input file in a weights variable
+
+        :return: None
+        """
+        global _weights
+        _weights = {}
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            items, weights = [], []
+            if self._wFile.empty:
                 print("its empty..")
-            i = self._nFile.columns.values.tolist()
-            if 'item' in i:
-                items = self._nFile['items'].tolist()
-            if 'Neighbours' in i:
-                data = self._nFile['Neighbours'].tolist()
-            for k in range(len(items)):
-                self._neighbourList[items[k][0]] = data[k]
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                items = self._wFile['items'].tolist()
+            if 'weights' in i:
+                weights = self._wFile['weights'].tolist()
+            for i in range(len(weights)):
+                _weights[items[i]] = weights[i]
+
             # print(self.Database)
-        if isinstance(self._nFile, str):
-            if _fp._validators.url(self._nFile):
-                data = _fp._urlopen(self._nFile)
+        if isinstance(self._wFile, str):
+            if _fp._validators.url(self._wFile):
+                data = _fp._urlopen(self._wFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self._neighbourList[temp[0]] = temp[1:]
+                    _weights[temp[0]] = temp[1]
             else:
                 try:
-                    with open(self._nFile, 'r', encoding='utf-8') as f:
+                    with open(self._wFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            self._neighbourList[temp[0]] = temp[1:]
+                            s = int(float(temp[1]))
+                            _weights[temp[0]] = s
                 except IOError:
-                    print("File Not Found2")
+                    print("File Not Found")
                     quit()
 
     def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
         """
-        to convert the type of user specified minWS value
+        To convert the type of user specified minSup value.
 
-        :param value: user specified minWS value
+        :param value: user specified minSup value
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self._Database) * value)
+            value = (len(self.__Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self._Database) * value)
+                value = (len(self.__Database) * value)
             else:
                 value = int(value)
         return value
 
     def __frequentOneItem(self) -> List[str]:
         """
         Generating One frequent items sets
-        :return: None
+
+        :return: list
         """
         global _maxWeight
-        self._mapSupport = {}
-        for tr in self._Database:
-            for i in tr:
-                nn = [j for j in tr if j.item in self._neighbourList[i.item]]
-                if i.item not in self._mapSupport:
-                    self._mapSupport[i.item] = i.weight
+        self.__mapSupport = {}
+        for tr in self.__Database:
+            for i in range(0, len(tr)):
+                if tr[i] not in self.__mapSupport:
+                    self.__mapSupport[tr[i]] = 1
                 else:
-                    self._mapSupport[i.item] += i.weight
-                for k in nn:
-                    self._mapSupport[i.item] += k.weight
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v >= self._minWS}
-        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse=True)]
+                    self.__mapSupport[tr[i]] += 1
+        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup and v * _maxWeight > self._minSup}
+        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
         self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
         return genList
 
-    def __updateTransactions(self, itemSet: List[str]) -> List[List[_WeightedItem]]:
+    def __updateTransactions(self, itemSet: List[str]) -> List[List[int]]:
         """
         Updates the items in transactions with rank of items according to their support
+
         :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
+                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+
         :param itemSet: list of one-frequent items
         :return: list
         """
         list1 = []
-        for tr in self._Database:
+        for tr in self.__Database:
             list2 = []
             for i in range(len(tr)):
-                if tr[i].item in itemSet:
-                    list2.append(tr[i])
+                if tr[i] in itemSet:
+                    list2.append(self.__rank[tr[i]])
             if len(list2) >= 1:
-                basket = list2
-                basket.sort(key=lambda val: self.__rank[val.item])
-                list1.append(basket)
+                list2.sort()
+                list1.append(list2)
         return list1
 
     @staticmethod
-    def __buildTree(transactions: List[List[_WeightedItem]], info: Dict[int, float]) -> _Tree:
+    def __buildTree(transactions: List[List[int]], info: Dict[int, int]) -> '_Tree':
         """
         Builds the tree with updated transactions
 
         :param transactions: updated transactions
         :param info: support details of each item in transactions.
-        :return: transactions compressed in fp-tree.
+        :return: Transactions compressed in fp-tree
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(transactions)):
             rootNode.addTransaction(transactions[i], 1)
         return rootNode
 
-    def __savePeriodic(self, itemSet: List[str]) -> str:
+    def __savePeriodic(self, itemSet: List[int]) -> str:
         """
         The duplication items and their ranks
 
         :param itemSet: frequent itemSet that generated
         :return: patterns with original item names.
-
         """
         temp = str()
         for i in itemSet:
             temp = temp + self.__rankDup[i] + "\t"
         return temp
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
         main program to start the operation
-        :return : None
 
+        :return: None
         """
-        global _minWS, _neighbourList, _rank
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minWS is None:
-            raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
-        self._scanNeighbours()
-        self._minWS = self.__convert(self._minWS)
-        _minWS = self._minWS
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
-        _rank = self.__rank
-        for x, y in self.__rank.items():
-            self.__rankDup[y] = x
-        _neighbourList = self._neighbourList
-        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
-        # for x, y in self._neighbourList.items():
-        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
-        #     _neighbourList[self.__rank[x]] = xx
-        # print(_neighbourList)
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self) -> None:
+    def mine(self) -> None:
         """
         main program to start the operation
-        :return : None
 
+        :return: None
         """
-        global _minWS, _neighbourList, _rank
+        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
         self.__startTime = _fp._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
-        if self._minWS is None:
+        if self._minSup is None:
             raise Exception("Please enter the Minimum Support")
         self.__creatingItemSets()
-        self._scanNeighbours()
-        self._minWS = self.__convert(self._minWS)
-        _minWS = self._minWS
+        self._scanningWeights()
+        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
+        _maxWeight = max([s for s in _weights.values()])
+        _miniWeight = min([s for s in _weights.values()])
+        self._minSup = self.__convert(self._minSup)
+        _minSup = self._minSup
         itemSet = self.__frequentOneItem()
         updatedTransactions = self.__updateTransactions(itemSet)
-        info = {self.__rank[k]: v for k, v in self._mapSupport.items()}
-        _rank = self.__rank
         for x, y in self.__rank.items():
             self.__rankDup[y] = x
-        _neighbourList = self._neighbourList
-        #self._neighbourList = {k:v for k, v in self._neighbourList.items() if k in self._mapSupport.keys()}
-        # for x, y in self._neighbourList.items():
-        #     xx = [self.__rank[i] for i in y if i in self._mapSupport.keys()]
-        #     _neighbourList[self.__rank[x]] = xx
-        # print(_neighbourList)
+        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
         __Tree = self.__buildTree(updatedTransactions, info)
         patterns = __Tree.generatePatterns([])
         self.__finalPatterns = {}
         for k in patterns:
             s = self.__savePeriodic(k[0])
             self.__finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent patterns were generated successfully using SWFPGrowth algorithm")
+        print("Weighted Frequent patterns were generated successfully using basic algorithm")
         self.__endTime = _fp._time.time()
         self.__memoryUSS = float()
         self.__memoryRSS = float()
         process = _fp._psutil.Process(_fp._os.getpid())
         self.__memoryUSS = process.memory_full_info().uss
         self.__memoryRSS = process.memory_info().rss
 
+
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self.__memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function
+
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function.
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self.__memoryRSS
 
     def getRuntime(self) -> float:
         """
-        Calculating the total amount of runtime taken by the mining process
 
+        Calculating the total amount of runtime taken by the mining process.
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self.__endTime - self.__startTime
 
-    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> pd.DataFrame:
         """
-        Storing final frequent patterns in a dataframe
+
+        Storing final frequent patterns in a dataframe.
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self.__finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of frequent patterns will be loaded in to an output file
+
+        Complete set of frequent patterns will be loaded in to an output file.
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
         for x, y in self.__finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self) -> Dict[str, int]:
         """
-        Function to send the set of frequent patterns after completion of the mining process
+
+        Function to send the set of frequent patterns after completion of the mining process.
 
         :return: returning frequent patterns
         :rtype: dict
         """
         return self.__finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of  Weighted Spatial Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
+        
+
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 7 or len(_fp._sys.argv) == 8:
-        if len(_fp._sys.argv) == 8:
-            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6],
-                             _fp._sys.argv[7])
+    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
         if len(_fp._sys.argv) == 7:
-            _ap = SWFPGrowth(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 6:
+            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
-        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Weighted Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:", _ap.getMemoryUSS())
-        print("Total Memory in RSS",  _ap.getMemoryRSS())
-        print("Total ExecutionTime in ms:", _ap.getRuntime())
-    else:
-        _ap = SWFPGrowth('sample.txt', 'neighbourSample.txt', 150, ' ')
-        _ap.startMine()
-        print("Total number of Weighted Spatial Frequent Patterns:", len(_ap.getPatterns()))
-        _ap.save('output.txt')
-        print("Total Memory in USS:", _ap.getMemoryUSS())
+        print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
+    else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/weightedFrequentNeighbourhoodPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/WFIM.py` & `pami-2024.5.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py`

 * *Files 16% similar despite different names*

```diff
@@ -1,42 +1,43 @@
-# WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
-# It stores the database in compressed fp-tree decreasing the memory usage and extracts the
-# patterns from tree.It employs downward closure property to  reduce the search space effectively.
+# WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database. It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
 #
 # **Importing this algorithm into a python program**
-# ----------------------------------------------------------
 #
+#             from PAMI.weightedFrequentRegularPattern.basic import WFRIMiner as alg
 #
-#             from PAMI.weightFrequentPattern.basic import basic as alg
+#             iFile = 'sampleDB.txt'
 #
-#             obj = alg.basic(iFile, wFile, minSup, minWeight)
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj.startMine()
+#             obj = alg.WFRIMiner(iFile, WS, regularity)
 #
-#             frequentPatterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of Frequent Patterns:", len(frequentPatterns))
+#             weightedFrequentRegularPatterns = obj.getPatterns()
 #
-#             obj.savePatterns(oFile)
+#             print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 #
-#             Df = obj.getPatternsAsDataFrame()
+#             obj.save(oFile)
+#
+#             Df = obj.getPatternInDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
 #             print("Total Memory in USS:", memUSS)
 #
 #             memRSS = obj.getMemoryRSS()
 #
 #             print("Total Memory in RSS", memRSS)
 #
 #             run = obj.getRuntime()
 #
 #             print("Total ExecutionTime in seconds:", run)
 #
 
+
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
 
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
@@ -44,86 +45,99 @@
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
 
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.weightedFrequentPattern.basic import abstract as _fp
-from typing import List, Dict, Tuple, Set, Union, Any, Generator
+from PAMI.weightedFrequentRegularPattern.basic import abstract as _fp
 import pandas as pd
 from deprecated import deprecated
+from typing import List, Dict
 
 
-_minSup = str()
-_minWeight = int()
-_miniWeight = int()
-_maxWeight = int()
+_WS = str()
+_regularity = str()
+_lno = int()
 _weights = {}
+_wf = {}
 _fp._sys.setrecursionlimit(20000)
 
 
 class _Node:
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
 
         itemId: int
             storing item of a node
+
         counter: int
             To maintain the support of node
+
         parent: node
             To maintain the parent of node
+
         children: list
             To maintain the children of node
 
     :Methods:
 
         addChild(node)
             Updates the nodes children list and parent for the given node
+
     """
 
-    def __init__(self, item: str, children: list) -> None:
-        self.itemId = item
-        self.counter = 1
-        self.parent = None
+    def __init__(self, item: int, children: dict) -> None:
+        """
+        Initializing the Node class
+
+        :param item: Storing the item of a node
+        :type item: int or None
+        :param children: To maintain the children of a node
+        :type children: dict
+        :return: None
+        """
+
+        self.item = item
         self.children = children
+        self.parent = None
+        self.timeStamps = []
 
-    def addChild(self, node: '_Node') -> None:
+    def addChild(self, node) -> None:
         """
-        Retrieving the child from the tree
+        To add the children to a node
 
-        :param node: Children node
-        :type node: Node
-        :return: Updates the children nodes and parent nodes
+        :param node: parent node in the tree
+        :return: None
         """
-        self.children[node.itemId] = node
+
+        self.children[node.item] = node
         node.parent = self
 
 
 class _Tree:
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
 
         root : Node
             The first node of the tree set to Null.
+
         summaries : dictionary
             Stores the nodes itemId which shares same itemId
+
         info : dictionary
             frequency of items in the transactions
 
     :Methods:
-
         addTransaction(transaction, freq)
             adding items of  transactions into the tree as nodes and freq is the count of nodes
         getFinalConditionalPatterns(node)
             getting the conditional patterns from fp-tree for a node
         getConditionalPatterns(patterns, frequencies)
             sort the patterns by removing the items with lower minSup
         generatePatterns(prefix)
@@ -131,593 +145,684 @@
     """
 
     def __init__(self) -> None:
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction: List[str], count: int) -> None:
+    def addTransaction(self, transaction: list, tid: list) -> None:
         """
-        Adding transaction into tree
+        Adding a transaction into tree
 
-        :param transaction: it represents the one transaction in database
+        :param transaction: To represent the complete database
         :type transaction: list
-        :param count: frequency of item
-        :type count: int
-        :return: None
+        :param tid: To represent the timestamp of a database
+        :type tid: list
+        :return: pfp-growth tree
         """
-        # This method takes transaction as input and returns the tree
         currentNode = self.root
         for i in range(len(transaction)):
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
-                newNode.freq = count
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-                currentNode.freq += count
+        currentNode.timeStamps = currentNode.timeStamps + tid
 
-    def getFinalConditionalPatterns(self, alpha: str) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+    def getConditionalPatterns(self, alpha, pattern) -> tuple:
         """
-        Generates the conditional patterns for a node
+        Generates all the conditional patterns of a respective node
 
-        :param alpha: node to generate conditional patterns
-        :return: returns conditional patterns, frequency of each item in conditional patterns
+        :param alpha: To represent a Node in the tree
+        :type alpha: Node
+        :param pattern: prefix of the pattern
+        :type alpha: list
+        :return: A tuple consisting of finalPatterns, conditional pattern base and information
         """
         finalPatterns = []
-        finalFreq = []
+        finalSets = []
         for i in self.summaries[alpha]:
-            set1 = i.freq
+            set1 = i.timeStamps
             set2 = []
-            while i.parent.itemId is not None:
-                set2.append(i.parent.itemId)
+            while i.parent.item is not None:
+                set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalFreq.append(set1)
-        finalPatterns, finalFreq, info = self.getConditionalTransactions(finalPatterns, finalFreq)
-        return finalPatterns, finalFreq, info
+                finalSets.append(set1)
+        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets, pattern)
+        return finalPatterns, finalSets, info
+
+    @staticmethod
+    def generateTimeStamps(node) -> list:
+        """
+        To get the timestamps of a node
+
+        :param node: A node in the tree
+        :return: Timestamps of a node
+        """
+
+        finalTimeStamps = node.timeStamps
+        return finalTimeStamps
+
+    def removeNode(self, nodeValue) -> None:
+        """
+        Removing the node from tree
+
+        :param nodeValue: To represent a node in the tree
+        :type nodeValue: node
+        :return: Tree with their nodes updated with timestamps
+        """
+
+        for i in self.summaries[nodeValue]:
+            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
+            del i.parent.children[nodeValue]
+
+    def getTimeStamps(self, alpha) -> list:
+        """
+        To get all the timestamps of the nodes which share same item name
+
+        :param alpha: Node in a tree
+        :return: Timestamps of a  node
+        """
+        temporary = []
+        for i in self.summaries[alpha]:
+            temporary += i.timeStamps
+        return temporary
 
     @staticmethod
-    def getConditionalTransactions(ConditionalPatterns: List[List[str]], conditionalFreq: List[int]) -> Tuple[List[List[str]], List[int], Dict[str, int]]:
+    def getSupportAndPeriod(timeStamps: list, pattern: list) -> list:
         """
-        To calculate the frequency of items in conditional patterns and sorting the patterns
+        To calculate the periodicity and support
 
-        :param ConditionalPatterns: paths of a node
-        :param conditionalFreq: frequency of each item in the path
-        :return: conditional patterns and frequency of each item in transactions
+        :param timeStamps: Timestamps of an item set
+        :type timeStamps: list
+        :param pattern: pattern to evaluate the weighted frequent regular or not
+        :type pattern: list
+        :return: support, periodicity
+        """
+        global _WS, _regularity, _lno, _weights
+        timeStamps.sort()
+        cur = 0
+        per = list()
+        sup = 0
+        for j in range(len(timeStamps)):
+            per.append(timeStamps[j] - cur)
+            cur = timeStamps[j]
+            sup += 1
+        per.append(_lno - cur)
+        l = int()
+        for i in pattern:
+            l = l + _weights[i]
+        wf = (l / (len(pattern))) * sup
+        if len(per) == 0:
+            return [0, 0]
+        return [sup, max(per), wf]
+
+    def conditionalDatabases(self, conditionalPatterns: list, conditionalTimeStamps: list, pattern: list) -> tuple:
+        """
+        It generates the conditional patterns with periodic-frequent items
+
+        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
+        :type conditionalPatterns: list
+        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
+        :type conditionalTimeStamps: list
+        :param pattern: prefix of the pattern
+        :type pattern: list
+        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
         """
-        global _minSup, _miniWeight
+        global _WS, _regularity
         pat = []
-        freq = []
+        timeStamps = []
         data1 = {}
-        for i in range(len(ConditionalPatterns)):
-            for j in ConditionalPatterns[i]:
+        for i in range(len(conditionalPatterns)):
+            for j in conditionalPatterns[i]:
                 if j in data1:
-                    data1[j] += conditionalFreq[i]
+                    data1[j] = data1[j] + conditionalTimeStamps[i]
                 else:
-                    data1[j] = conditionalFreq[i]
-        up_dict = {k: v for k, v in data1.items() if v >= _minSup and v * _miniWeight > _minSup}
+                    data1[j] = conditionalTimeStamps[i]
+        updatedDictionary = {}
+        for m in data1:
+            updatedDictionary[m] = self.getSupportAndPeriod(data1[m], pattern + [m])
+        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _WS and v[1] <= _regularity}
         count = 0
-        for p in ConditionalPatterns:
-            p1 = [v for v in p if v in up_dict]
-            trans = sorted(p1, key=lambda x: (up_dict.get(x), -x), reverse=True)
+        for p in conditionalPatterns:
+            p1 = [v for v in p if v in updatedDictionary]
+            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                freq.append(conditionalFreq[count])
+                timeStamps.append(conditionalTimeStamps[count])
             count += 1
-        return pat, freq, up_dict
+        return pat, timeStamps, updatedDictionary
 
-    def generatePatterns(self, prefix: List[str]) -> Generator[Tuple[List[str], int], None, None]:
+    def generatePatterns(self, prefix: list) -> None:
         """
-        To generate the frequent patterns
+        Generates the patterns
 
-        :param prefix: an empty list
-        :return: Frequent patterns that are extracted from fp-tree
+        :param prefix: Forms the combination of items
+        :type prefix: list
+        :returns: yields patterns with their support and periodicity
         """
-        global _miniWeight, _maxWeight, _minWeight, _minSup
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x), -x)):
+        global _WS
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
             pattern = prefix[:]
             pattern.append(i)
-            yield pattern, self.info[i]
-            patterns, freq, info = self.getFinalConditionalPatterns(i)
-            conditionalTree = _Tree()
-            conditionalTree.info = info.copy()
-            for pat in range(len(patterns)):
-                conditionalTree.addTransaction(patterns[pat], freq[pat])
-            if len(patterns) > 0:
-                for q in conditionalTree.generatePatterns(pattern):
-                    yield q
+            if self.info[i][2] >= _WS:
+                yield pattern, self.info[i]
+                patterns, timeStamps, info = self.getConditionalPatterns(i, pattern)
+                conditionalTree = _Tree()
+                conditionalTree.info = info.copy()
+                for pat in range(len(patterns)):
+                    conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+                if len(patterns) > 0:
+                    for q in conditionalTree.generatePatterns(pattern):
+                        yield q
+            self.removeNode(i)
 
 
-class WFIM(_fp._weightedFrequentPatterns):
+class WFRIMiner(_fp._weightedFrequentRegularPatterns):
     """
-    :Description:
-       * WFMiner is one of the fundamental algorithm to discover weighted frequent patterns in a transactional database.
-       * It stores the database in compressed fp-tree decreasing the memory usage and extracts the patterns from tree.It employs employs downward closure property to  reduce the search space effectively.
-
-    :Reference :
-           U. Yun and J. J. Leggett, “Wfim: weighted frequent itemset mining with a weight range and a minimum weight,”
-           in Proceedings of the 2005 SIAM International Conference on Data Mining. SIAM, 2005, pp. 636–640.
-           https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.76
+    About this algorithm
+    ====================
+
+    :Description: WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
+       * It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
+
+    :Reference:  K. Klangwisan and K. Amphawan, "Mining weighted-frequent-regular itemsets from transactional database,"
+                 2017 9th International Conference on Knowledge and Smart Technology (KST), 2017, pp. 66-71,
+                 doi: 10.1109/KST.2017.7886090.
 
     :param  iFile: str :
-                   Name of the Input file to mine complete set of weighted Frequent Patterns.
+                   Name of the Input file to mine complete set of Weighted Frequent Regular Patterns.
     :param  oFile: str :
-                   Name of the output file to store complete set of weighted Frequent Patterns.
-    :param  minSup: str or int or float:
-                   minimum support thresholds were tuned to find the appropriate ranges in the limited memory
+                   Name of the output file to store complete set of Weighted Frequent Regular Patterns.
     :param  sep: str :
                    This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
+    :param  wFile: str :
+                   This is a weighted file.
+
 
 
-    :Attributes :
+    :Attributes:
 
         iFile : file
             Input file name or path of the input file
-        minSup: float or int or str
-            The user can specify minSup either in count or proportion of database size.
-            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
+
+        WS: float or int or str
+            The user can specify WS either in count or proportion of database size.
+            If the program detects the data type of WS is integer, then it treats WS is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
-        minWeight: float or int or str
-            The user can specify minWeight either in count or proportion of database size.
-            If the program detects the data type of minWeight is integer, then it treats minWeight is expressed in count.
+            Example: WS=10 will be treated as integer, while WS=10.0 will be treated as float
+
+        regularity: float or int or str
+            The user can specify regularity either in count or proportion of database size.
+            If the program detects the data type of regularity is integer, then it treats regularity is expressed in count.
             Otherwise, it will be treated as float.
-            Example: minWeight=10 will be treated as integer, while minWeight=10.0 will be treated as float
+            Example: regularity=10 will be treated as integer, while regularity=10.0 will be treated as float
+
         sep : str
             This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
             However, the users can override their default separator.
+
         oFile : file
             Name of the output file or the path of the output file
+
         startTime:float
             To record the start time of the mining process
+
         endTime:float
             To record the completion time of the mining process
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
             it represents the total no of transactions
+
         tree : class
             it represents the Tree class
+
         finalPatterns : dict
             it represents to store the patterns
 
-    :Methods :
+    :Methods:
 
         startMine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to a output file
+            Complete set of frequent patterns will be loaded in to an output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
         creatingItemSets()
             Scans the dataset or dataframes and stores in list format
         frequentOneItem()
             Extracts the one-frequent patterns from transactions
 
-    **Methods to execute code on terminal**
-    -------------------------------------------
-    .. code-block:: console
 
+    Execution methods
+    =================
+
+
+    **Terminal command**
+
+    .. code-block:: console
 
-       Format:
+      Format:
 
-       (.venv) $ python3 basic.py <inputFile> <weightFile> <outputFile> <minSup> <minWeight>
+      (.venv) $ python3 WFRIMiner.py <inputFile> <outputFile> <weightSupport> <regularity>
 
-       Example Usage:
+      Example Usage:
 
-       (.venv) $ python3 basic.py sampleDB.txt weightSample.txt patterns.txt 10.0 3.4
+      (.venv) $ python3 WFRIMiner.py sampleDB.txt patterns.txt 10 5
 
+    .. note:: WS & regularity will be considered in support count or frequency
 
-               .. note:: minSup and maxPer will be considered in support count or frequency
 
+    **Calling from a python program**
 
-    **Importing this algorithm into a python program**
-    -----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.weightFrequentPattern.basic import basic as alg
+            from PAMI.weightedFrequentRegularpattern.basic import WFRIMiner as alg
 
-            obj = alg.basic(iFile, wFile, minSup, minWeight)
+            iFile = 'sampleDB.txt'
 
-            obj.startMine()
+            minSup = 10  # can also be specified between 0 and 1
 
-            frequentPatterns = obj.getPatterns()
+            obj = alg.WFRIMiner(iFile, WS, regularity)
 
-            print("Total number of Frequent Patterns:", len(frequentPatterns))
+            obj.mine()
 
-            obj.savePatterns(oFile)
+            weightedFrequentRegularPatterns = obj.getPatterns()
 
-            Df = obj.getPatternsAsDataFrame()
+            print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
 
-            memUSS = obj.getmemoryUSS()
+            obj.save(oFile)
+
+            Df = obj.getPatternInDataFrame()
+
+            memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    ----------------------
+    Credits
+    =======
+
              The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
 
-        """
+    """
 
-    __startTime = float()
-    __endTime = float()
-    _minSup = str()
-    __finalPatterns = {}
+    _startTime = float()
+    _endTime = float()
+    _WS = str()
+    _regularity = str()
+    _weight = {}
+    _finalPatterns = {}
+    _wFile = " "
     _iFile = " "
     _oFile = " "
     _sep = " "
-    __memoryUSS = float()
-    __memoryRSS = float()
-    __Database = []
-    __mapSupport = {}
-    __lno = 0
-    __tree = _Tree()
-    __rank = {}
-    __rankDup = {}
+    _memoryUSS = float()
+    _memoryRSS = float()
+    _Database = []
+    _mapSupport = {}
+    _lno = 0
+    _tree = _Tree()
+    _rank = {}
+    _rankDup = {}
 
-    def __init__(self, iFile: str, wFile: str, minSup: str, minWeight: int, sep: str='\t') -> None:
-        super().__init__(iFile, wFile, minSup, minWeight, sep)
+    def __init__(self, iFile, _wFile, WS, regularity, sep='\t') -> None:
+        super().__init__(iFile, _wFile, WS, regularity, sep)
 
-    def __creatingItemSets(self) -> None:
+    def _creatingItemSets(self) -> None:
         """
         Storing the complete transactions of the database/input file in a database variable
+
         :return: None
         """
-        self.__Database = []
+        self._Database = []
+        self._weight = {}
         if isinstance(self._iFile, _fp._pd.DataFrame):
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
-                self.__Database = self._iFile['Transactions'].tolist()
+                self._Database = self._iFile['Transactions'].tolist()
+
+        if isinstance(self._wFile, _fp._pd.DataFrame):
+            _items, _weights = [], []
+            if self._wFile.empty:
+                print("its empty..")
+            i = self._wFile.columns.values.tolist()
+            if 'items' in i:
+                _items = self._wFile['items'].tolist()
+            if 'weight' in i:
+                _weights = self._wFile['weight'].tolist()
+            for i in range(len(_items)):
+                self._weight[_items[i]] = _weights[i]
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _fp._validators.url(self._iFile):
                 data = _fp._urlopen(self._iFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    self.__Database.append(temp)
+                    self._Database.append(temp)
             else:
                 try:
                     with open(self._iFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            # print(len(temp))
-                            self.__Database.append(temp)
+                            self._Database.append(temp)
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def _scanningWeights(self) -> None:
-        """
-        Storing the weights of the variables in input file in a weights variable
-        :return: None
-        """
-        global _weights
-        _weights = {}
-        if isinstance(self._wFile, _fp._pd.DataFrame):
-            items, weights = [], []
-            if self._wFile.empty:
-                print("its empty..")
-            i = self._wFile.columns.values.tolist()
-            if 'items' in i:
-                items = self._wFile['items'].tolist()
-            if 'weights' in i:
-                weights = self._wFile['weights'].tolist()
-            for i in range(len(weights)):
-                _weights[items[i]] = weights[i]
-
-            # print(self.Database)
         if isinstance(self._wFile, str):
             if _fp._validators.url(self._wFile):
                 data = _fp._urlopen(self._wFile)
                 for line in data:
                     line.strip()
                     line = line.decode("utf-8")
                     temp = [i.rstrip() for i in line.split(self._sep)]
                     temp = [x for x in temp if x]
-                    _weights[temp[0]] = temp[1]
+                    self._weight[temp[0]] = float(temp[1])
             else:
                 try:
                     with open(self._wFile, 'r', encoding='utf-8') as f:
                         for line in f:
                             line.strip()
                             temp = [i.rstrip() for i in line.split(self._sep)]
                             temp = [x for x in temp if x]
-                            s = int(float(temp[1]))
-                            _weights[temp[0]] = s
+                            self._weight[temp[0]] = float(temp[1])
                 except IOError:
                     print("File Not Found")
                     quit()
 
-    def __convert(self, value: Union[int, float, str]) -> Union[int, float]:
+    def _convert(self, value) -> float:
         """
-        To convert the type of user specified minSup value.
+        To convert the type of user specified minSup value
 
         :param value: user specified minSup value
         :return: converted type
         """
         if type(value) is int:
             value = int(value)
         if type(value) is float:
-            value = (len(self.__Database) * value)
+            value = (len(self._Database) * value)
         if type(value) is str:
             if '.' in value:
                 value = float(value)
-                value = (len(self.__Database) * value)
+                value = (len(self._Database) * value)
             else:
                 value = int(value)
         return value
 
-    def __frequentOneItem(self) -> List[str]:
+    def _frequentOneItem(self) -> List[str]:
         """
         Generating One frequent items sets
+
         :return: list
         """
-        global _maxWeight
-        self.__mapSupport = {}
-        for tr in self.__Database:
-            for i in range(0, len(tr)):
-                if tr[i] not in self.__mapSupport:
-                    self.__mapSupport[tr[i]] = 1
+        global _lno, _wf, _weights
+        self._mapSupport = {}
+        _owf = {}
+        for tr in self._Database:
+            for i in range(1, len(tr)):
+                if tr[i] not in self._mapSupport:
+                    self._mapSupport[tr[i]] = [int(tr[0]), int(tr[0]), 1]
                 else:
-                    self.__mapSupport[tr[i]] += 1
-        self.__mapSupport = {k: v for k, v in self.__mapSupport.items() if v >= self._minSup and v * _maxWeight > self._minSup}
-        genList = [k for k, v in sorted(self.__mapSupport.items(), key=lambda x: x[1], reverse=True)]
-        self.__rank = dict([(index, item) for (item, index) in enumerate(genList)])
+                    self._mapSupport[tr[i]][0] = max(self._mapSupport[tr[i]][0], (int(tr[0]) - self._mapSupport[tr[i]][1]))
+                    self._mapSupport[tr[i]][1] = int(tr[0])
+                    self._mapSupport[tr[i]][2] += 1
+        for key in self._mapSupport:
+            self._mapSupport[key][0] = max(self._mapSupport[key][0], abs(len(self._Database) - self._mapSupport[key][1]))
+        _lno = len(self._Database)
+        self._mapSupport = {k: [v[2], v[0]] for k, v in self._mapSupport.items() if v[0] <= self._regularity}
+        for x, y in self._mapSupport.items():
+            if self._weight.get(x) is None:
+                self._weight[x] = 0
+        gmax = max([self._weight[values] for values in self._mapSupport.keys()])
+        for x, y in self._mapSupport.items():
+            _owf[x] = y[0] * gmax
+        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v[0] * _owf[k] >= self._WS}
+        for x, y in self._mapSupport.items():
+            temp = self._weight[x] * y[0]
+            _wf[x] = temp
+            self._mapSupport[x].append(temp)
+        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse= True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
+        for x, y in self._rank.items():
+            _weights[y] = self._weight[x]
         return genList
 
-    def __updateTransactions(self, itemSet: List[str]) -> List[List[int]]:
+    def _updateTransactions(self, itemSet) -> List[List[int]]:
         """
         Updates the items in transactions with rank of items according to their support
+
         :Example: oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-                    rank = {'a':0, 'b':1, 'c':2, 'd':3}
+                  rank = {'a':0, 'b':1, 'c':2, 'd':3}
 
         :param itemSet: list of one-frequent items
-        :return: list
+        :return: None
         """
         list1 = []
-        for tr in self.__Database:
-            list2 = []
-            for i in range(len(tr)):
+        for tr in self._Database:
+            list2 = [int(tr[0])]
+            for i in range(1, len(tr)):
                 if tr[i] in itemSet:
-                    list2.append(self.__rank[tr[i]])
-            if len(list2) >= 1:
-                list2.sort()
+                    list2.append(self._rank[tr[i]])
+            if len(list2) >= 2:
+                basket = list2[1:]
+                basket.sort()
+                list2[1:] = basket[0:]
                 list1.append(list2)
         return list1
 
     @staticmethod
-    def __buildTree(transactions: List[List[int]], info: Dict[int, int]) -> '_Tree':
+    def _buildTree(transactions, info) -> _Tree:
         """
         Builds the tree with updated transactions
 
         :param transactions: updated transactions
-        :param info: support details of each item in transactions.
-        :return: Transactions compressed in fp-tree
+        :param info: support details of each item in transactions
+        :return: transactions compressed in fp-tree
+
         """
         rootNode = _Tree()
         rootNode.info = info.copy()
         for i in range(len(transactions)):
-            rootNode.addTransaction(transactions[i], 1)
+            set1 = [transactions[i][0]]
+            rootNode.addTransaction(transactions[i][1:], set1)
         return rootNode
 
-    def __savePeriodic(self, itemSet: List[int]) -> str:
+    def _savePeriodic(self, itemSet) -> str:
         """
         The duplication items and their ranks
 
         :param itemSet: frequent itemSet that generated
         :return: patterns with original item names.
+
         """
         temp = str()
         for i in itemSet:
-            temp = temp + self.__rankDup[i] + "\t"
+            temp = temp + self._rankDup[i] + "\t"
         return temp
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
+    @deprecated(
+        "It is recommended to use 'mine()' instead of 'startMine()' for mining process. Starting from January 2025, 'startMine()' will be completely terminated.")
     def startMine(self) -> None:
         """
-        main program to start the operation
-        :return: None
+        Frequent pattern mining process will start from here
         """
-        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
-        self.__startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
-            raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
-        self._scanningWeights()
-        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
-        _maxWeight = max([s for s in _weights.values()])
-        _miniWeight = min([s for s in _weights.values()])
-        self._minSup = self.__convert(self._minSup)
-        _minSup = self._minSup
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
-        for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent patterns were generated successfully using basic algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
+        self.mine()
 
-    def Mine(self) -> None:
+    def mine(self) -> None:
         """
-        main program to start the operation
-        :return: None
+        Frequent pattern mining process will start from here
         """
-        global _minSup, _minWeight, _miniWeight, _maxWeight, _weights
-        self.__startTime = _fp._time.time()
+        global _WS, _regularity, _weights
+        self._startTime = _fp._time.time()
         if self._iFile is None:
             raise Exception("Please enter the file path or file name:")
-        if self._minSup is None:
+        if self._WS is None:
             raise Exception("Please enter the Minimum Support")
-        self.__creatingItemSets()
-        self._scanningWeights()
-        _weights = {k: v for k, v in _weights.items() if v >= _minWeight}
-        _maxWeight = max([s for s in _weights.values()])
-        _miniWeight = min([s for s in _weights.values()])
-        self._minSup = self.__convert(self._minSup)
-        _minSup = self._minSup
-        itemSet = self.__frequentOneItem()
-        updatedTransactions = self.__updateTransactions(itemSet)
-        for x, y in self.__rank.items():
-            self.__rankDup[y] = x
-        info = {self.__rank[k]: v for k, v in self.__mapSupport.items()}
-        __Tree = self.__buildTree(updatedTransactions, info)
-        patterns = __Tree.generatePatterns([])
-        self.__finalPatterns = {}
+        self._creatingItemSets()
+        self._WS = self._convert(self._WS)
+        self._regularity = self._convert(self._regularity)
+        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
+        itemSet = self._frequentOneItem()
+        updatedTransactions = self._updateTransactions(itemSet)
+        for x, y in self._rank.items():
+            self._rankDup[y] = x
+        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
+        _Tree = self._buildTree(updatedTransactions, info)
+        patterns = _Tree.generatePatterns([])
+        self._finalPatterns = {}
         for k in patterns:
-            s = self.__savePeriodic(k[0])
-            self.__finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent patterns were generated successfully using basic algorithm")
-        self.__endTime = _fp._time.time()
-        self.__memoryUSS = float()
-        self.__memoryRSS = float()
+            s = self._savePeriodic(k[0])
+            self._finalPatterns[str(s)] = k[1]
+        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        self._endTime = _fp._time.time()
+        self._memoryUSS = float()
+        self._memoryRSS = float()
         process = _fp._psutil.Process(_fp._os.getpid())
-        self.__memoryUSS = process.memory_full_info().uss
-        self.__memoryRSS = process.memory_info().rss
-
+        self._memoryRSS = float()
+        self._memoryUSS = float()
+        self._memoryUSS = process.memory_full_info().uss
+        self._memoryRSS = process.memory_info().rss
 
     def getMemoryUSS(self) -> float:
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memoryUSS
+        return self._memoryUSS
 
     def getMemoryRSS(self) -> float:
         """
-        Total amount of RSS memory consumed by the mining process will be retrieved from this function.
+
+        Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
-        return self.__memoryRSS
+        return self._memoryRSS
 
     def getRuntime(self) -> float:
         """
-        Calculating the total amount of runtime taken by the mining process.
+
+        Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
-        return self.__endTime - self.__startTime
+        return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> pd.DataFrame:
+    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
         """
-        Storing final frequent patterns in a dataframe.
+
+        Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
-        for a, b in self.__finalPatterns.items():
+        for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
     def save(self, outFile: str) -> None:
         """
-        Complete set of frequent patterns will be loaded in to an output file.
+
+        Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
         :type outFile: csv file
         :return: None
         """
         self._oFile = outFile
         writer = open(self._oFile, 'w+')
-        for x, y in self.__finalPatterns.items():
+        for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, int]:
+    def getPatterns(self) -> Dict[str, float]:
         """
-        Function to send the set of frequent patterns after completion of the mining process.
+
+        Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self.__finalPatterns
+        return self._finalPatterns
 
     def printResults(self) -> None:
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of  Weighted Frequent Patterns:", len(self.getPatterns()))
+        print("Total number of  Weighted Frequent Regular Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
-        
-
 
 if __name__ == "__main__":
     _ap = str()
     if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
         if len(_fp._sys.argv) == 7:
-            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
-        if len(_fp._sys.argv) == 6:
-            _ap = WFIM(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+        if len(_fp._sys.argv) == 5:
+            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
         _ap.startMine()
-        print("Total number of Weighted Frequent Patterns:", len(_ap.getPatterns()))
+        _ap.mine()
+        print("Total number of Weighted Frequent Regular Patterns:", len(_ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
         print("Total Memory in USS:",  _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-utf-8
+us-ascii
```

### Comparing `pami-2024.4.9.1/PAMI/weightedFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/weightedFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/WFRIMiner.py` & `pami-2024.5.1/PAMI/uncertainFrequentPattern/basic/TubeS.py`

 * *Files 17% similar despite different names*

```diff
@@ -1,819 +1,819 @@
-# WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
-# It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
+# TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
 #
 # **Importing this algorithm into a python program**
-# --------------------------------------------------------
 #
+#             from PAMI.uncertainFrequentPattern.basic import TubeS as alg
 #
-#             from PAMI.weightedFrequentRegularPattern.basic import WFRIMiner as alg
+#             iFile = 'sampleDB.txt'
 #
-#             obj = alg.WFRIMiner(iFile, WS, regularity)
+#             minSup = 10  # can also be specified between 0 and 1
 #
-#             obj.startMine()
+#             obj = alg.TubeS(iFile, minSup)
 #
-#             weightedFrequentRegularPatterns = obj.getPatterns()
+#             obj.mine()
 #
-#             print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
+#             frequentPatterns = obj.getPatterns()
+#
+#             print("Total number of Frequent Patterns:", len(frequentPatterns))
 #
 #             obj.save(oFile)
 #
-#             Df = obj.getPatternInDataFrame()
+#             Df = obj.getPatternsAsDataFrame()
 #
 #             memUSS = obj.getMemoryUSS()
 #
-#             print("Total Memory in USS:", memUSS)
+#              print("Total Memory in USS:", memUSS)
 #
-#             memRSS = obj.getMemoryRSS()
+#              memRSS = obj.getMemoryRSS()
 #
-#             print("Total Memory in RSS", memRSS)
+#              print("Total Memory in RSS", memRSS)
 #
-#             run = obj.getRuntime()
+#              run = obj.getRuntime()
 #
-#             print("Total ExecutionTime in seconds:", run)
+#              print("Total ExecutionTime in seconds:", run)
 #
 
 
 __copyright__ = """
  Copyright (C)  2021 Rage Uday Kiran
-
      This program is free software: you can redistribute it and/or modify
      it under the terms of the GNU General Public License as published by
      the Free Software Foundation, either version 3 of the License, or
      (at your option) any later version.
-
+     
      This program is distributed in the hope that it will be useful,
      but WITHOUT ANY WARRANTY; without even the implied warranty of
      MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
      GNU General Public License for more details.
-
+     
      You should have received a copy of the GNU General Public License
      along with this program.  If not, see <https://www.gnu.org/licenses/>.
-     Copyright (C)  2021 Rage Uday Kiran
-
 """
 
-from PAMI.weightedFrequentRegularPattern.basic import abstract as _fp
-import pandas as pd
-from deprecated import deprecated
-from typing import List, Dict
-
+from PAMI.uncertainFrequentPattern.basic import abstract as _fp
+import deprecated
 
-_WS = str()
-_regularity = str()
-_lno = int()
-_weights = {}
-_wf = {}
+_minSup = float()
 _fp._sys.setrecursionlimit(20000)
+_finalPatterns = {}
 
 
-class _Node:
+class _Item:
+    """
+    A class used to represent the item with probability in transaction of dataset
+
+    :Attributes:
+
+        item : int or word
+          Represents the name of the item
+
+        probability : float
+          Represent the existential probability(likelihood presence) of an item
+    """
+
+    def __init__(self, item, probability):
+        self.item = item
+        self.probability = probability
+
+
+class _Node(object):
     """
     A class used to represent the node of frequentPatternTree
 
     :Attributes:
-        itemId: int
+
+        item : int
             storing item of a node
-        counter: int
-            To maintain the support of node
-        parent: node
-            To maintain the parent of node
-        children: list
+
+        probability : int
+            To maintain the expected support of node
+
+        parent : node
+            To maintain the parent of every node
+
+        children : list
             To maintain the children of node
 
     :Methods:
-        addChild(node)
-            Updates the nodes children list and parent for the given node
 
+        addChild(itemName)
+             storing the children to their respective parent nodes
     """
 
-    def __init__(self, item: int, children: dict) -> None:
-        """
-        Initializing the Node class
-
-        :param item: Storing the item of a node
-        :type item: int or None
-        :param children: To maintain the children of a node
-        :type children: dict
-        :return: None
-        """
-
+    def __init__(self, item, children):
         self.item = item
+        self.probability = 1
+        self.secondProbability = 1
         self.children = children
         self.parent = None
-        self.timeStamps = []
 
-    def addChild(self, node) -> None:
+    def addChild(self, node):
         """
-        To add the children to a node
-
-        :param node: parent node in the tree
-        :return: None
+        This function is used to add child
         """
-
         self.children[node.item] = node
         node.parent = self
 
 
-class _Tree:
+def Second(transaction, i):
+    """
+    To calculate the second probability of a node in transaction
+
+    :param transaction: transaction in a database
+    :param i: index of item in transaction
+    :return: second probability of a node
+    """
+    temp = []
+    for j in range(0, i):
+        temp.append(transaction[j].probability)
+    l1 = max(temp)
+    temp.remove(l1)
+    l2 = max(temp)
+    return l2 * l2
+
+
+def printTree(root):
+    """
+    To print the tree with root node through recursion
+
+    :param root: root node of  tree
+    :return: details of tree
+    """
+    for x, y in root.children.items():
+        print(x, y.item, y.probability, y.parent.item, y.tids, y.secondProbability)
+        printTree(y)
+
+
+class _Tree(object):
     """
     A class used to represent the frequentPatternGrowth tree structure
 
     :Attributes:
+
         root : Node
-            The first node of the tree set to Null.
+          Represents the root node of the tree
+
         summaries : dictionary
-            Stores the nodes itemId which shares same itemId
+          storing the nodes with same item name
+
         info : dictionary
-            frequency of items in the transactions
+          stores the support of items
 
     :Methods:
-        addTransaction(transaction, freq)
-            adding items of  transactions into the tree as nodes and freq is the count of nodes
-        getFinalConditionalPatterns(node)
-            getting the conditional patterns from fp-tree for a node
-        getConditionalPatterns(patterns, frequencies)
-            sort the patterns by removing the items with lower minSup
-        generatePatterns(prefix)
-            generating the patterns from fp-tree
-    """
+        addTransaction(transaction)
+            creating transaction as a branch in frequentPatternTree
+        addConditionalTransaction(prefixPaths, supportOfItems)
+            construct the conditional tree for prefix paths
+        conditionalPatterns(Node)
+            generates the conditional patterns from tree for specific node
+        conditionalTransactions(prefixPaths,Support)
+            takes the prefixPath of a node and support at child of the path and extract the frequent items from
+            prefixPaths and generates prefixPaths with items which are frequent
+        removeNode(Node)
+            removes the node from tree once after generating all the patterns respective to the node
+        generate_patterns(Node)
+            starts from the root node of the tree and mines the frequent patterns
+            """
 
-    def __init__(self) -> None:
+    def __init__(self):
         self.root = _Node(None, {})
         self.summaries = {}
         self.info = {}
 
-    def addTransaction(self, transaction: list, tid: list) -> None:
+    def addTransaction(self, transaction):
+        """
+        Adding transaction into tree
+
+        :param transaction : it represents the one transactions in database
+        :type transaction : list
+        """
+        currentNode = self.root
+        k = 0
+        for i in range(len(transaction)):
+            k += 1
+            if transaction[i].item not in currentNode.children:
+                newNode = _Node(transaction[i].item, {})
+                newNode.k = k
+                if k >= 3:
+                    newNode.secondProbability = Second(transaction, i)
+                l1 = i - 1
+                temp = []
+                while l1 >= 0:
+                    temp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(temp) == 0:
+                    newNode.probability = round(transaction[i].probability, 2)
+                else:
+                    newNode.probability = round(max(temp) * transaction[i].probability, 2)
+                currentNode.addChild(newNode)
+                if transaction[i].item in self.summaries:
+                    self.summaries[transaction[i].item].append(newNode)
+                else:
+                    self.summaries[transaction[i].item] = [newNode]
+                currentNode = newNode
+            else:
+                currentNode = currentNode.children[transaction[i].item]
+                if k >= 3:
+                    currentNode.secondProbability = max(transaction[i].probability, currentNode.secondProbability)
+                currentNode.k = k
+                l1 = i - 1
+                temp = []
+                while l1 >= 0:
+                    temp.append(transaction[l1].probability)
+                    l1 -= 1
+                if len(temp) == 0:
+                    currentNode.probability += round(transaction[i].probability, 2)
+                else:
+                    nn = max(temp) * transaction[i].probability
+                    currentNode.probability += round(nn, 2)
+
+    def addConditionalTransaction(self, transaction, sup, second):
         """
-        Adding a transaction into tree
+        Constructing conditional tree from prefixPaths
 
-        :param transaction: To represent the complete database
-        :type transaction: list
-        :param tid: To represent the timestamp of a database
-        :type tid: list
-        :return: pfp-growth tree
+        :param transaction : it represents the one transactions in database
+        :type transaction : list
+        :param sup : support of prefixPath taken at last child of the path
+        :type sup : int
+        :param second: second probability of the leaf node
+        :type second: float
         """
         currentNode = self.root
+        k = 0
         for i in range(len(transaction)):
+            k += 1
             if transaction[i] not in currentNode.children:
                 newNode = _Node(transaction[i], {})
+                newNode.k = k
+                newNode.secondProbability = second
+                newNode.probability = sup
                 currentNode.addChild(newNode)
                 if transaction[i] in self.summaries:
                     self.summaries[transaction[i]].append(newNode)
                 else:
                     self.summaries[transaction[i]] = [newNode]
                 currentNode = newNode
             else:
                 currentNode = currentNode.children[transaction[i]]
-        currentNode.timeStamps = currentNode.timeStamps + tid
+                currentNode.k = k
+                currentNode.secondProbability = max(currentNode.secondProbability, second)
+                currentNode.probability += sup
 
-    def getConditionalPatterns(self, alpha, pattern) -> tuple:
+    def conditionalPatterns(self, alpha):
         """
-        Generates all the conditional patterns of a respective node
+        Generates all the conditional patterns of respective node
 
-        :param alpha: To represent a Node in the tree
-        :type alpha: Node
-        :param pattern: prefix of the pattern
-        :type alpha: list
-        :return: A tuple consisting of finalPatterns, conditional pattern base and information
+        :param alpha : it represents the Node in tree
+        :type alpha : _Node
         """
         finalPatterns = []
-        finalSets = []
+        sup = []
+        second = []
         for i in self.summaries[alpha]:
-            set1 = i.timeStamps
+            s = i.probability
+            s1 = i.secondProbability
             set2 = []
             while i.parent.item is not None:
                 set2.append(i.parent.item)
                 i = i.parent
             if len(set2) > 0:
                 set2.reverse()
                 finalPatterns.append(set2)
-                finalSets.append(set1)
-        finalPatterns, finalSets, info = self.conditionalDatabases(finalPatterns, finalSets, pattern)
-        return finalPatterns, finalSets, info
-
-    @staticmethod
-    def generateTimeStamps(node) -> list:
-        """
-        To get the timestamps of a node
-
-        :param node: A node in the tree
-        :return: Timestamps of a node
-        """
-
-        finalTimeStamps = node.timeStamps
-        return finalTimeStamps
-
-    def removeNode(self, nodeValue) -> None:
-        """
-        Removing the node from tree
-
-        :param nodeValue: To represent a node in the tree
-        :type nodeValue: node
-        :return: Tree with their nodes updated with timestamps
-        """
-
-        for i in self.summaries[nodeValue]:
-            i.parent.timeStamps = i.parent.timeStamps + i.timeStamps
-            del i.parent.children[nodeValue]
-
-    def getTimeStamps(self, alpha) -> list:
-        """
-        To get all the timestamps of the nodes which share same item name
-
-        :param alpha: Node in a tree
-        :return: Timestamps of a  node
-        """
-        temporary = []
-        for i in self.summaries[alpha]:
-            temporary += i.timeStamps
-        return temporary
+                second.append(s1)
+                sup.append(s)
+        finalPatterns, support, info = self.conditionalTransactions(finalPatterns, sup)
+        return finalPatterns, support, info, second
 
-    @staticmethod
-    def getSupportAndPeriod(timeStamps: list, pattern: list) -> list:
+    def conditionalTransactions(self, condPatterns, support):
         """
-        To calculate the periodicity and support
+        It generates the conditional patterns with frequent items
 
-        :param timeStamps: Timestamps of an item set
-        :type timeStamps: list
-        :param pattern: pattern to evaluate the weighted frequent regular or not
-        :type pattern: list
-        :return: support, periodicity
-        """
-        global _WS, _regularity, _lno, _weights
-        timeStamps.sort()
-        cur = 0
-        per = list()
-        sup = 0
-        for j in range(len(timeStamps)):
-            per.append(timeStamps[j] - cur)
-            cur = timeStamps[j]
-            sup += 1
-        per.append(_lno - cur)
-        l = int()
-        for i in pattern:
-            l = l + _weights[i]
-        wf = (l / (len(pattern))) * sup
-        if len(per) == 0:
-            return [0, 0]
-        return [sup, max(per), wf]
-
-    def conditionalDatabases(self, conditionalPatterns: list, conditionalTimeStamps: list, pattern: list) -> tuple:
-        """
-        It generates the conditional patterns with periodic-frequent items
-
-        :param conditionalPatterns: conditionalPatterns generated from conditionPattern method of a respective node
-        :type conditionalPatterns: list
-        :param conditionalTimeStamps: Represents the timestamps of a conditional patterns of a node
-        :type conditionalTimeStamps: list
-        :param pattern: prefix of the pattern
-        :type pattern: list
-        :returns: Returns conditional transactions by removing non-periodic and non-frequent items
+        :param condPatterns : conditional patterns generated from conditionalPatterns() method for respective node
+        :type condPatterns : list
+        :param support : the support of conditional pattern in tree
+        :type support : list
         """
-        global _WS, _regularity
+        global _minSup
         pat = []
-        timeStamps = []
+        sup = []
         data1 = {}
-        for i in range(len(conditionalPatterns)):
-            for j in conditionalPatterns[i]:
+        for i in range(len(condPatterns)):
+            for j in condPatterns[i]:
                 if j in data1:
-                    data1[j] = data1[j] + conditionalTimeStamps[i]
+                    data1[j] += support[i]
                 else:
-                    data1[j] = conditionalTimeStamps[i]
-        updatedDictionary = {}
-        for m in data1:
-            updatedDictionary[m] = self.getSupportAndPeriod(data1[m], pattern + [m])
-        updatedDictionary = {k: v for k, v in updatedDictionary.items() if v[0] >= _WS and v[1] <= _regularity}
+                    data1[j] = support[i]
+        updatedDict = {}
+        updatedDict = {k: v for k, v in data1.items() if v >= _minSup}
         count = 0
-        for p in conditionalPatterns:
-            p1 = [v for v in p if v in updatedDictionary]
-            trans = sorted(p1, key=lambda x: (updatedDictionary.get(x)[0], -x), reverse=True)
+        for p in condPatterns:
+            p1 = [v for v in p if v in updatedDict]
+            trans = sorted(p1, key=lambda x: (updatedDict.get(x)), reverse=True)
             if len(trans) > 0:
                 pat.append(trans)
-                timeStamps.append(conditionalTimeStamps[count])
+                sup.append(support[count])
             count += 1
-        return pat, timeStamps, updatedDictionary
+        return pat, sup, updatedDict
+
+    def removeNode(self, nodeValue):
+        """
+        Removing the node from tree
+
+        :param nodeValue : it represents the node in tree
+        :type nodeValue : node
+        """
+        for i in self.summaries[nodeValue]:
+            del i.parent.children[nodeValue]
 
-    def generatePatterns(self, prefix: list) -> None:
+    def generatePatterns(self, prefix):
         """
         Generates the patterns
 
-        :param prefix: Forms the combination of items
-        :type prefix: list
-        :returns: yields patterns with their support and periodicity
+        :param prefix : forms the combination of items
+        :type prefix : list
         """
-        global _WS
-        for i in sorted(self.summaries, key=lambda x: (self.info.get(x)[0], -x)):
+        global _finalPatterns, _minSup
+        for i in sorted(self.summaries, key=lambda x: (self.info.get(x))):
             pattern = prefix[:]
             pattern.append(i)
-            if self.info[i][2] >= _WS:
-                yield pattern, self.info[i]
-                patterns, timeStamps, info = self.getConditionalPatterns(i, pattern)
+            s = 0
+            for x in self.summaries[i]:
+                #if x.k <= 2:
+                    #s += x.probability
+                #elif x.k >= 3:
+                    #n = x.probability * pow(x.secondProbability, (x.k - 2))
+                    #s += n
+                if len(pattern) <= 2:
+                    s += x.probability
+                elif len(pattern) >= 3:
+                    n = x.probability * pow(x.secondProbability, (x.k - 2))
+                    s += n
+            _finalPatterns[tuple(pattern)] = self.info[i]
+            if s >= _minSup:
+                patterns, support, info, second = self.conditionalPatterns(i)
                 conditionalTree = _Tree()
                 conditionalTree.info = info.copy()
                 for pat in range(len(patterns)):
-                    conditionalTree.addTransaction(patterns[pat], timeStamps[pat])
+                    conditionalTree.addConditionalTransaction(patterns[pat], support[pat], second[pat])
                 if len(patterns) > 0:
-                    for q in conditionalTree.generatePatterns(pattern):
-                        yield q
+                    conditionalTree.generatePatterns(pattern)
             self.removeNode(i)
 
 
-class WFRIMiner(_fp._weightedFrequentRegularPatterns):
+class TubeS(_fp._frequentPatterns):
     """
-    :Description: WFRIMiner is one of the fundamental algorithm to discover weighted frequent regular patterns in a transactional database.
-       * It stores the database in compressed WFRI-tree decreasing the memory usage and extracts the patterns from tree.It employs downward closure property to  reduce the search space effectively.
+    About this algorithm
+    ====================
+
+    :Description: TubeS is one of the fastest algorithm to discover frequent patterns in a uncertain transactional database.
 
-    :Reference:
-           K. Klangwisan and K. Amphawan, "Mining weighted-frequent-regular itemsets from transactional database,"
-           2017 9th International Conference on Knowledge and Smart Technology (KST), 2017, pp. 66-71,
-           doi: 10.1109/KST.2017.7886090.
-
-    :param  iFile: str :
-                   Name of the Input file to mine complete set of Weighted Frequent Regular Patterns.
-    :param  oFile: str :
-                   Name of the output file to store complete set of Weighted Frequent Regular Patterns.
-    :param  sep: str :
-                   This variable is used to distinguish items from one another in a transaction. The default seperator is tab space. However, the users can override their default separator.
-    :param  wFile: str :
-                This is a weighted file.
+    :Reference:  Carson Kai-Sang Leung and Richard Kyle MacKinnon. 2014. Fast Algorithms for Frequent Itemset Mining from Uncertain Data.
+                 In Proceedings of the 2014 IEEE International Conference on Data Mining (ICDM '14). IEEE Computer Society, USA, 893–898. https://doi.org/10.1109/ICDM.2014.146
 
     :Attributes:
 
         iFile : file
-            Input file name or path of the input file
-        WS: float or int or str
-            The user can specify WS either in count or proportion of database size.
-            If the program detects the data type of WS is integer, then it treats WS is expressed in count.
-            Otherwise, it will be treated as float.
-            Example: WS=10 will be treated as integer, while WS=10.0 will be treated as float
-        regularity: float or int or str
-            The user can specify regularity either in count or proportion of database size.
-            If the program detects the data type of regularity is integer, then it treats regularity is expressed in count.
+            Name of the Input file or path of the input file
+
+        oFile : file
+            Name of the output file or path of the output file
+
+        minSup : float or int or str
+            The user can specify minSup either in count or proportion of database size.
+            If the program detects the data type of minSup is integer, then it treats minSup is expressed in count.
             Otherwise, it will be treated as float.
-            Example: regularity=10 will be treated as integer, while regularity=10.0 will be treated as float
+            Example: minSup=10 will be treated as integer, while minSup=10.0 will be treated as float
+
         sep : str
-            This variable is used to distinguish items from one another in a transaction. The default separator is tab space or \t.
+            This variable is used to distinguish items from one another in a transaction. The default seperator is tab space or \t.
             However, the users can override their default separator.
-        oFile : file
-            Name of the output file or the path of the output file
-        startTime:float
-            To record the start time of the mining process
-        endTime:float
-            To record the completion time of the mining process
+
         memoryUSS : float
             To store the total amount of USS memory consumed by the program
+
         memoryRSS : float
             To store the total amount of RSS memory consumed by the program
+
+        startTime : float
+            To record the start time of the mining process
+
+        endTime : float
+            To record the completion time of the mining process
+
         Database : list
             To store the transactions of a database in list
+
         mapSupport : Dictionary
             To maintain the information of item and their frequency
+
         lno : int
-            it represents the total no of transactions
+            To represent the total no of transaction
+
         tree : class
-            it represents the Tree class
+            To represents the Tree class
+
+        itemSetCount : int
+            To represents the total no of patterns
+
         finalPatterns : dict
-            it represents to store the patterns
+            To store the complete patterns
 
     :Methods:
-
-        startMine()
+        mine()
             Mining process will start from here
         getPatterns()
             Complete set of patterns will be retrieved with this function
         save(oFile)
-            Complete set of frequent patterns will be loaded in to an output file
+            Complete set of frequent patterns will be loaded in to a output file
         getPatternsAsDataFrame()
             Complete set of frequent patterns will be loaded in to a dataframe
         getMemoryUSS()
             Total amount of USS memory consumed by the mining process will be retrieved from this function
         getMemoryRSS()
             Total amount of RSS memory consumed by the mining process will be retrieved from this function
         getRuntime()
             Total amount of runtime taken by the mining process will be retrieved from this function
-        creatingItemSets()
-            Scans the dataset or dataframes and stores in list format
+        creatingItemSets(fileName)
+            Scans the dataset and stores in a list format
         frequentOneItem()
-            Extracts the one-frequent patterns from transactions
+            Extracts the one-length frequent patterns from database
+        updateTransactions()
+            Update the transactions by removing non-frequent items and sort the Database by item decreased support
+        buildTree()
+            After updating the Database, remaining items will be added into the tree by setting root node as null
+        convert()
+            to convert the user specified value
+
+    Execution methods
+    =================
 
-    **Methods to execute code on terminal**
-    -------------------------------------------
-    .. code-block:: console
 
+    **Terminal command**
+
+
+    .. code-block:: console
 
       Format:
 
-      (.venv) $ python3 WFRIMiner.py <inputFile> <outputFile> <weightSupport> <regularity>
+      (.venv) $ python3 TubeS.py <inputFile> <outputFile> <minSup>
 
       Example Usage:
 
-      (.venv) $ python3 WFRIMiner.py sampleDB.txt patterns.txt 10 5
+      (.venv) $ python3 TubeS.py sampleDB.txt patterns.txt 10.0
 
+    .. note:: minSup can be specified  in support count or a value between 0 and 1.
 
-              .. note:: WS & regularity will be considered in support count or frequency
+    **Calling from a python program**
 
-
-    **Importing this algorithm into a python program**
-    ----------------------------------------------------
     .. code-block:: python
 
-            from PAMI.weightedFrequentRegularpattern.basic import WFRIMiner as alg
+            from PAMI.uncertainFrequentPattern.basic import TubeS as alg
+
+            iFile = 'sampleDB.txt'
+
+            minSup = 10  # can also be specified between 0 and 1
 
-            obj = alg.WFRIMiner(iFile, WS, regularity)
+            obj = alg.TubeS(iFile, minSup)
 
-            obj.startMine()
+            obj.mine()
 
-            weightedFrequentRegularPatterns = obj.getPatterns()
+            frequentPatterns = obj.getPatterns()
 
-            print("Total number of Frequent Patterns:", len(weightedFrequentRegularPatterns))
+            print("Total number of Frequent Patterns:", len(frequentPatterns))
 
             obj.save(oFile)
 
-            Df = obj.getPatternInDataFrame()
+            Df = obj.getPatternsAsDataFrame()
 
             memUSS = obj.getMemoryUSS()
 
             print("Total Memory in USS:", memUSS)
 
             memRSS = obj.getMemoryRSS()
 
             print("Total Memory in RSS", memRSS)
 
             run = obj.getRuntime()
 
             print("Total ExecutionTime in seconds:", run)
 
-    **Credits:**
-    ----------------
-             The complete program was written by P.Likhitha  under the supervision of Professor Rage Uday Kiran.
+    Credits
+    =======
 
-        """
+
+            The complete program was written by  P.Likhitha   under the supervision of Professor Rage Uday Kiran.
+    """
 
     _startTime = float()
     _endTime = float()
-    _WS = str()
-    _regularity = str()
-    _weight = {}
+    _minSup = float()
     _finalPatterns = {}
-    _wFile = " "
     _iFile = " "
     _oFile = " "
     _sep = " "
     _memoryUSS = float()
     _memoryRSS = float()
     _Database = []
-    _mapSupport = {}
-    _lno = 0
-    _tree = _Tree()
     _rank = {}
-    _rankDup = {}
-
-    def __init__(self, iFile, _wFile, WS, regularity, sep='\t') -> None:
-        super().__init__(iFile, _wFile, WS, regularity, sep)
-
-    def _creatingItemSets(self) -> None:
+    _lno = 0
+    def __init__(self, iFile, minSup, sep='\t'):
+        super().__init__(iFile, minSup, sep)
+    def _creatingItemSets(self):
         """
-        Storing the complete transactions of the database/input file in a database variable
-        :return: None
+        Scans the databases and stores the transactions into Database variable
         """
         self._Database = []
-        self._weight = {}
         if isinstance(self._iFile, _fp._pd.DataFrame):
+            uncertain, data = [], []
             if self._iFile.empty:
                 print("its empty..")
             i = self._iFile.columns.values.tolist()
             if 'Transactions' in i:
                 self._Database = self._iFile['Transactions'].tolist()
-
-        if isinstance(self._wFile, _fp._pd.DataFrame):
-            _items, _weights = [], []
-            if self._wFile.empty:
-                print("its empty..")
-            i = self._wFile.columns.values.tolist()
-            if 'items' in i:
-                _items = self._wFile['items'].tolist()
-            if 'weight' in i:
-                _weights = self._wFile['weight'].tolist()
-            for i in range(len(_items)):
-                self._weight[_items[i]] = _weights[i]
+            if 'uncertain' in i:
+                uncertain = self._iFile['uncertain'].tolist()
+            for k in range(len(data)):
+                tr = []
+                for j in range(len(data[k])):
+                    product = _Item(data[k][j], uncertain[k][j])
+                    tr.append(product)
+                self._Database.append(tr)
+                self._lno += 1
 
             # print(self.Database)
         if isinstance(self._iFile, str):
             if _fp._validators.url(self._iFile):
                 data = _fp._urlopen(self._iFile)
                 for line in data:
-                    line.strip()
+                    line = line.strip()
                     line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
+                    temp1 = line.split(':')
+                    temp = [i.rstrip() for i in temp[0].split(self._sep)]
+                    uncertain = [float(i.rstrip()) for i in temp[1].split(self._sep)]
+                    tr = []
+                    for i in range(len(temp)):
+                        item = temp[i]
+                        probability = uncertain[i]
+                        product = _Item(item, probability)
+                        tr.append(product)
+                    self._lno += 1
                     self._Database.append(temp)
             else:
                 try:
-                    with open(self._iFile, 'r', encoding='utf-8') as f:
-                        for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._Database.append(temp)
-                except IOError:
-                    print("File Not Found")
-                    quit()
-
-        if isinstance(self._wFile, str):
-            if _fp._validators.url(self._wFile):
-                data = _fp._urlopen(self._wFile)
-                for line in data:
-                    line.strip()
-                    line = line.decode("utf-8")
-                    temp = [i.rstrip() for i in line.split(self._sep)]
-                    temp = [x for x in temp if x]
-                    self._weight[temp[0]] = float(temp[1])
-            else:
-                try:
-                    with open(self._wFile, 'r', encoding='utf-8') as f:
+                    with open(self._iFile, 'r') as f:
                         for line in f:
-                            line.strip()
-                            temp = [i.rstrip() for i in line.split(self._sep)]
-                            temp = [x for x in temp if x]
-                            self._weight[temp[0]] = float(temp[1])
+                            temp1 = line.strip()
+                            temp1 = temp1.split(':')
+                            temp = [i.rstrip() for i in temp1[0].split(self._sep)]
+                            uncertain = [float(i.rstrip()) for i in temp1[1].split(self._sep)]
+                            tr = []
+                            for i in range(len(temp)):
+                                item = temp[i]
+                                probability = uncertain[i]
+                                product = _Item(item, probability)
+                                tr.append(product)
+                            self._lno += 1
+                            self._Database.append(tr)
                 except IOError:
                     print("File Not Found")
-                    quit()
 
-    def _convert(self, value) -> float:
+    def _frequentOneItem(self):
         """
-        To convert the type of user specified minSup value
-
-        :param value: user specified minSup value
-        :return: converted type
+        Takes the transactions and calculates the support of each item in the dataset and assign the ranks to the items by decreasing support and returns the frequent items list
         """
-        if type(value) is int:
-            value = int(value)
-        if type(value) is float:
-            value = (len(self._Database) * value)
-        if type(value) is str:
-            if '.' in value:
-                value = float(value)
-                value = (len(self._Database) * value)
-            else:
-                value = int(value)
-        return value
-
-    def _frequentOneItem(self) -> List[str]:
+        global _minSup
+        mapSupport = {}
+        for i in self._Database:
+            for j in i:
+                if j.item not in mapSupport:
+                    mapSupport[j.item] = round(j.probability, 2)
+                else:
+                    mapSupport[j.item] += round(j.probability, 2)
+        mapSupport = {k: round(v, 2) for k, v in mapSupport.items() if v >= self._minSup}
+        plist = [k for k, v in sorted(mapSupport.items(), key=lambda x: x[1], reverse=True)]
+        self._rank = dict([(index, item) for (item, index) in enumerate(plist)])
+        return mapSupport, plist
+
+    def _buildTree(self, data, info):
+        """
+        It takes the transactions and support of each item and construct the main tree with setting root node as null
+
+        :param data : it represents the one transactions in database
+        :type data : list
+        :param info : it represents the support of each item
+        :type info : dictionary
         """
-        Generating One frequent items sets
-        :return: list
+        rootNode = _Tree()
+        rootNode.info = info.copy()
+        for i in range(len(data)):
+            rootNode.addTransaction(data[i])
+        return rootNode
+
+    def updateTransactions(self, dict1):
         """
-        global _lno, _wf, _weights
-        self._mapSupport = {}
-        _owf = {}
-        for tr in self._Database:
-            for i in range(1, len(tr)):
-                if tr[i] not in self._mapSupport:
-                    self._mapSupport[tr[i]] = [int(tr[0]), int(tr[0]), 1]
-                else:
-                    self._mapSupport[tr[i]][0] = max(self._mapSupport[tr[i]][0], (int(tr[0]) - self._mapSupport[tr[i]][1]))
-                    self._mapSupport[tr[i]][1] = int(tr[0])
-                    self._mapSupport[tr[i]][2] += 1
-        for key in self._mapSupport:
-            self._mapSupport[key][0] = max(self._mapSupport[key][0], abs(len(self._Database) - self._mapSupport[key][1]))
-        _lno = len(self._Database)
-        self._mapSupport = {k: [v[2], v[0]] for k, v in self._mapSupport.items() if v[0] <= self._regularity}
-        for x, y in self._mapSupport.items():
-            if self._weight.get(x) is None:
-                self._weight[x] = 0
-        gmax = max([self._weight[values] for values in self._mapSupport.keys()])
-        for x, y in self._mapSupport.items():
-            _owf[x] = y[0] * gmax
-        self._mapSupport = {k: v for k, v in self._mapSupport.items() if v[0] * _owf[k] >= self._WS}
-        for x, y in self._mapSupport.items():
-            temp = self._weight[x] * y[0]
-            _wf[x] = temp
-            self._mapSupport[x].append(temp)
-        genList = [k for k, v in sorted(self._mapSupport.items(), key=lambda x: x[1], reverse= True)]
-        self._rank = dict([(index, item) for (item, index) in enumerate(genList)])
-        for x, y in self._rank.items():
-            _weights[y] = self._weight[x]
-        return genList
-
-    def _updateTransactions(self, itemSet) -> List[List[int]]:
-        """
-        Updates the items in transactions with rank of items according to their support
-
-        :Example:
-        oneLength = {'a':7, 'b': 5, 'c':'4', 'd':3}
-        rank = {'a':0, 'b':1, 'c':2, 'd':3}
+        Remove the items which are not frequent from transactions and updates the transactions with rank of items
 
-        :param itemSet: list of one-frequent items
-        :return: None
+        :param dict1 : frequent items with support
+        :type dict1 : dictionary
         """
         list1 = []
         for tr in self._Database:
-            list2 = [int(tr[0])]
-            for i in range(1, len(tr)):
-                if tr[i] in itemSet:
-                    list2.append(self._rank[tr[i]])
-            if len(list2) >= 2:
-                basket = list2[1:]
-                basket.sort()
-                list2[1:] = basket[0:]
+            list2 = []
+            for i in range(0, len(tr)):
+                if tr[i].item in dict1:
+                    list2.append(tr[i])
+            if (len(list2) >= 2):
+                basket = list2
+                basket.sort(key=lambda val: self._rank[val.item])
+                list2 = basket
                 list1.append(list2)
         return list1
 
-    @staticmethod
-    def _buildTree(transactions, info) -> _Tree:
+    def _Check(self, i, x):
         """
-        Builds the tree with updated transactions
+        To check the presence of item or pattern in transaction
 
-        :param transactions: updated transactions
-        :param info: support details of each item in transactions
-        :return: transactions compressed in fp-tree
+        :param x: it represents the pattern
+        :type x : list
+        :param i : represents the uncertain transactions
+        :type i : list
+        """
+        for m in x:
+            k = 0
+            for n in i:
+                if m == n.item:
+                    k += 1
+            if k == 0:
+                return 0
+        return 1
 
+    def _convert(self, value):
         """
-        rootNode = _Tree()
-        rootNode.info = info.copy()
-        for i in range(len(transactions)):
-            set1 = [transactions[i][0]]
-            rootNode.addTransaction(transactions[i][1:], set1)
-        return rootNode
+        To convert the type of user specified minSup value
 
-    def _savePeriodic(self, itemSet) -> str:
+        :param value: user specified minSup value
+        :return: converted type minSup value
         """
-        The duplication items and their ranks
-
-        :param itemSet: frequent itemSet that generated
-        :return: patterns with original item names.
+        if type(value) is int:
+            value = int(value)
+        if type(value) is float:
+            value = (len(self._Database) * value)
+        if type(value) is str:
+            if '.' in value:
+                value = (len(self._Database) * value)
+            else:
+                value = int(value)
+        return value
 
+    def _removeFalsePositives(self):
         """
-        temp = str()
-        for i in itemSet:
-            temp = temp + self._rankDup[i] + "\t"
-        return temp
+        To remove the false positive patterns generated in frequent patterns.
 
-    @deprecated("It is recommended to use mine() instead of startMine() for mining process")
-    def startMine(self) -> None:
+        :return: Patterns with accurate probability
         """
-        main program to start the operation
-        :return: None
-        """
-        global _WS, _regularity, _weights
-        self._startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._WS is None:
-            raise Exception("Please enter the Minimum Support")
-        self._creatingItemSets()
-        self._WS = self._convert(self._WS)
-        self._regularity = self._convert(self._regularity)
-        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
-        itemSet = self._frequentOneItem()
-        updatedTransactions = self._updateTransactions(itemSet)
-        for x, y in self._rank.items():
-            self._rankDup[y] = x
-        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
-        _Tree = self._buildTree(updatedTransactions, info)
-        patterns = _Tree.generatePatterns([])
-        self._finalPatterns = {}
-        for k in patterns:
-            s = self._savePeriodic(k[0])
-            self._finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
-        self._endTime = _fp._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
-        process = _fp._psutil.Process(_fp._os.getpid())
-        self._memoryRSS = float()
-        self._memoryUSS = float()
-        self._memoryUSS = process.memory_full_info().uss
-        self._memoryRSS = process.memory_info().rss
+        global _finalPatterns
+        periods = {}
+        for i in self._Database:
+            for x, y in _finalPatterns.items():
+                if len(x) == 1:
+                    periods[x] = y
+                else:
+                    s = 1
+                    check = self._Check(i, x)
+                    if check == 1:
+                        for j in i:
+                            if j.item in x:
+                                s *= j.probability
+                        if x in periods:
+                            periods[x] += s
+                        else:
+                            periods[x] = s
+        for x, y in periods.items():
+            if y >= self._minSup:
+                sample = str()
+                for i in x:
+                    sample = sample + i + "\t"
+                self._finalPatterns[sample] = y
 
-    def Mine(self) -> None:
+    def mine(self):
         """
-        main program to start the operation
-        :return: None
+        Main method where the patterns are mined by constructing tree and remove the false patterns by counting the original support of a patterns
         """
-        global _WS, _regularity, _weights
+        global _minSup
         self._startTime = _fp._time.time()
-        if self._iFile is None:
-            raise Exception("Please enter the file path or file name:")
-        if self._WS is None:
-            raise Exception("Please enter the Minimum Support")
         self._creatingItemSets()
-        self._WS = self._convert(self._WS)
-        self._regularity = self._convert(self._regularity)
-        _WS, _regularity, _weights = self._WS, self._regularity, self._weight
-        itemSet = self._frequentOneItem()
-        updatedTransactions = self._updateTransactions(itemSet)
-        for x, y in self._rank.items():
-            self._rankDup[y] = x
-        info = {self._rank[k]: v for k, v in self._mapSupport.items()}
-        _Tree = self._buildTree(updatedTransactions, info)
-        patterns = _Tree.generatePatterns([])
+        self._minSup = self._convert(self._minSup)
+        _minSup = self._minSup
         self._finalPatterns = {}
-        for k in patterns:
-            s = self._savePeriodic(k[0])
-            self._finalPatterns[str(s)] = k[1]
-        print("Weighted Frequent Regular patterns were generated successfully using WFRIM algorithm")
+        mapSupport, plist = self._frequentOneItem()
+        transactions1 = self.updateTransactions(mapSupport)
+        info = {k: v for k, v in mapSupport.items()}
+        Tree1 = self._buildTree(transactions1, info)
+        Tree1.generatePatterns([])
+        self._removeFalsePositives()
+        print("Uncertain Frequent patterns were generated successfully using TubeS algorithm")
         self._endTime = _fp._time.time()
-        self._memoryUSS = float()
-        self._memoryRSS = float()
         process = _fp._psutil.Process(_fp._os.getpid())
-        self._memoryRSS = float()
         self._memoryUSS = float()
+        self._memoryRSS = float()
         self._memoryUSS = process.memory_full_info().uss
         self._memoryRSS = process.memory_info().rss
 
-    def getMemoryUSS(self) -> float:
+    def getMemoryUSS(self):
         """
+
         Total amount of USS memory consumed by the mining process will be retrieved from this function
 
         :return: returning USS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryUSS
 
-    def getMemoryRSS(self) -> float:
+    def getMemoryRSS(self):
         """
+
         Total amount of RSS memory consumed by the mining process will be retrieved from this function
 
         :return: returning RSS memory consumed by the mining process
         :rtype: float
         """
 
         return self._memoryRSS
 
-    def getRuntime(self) -> float:
+    def getRuntime(self):
         """
+
         Calculating the total amount of runtime taken by the mining process
 
         :return: returning total amount of runtime taken by the mining process
         :rtype: float
         """
 
         return self._endTime - self._startTime
 
-    def getPatternsAsDataFrame(self) -> _fp._pd.DataFrame:
+    def getPatternsAsDataFrame(self):
         """
+
         Storing final frequent patterns in a dataframe
 
         :return: returning frequent patterns in a dataframe
         :rtype: pd.DataFrame
         """
 
         dataframe = {}
         data = []
         for a, b in self._finalPatterns.items():
             data.append([a.replace('\t', ' '), b])
             dataframe = _fp._pd.DataFrame(data, columns=['Patterns', 'Support'])
         return dataframe
 
-    def save(self, outFile: str) -> None:
+    def save(self, outFile):
         """
+
         Complete set of frequent patterns will be loaded in to an output file
 
         :param outFile: name of the output file
-        :type outFile: csv file
-        :return: None
+        :type outFile: file
         """
-        self._oFile = outFile
-        writer = open(self._oFile, 'w+')
+        self.oFile = outFile
+        writer = open(self.oFile, 'w+')
         for x, y in self._finalPatterns.items():
             s1 = x.strip() + ":" + str(y)
             writer.write("%s \n" % s1)
 
-    def getPatterns(self) -> Dict[str, float]:
+    def getPatterns(self):
         """
+
         Function to send the set of frequent patterns after completion of the mining process
 
         :return: returning frequent patterns
         :rtype: dict
         """
-        return self._finalPatterns
+        return len(self._finalPatterns)
 
-    def printResults(self) -> None:
+    def printResults(self):
         """
         This function is used to print the results
-        :return: None
         """
-        print("Total number of  Weighted Frequent Regular Patterns:", len(self.getPatterns()))
+        print("Total number of  Uncertain Frequent Patterns:", len(self.getPatterns()))
         print("Total Memory in USS:", self.getMemoryUSS())
         print("Total Memory in RSS", self.getMemoryRSS())
         print("Total ExecutionTime in ms:",  self.getRuntime())
 
 
 if __name__ == "__main__":
     _ap = str()
-    if len(_fp._sys.argv) == 6 or len(_fp._sys.argv) == 7:
-        if len(_fp._sys.argv) == 7:
-            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5], _fp._sys.argv[6])
+    if len(_fp._sys.argv) == 4 or len(_fp._sys.argv) == 5:
         if len(_fp._sys.argv) == 5:
-            _ap = WFRIMiner(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4], _fp._sys.argv[5])
-        _ap.startMine()
-        print("Total number of Weighted Frequent Regular Patterns:", len(_ap.getPatterns()))
+            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3], _fp._sys.argv[4])
+        if len(_fp._sys.argv) == 4:
+            _ap = TubeS(_fp._sys.argv[1], _fp._sys.argv[3])
+        _ap.mine()
+        print("Total number of Uncertain Frequent Patterns:", len(_ap.getPatterns()))
         _ap.save(_fp._sys.argv[2])
-        print("Total Memory in USS:",  _ap.getMemoryUSS())
+        print("Total Memory in USS:", _ap.getMemoryUSS())
         print("Total Memory in RSS", _ap.getMemoryRSS())
         print("Total ExecutionTime in ms:", _ap.getRuntime())
     else:
         print("Error! The number of input parameters do not match the total number of parameters provided")
```

#### encoding

```diff
@@ -1 +1 @@
-us-ascii
+utf-8
```

### Comparing `pami-2024.4.9.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/weightedFrequentRegularPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py` & `pami-2024.5.1/PAMI/weightedUncertainFrequentPattern/basic/abstract.py`

 * *Files identical despite different names*

### Comparing `pami-2024.4.9.1/PKG-INFO` & `pami-2024.5.1/PKG-INFO`

 * *Files 3% similar despite different names*

```diff
@@ -1,10 +1,10 @@
 Metadata-Version: 2.1
 Name: pami
-Version: 2024.4.9.1
+Version: 2024.5.1
 Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
 Home-page: https://github.com/udayLab/PAMI
 Author: Rage Uday Kiran
 Author-email: uday.rage@gmail.com
 License: GPLv3
 Classifier: Development Status :: 5 - Production/Stable
 Classifier: Programming Language :: Python :: 3
@@ -18,18 +18,20 @@
 Requires-Dist: plotly
 Requires-Dist: matplotlib
 Requires-Dist: resource
 Requires-Dist: validators
 Requires-Dist: urllib3
 Requires-Dist: Pillow
 Requires-Dist: numpy
+Requires-Dist: sphinx
 Requires-Dist: sphinx-rtd-theme
 Requires-Dist: validators
 Requires-Dist: discord.py
 Requires-Dist: networkx
+Requires-Dist: deprecated
 Provides-Extra: gpu
 Requires-Dist: cupy; extra == "gpu"
 Requires-Dist: pycuda; extra == "gpu"
 Provides-Extra: spark
 Requires-Dist: pyspark; extra == "spark"
 Provides-Extra: dev
 Requires-Dist: twine; extra == "dev"
@@ -56,102 +58,113 @@
 [![Downloads](https://static.pepy.tech/badge/pami)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/month)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/week)](https://pepy.tech/project/pami)
 
 [Click here for more information](https://pepy.tech/project/pami)
 
 
+***
+
+# Table of Contents
+
+- [Introduction](#introduction)
+- [Development process](#process-flow-chart)
+- [Recent updates](#recent-updates)
+- [Features](#features)
+- [Maintenance](#Maintenance)
+- [Try your first PAMI program](#try-your-first-PAMI-program)
+- [Evaluation](#evaluation)
+- [Reading Material](#Reading-Material)
+- [License](#License)
+- [Documentation](#Documentation)
+- [Background](#Background)
+- [Getting Help](#Getting-Help)
+- [Discussion and Development](#Discussion-and-Development)
+- [Contribution to PAMI](#Contribution-to-PAMI)
+- [Tutorials](#tutorials)
+- [Real-World Case Studies](#real-world-case-studies)
+
+
+***
 # Introduction
+
 PAttern MIning (PAMI) is a Python library containing several algorithms to discover user interest-based patterns in a wide-spectrum of datasets across multiple computing platforms. Useful links to utilize the services of this library were provided below:
 
 
 1. Youtube tutorial https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ
 
 2. Tutorials (Notebooks) https://github.com/UdayLab/PAMI/tree/main/notebooks
    
 3. User manual https://udaylab.github.io/PAMI/manuals/index.html
 
 4. Coders manual https://udaylab.github.io/PAMI/codersManual/index.html
 
-5. Code documentation https://pami-1.readthedocs.io 
+5. Code documentation https://pami-1.readthedocs.io
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
-# Recent Updates  
+***
+# Process Flow Chart
+
+![PAMI's production process](./images/pamiDevelopmentSteps.png?raw=true)
+
+<!--- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) ---> 
+***
+# Recent Updates
 
 - Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
 - Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
 - Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
+***
 # Features
 
 - ✅ Well-tested and production-ready
 - 🔋 Highly optimized to our best effort, light-weight, and energy-efficient
 - 👀 Proper code documentation
 - 🍼 Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 - 🤖 Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 - ⚡️ Supports Cuda and PySpark 
 - 🖥️ Operating System Independence
 - 🔬 Knowledge discovery in static data and streams
 - 🐎 Snappy
 - 🐻 Ease of use
 
-# Table of Content
-
-- [Maintenance](#Maintenance)
-- [Try your first PAMI program](#try-your-first-PAMI-program)
-- [Reading Material](#Reading-Material)
-- [Tutorials](#Tutorials)
-- [License](#License)
-- [Documentation](#Documentation)
-- [Background](#Background)
-- [Getting Help](#Getting-Help)
-- [Discussion and Development](#Discussion-and-Development)
-- [Contribution to PAMI](#Contribution-to-PAMI)
+***
 
 # Maintenance
 
   __Installation__
   
   1. Installing basic pami package (recommended)
 
-
          pip install pami
 
-
   2. Installing pami package in a GPU machine that supports CUDA
 
-
          pip install 'pami[gpu]'
 
-
   3. Installing pami package in a distributed network environment supporting Spark
 
-
          pip install 'pami[spark]'
 
-
   4. Installing pami package for developing purpose
-          
 
          pip install 'pami[dev]'
 
-
-  5. Installing complete Library of pami 
-
+  5. Installing complete Library of pami
 
          pip install 'pami[all]'
 
-
   __Upgradation__
 
   
         pip install --upgrade pami
   
 
   __Uninstallation__
@@ -161,14 +174,15 @@
        
 
   __Information__ 
 
 
         pip show pami
 
+***
 # *Try your first PAMI program*
 
 ```shell
 $ python
 ```
 
 ```python
@@ -191,20 +205,78 @@
 Frequent patterns were generated successfully using frequentPatternGrowth algorithm
 Total No of patterns: 4540
 Runtime: 8.749667644500732
 Memory (RSS): 522911744
 Memory (USS): 475353088
 ```
 
+***
+
+# Evaluation:
+
+1. we compared three different Python libraries such as PAMI, mlxtend and efficient-apriori for Apriori.
+2. (Transactional_T10I4D100K.csv)is a transactional database downloaded from PAMI and
+used as an input file for all libraries.
+3. Minimum support values and seperator are also same.
+
+* The performance of the **Apriori algorithm** is shown in the graphical results below:
+1. Comparing the **Patterns Generated** by different Python libraries for the Apriori algorithm:
+
+   <img width="573" alt="Screenshot 2024-04-11 at 13 31 31" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/fd7974bc-ffe2-44dd-82e3-a5306a8a23bd">
+   
+2. Evaluating the **Runtime** of the Apriori algorithm across different Python libraries:
+
+   <img width="567" alt="Screenshot 2024-04-11 at 13 31 20" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d615ae3-dc0d-49ba-a880-4890bb1f11c5">
+
+3. Comparing the **Memory Consumption** of the Apriori algorithm across different Python libraries:
+
+   <img width="570" alt="Screenshot 2024-04-11 at 13 31 08" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d5991ca-51ae-442d-9b5e-2d21bbebfedd">
+
+For more information, we have uploaded the evaluation file in two formats:
+- One **ipynb** file format, please check it here. [Evaluation File ipynb](https://github.com/UdayLab/PAMI/blob/main/notebooks/Evaluation-neverDelete.ipynb) 
+- Two **pdf** file format, check here. [Evaluation File Pdf](https://github.com/UdayLab/PAMI/blob/main/notebooks/evaluation.pdf)
+
+***
 # Reading Material
+
 For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
- 
+
+***
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+***
+
+# Documentation
+
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+***
+
+# Background
+
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+***
+# Getting Help
+
+For any queries, the best place to go to is Github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
 ***
+# Discussion and Development
 
-# Tutorials 
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+***
+# Contribution to PAMI
+
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+***
+# Tutorials
 
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
@@ -394,16 +466,14 @@
 #### 5.1. Fuzzy Frequent pattern mining: [Sample](https://github.com/UdayLab/PAMI/fuzzyFrequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                   |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FFI-Miner <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyFrequentPattern/basic/FFIMiner.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
-
-
 #### 5.2. Fuzzy correlated pattern mining: [Sample](https://udaylab.github.io/PAMI/fuzzyCorrelatedPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                       |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FCP-growth <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyCorrelatedPattern/basic/FCPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
@@ -501,28 +571,43 @@
 ## 11. Mining patterns from Graphs
 
 #### 11.1. Frequent sub-graph mining
 | Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-# License
+## 12. Additional Features
 
-[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+#### 12.1. Creation of synthetic databases
 
-# Documentation
-The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+| Database type                                                                                                                                                                                                                                                                        |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TransactionalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> | |
+| Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TemporalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
+| Utility database (coming soon)                                                                                                                                                                                                                                                       |
+
+#### 12.2. Converting a dataframe into a specific database type
+| Approaches                                  |
+|---------------------------------------------|
+| Dense dataframe to databases (coming soon)  |
+| Sparse dataframe to databases (coming soon) |
+
+#### 12.3. Gathering the statistical details of a database
+| Approaches                           |
+|--------------------------------------|
+| Transactional database (coming soon) |
+| Temporal database (coming soon)      |
+| Utility database (coming soon)       |
+
+#### 12.4. Generating Latex code for the experimental results
+| Approaches               |
+|--------------------------|
+| Latex code (coming soon) |
 
-# Background
-The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
-has been under active development since then.
+***
 
-# Getting Help
-For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+# Real World Case Studies
 
-# Discussion and Development
-In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/vanithakattumuri/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
 
-# Contribution to PAMI
-We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
 
 [Go to Top](#table-of-contents)
```

### Comparing `pami-2024.4.9.1/README.md` & `pami-2024.5.1/pami.egg-info/PKG-INFO`

 * *Files 8% similar despite different names*

```diff
@@ -1,7 +1,54 @@
+Metadata-Version: 2.1
+Name: pami
+Version: 2024.5.1
+Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
+Home-page: https://github.com/udayLab/PAMI
+Author: Rage Uday Kiran
+Author-email: uday.rage@gmail.com
+License: GPLv3
+Classifier: Development Status :: 5 - Production/Stable
+Classifier: Programming Language :: Python :: 3
+Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
+Classifier: Operating System :: OS Independent
+Requires-Python: >=3.5
+Description-Content-Type: text/markdown
+License-File: LICENSE
+Requires-Dist: psutil
+Requires-Dist: pandas
+Requires-Dist: plotly
+Requires-Dist: matplotlib
+Requires-Dist: resource
+Requires-Dist: validators
+Requires-Dist: urllib3
+Requires-Dist: Pillow
+Requires-Dist: numpy
+Requires-Dist: sphinx
+Requires-Dist: sphinx-rtd-theme
+Requires-Dist: validators
+Requires-Dist: discord.py
+Requires-Dist: networkx
+Requires-Dist: deprecated
+Provides-Extra: gpu
+Requires-Dist: cupy; extra == "gpu"
+Requires-Dist: pycuda; extra == "gpu"
+Provides-Extra: spark
+Requires-Dist: pyspark; extra == "spark"
+Provides-Extra: dev
+Requires-Dist: twine; extra == "dev"
+Requires-Dist: setuptools; extra == "dev"
+Requires-Dist: build; extra == "dev"
+Provides-Extra: all
+Requires-Dist: cupy; extra == "all"
+Requires-Dist: pycuda; extra == "all"
+Requires-Dist: pyspark; extra == "all"
+Requires-Dist: twine; extra == "all"
+Requires-Dist: setuptools; extra == "all"
+Requires-Dist: build; extra == "all"
+
 ![PyPI](https://img.shields.io/pypi/v/PAMI)
 ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/PAMI)
 [![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
 ![PyPI - Implementation](https://img.shields.io/pypi/implementation/PAMI)
 [![Documentation Status](https://readthedocs.org/projects/pami-1/badge/?version=latest)](https://pami-1.readthedocs.io/en/latest/?badge=latest)
 ![PyPI - Wheel](https://img.shields.io/pypi/wheel/PAMI)
 ![PyPI - Status](https://img.shields.io/pypi/status/PAMI)
@@ -11,102 +58,113 @@
 [![Downloads](https://static.pepy.tech/badge/pami)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/month)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/week)](https://pepy.tech/project/pami)
 
 [Click here for more information](https://pepy.tech/project/pami)
 
 
+***
+
+# Table of Contents
+
+- [Introduction](#introduction)
+- [Development process](#process-flow-chart)
+- [Recent updates](#recent-updates)
+- [Features](#features)
+- [Maintenance](#Maintenance)
+- [Try your first PAMI program](#try-your-first-PAMI-program)
+- [Evaluation](#evaluation)
+- [Reading Material](#Reading-Material)
+- [License](#License)
+- [Documentation](#Documentation)
+- [Background](#Background)
+- [Getting Help](#Getting-Help)
+- [Discussion and Development](#Discussion-and-Development)
+- [Contribution to PAMI](#Contribution-to-PAMI)
+- [Tutorials](#tutorials)
+- [Real-World Case Studies](#real-world-case-studies)
+
+
+***
 # Introduction
+
 PAttern MIning (PAMI) is a Python library containing several algorithms to discover user interest-based patterns in a wide-spectrum of datasets across multiple computing platforms. Useful links to utilize the services of this library were provided below:
 
 
 1. Youtube tutorial https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ
 
 2. Tutorials (Notebooks) https://github.com/UdayLab/PAMI/tree/main/notebooks
    
 3. User manual https://udaylab.github.io/PAMI/manuals/index.html
 
 4. Coders manual https://udaylab.github.io/PAMI/codersManual/index.html
 
-5. Code documentation https://pami-1.readthedocs.io 
+5. Code documentation https://pami-1.readthedocs.io
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
-# Recent Updates  
+***
+# Process Flow Chart
+
+![PAMI's production process](./images/pamiDevelopmentSteps.png?raw=true)
+
+<!--- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) ---> 
+***
+# Recent Updates
 
 - Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
 - Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
 - Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
+***
 # Features
 
 - ✅ Well-tested and production-ready
 - 🔋 Highly optimized to our best effort, light-weight, and energy-efficient
 - 👀 Proper code documentation
 - 🍼 Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 - 🤖 Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 - ⚡️ Supports Cuda and PySpark 
 - 🖥️ Operating System Independence
 - 🔬 Knowledge discovery in static data and streams
 - 🐎 Snappy
 - 🐻 Ease of use
 
-# Table of Content
-
-- [Maintenance](#Maintenance)
-- [Try your first PAMI program](#try-your-first-PAMI-program)
-- [Reading Material](#Reading-Material)
-- [Tutorials](#Tutorials)
-- [License](#License)
-- [Documentation](#Documentation)
-- [Background](#Background)
-- [Getting Help](#Getting-Help)
-- [Discussion and Development](#Discussion-and-Development)
-- [Contribution to PAMI](#Contribution-to-PAMI)
+***
 
 # Maintenance
 
   __Installation__
   
   1. Installing basic pami package (recommended)
 
-
          pip install pami
 
-
   2. Installing pami package in a GPU machine that supports CUDA
 
-
          pip install 'pami[gpu]'
 
-
   3. Installing pami package in a distributed network environment supporting Spark
 
-
          pip install 'pami[spark]'
 
-
   4. Installing pami package for developing purpose
-          
 
          pip install 'pami[dev]'
 
-
-  5. Installing complete Library of pami 
-
+  5. Installing complete Library of pami
 
          pip install 'pami[all]'
 
-
   __Upgradation__
 
   
         pip install --upgrade pami
   
 
   __Uninstallation__
@@ -116,14 +174,15 @@
        
 
   __Information__ 
 
 
         pip show pami
 
+***
 # *Try your first PAMI program*
 
 ```shell
 $ python
 ```
 
 ```python
@@ -146,20 +205,78 @@
 Frequent patterns were generated successfully using frequentPatternGrowth algorithm
 Total No of patterns: 4540
 Runtime: 8.749667644500732
 Memory (RSS): 522911744
 Memory (USS): 475353088
 ```
 
+***
+
+# Evaluation:
+
+1. we compared three different Python libraries such as PAMI, mlxtend and efficient-apriori for Apriori.
+2. (Transactional_T10I4D100K.csv)is a transactional database downloaded from PAMI and
+used as an input file for all libraries.
+3. Minimum support values and seperator are also same.
+
+* The performance of the **Apriori algorithm** is shown in the graphical results below:
+1. Comparing the **Patterns Generated** by different Python libraries for the Apriori algorithm:
+
+   <img width="573" alt="Screenshot 2024-04-11 at 13 31 31" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/fd7974bc-ffe2-44dd-82e3-a5306a8a23bd">
+   
+2. Evaluating the **Runtime** of the Apriori algorithm across different Python libraries:
+
+   <img width="567" alt="Screenshot 2024-04-11 at 13 31 20" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d615ae3-dc0d-49ba-a880-4890bb1f11c5">
+
+3. Comparing the **Memory Consumption** of the Apriori algorithm across different Python libraries:
+
+   <img width="570" alt="Screenshot 2024-04-11 at 13 31 08" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d5991ca-51ae-442d-9b5e-2d21bbebfedd">
+
+For more information, we have uploaded the evaluation file in two formats:
+- One **ipynb** file format, please check it here. [Evaluation File ipynb](https://github.com/UdayLab/PAMI/blob/main/notebooks/Evaluation-neverDelete.ipynb) 
+- Two **pdf** file format, check here. [Evaluation File Pdf](https://github.com/UdayLab/PAMI/blob/main/notebooks/evaluation.pdf)
+
+***
 # Reading Material
+
 For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
- 
+
+***
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+***
+
+# Documentation
+
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+***
+
+# Background
+
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+***
+# Getting Help
+
+For any queries, the best place to go to is Github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
 ***
+# Discussion and Development
 
-# Tutorials 
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+***
+# Contribution to PAMI
+
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+***
+# Tutorials
 
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
@@ -349,16 +466,14 @@
 #### 5.1. Fuzzy Frequent pattern mining: [Sample](https://github.com/UdayLab/PAMI/fuzzyFrequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                   |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FFI-Miner <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyFrequentPattern/basic/FFIMiner.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
-
-
 #### 5.2. Fuzzy correlated pattern mining: [Sample](https://udaylab.github.io/PAMI/fuzzyCorrelatedPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                       |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FCP-growth <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyCorrelatedPattern/basic/FCPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
@@ -456,28 +571,43 @@
 ## 11. Mining patterns from Graphs
 
 #### 11.1. Frequent sub-graph mining
 | Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-# License
+## 12. Additional Features
 
-[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+#### 12.1. Creation of synthetic databases
 
-# Documentation
-The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+| Database type                                                                                                                                                                                                                                                                        |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TransactionalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> | |
+| Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TemporalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
+| Utility database (coming soon)                                                                                                                                                                                                                                                       |
+
+#### 12.2. Converting a dataframe into a specific database type
+| Approaches                                  |
+|---------------------------------------------|
+| Dense dataframe to databases (coming soon)  |
+| Sparse dataframe to databases (coming soon) |
+
+#### 12.3. Gathering the statistical details of a database
+| Approaches                           |
+|--------------------------------------|
+| Transactional database (coming soon) |
+| Temporal database (coming soon)      |
+| Utility database (coming soon)       |
+
+#### 12.4. Generating Latex code for the experimental results
+| Approaches               |
+|--------------------------|
+| Latex code (coming soon) |
 
-# Background
-The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
-has been under active development since then.
+***
 
-# Getting Help
-For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+# Real World Case Studies
 
-# Discussion and Development
-In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/vanithakattumuri/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
 
-# Contribution to PAMI
-We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
 
-[Go to Top](#table-of-contents)
+[Go to Top](#table-of-contents)
```

### Comparing `pami-2024.4.9.1/pami.egg-info/PKG-INFO` & `pami-2024.5.1/README.md`

 * *Files 9% similar despite different names*

```diff
@@ -1,52 +1,7 @@
-Metadata-Version: 2.1
-Name: pami
-Version: 2024.4.9.1
-Summary: This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan
-Home-page: https://github.com/udayLab/PAMI
-Author: Rage Uday Kiran
-Author-email: uday.rage@gmail.com
-License: GPLv3
-Classifier: Development Status :: 5 - Production/Stable
-Classifier: Programming Language :: Python :: 3
-Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
-Classifier: Operating System :: OS Independent
-Requires-Python: >=3.5
-Description-Content-Type: text/markdown
-License-File: LICENSE
-Requires-Dist: psutil
-Requires-Dist: pandas
-Requires-Dist: plotly
-Requires-Dist: matplotlib
-Requires-Dist: resource
-Requires-Dist: validators
-Requires-Dist: urllib3
-Requires-Dist: Pillow
-Requires-Dist: numpy
-Requires-Dist: sphinx-rtd-theme
-Requires-Dist: validators
-Requires-Dist: discord.py
-Requires-Dist: networkx
-Provides-Extra: gpu
-Requires-Dist: cupy; extra == "gpu"
-Requires-Dist: pycuda; extra == "gpu"
-Provides-Extra: spark
-Requires-Dist: pyspark; extra == "spark"
-Provides-Extra: dev
-Requires-Dist: twine; extra == "dev"
-Requires-Dist: setuptools; extra == "dev"
-Requires-Dist: build; extra == "dev"
-Provides-Extra: all
-Requires-Dist: cupy; extra == "all"
-Requires-Dist: pycuda; extra == "all"
-Requires-Dist: pyspark; extra == "all"
-Requires-Dist: twine; extra == "all"
-Requires-Dist: setuptools; extra == "all"
-Requires-Dist: build; extra == "all"
-
 ![PyPI](https://img.shields.io/pypi/v/PAMI)
 ![PyPI - Python Version](https://img.shields.io/pypi/pyversions/PAMI)
 [![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
 ![PyPI - Implementation](https://img.shields.io/pypi/implementation/PAMI)
 [![Documentation Status](https://readthedocs.org/projects/pami-1/badge/?version=latest)](https://pami-1.readthedocs.io/en/latest/?badge=latest)
 ![PyPI - Wheel](https://img.shields.io/pypi/wheel/PAMI)
 ![PyPI - Status](https://img.shields.io/pypi/status/PAMI)
@@ -56,102 +11,113 @@
 [![Downloads](https://static.pepy.tech/badge/pami)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/month)](https://pepy.tech/project/pami)
 [![Downloads](https://static.pepy.tech/badge/pami/week)](https://pepy.tech/project/pami)
 
 [Click here for more information](https://pepy.tech/project/pami)
 
 
+***
+
+# Table of Contents
+
+- [Introduction](#introduction)
+- [Development process](#process-flow-chart)
+- [Recent updates](#recent-updates)
+- [Features](#features)
+- [Maintenance](#Maintenance)
+- [Try your first PAMI program](#try-your-first-PAMI-program)
+- [Evaluation](#evaluation)
+- [Reading Material](#Reading-Material)
+- [License](#License)
+- [Documentation](#Documentation)
+- [Background](#Background)
+- [Getting Help](#Getting-Help)
+- [Discussion and Development](#Discussion-and-Development)
+- [Contribution to PAMI](#Contribution-to-PAMI)
+- [Tutorials](#tutorials)
+- [Real-World Case Studies](#real-world-case-studies)
+
+
+***
 # Introduction
+
 PAttern MIning (PAMI) is a Python library containing several algorithms to discover user interest-based patterns in a wide-spectrum of datasets across multiple computing platforms. Useful links to utilize the services of this library were provided below:
 
 
 1. Youtube tutorial https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ
 
 2. Tutorials (Notebooks) https://github.com/UdayLab/PAMI/tree/main/notebooks
    
 3. User manual https://udaylab.github.io/PAMI/manuals/index.html
 
 4. Coders manual https://udaylab.github.io/PAMI/codersManual/index.html
 
-5. Code documentation https://pami-1.readthedocs.io 
+5. Code documentation https://pami-1.readthedocs.io
 
 6. Datasets   https://u-aizu.ac.jp/~udayrage/datasets.html
 
 7. Discussions on PAMI usage https://github.com/UdayLab/PAMI/discussions
 
 8. Report issues https://github.com/UdayLab/PAMI/issues
 
-# Recent Updates  
+***
+# Process Flow Chart
+
+![PAMI's production process](./images/pamiDevelopmentSteps.png?raw=true)
+
+<!--- ![alt text](https://github.com/[username]/[reponame]/blob/[branch]/image.jpg?raw=true) ---> 
+***
+# Recent Updates
 
 - Version 2023.07.07: New algorithms: cuApriroi, cuAprioriBit, cuEclat, cuEclatBit, gPPMiner, cuGPFMiner, FPStream, HUPMS, SHUPGrowth New codes to generate synthetic databases
 - Version 2023.06.20: Fuzzy Partial Periodic, Periodic Patterns in High Utility, Code Documentation, help() function Update 
 - Version 2023.03.01: prefixSpan and SPADE   
 
 Total number of algorithms: 83
 
+***
 # Features
 
 - ✅ Well-tested and production-ready
 - 🔋 Highly optimized to our best effort, light-weight, and energy-efficient
 - 👀 Proper code documentation
 - 🍼 Ample examples of using various algorithms at [./notebooks](https://github.com/UdayLab/PAMI/tree/main/notebooks) folder
 - 🤖 Works with AI libraries such as TensorFlow, PyTorch, and sklearn. 
 - ⚡️ Supports Cuda and PySpark 
 - 🖥️ Operating System Independence
 - 🔬 Knowledge discovery in static data and streams
 - 🐎 Snappy
 - 🐻 Ease of use
 
-# Table of Content
-
-- [Maintenance](#Maintenance)
-- [Try your first PAMI program](#try-your-first-PAMI-program)
-- [Reading Material](#Reading-Material)
-- [Tutorials](#Tutorials)
-- [License](#License)
-- [Documentation](#Documentation)
-- [Background](#Background)
-- [Getting Help](#Getting-Help)
-- [Discussion and Development](#Discussion-and-Development)
-- [Contribution to PAMI](#Contribution-to-PAMI)
+***
 
 # Maintenance
 
   __Installation__
   
   1. Installing basic pami package (recommended)
 
-
          pip install pami
 
-
   2. Installing pami package in a GPU machine that supports CUDA
 
-
          pip install 'pami[gpu]'
 
-
   3. Installing pami package in a distributed network environment supporting Spark
 
-
          pip install 'pami[spark]'
 
-
   4. Installing pami package for developing purpose
-          
 
          pip install 'pami[dev]'
 
-
-  5. Installing complete Library of pami 
-
+  5. Installing complete Library of pami
 
          pip install 'pami[all]'
 
-
   __Upgradation__
 
   
         pip install --upgrade pami
   
 
   __Uninstallation__
@@ -161,14 +127,15 @@
        
 
   __Information__ 
 
 
         pip show pami
 
+***
 # *Try your first PAMI program*
 
 ```shell
 $ python
 ```
 
 ```python
@@ -191,20 +158,78 @@
 Frequent patterns were generated successfully using frequentPatternGrowth algorithm
 Total No of patterns: 4540
 Runtime: 8.749667644500732
 Memory (RSS): 522911744
 Memory (USS): 475353088
 ```
 
+***
+
+# Evaluation:
+
+1. we compared three different Python libraries such as PAMI, mlxtend and efficient-apriori for Apriori.
+2. (Transactional_T10I4D100K.csv)is a transactional database downloaded from PAMI and
+used as an input file for all libraries.
+3. Minimum support values and seperator are also same.
+
+* The performance of the **Apriori algorithm** is shown in the graphical results below:
+1. Comparing the **Patterns Generated** by different Python libraries for the Apriori algorithm:
+
+   <img width="573" alt="Screenshot 2024-04-11 at 13 31 31" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/fd7974bc-ffe2-44dd-82e3-a5306a8a23bd">
+   
+2. Evaluating the **Runtime** of the Apriori algorithm across different Python libraries:
+
+   <img width="567" alt="Screenshot 2024-04-11 at 13 31 20" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d615ae3-dc0d-49ba-a880-4890bb1f11c5">
+
+3. Comparing the **Memory Consumption** of the Apriori algorithm across different Python libraries:
+
+   <img width="570" alt="Screenshot 2024-04-11 at 13 31 08" src="https://github.com/vanithakattumuri/PAMI/assets/134862983/5d5991ca-51ae-442d-9b5e-2d21bbebfedd">
+
+For more information, we have uploaded the evaluation file in two formats:
+- One **ipynb** file format, please check it here. [Evaluation File ipynb](https://github.com/UdayLab/PAMI/blob/main/notebooks/Evaluation-neverDelete.ipynb) 
+- Two **pdf** file format, check here. [Evaluation File Pdf](https://github.com/UdayLab/PAMI/blob/main/notebooks/evaluation.pdf)
+
+***
 # Reading Material
+
 For more examples, refer this YouTube link [YouTube](https://www.youtube.com/playlist?list=PLKP768gjVJmDer6MajaLbwtfC9ULVuaCZ)
- 
+
+***
+# License
+
+[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+***
+
+# Documentation
+
+The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+***
+
+# Background
+
+The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
+has been under active development since then.
+
+***
+# Getting Help
+
+For any queries, the best place to go to is Github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+
 ***
+# Discussion and Development
 
-# Tutorials 
+In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+
+***
+# Contribution to PAMI
+
+We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
+
+***
+# Tutorials
 
 ### 1. Pattern mining in binary transactional databases
 
 #### 1.1. Frequent pattern mining: [Sample](https://udaylab.github.io/PAMI/frequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                      | Closed                                                                                                                                                                                                                                       | Maximal                                                                                                                                                                                                                                                     | Top-k                                                                                                                                                                                                                                  | CUDA           | pyspark                                                                                                                                                                                                                                                             |
 |------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
@@ -394,16 +419,14 @@
 #### 5.1. Fuzzy Frequent pattern mining: [Sample](https://github.com/UdayLab/PAMI/fuzzyFrequentPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                   |
 |---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FFI-Miner <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyFrequentPattern/basic/FFIMiner.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
-
-
 #### 5.2. Fuzzy correlated pattern mining: [Sample](https://udaylab.github.io/PAMI/fuzzyCorrelatedPatternMining.html)
 
 | Basic                                                                                                                                                                                                                                                       |
 |-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
 | FCP-growth <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/fuzzyCorrelatedPattern/basic/FCPGrowth.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
 
@@ -501,28 +524,43 @@
 ## 11. Mining patterns from Graphs
 
 #### 11.1. Frequent sub-graph mining
 | Basic                                                                                                                                                                                                                                      | topk                                                                                                                                                                                                                                  |
 |--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
  | Gspan <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/basic/gspan.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> | TKG <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/subgraphMining/topk/tkg.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/> </a> |
 
-# License
+## 12. Additional Features
 
-[![GitHub license](https://img.shields.io/github/license/UdayLab/PAMI)](https://github.com/UdayLab/PAMI/blob/main/LICENSE)
+#### 12.1. Creation of synthetic databases
 
-# Documentation
-The official documentation is hosted on [PAMI](https://pami-1.readthedocs.io).
+| Database type                                                                                                                                                                                                                                                                        |
+|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
+| Transactional database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TransactionalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a> | |
+| Temporal database <a target="_blank" href="https://colab.research.google.com/github/UdayLab/PAMI/blob/main/notebooks/extras/syntheticDataGenerators/TemporalDatabase.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>           |
+| Utility database (coming soon)                                                                                                                                                                                                                                                       |
+
+#### 12.2. Converting a dataframe into a specific database type
+| Approaches                                  |
+|---------------------------------------------|
+| Dense dataframe to databases (coming soon)  |
+| Sparse dataframe to databases (coming soon) |
+
+#### 12.3. Gathering the statistical details of a database
+| Approaches                           |
+|--------------------------------------|
+| Transactional database (coming soon) |
+| Temporal database (coming soon)      |
+| Utility database (coming soon)       |
+
+#### 12.4. Generating Latex code for the experimental results
+| Approaches               |
+|--------------------------|
+| Latex code (coming soon) |
 
-# Background
-The idea and motivation to develop PAMI was from [Kitsuregawa Lab](https://www.tkl.iis.u-tokyo.ac.jp/new/resources?lang=en) at the University of Tokyo. Work on ``PAMI`` started at [University of Aizu](https://u-aizu.ac.jp/en/) in 2020 and
-has been under active development since then.
+***
 
-# Getting Help
-For any queries, the best place to go to is github Issues [GithubIssues](https://github.com/orgs/UdayLab/discussions/categories/q-a).
+# Real World Case Studies
 
-# Discussion and Development
-In our GitHub repository, the primary platform for discussing development-related matters is the university lab. We encourage our team members and contributors to utilize this platform for a wide range of discussions, including bug reports, feature requests, design decisions, and implementation details.
+1. Air pollution analytics <a target="_blank" href="https://colab.research.google.com/github/vanithakattumuri/PAMI/blob/main/notebooks/airPollutionAnalytics.ipynb"> <img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
 
-# Contribution to PAMI
-We invite and encourage all community members to contribute, report bugs, fix bugs, enhance documentation, propose improvements, and share their creative ideas.
 
 [Go to Top](#table-of-contents)
```

### Comparing `pami-2024.4.9.1/pami.egg-info/SOURCES.txt` & `pami-2024.5.1/pami.egg-info/SOURCES.txt`

 * *Files 2% similar despite different names*

```diff
@@ -109,18 +109,22 @@
 PAMI/faultTolerantFrequentPattern/__init__.py
 PAMI/faultTolerantFrequentPattern/basic/FTApriori.py
 PAMI/faultTolerantFrequentPattern/basic/FTFPGrowth.py
 PAMI/faultTolerantFrequentPattern/basic/__init__.py
 PAMI/faultTolerantFrequentPattern/basic/abstract.py
 PAMI/frequentPattern/__init__.py
 PAMI/frequentPattern/basic/Apriori.py
+PAMI/frequentPattern/basic/Apriori2.py
+PAMI/frequentPattern/basic/Aprioribitset.py
 PAMI/frequentPattern/basic/ECLAT.py
 PAMI/frequentPattern/basic/ECLATDiffset.py
 PAMI/frequentPattern/basic/ECLATbitset.py
 PAMI/frequentPattern/basic/FPGrowth.py
+PAMI/frequentPattern/basic/_Apriori.py
+PAMI/frequentPattern/basic/_FPGrowth.py
 PAMI/frequentPattern/basic/__init__.py
 PAMI/frequentPattern/basic/abstract.py
 PAMI/frequentPattern/closed/CHARM.py
 PAMI/frequentPattern/closed/__init__.py
 PAMI/frequentPattern/closed/abstract.py
 PAMI/frequentPattern/cuda/__init__.py
 PAMI/frequentPattern/cuda/abstract.py
@@ -260,14 +264,16 @@
 PAMI/periodicCorrelatedPattern/basic/abstract.py
 PAMI/periodicFrequentPattern/__init__.py
 PAMI/periodicFrequentPattern/basic/PFECLAT.py
 PAMI/periodicFrequentPattern/basic/PFPGrowth.py
 PAMI/periodicFrequentPattern/basic/PFPGrowthPlus.py
 PAMI/periodicFrequentPattern/basic/PFPMC.py
 PAMI/periodicFrequentPattern/basic/PSGrowth.py
+PAMI/periodicFrequentPattern/basic/_PFECLAT.py
+PAMI/periodicFrequentPattern/basic/_PFPGrowth.py
 PAMI/periodicFrequentPattern/basic/__init__.py
 PAMI/periodicFrequentPattern/basic/abstract.py
 PAMI/periodicFrequentPattern/basic/parallelPFPGrowth.py
 PAMI/periodicFrequentPattern/closed/CPFPMiner.py
 PAMI/periodicFrequentPattern/closed/__init__.py
 PAMI/periodicFrequentPattern/closed/abstract.py
 PAMI/periodicFrequentPattern/cuda/__init__.py
```

### Comparing `pami-2024.4.9.1/setup.py` & `pami-2024.5.1/setup.py`

 * *Files 4% similar despite different names*

```diff
@@ -1,15 +1,15 @@
 import setuptools
 
 with open('README.md', 'r') as fh:
     long_description = fh.read()
 
 setuptools.setup(
     name='pami',
-    version='2024.4.9.1',
+    version='2024.5.1',
     author='Rage Uday Kiran',
     author_email='uday.rage@gmail.com',
     description='This software is being developed at the University of Aizu, Aizu-Wakamatsu, Fukushima, Japan',
     long_description=long_description,
     long_description_content_type='text/markdown',
     packages=setuptools.find_packages(),
     url='https://github.com/udayLab/PAMI',
@@ -20,18 +20,20 @@
         'plotly',
         'matplotlib',
         'resource',
         'validators',
         'urllib3',
         'Pillow',
         'numpy',
+        'sphinx',
         'sphinx-rtd-theme',
         'validators',
         'discord.py',
         'networkx',
+        'deprecated',
     ],
     extras_require={
         'gpu':  ['cupy', 'pycuda'],
         'spark': ['pyspark'],
         'dev': ['twine', 'setuptools', 'build'],
         'all': ['cupy', 'pycuda', 'pyspark', 'twine', 'setuptools', 'build']
     },
@@ -39,7 +41,8 @@
         'Development Status :: 5 - Production/Stable',      # Chose either "3 - Alpha", "4 - Beta" or "5 - Production/Stable" as the current state of your package
         'Programming Language :: Python :: 3',
         'License :: OSI Approved :: GNU General Public License v3 (GPLv3)',
         'Operating System :: OS Independent',
     ],
     python_requires='>=3.5',
 )
+
```

